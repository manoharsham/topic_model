{"id": "47527969", "url": "https://en.wikipedia.org/wiki?curid=47527969", "title": "Word2vec", "text": "Word2vec\n\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n\nWord2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google and patented. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms such as latent semantic analysis.\n\nWord2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram is slower but does a better job for infrequent words.\n\nResults of word2vec training can be sensitive to parametrization. The following are some important parameters in word2vec training.\n\nA Word2vec model can be trained with hierarchical softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors. As training epochs increase, hierarchical softmax stops being useful.\n\nHigh frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed.\n\nQuality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish. Typically, the dimensionality of the vectors is set to be between 100 and 1,000.\n\nThe size of the context window determines how many words before and after a given word would be included as context words of the given word. According to the authors' note, the recommended value is 10 for skip-gram and 5 for CBOW.\n\nAn extension of word2vec to construct embeddings from entire documents (rather than the individual words) has been proposed. This extension is called paragraph2vec or doc2vec and has been implemented in the C, Python and Java/Scala tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.\n\nAn extension of word vectors for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns. A similar variant, dna2vec, has shown that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec word vectors.\n\nAn extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by Banerjee et al. One of the biggest challenges with Word2Vec is how to handle unknown or out-of-vocabulary (OOV) words and morphologically similar words. This can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist, and words may have been used infrequently in a large corpus. If the word2vec model has not encountered a particular word before, it will be forced to use a random vector, which is generally far from its ideal representation.\n\nIWE combines Word2vec with a semantic dictionary mapping technique to tackle the major challenges of information extraction from clinical texts, which include ambiguity of free text narrative style, lexical variations, use of ungrammatical and telegraphic phases, arbitrary ordering of words, and frequent appearance of abbreviations and acronyms. Of particular interest, the IWE model (trained on the one institutional dataset) successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions.\n\nThe reasons for successful word embedding learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth's distributional hypothesis. However, they note that this explanation is \"very hand-wavy\" and argue that a more formal explanation would be preferable.\n\nLevy et al. (2015) show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks. Arora et al. (2016) explain word2vec and related algorithms as performing inference for a simple generative model for text, which involves a random walk generation process based upon loglinear topic model. They use this to explain some properties of word embeddings, including their use to solve analogies.\n\nThe word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013) found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated for a range of semantic relations (such as Country–Capital) as well as syntactic relations (e.g. present tense–past tense)\n\nMikolov et al. (2013) develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They developed a set of 8,869 semantic relations and 10,675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec, or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.\n\nThe use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.\n\nIn models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.\n\nAccuracy increases overall as the number of words used increases, and as the number of dimensions increases. Mikolov et al. report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions.\n\nAltszyler and coauthors (2017) studied Word2vec performance in two semantic tests for different corpus size. They found that Word2vec has a steep learning curve, outperforming another word-embedding technique (LSA) when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting.\n\n- C\n- C#\n- Python (TensorFlow)\n- Python (Gensim)\n- Java/Scala\n- Wikipedia2Vec (introduction)\n\n", "related": "\n- Autoencoder\n- Document-term matrix\n- Feature extraction\n- Feature learning\n- Neural network language models\n- Vector space model\n- Thought vector\n- fastText\n- GloVe\n- Normalized compression distance\n"}
{"id": "47577902", "url": "https://en.wikipedia.org/wiki?curid=47577902", "title": "Trax Retail", "text": "Trax Retail\n\nTrax is a technology company headquartered in Singapore, with offices throughout the Asia-Pacific, Europe, the Middle East, North America, and South America. Founded in 2010 by Joel Bar-El and Dror Feldheim, Trax has more than 150 customers in the retail and FMCG industries, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Customers use the company’s computer vision technology to collect, measure, and analyze what’s happening on physical store shelves. Trax’s services are available in 45 markets. \n\nTrax's development centre, which opened in July 2012, is in Tel Aviv. In 2015, the company opened its first two regional offices: London in January and São Paulo, Brazil, in April. In March 2016, Trax established its US headquarters in Atlanta, Georgia. The company opened two more regional offices in Shanghai and Mexico City in June and September 2016, respectively. \n\nTrax closed its first round of funding for US$1.1 million in June 2011, its second round of funding for US$6.4 million in December 2012, and its third round of funding for US$15.7 million in February 2014. In December 2014, the company announced its fourth round of investment for US$15 million and a fifth round of funding for US$40 million on June 8, 2016.On February 8, 2017, Trax closed its sixth round of funding for US$19.5 million. On June 30, 2017, Trax announced its seventh round of funding for US$64 million led by global private equity giant Warburg Pincus.\n\nIn 2018, Trax raised $125 million in an investment round led by Chinese private equity firm Boyu Capital, and Singapore’s GIC Pte later came on board as a shareholder.\n\nIn July 2019, Trax announced that it had closed a US$100 million Series D investment. HOPU Investment Management, an Asian alternative asset manager, led the transaction. Trax will use the investment from HOPU to develop its presence both in China and globally and to help deploy its retailer solutions at scale.\n\nOn July 12, 2017, Trax announced that it had acquired Nielsen Store Observations (NSO) project-based services in the US from Nielsen Corporation.\n\nOn January 18, 2018, Trax announced that it had acquired US-based Quri, a provider of crowdsourced data on in-store conditions for the consumer packaged goods (CPG) industry.\n\nIn June 2019, Trax announced its acquisition of LenzTech, a Chinese-based company providing artificial intelligence (AI), crowdsourcing services, and Big Data analysis to the retail market.\n\nIn July 2019, Trax announced that it had acquired Shopkick, a US-based company whose shopping app for smartphones and tablets allows users to earn rewards for their online and in-store shopping activities. Later that month, Trax announced the acquisition of its main global competitor, Planorama, a European provider of image recognition for retail execution.\n\nIn January 2019, Trax announced a partnership with Google Cloud Platform. Using Google Cloud Platform, Trax aims to deliver its Retail Watch image recognition product and machine learning capability to retailers to provide real-time shelf insights, such as out-of-stock products.\n\nIn April 2019, Trax and IRI formed an alliance to provide CPG manufacturers with greater insights for better and faster in-store execution and analytics. \n\nIn July 2019, Trax announced a strategic partnership agreement with Kantar Group to help CPG manufacturers and retailers optimise category management and product assortment. \n\nTrax offers Computer Vision-led products and services to CPG manufacturers and retailers. To CPGs, Trax delivers in-store execution, market measurement and shelf strategy solutions, while retailers can leverage store management offerings. These products and services draw on store digitization, and data discovery and visualization. \nTrax’s computer vision technology uses artificial intelligence, fine-grained image recognition, and machine learning engines to convert store images into shelf insights. The technology can recognise products that are similar or identical, such as branded drinks or shampoo bottles but can also differentiate between them based on variety and size. \n\nTrax piloted its machine learning algorithms with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better the technology gets at recognizing the same products in different shapes and sizes.\n\nTrax has received a number of awards and recognition, such as the Deloitte 2016 Technology Fast 50 Award and Frost & Sullivan's 2017 Product Innovation Award.\n\n", "related": "NONE"}
{"id": "47845063", "url": "https://en.wikipedia.org/wiki?curid=47845063", "title": "Stochastic block model", "text": "Stochastic block model\n\nThe stochastic block model is a generative model for random graphs. This model tends to produce graphs containing \"communities\", subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data.\n\nThe stochastic block model takes the following parameters:\n- The number formula_1 of vertices;\n- a partition of the vertex set formula_2 into disjoint subsets formula_3, called \"communities\";\n- a symmetric formula_4 matrix formula_5 of edge probabilities.\n\nThe edge set is then sampled at random as follows: any two vertices formula_6 and formula_7 are connected by an edge with probability formula_8. An example problem is: given a graph with formula_1 vertices, where the edges are sampled as described, recover the groups formula_3.\n\nIf the probability matrix is a constant, in the sense that formula_11 for all formula_12, then the result is the Erdős–Rényi model formula_13. This case is degenerate—the partition into communities becomes irrelevant—but it illustrates a close relationship to the Erdős–Rényi model.\n\nThe \"planted partition model\" is the special case that the values of the probability matrix formula_5 are a constant formula_15 on the diagonal and another constant formula_16 off the diagonal. Thus two vertices within the same community share an edge with probability formula_15, while two vertices in different communities share an edge with probability formula_16. Sometimes it is this restricted model that is called the stochastic block model. The case where formula_19 is called an \"assortative\" model, while the case formula_20 is called \"disassortative\".\n\nReturning to the general stochastic block model, a model is called \"strongly assortative\" if formula_21 whenever formula_22: all diagonal entries dominate all off-diagonal entries. A model is called \"weakly assortative\" if formula_23 whenever formula_24: each diagonal entry is only required to dominate the rest of its own row and column. \"Disassortative\" forms of this terminology exist, by reversing all inequalities. Algorithmic recovery is often easier against block models with assortative or disassortative conditions of this form.\n\nMuch of the literature on algorithmic community detection addresses three statistical tasks: detection, partial recovery, and exact recovery.\n\nThe goal of detection algorithms is simply to determine, given a sampled graph, whether the graph has latent community structure. More precisely, a graph might be generated, with some known prior probability, from a known stochastic block model, and otherwise from a similar Erdos-Renyi model. The algorithmic task is to correctly identify which of these two underlying models generated the graph.\n\nIn partial recovery, the goal is to approximately determine the latent partition into communities, in the sense of finding a partition that is correlated with the true partition significantly better than a random guess.\n\nIn exact recovery, the goal is to recover the latent partition into communities exactly. The community sizes and probability matrix may be known or unknown.\n\nStochastic block models exhibit a sharp threshold effect reminiscent of percolation thresholds. Suppose that we allow the size formula_1 of the graph to grow, keeping the community sizes in fixed proportions. If the probability matrix remains fixed, tasks such as partial and exact recovery become feasible for all non-degenerate parameter settings. However, if we scale down the probability matrix at a suitable rate as formula_1 increases, we observe a sharp phase transition: for certain settings of the parameters, it will become possible to achieve recovery with probability tending to 1, whereas on the opposite side of the parameter threshold, the probability of recovery tends to 0 no matter what algorithm is used.\n\nFor partial recovery, the appropriate scaling is to take formula_27 for fixed formula_28, resulting in graphs of constant average degree. In the case of two equal-sized communities, in the assortative planted partition model with probability matrix\nformula_29\npartial recovery is feasible with probability formula_30 whenever formula_31, whereas any estimator fails partial recovery with probability formula_32 whenever formula_33.\n\nFor exact recovery, the appropriate scaling is to take formula_34, resulting in graphs of logarithmic average degree. Here a similar threshold exists: for the assortative planted partition model with formula_35 equal-sized communities, the threshold lies at formula_36. In fact, the exact recovery threshold is known for the fully general stochastic block model.\nIn principle, exact recovery can be solved in its feasible range using maximum likelihood, but this amounts to solving a constrained or regularized cut problem such as minimum bisection that is typically NP-complete. Hence, no known efficient algorithms will correctly compute the maximum-likelihood estimate in the worst case.\n\nHowever, a wide variety of algorithms perform well in the average case, and many high-probability performance guarantees have been proven for algorithms in both the partial and exact recovery settings. Successful algorithms include spectral clustering of the vertices, semidefinite programming, forms of belief propagation, and community detection among others.\n\nSeveral variants of the model exist. One minor tweak allocates vertices to communities randomly, according to a categorical distribution, rather than in a fixed partition. More significant variants include the censored block model and the mixed-membership block model.\n", "related": "NONE"}
{"id": "28650287", "url": "https://en.wikipedia.org/wiki?curid=28650287", "title": "Cleverbot", "text": "Cleverbot\n\nCleverbot is a chatterbot web application that uses an artificial intelligence (AI) algorithm to have conversations with humans. It was created by British AI scientist Rollo Carpenter. It was preceded by Jabberwacky, a chatbot project that began in 1986 and went online in 1997. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web, the number of conversations held has exceeded 150 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app.\n\nUnlike some other chatterbots, Cleverbot's responses are not pre-programmed. Instead, it learns from human input: Humans type into the box below the Cleverbot logo and the system finds all keywords or an exact phrase matching the input. After searching through its saved conversations, it responds to the input by finding how a human responded to that input when it was asked, in part or in full, by Cleverbot.\n\nCleverbot participated in a formal Turing test at the 2011 Techniche festival at the Indian Institute of Technology Guwahati on 3 September 2011. Out of the 1334 votes cast, Cleverbot was judged to be 59.3% human, compared to the rating of 63.3% human achieved by human participants. A score of 50.05% or higher is often considered to be a passing grade. The software running for the event had to handle just 1 or 2 simultaneous requests, whereas online Cleverbot is usually talking to around 80,000 people at once.\n\nCleverbot is constantly being controversial growing in data size at a rate of 400 to 7 million interactions per second. Updates to the software have been mostly behind the scenes. In 2014, Cleverbot was upgraded to use GPU serving techniques. Unlike Eliza, the program does not respond in a fixed way, instead choosing its responses heuristically using fuzzy logic, the whole of the conversation being compared to the millions that have taken place before. Cleverbot now uses over 279 million interactions, about 3-4% of the data it has already accumulated. The developers of Cleverbot are attempting to build a new version using machine learning techniques.\n\nA significant part of the engine behind Cleverbot and an API for accessing it has been made available to developers in the form of Cleverscript. A service for directly accessing Cleverbot has been made available to developers in the form of Cleverbot.io.\n\nAn app that uses the Cleverscript engine to play a game of 20 Questions, has been launched under the name \"Clevernator\". Unlike other such games, the player asks the questions and it is the role of the AI to understand, and answer factually. An app that allows owners to create and talk to their own small Cleverbot-like AI has been launched, called \"Cleverme!\" for Apple products.\n\nIn early 2017, a Twitch stream of two Google Home devices modified to talk to each other using Cleverbot.io garnered over 700,000 visitors and over 30,000 peak concurrent viewers.\n\n", "related": "\n- Omegle\n\n- Cleverscript website\n- Cleverbot.io website\n- Livestream of 2 cleverbots chatting with each other on Twitch\n"}
{"id": "926722", "url": "https://en.wikipedia.org/wiki?curid=926722", "title": "Relational data mining", "text": "Relational data mining\n\nRelational data mining is the data mining technique for relational\ndatabases. Unlike traditional data mining algorithms, which look for\npatterns in a single table (propositional patterns), \nrelational data mining algorithms look for patterns among multiple tables\n(relational patterns). For most types of propositional\npatterns, there are corresponding relational patterns. For example,\nthere are relational classification rules (relational classification), relational regression tree, and relational association rules.\n\nThere are several approaches to relational data mining:\n1. Inductive Logic Programming (ILP)\n2. Statistical Relational Learning (SRL)\n3. Graph Mining\n4. Propositionalization\n5. Multi-view learning\n\nMulti-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations \"live in\", \"nearby\" and \"humid\": “Those who \"live in\" a place which is \"near by\" a city with \"humid\" climate type and also are \"younger\" than 20 -> their \"health condition\" is good”. Such association rules are extractable from RDBMS data or semantic web data.\n\n- Safarii: a Data Mining environment for analysing large relational databases based on a multi-relational data mining engine.\n- Dataconda: a software, free for research and teaching purposes, that helps mining relational databases without the use of SQL.\n\n- Relational dataset repository: a collection of publicly available relational datasets.\n\n", "related": "\n- Data mining\n- Structure mining\n- Database mining\n\n- Web page for a text book on relational data mining\n"}
{"id": "47937215", "url": "https://en.wikipedia.org/wiki?curid=47937215", "title": "The Master Algorithm", "text": "The Master Algorithm\n\nThe Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n\nThe book outlines five tribes of machine learning: inductive reasoning, connectionism, evolutionary computation, Bayes' theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgements. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying \"master algorithm\".\n\nTowards the end of the book the author pictures a \"master algorithm\" in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesn't yet exist, he briefly reviews his own invention of the Markov logic network.\n\nIn 2016 Bill Gates recommended the book, alongside Nick Bostrom's \"Superintelligence\", as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese President Xi Jinping's bookshelf.\n\nA computer science educator stated in \"Times Higher Education\" that the examples are clear and accessible. In contrast, \"The Economist\" agreed Domingos \"does a good job\" but complained that he \"constantly invents metaphors that grate or confuse\". \"Kirkus Reviews\" praised the book, stating \"Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights.\"\n\nA \"New Scientist\" review called it \"compelling but rather unquestioning\".\n\n- https://www.wsj.com/articles/the-sum-of-human-knowledge-1442610803\n- http://www.kdnuggets.com/2015/09/book-master-algorithm-pedro-domingos.html\n- http://www.kdnuggets.com/2014/08/interview-pedro-domingos-master-algorithm-new-deep-learning.html (interview)\n", "related": "NONE"}
{"id": "9583985", "url": "https://en.wikipedia.org/wiki?curid=9583985", "title": "Committee machine", "text": "Committee machine\n\nA committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers.\n\nIn this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes the following methods:\n- Ensemble averaging\nIn ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.\n- Boosting\nIn boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.\n\nIn this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:\n- Mixture of experts\nIn mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.\n- Hierarchical mixture of experts\nIn hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion.\n", "related": "NONE"}
{"id": "44628821", "url": "https://en.wikipedia.org/wiki?curid=44628821", "title": "Matrix regularization", "text": "Matrix regularization\n\nIn the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over\n\nto find a vector formula_2 that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem can be written as\n\nwhere the vector norm enforcing a regularization penalty on formula_2 has been extended to a matrix norm on formula_5.\n\nMatrix regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning.\n\nConsider a matrix formula_6 to be learned from a set of examples, formula_7, where formula_8 goes from formula_9 to formula_10, and formula_11 goes from formula_9 to formula_13. Let each input matrix formula_14 be formula_15, and let formula_6 be of size formula_17. A general model for the output formula_18 can be posed as\n\nwhere the inner product is the Frobenius inner product. For different applications the matrices formula_14 will have different forms, but for each of these the optimization problem to infer formula_6 can be written as\n\nwhere formula_23 defines the empirical error for a given formula_6, and formula_25 is a matrix regularization penalty. The function formula_25 is typically chosen to be convex and is often selected to enforce sparsity (using formula_27-norms) and/or smoothness (using formula_28-norms). Finally, formula_6 is in the space of matrices formula_30 with Frobenius inner product formula_31.\n\nIn the problem of matrix completion, the matrix formula_32 takes the form\n\nwhere formula_34 and formula_35 are the canonical basis in formula_36 and formula_37. In this case the role of the Frobenius inner product is to select individual elements formula_38 from the matrix formula_6. Thus, the output formula_18 is a sampling of entries from the matrix formula_6.\n\nThe problem of reconstructing formula_6 from a small set of sampled entries is possible only under certain restrictions on the matrix, and these restrictions can be enforced by a regularization function. For example, it might be assumed that formula_6 is low-rank, in which case the regularization penalty can take the form of a nuclear norm.\n\nwhere formula_45, with formula_8 from formula_9 to formula_48, are the singular values of formula_6.\n\nModels used in multivariate regression are parameterized by a matrix of coefficients. In the Frobenius inner product above, each matrix formula_5 is\n\nsuch that the output of the inner product is the dot product of one row of the input with one column of the coefficient matrix. The familiar form of such models is\n\nMany of the vector norms used in single variable regression can be extended to the multivariate case. One example is the squared Frobenius norm, which can be viewed as an formula_28-norm acting either entrywise, or on the singular values of the matrix:\n\nIn the multivariate case the effect of regularizing with the Frobenius norm is the same as the vector case; very complex models will have larger norms, and, thus, will be penalized more.\n\nThe setup for multi-task learning is almost the same as the setup for multivariate regression. The primary difference is that the input variables are also indexed by task (columns of formula_55). The representation with the Frobenius inner product is then\n\nThe role of matrix regularization in this setting can be the same as in multivariate regression, but matrix norms can also be used to couple learning problems across tasks. In particular, note that for the optimization problem\n\nthe solutions corresponding to each column of formula_55 are decoupled. That is, the same solution can be found by solving the joint problem, or by solving an isolated regression problem for each column. The problems can be coupled by adding an additional regulatization penalty on the covariance of solutions\n\nwhere formula_60 models the relationship between tasks. This scheme can be used to both enforce similarity of solutions across tasks, and to learn the specific structure of task similarity by alternating between optimizations of formula_6 and formula_60. When the relationship between tasks is known to lie on a graph, the Laplacian matrix of the graph can be used to couple the learning problems.\n\nRegularization by spectral filtering has been used to find stable solutions to problems such as those discussed above by addressing ill-posed matrix inversions (see for example Filter function for Tikhonov regularization). In many cases the regularization function acts on the input (or kernel) to ensure a bounded inverse by eliminating small singular values, but it can also be useful to have spectral norms that act on the matrix that is to be learned.\n\nThere are a number of matrix norms that act on the singular values of the matrix. Frequently used examples include the Schatten p-norms, with \"p\" = 1 or 2. For example, matrix regularization with a Schatten 1-norm, also called the nuclear norm, can be used to enforce sparsity in the spectrum of a matrix. This has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank. In this case the optimization problem becomes:\n\nSpectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression. In this setting, a reduced rank coefficient matrix can be found by keeping just the top formula_10 singular values, but this can be extended to keep any reduced set of singular values and vectors.\n\nSparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the Lasso method). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise formula_66-norm of the matrix, but the formula_66-norm is not convex. In practice this can be implemented by convex relaxation to the formula_27-norm. While entry-wise regularization with an formula_27-norm will find solutions with a small number of nonzero elements, applying an formula_27-norm to different groups of variables can enforce structure in the sparsity of solutions.\n\nThe most straightforward example of structured sparsity uses the formula_71 norm with formula_72 and formula_73:\n\nFor example, the formula_75 norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group. The grouping effect is achieved by taking the formula_28-norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the formula_28-norms of each column.\n\nMore generally, the formula_75 norm can be applied to arbitrary groups of variables:\n\nwhere the index formula_80 is across groups of variables, and formula_81 indicates the cardinality of group formula_80.\n\nAlgorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via matching pursuit: and proximal gradient methods. By writing the proximal gradient with respect to a given coefficient, formula_83, it can be seen that this norm enforces a group-wise soft threshold\n\nwhere formula_85 is the indicator function for group norms formula_86.\n\nThus, using formula_75 norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix formula_55) will depend on the same sparse set of input variables.\n\nThe ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning. This can be useful when there are multiple types of input data (color and texture, for example) with different appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps formula_89 and formula_90 that lie in corresponding reproducing kernel Hilbert spaces formula_91, then a larger space, formula_92, can be created as the sum of two spaces:\n\nassuming linear independence in formula_89 and formula_90. In this case the formula_75-norm is again the sum of norms:\n\nThus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width.\n\n", "related": "\n- Regularization (mathematics)\n"}
{"id": "48777199", "url": "https://en.wikipedia.org/wiki?curid=48777199", "title": "Manifold regularization", "text": "Manifold regularization\n\nIn machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is \"smooth\": data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n\nManifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function formula_1 from among a hypothesis space of functions formula_2. The hypothesis space is an RKHS, meaning that it is associated with a kernel formula_3, and so every candidate function formula_1 has a norm formula_5, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.\n\nFormally, given a set of labeled training data formula_6 with formula_7 and a loss function formula_8, a learning algorithm using Tikhonov regularization will attempt to solve the expression\n\nwhere formula_10 is a hyperparameter that controls how much the algorithm will prefer simpler functions to functions that fit the data better.\n\nManifold regularization adds a second regularization term, the \"intrinsic regularizer\", to the \"ambient regularizer\" used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space formula_11, but instead from a nonlinear manifold formula_12. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.\n\nThere are many possible choices for formula_13. Many natural choices involve the gradient on the manifold formula_14, which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient formula_15 should be small where the \"marginal probability density\" formula_16, the probability density of a randomly drawn data point appearing at formula_17, is large. This gives one appropriate choice for the intrinsic regularizer:\n\nIn practice, this norm cannot be computed directly because the marginal distribution formula_19 is unknown, but it can be estimated from the provided data. In particular, if the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include formula_20 labeled examples (pairs of an input formula_17 and a label formula_22) and formula_23 unlabeled examples (inputs without associated labels). Define formula_24 to be a matrix of edge weights for a graph, where formula_25 is a distance measure between the data points formula_26 and formula_27. Define formula_28 to be a diagonal matrix with formula_29 and formula_30 to be the Laplacian matrix formula_31. Then, as the number of data points formula_32 increases, formula_30 converges to the Laplace–Beltrami operator formula_34, which is the divergence of the gradient formula_35. Then, if formula_36 is a vector of the values of formula_1 at the data, formula_38, the intrinsic norm can be estimated:\n\nAs the number of data points formula_32 increases, this empirical definition of formula_41 converges to the definition when formula_19 is known.\n\nUsing the weights formula_43 and formula_44 for the ambient and intrinsic regularizers, the final expression to be solved becomes:\n\nAs with other kernel methods, formula_2 may be an infinite-dimensional space, so if the regularization expression cannot be solved explicitly, it is impossible to search the entire space for a solution. Instead, a representer theorem shows that under certain conditions on the choice of the norm formula_13, the optimal solution formula_48 must be a linear combination of the kernel centered at each of the input points: for some weights formula_49,\n\nUsing this result, it is possible to search for the optimal solution formula_48 by searching the finite-dimensional space defined by the possible choices of formula_49.\n\nManifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function formula_8 and hypothesis space formula_2. Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.\n\nRegularized least squares (RLS) is a family of regression algorithms: algorithms that predict a value formula_55 for its inputs formula_17, with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the kernel method. The problem statement for RLS results from choosing the loss function formula_8 in Tikhonov regularization to be the mean squared error:\n\nThanks to the representer theorem, the solution can be written as a weighted sum of the kernel evaluated at the data points:\n\nand solving for formula_60 gives:\n\nwhere formula_3 is defined to be the kernel matrix, with formula_63, and formula_64 is the vector of data labels.\n\nAdding a Laplacian term for manifold regularization gives the Laplacian RLS statement:\n\nThe representer theorem for manifold regularization again gives\n\nand this yields an expression for the vector formula_60. Letting formula_3 be the kernel matrix as above, formula_64 be the vector of data labels, and formula_70 be the formula_71 block matrix formula_72:\n\nwith a solution of\n\nLapRLS has been applied to problems including sensor networks,\nmedical imaging,\nobject detection,\nspectroscopy,\ndocument classification,\ndrug-protein interactions,\nand compressing images and videos.\n\nSupport vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or \"classes\". Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, formula_75:\n\nAdding the intrinsic regularization term to this expression gives the LapSVM problem statement:\n\nAgain, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:\n\nformula_79 can be found by writing the problem as a linear program and solving the dual problem. Again letting formula_3 be the kernel matrix and formula_70 be the block matrix formula_72, the solution can be shown to be\n\nwhere formula_84 is the solution to the dual problem\n\nand formula_86 is defined by\n\nLapSVM has been applied to problems including geographical imaging,\nmedical imaging,\nface recognition,\nmachine maintenance,\nand brain-computer interfaces.\n\n- Manifold regularization assumes that data with different labels are not likely to be close together. This assumption is what allows the technique to draw information from unlabeled data, but it only applies to some problem domains. Depending on the structure of the data, it may be necessary to use a different semi-supervised or transductive learning algorithm.\n- In some datasets, the intrinsic norm of a function formula_13 can be very close to the ambient norm formula_5: for example, if the data consist of two classes that lie on perpendicular lines, the intrinsic norm will be equal to the ambient norm. In this case, unlabeled data have no effect on the solution learned by manifold regularization, even if the data fit the algorithm's assumption that the separator should be smooth. Approaches related to co-training have been proposed to address this limitation.\n- If there are a very large number of unlabeled examples, the kernel matrix formula_3 becomes very large, and a manifold regularization algorithm may become prohibitively slow to compute. Online algorithms and sparse approximations of the manifold may help in this case.\n\n- The ManifoldLearn library and the Primal LapSVM library implement LapRLS and LapSVM in MATLAB.\n- The Dlib library for C++ includes a linear manifold regularization function.\n\n", "related": "\n- Manifold learning\n- Semi-supervised learning\n- Transduction (machine learning)\n- Spectral graph theory\n- Reproducing kernel Hilbert space\n- Tikhonov regularization\n- Differential geometry\n"}
{"id": "48833041", "url": "https://en.wikipedia.org/wiki?curid=48833041", "title": "Error tolerance (PAC learning)", "text": "Error tolerance (PAC learning)\n\nIn PAC learning, error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated.\n\nIn the following, let formula_1 be our formula_2-dimensional input space. Let formula_3 be a class of functions that we wish to use in order to learn a formula_4-valued target function formula_5 defined over formula_1. Let formula_7 be the distribution of the inputs over formula_1. The goal of a learning algorithm formula_9 is to choose the best function formula_10 such that it minimizes formula_11. Let us suppose we have a function formula_12 that can measure the complexity of formula_5. Let formula_14 be an oracle that, whenever called, returns an example formula_15 and its correct label formula_16.\n\nWhen no noise corrupts the data, we can define learning in the Valiant setting:\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the Valiant setting if there exists a learning algorithm formula_9 that has access to formula_14 and a polynomial formula_21 such that for any formula_22 and formula_23 it outputs, in a number of calls to the oracle bounded by formula_24 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_27.\n\nIn the following we will define learnability of formula_5 when data have suffered some modification.\n\nIn the classification noise model a noise rate formula_29 is introduced. Then, instead of formula_30 that returns always the correct label of example formula_15, algorithm formula_32 can only call a faulty oracle formula_33 that will flip the label of formula_15 with probability formula_35. As in the Valiant case, the goal of a learning algorithm formula_9 is to choose the best function formula_10 such that it minimizes formula_11. In applications it is difficult to have access to the real value of formula_35, but we assume we have access to its upperbound formula_40. Note that if we allow the noise rate to be formula_41, then learning becomes impossible in any amount of computation time, because every label conveys no information about the target function.\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the classification noise model if there exists a learning algorithm formula_9 that has access to formula_33 and a polynomial formula_21 such that for any formula_47, formula_48 and formula_49 it outputs, in a number of calls to the oracle bounded by formula_50 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_53.\n\nStatistical Query Learning is a kind of active learning problem in which the learning algorithm formula_9 can decide if to request information about the likelihood formula_55 that a function formula_5 correctly labels example formula_15, and receives an answer accurate within a tolerance formula_58. Formally, whenever the learning algorithm formula_9 calls the oracle formula_60, it receives as feedback probability formula_61, such that formula_62.\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the statistical query learning model if there exists a learning algorithm formula_9 that has access to formula_60 and polynomials formula_67, formula_68, and formula_69 such that for any formula_22 the following hold:\n1. formula_60 can evaluate formula_55 in time formula_73;\n2. formula_74 is bounded by formula_75\n3. formula_9 outputs a model formula_77 such that formula_78, in a number of calls to the oracle bounded by formula_79.\n\nNote that the confidence parameter formula_80 does not appear in the definition of learning. This is because the main purpose of formula_80 is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now formula_60 always guarantees to meet the approximation criterion formula_62, the failure probability is no longer needed.\n\nThe statistical query model is strictly weaker than the PAC model: any efficiently SQ-learnable class is efficiently PAC learnable in the presence of classification noise, but there exist efficient PAC-learnable problems such as parity that are not efficiently SQ-learnable.\n\nIn the malicious classification model an adversary generates errors to foil the learning algorithm. This setting describes situations of error burst, which may occur when for a limited time transmission equipment malfunctions repeatedly. Formally, algorithm formula_9 calls an oracle formula_85 that returns a correctly labeled example formula_15 drawn, as usual, from distribution formula_7 over the input space with probability formula_88, but it returns with probability formula_89 an example drawn from a distribution that is not related to formula_7. \nMoreover, this maliciously chosen example may strategically selected by an adversary who has knowledge of formula_5, formula_89, formula_7, or the current progress of the learning algorithm.\n\nDefinition:\nGiven a bound formula_94 for formula_95, we say that formula_5 is efficiently learnable using formula_3 in the malicious classification model, if there exist a learning algorithm formula_9 that has access to formula_85 and a polynomial formula_100 such that for any formula_22, formula_23 it outputs, in a number of calls to the oracle bounded by formula_103 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_53.\n\nIn the nonuniform random attribute noise model the algorithm is learning a Boolean function, a malicious oracle formula_107 may flip each formula_108-th bit of example formula_109 independently with probability formula_110.\n\nThis type of error can irreparably foil the algorithm, in fact the following theorem holds:\n\nIn the nonuniform random attribute noise setting, an algorithm formula_9 can output a function formula_10 such that formula_113 only if formula_114.\n", "related": "NONE"}
{"id": "48841414", "url": "https://en.wikipedia.org/wiki?curid=48841414", "title": "Multiple instance learning", "text": "Multiple instance learning\n\nIn machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled \"bags\", each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.\n\nBabenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn't.\n\nDepending on the type and variation in training data, machine learning can be roughly categorized into three frameworks: supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags.\n\nKeeler et al., in his work in the early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn't say exactly which of its low-energy shapes are responsible for that.\nOne of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn't really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning. \nSolution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but APR was designed with Musk data in mind.\n\nProblem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework. Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction.\n\nTake image classification for example. Given an image, we want to know its target class based on its visual content. For instance, the target class might be \"beach\", where the image contains both \"sand\" and \"water\". In MIL terms, the image is described as a \"bag\" formula_1, where each formula_2 is the feature vector (called \"instance\") extracted from the corresponding formula_3-th region in the image and formula_4 is the total regions (instances) partitioning the image. The bag is labeled \"positive\" (\"beach\") if it contains both \"sand\" region instances and \"water\" region instances.\n\nExamples of where MIL is applied are:\n- Molecule activity\n- Predicting binding sites of Calmodulin binding proteins\n- Predicting function for alternatively spliced isoforms ,\n- Image classification\n- Text or document categorization\n- Predicting functional binding sites of MicroRNA targets\n- Medical image classification ,\nNumerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning.\n\nIf the space of instances is formula_5, then the set of bags is the set of functions formula_6, which is isomorphic to the set of multi-subsets of formula_5. For each bag formula_8 and each instance formula_9, formula_10 is viewed as the number of times formula_11 occurs in formula_12. Let formula_13 be the space of labels, then a \"multiple instance concept\" is a map formula_14. The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where formula_15.\n\nMost of the work on multiple instance learning, including Dietterich et al. (1997) and Maron & Lozano-P´erez (1997) early papers, make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption.\n\nThe standard assumption takes each instance formula_16 to have an associated label formula_17 which is hidden to the learner. The pair formula_18 is called an \"instance-level concept\". A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let formula_19 be a bag. The label of formula_12 is then formula_21. Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one.\n\nStandard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions. Reason for this is the belief that standard MI assumption is appropriate for the Musk dataset, but since MIL can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, standard formula_22 presence-based formula_22 threshold-based formula_22 count-based, with the count-based assumption being the most general and the standard assumption being the least general. One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions.\n\nThe presence-based assumption is a generalization of the standard assumption, wherein a bag must contain one or more instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let formula_25 be the set of required instance-level concepts, and let formula_26 denote the number of times the instance-level concept formula_27 occurs in the bag formula_12. Then formula_29 for all formula_30. Note that, by taking formula_31 to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption.\n\nA further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept formula_30 is associated a threshold formula_33. For a bag formula_12, formula_35 for all formula_30.\n\nThe count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept formula_30 has a lower threshold formula_33 and upper threshold formula_39 with formula_40. A bag formula_12 is labeled according to formula_42 for all formula_30.\n\nScott, Zhang, and Brown (2005) describe another generalization of the standard model, which they call \"generalized multiple instance learning\" (GMIL). The GMIL assumption specifies a set of required instances formula_44. A bag formula_45 is labeled positive if it contains instances which are sufficiently close to at least formula_46 of the required instances formula_47. Under only this condition, the GMIL assumption is equivalent to the presence-based assumption. However, Scott et al. describe a further generalization in which there is a set of attraction points formula_44 and a set of repulsion points formula_49. A bag is labeled positive if and only if it contains instances which are sufficiently close to at least formula_46 of the attraction points and are sufficiently close to at most formula_51 of the repulsion points. This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy.\n\nIn contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag formula_12 as a distribution formula_53 over instances formula_5, and similarly view labels as a distribution formula_55 over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution formula_56.\n\nSince formula_53 is typically considered fixed but unknown, algorithms instead focus on computing the empirical version: formula_58, where formula_59 is the number of instances in bag formula_12. Since formula_55 is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version.\n\nWhile the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that formula_62, where formula_63 is a weight function over instances and formula_64.\n\n There are two major flavors of algorithms for Multiple Instance Learning: instance-based and metadata-based, or embedding-based algorithms. The term \"instance-based\" denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept. For a survey of some of the modern MI algorithms see Foulds and Frank \nThe earliest proposed MI algorithms were a set of \"iterated-discrimination\" algorithms developed by Dietterich et al., and Diverse Density developed by Maron and Lozano-Pérez. Both of these algorithms operated under the standard assumption.\n\nBroadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an axis parallel rectangle (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively: starting from a random instance formula_65 in a positive bag, the APR is expanded to the smallest APR covering any instance formula_66 in a new positive bag formula_67. This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance formula_68 contained in the APR is given a \"relevance\", corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives.\n\nAfter the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows: a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability. Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions.\n\nIn its simplest form, Diverse Density (DD) assumes a single representative instance formula_69 as the concept. This representative instance must be \"dense\" in that it is much closer to instances from positive bags than from negative bags, as well as \"diverse\" in that it is close to at least one instance from each positive bag.\n\nLet formula_70 be the set of positively labeled bags and let formula_71 be the set of negatively labeled bags, then the best candidate for the representative instance is given by formula_72, where the diverse density formula_73 under the assumption that bags are independently distributed given the concept formula_69. Letting formula_75 denote the jth instance of bag i, the noisy-or model gives:\nformula_78 is taken to be the scaled distance formula_79 where formula_80 is the scaling vector. This way, if every positive bag has an instance close to formula_81, then formula_82 will be high for each formula_3, but if any negative bag formula_84 has an instance close to formula_81, formula_86 will be low. Hence, formula_87 is high only if every positive bag has an instance close to formula_81 and no negative bags have an instance close to formula_81. The candidate concept formula_90 can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to formula_90. Though Diverse Density was originally proposed by Maron et al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 and DD-SVM in 2004, and MILES in 2006 \n\nA number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including\n- Support vector machines\n- Artificial neural networks\n- Decision trees\n- Boosting\n\nPost 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above.\n\n- Weidmann proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept\n- Scott et al. proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles formula_92 in the original space of instances, and defines a new feature space of Boolean vectors. A bag formula_12 is mapped to a vector formula_94 in this new feature space, where formula_95 if APR formula_96 covers formula_12, and formula_98 otherwise. A single-instance algorithm can then be applied to learn the concept in this new feature space.\n\nBecause of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements.\n\n- Xu (2003) proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption.\n\nBy mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based.\n\n- One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity.\n- Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags.\n- A modification of k-nearest neighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) suggest the (maximum and minimum, respectively) Hausdorff metrics for bags formula_99 and formula_12:\nThey define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting.\n\nSo far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case.\n\n- One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if formula_5 is the space of features and formula_13 is the space of labels, an MIML concept is a map formula_105. Zhou and Zhang (2006) propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem.\n- Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the \"prime instance\", which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem.\n\n", "related": "\n- Supervised learning\n- Multi-label classification\n\nRecent reviews of the MIL literature include:\n- , which provides an extensive review and comparative study of the different paradigms,\n- , which provides a thorough review of the different assumptions used by different paradigms in the literature.\n"}
{"id": "48827727", "url": "https://en.wikipedia.org/wiki?curid=48827727", "title": "Learnable function class", "text": "Learnable function class\n\nIn statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.\n\nLet formula_1 be the sample space, where formula_2 are the labels and formula_3 are the covariates (predictors). formula_4 is a collection of mappings (functions) under consideration to link formula_3 to formula_2. formula_7 is a pre-given loss function (usually non-negative). Given a probability distribution formula_8 on formula_9, define the expected risk formula_10 to be:\nThe general goal in statistical learning is to find the function in formula_12 that minimizes the expected risk. That is, to find solutions to the following problem:\nBut in practice the distribution formula_14 is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions formula_15 that satisfies\nOne usual algorithm to find such a sequence is through empirical risk minimization.\n\nWe can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is:\n\nThe intuition behind the more strict requirement is as such: the rate at which sequence formula_17 converges to the minimizer of the expected risk can be very different for different formula_8. Because in real world the true distribution formula_14 is always unknown, we would want to select a sequence that performs well under all cases.\n\nHowever, by the no free lunch theorem, such a sequence that satisfies () does not exist if formula_12 is too complex. This means we need to be careful and not allow too \"many\" functions in formula_12 if we want () to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence formula_17 that satisfies () are known as learnable classes.\n\nIt is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies (). Thus in these settings not only do we know that the problem posed by () is solvable, we also immediately have an algorithm that gives the solution.\n\nIf the true relationship between formula_2 and formula_3 is formula_25, then by selecting the appropriate loss function, formula_26 can always be expressed as the minimizer of the expected loss across all possible functions. That is,\n\nHere we let formula_28 be the collection of all possible functions mapping formula_29 onto formula_30. formula_26 can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over formula_28. Thus we often consider a subset of formula_28, formula_12, to carry out searches on. By doing so, we risk that formula_26 might not be an element of formula_12. This tradeoff can be mathematically expressed as\n\nIn the above decomposition, part formula_37 does not depend on the data and is non-stochastic. It describes how far away our assumptions (formula_12) are from the truth (formula_28). formula_37 will be strictly greater than 0 if we make assumptions that are too strong (formula_12 too small). On the other hand, failing to put enough restrictions on formula_12 will cause it to be not learnable, and part formula_43 will not stochastically converge to 0. This is the well-known overfitting problem in statistics and machine learning literature.\n\nA good example where learnable classes are used is the so-called Tikhonov regularization in reproducing kernel Hilbert space (RKHS). Specifically, let formula_44 be an RKHS, and formula_45 be the norm on formula_44 given by its inner product. It is shown in that formula_47 is a learnable class for any finite, positive formula_48. The empirical minimization algorithm to the dual form of this problem is\n\nThis was first introduced by Tikhonov to solve ill-posed problems. Many statistical learning algorithms can be expressed in such a form (for example, the well-known ridge regression).\n\nThe tradeoff between formula_43 and formula_37 in () is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of formula_52, which are essentially balls in formula_44 with centers at 0. As formula_48 gets larger, formula_55 gets closer to the entire space, and formula_37 is likely to become smaller. However we will also suffer smaller convergence rates in formula_43. The way to choose an optimal formula_48 in finite sample settings is usually through cross-validation.\n\nPart formula_43 in () is closely linked to empirical process theory in statistics, where the empirical risk formula_60 are known as empirical processes. In this field, the function class formula_12 that satisfies the stochastic convergence\n\nare known as uniform Glivenko–Cantelli classes. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent. Interplay between formula_43 and formula_37 in statistics literature is often known as the bias-variance tradeoff.\n\nHowever, note that in the authors gave an example of stochastic convex optimization for General Setting of Learning where learnability is not equivalent with uniform convergence.\n", "related": "NONE"}
{"id": "48987892", "url": "https://en.wikipedia.org/wiki?curid=48987892", "title": "Isotropic position", "text": "Isotropic position\n\nIn the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix. \n\nLet formula_1 be a distribution over vectors in the vector space formula_2.\nThen formula_1 is in isotropic position if, for vector formula_4 sampled from the distribution,\n\nA \"set\" of vectors is said to be in isotropic position if the uniform distribution over that set is in isotropic position. In particular, every orthonormal set of vectors is isotropic.\n\nAs a related definition, a convex body formula_6 in formula_2 is called isotropic if it has volume formula_8, center of mass at the origin, and there is a constant formula_9 such that\n\nfor all vectors formula_11 in formula_2; here formula_13 stands \nfor the standard Euclidean norm.\n\n", "related": "\n- Whitening transformation\n"}
{"id": "48844125", "url": "https://en.wikipedia.org/wiki?curid=48844125", "title": "Structured sparsity regularization", "text": "Structured sparsity regularization\n\nStructured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable formula_1 (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space formula_2 (i.e., the domain, space of features or explanatory variables). \"Sparsity regularization methods\" focus on selecting the input variables that best describe the output. \"Structured sparsity regularization methods\" generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in formula_2.\n\nCommon motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of formula_2 may be higher than the number of observations formula_5), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer.\n\nConsider the linear kernel regularized empirical risk minimization problem with a loss function formula_6 and the formula_7 \"norm\" as the regularization penalty:\nwhere formula_9, and formula_10 denotes the formula_7 \"norm\", defined as the number of nonzero entries of the vector formula_12. formula_13 is said to be sparse if formula_14. Which means that the output formula_15 can be described by a small subset of input variables.\n\nMore generally, assume a dictionary formula_16 with formula_17 is given, such that the target function formula_18 of a learning problem can be written as:\nThe formula_7 norm formula_22 as the number of non-zero components of formula_12 is defined as \nformula_27 is said to be sparse if formula_28.\n\nHowever, while using the formula_7 norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the formula_30 norm; this has been shown to still favor sparser solutions and is additionally convex.\n\nStructured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization. Consider the above regularized empirical risk minimization problem with a general kernel and associated feature map formula_16 with formula_17.\nThe regularization term formula_34 penalizes each formula_35 component independently, which means that the algorithm will suppress input variables independently from each other.\n\nIn several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. Structured sparsity regularization methods allow to impose such structure by adding structure to the norms defining the regularization term.\n\nThe non-overlapping group case is the most basic instance of structured sparsity. In it, an \"a priori\" partition of the coefficient vector formula_12 in formula_37 non-overlapping groups is assumed. Let formula_38 be the vector of coefficients in group formula_39, we can define a regularization term and its group norm as\nwhere formula_41 is the group formula_42 norm formula_43 , formula_44 is group formula_39, and formula_46 is the \"j-th\" component of group formula_44.\n\nThe above norm is also referred to as group Lasso. This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.\n\nOverlapping groups is the structure sparsity case where a variable can belong to more than one group formula_39. This case is often of interest as it can represent a more general class of relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.\n\nThere are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:\n\nThe \"intersection of complements\" approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to. Consider again the group Lasso for a regularized empirical risk minimization problem:\nwhere formula_41 is the group formula_42 norm, formula_44 is group formula_39, and formula_46 is the \"j-th\" component of group formula_44.\n\nAs in the non-overlapping groups case, the \"group Lasso\" regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients formula_56. However, as in this case groups may overlap, we take the intersection of the complements of those groups that are not set to zero.\n\nThis \"intersection of complements\" selection criteria implies the modeling choice that we allow some coefficients within a particular group formula_39 to be set to zero, while others within the same group formula_39 may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.\n\nA different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.\n\nThe formulation of the union of groups approach is also referred to as latent group Lasso, and requires to modify the group formula_42 norm considered above and introduce the following regularizer \nwhere formula_61, formula_62 is the vector of coefficients of group g, and formula_63 is a vector with coefficients formula_46 for all variables formula_65 in group formula_39 , and formula_67 in all others, i.e., formula_68 if formula_65 in group formula_39 and formula_71 otherwise.\n\nThis regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring formula_72 produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.\n\nThe objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group formula_30 regularization term. An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.\n\nAn example of a way to fix this is to introduce the squared formula_42 norm of the weight vector as an additional regularization term while keeping the formula_30 regularization term from the group lasso approach. If the coefficient of the squared formula_42 norm term is greater than formula_67, then because the squared formula_42 norm term is strongly convex, the resulting objective function will also be strongly convex. Provided that the formula_42 coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group formula_42 regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach. Thus this approach allows for simpler optimization while maintaining sparsity.\n\n\"See: Submodular set function\"\n\nBesides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a directed acyclic graph over the variables while in the context of grid-based norms, the structure can be represented using a grid.\n\n\"See:\" Unsupervised learning\n\nUnsupervised learning methods are often used to learn the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, \"hierarchies\" are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.\n\nHierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents. Hierarchical models using Bayesian non-parametric methods have been used to learn topic models, which are statistical models for discovering the abstract \"topics\" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods. Hierarchical norms have been applied to bioinformatics, computer vision and topic models.\n\nIf the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes. Such methods have applications in computer vision\n\nThe problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:\nWhere formula_10 denotes the formula_7 \"norm\", defined as the number of nonzero entries of the vector formula_12.\n\nAlthough this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.\n\nTwo main approaches for solving the optimization problem are: 1) greedy methods, such as step-wise regression in statistics, or matching pursuit in signal processing; and 2) convex relaxation formulation approaches and proximal gradient optimization methods.\n\nA natural approximation for the best subset selection problem is the formula_30 norm regularization:\nSuch as scheme is called basis pursuit or the Lasso, which substitutes the formula_7 \"norm\" for the convex, non-differentiable formula_30 norm.\n\nProximal gradient methods, also called forward-backward splitting, are optimization methods useful for minimizing functions with a convex and differentiable component, and a convex potentially non-differentiable component.\n\nAs such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems of the following form: \nWhere formula_90 is a convex and differentiable loss function like the quadratic loss, and formula_91 is a convex potentially non-differentiable regularizer such as the formula_30 norm.\n\nStructured Sparsity regularization can be applied in the context of multiple kernel learning. Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.\n\nIn the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown. Assume for this example that rather than only one dictionary, several finite dictionaries are considered.\n\nFor simplicity, the case in which there are only two dictionaries formula_93 and formula_94 where formula_95 and formula_96 are integers, will be considered. The atoms in formula_26 as well as the atoms in formula_98 are assumed to be linearly independent. Let formula_99 be the union of the two dictionaries. Consider the linear space of functions formula_100 given by linear combinations of the form\n\nformula_101\n\nfor some coefficient vectors formula_102, where formula_103. Assume the atoms in formula_104 to still be linearly independent, or equivalently, that the map formula_105 is one to one. The functions in the space formula_100 can be seen as the sums of two components, one in the space formula_107, the linear combinations of atoms in formula_26 and one in formula_109, the linear combinations of the atoms in formula_98.\n\nOne choice of norm on this space is formula_111. Note that we can now view formula_100 as a function space in which formula_107, formula_109 are subspaces. In view of the linear independence assumption, formula_100 can be identified with formula_116 and formula_117 with formula_118 respectively. The norm mentioned above can be seen as the group norm in formula_100associated to the subspaces formula_107, formula_109, providing a connection to structured sparsity regularization.\n\nHere, formula_107, formula_109 and formula_100 can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps formula_125, given by formula_126, formula_127, given by formula_128, and formula_129, given by the concatenation of formula_130, respectively.\n\nIn the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces formula_107 and formula_109. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.\n\nThe above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis\n\nspaces.\n\nConsidering sparse multiple kernel learning is useful in several situations including the following:\n- Data fusion: When each kernel corresponds to a different kind of modality/feature.\n- Nonlinear variable selection: Consider kernels formula_133 depending only one dimension of the input.\n\nGenerally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.\n\nStructured sparsity regularization methods have been used in a number of settings where it is desired to impose an \"a priori\" input variable structure to the regularization process. Some such applications are:\n- Compressive sensing in magnetic resonance imaging (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time\n- Robust face recognition in the presence of misalignment, occlusion and illumination variation\n- Uncovering socio-linguistic associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities\n- Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets\n\n", "related": "\n- Statistical learning theory\n- Regularization\n- Sparse approximation\n- Proximal gradient methods\n- Convex analysis\n- Feature selection\n"}
{"id": "48813654", "url": "https://en.wikipedia.org/wiki?curid=48813654", "title": "Sparse dictionary learning", "text": "Sparse dictionary learning\n\nSparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as \"sparse coding\") in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called \"atoms\" and they compose a \"dictionary\". Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation. \n\nOne of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP or fast non-iterative algorithms can be used to recover the signal. \n\nOne of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting. \nGiven the input dataset formula_1 we wish to find a dictionary formula_2 and a representation formula_3 such that both formula_4 is minimized and the representations formula_5 are sparse enough. This can be formulated as the following optimization problem:\n\nformula_6, where formula_7, formula_8\n\nformula_9 is required to constrain formula_10 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of formula_5.formula_12 controls the trade off between the sparsity and the minimization error.\n\nThe minimization problem above is not convex because of the ℓ-\"norm\" and solving this problem is NP-hard. In some cases \"L\"-norm is known to ensure sparsity and so the above becomes a convex optimization problem with respect to each of the variables formula_10 and formula_14 when the other one is fixed, but it is not jointly convex in formula_15.\nThe dictionary formula_10 defined above can be \"undercomplete\" if formula_17 or \"overcomplete\" in case formula_18 with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.\n\nUndercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms formula_19 to be orthogonal. The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial. And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.\n\nOvercomplete dictionaries, however, do not require the atoms to be orthogonal (they will never be a basis anyway) thus allowing for more flexible dictionaries and richer data representations. \n\nAn overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents the given signal in a best way. Learned dictionaries are capable of giving sparser solutions as compared to predefined transform matrices.\n\nAs the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.\n\nThe problem of finding an optimal sparse coding formula_20 with a given dictionary formula_10 is known as sparse approximation (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as matching pursuit and LASSO) which are incorporated into the algorithms described below.\n\nThe method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem. The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:\n\nformula_22\n\nHere, formula_23 denotes the Frobenius norm. MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by formula_24 where formula_25 is a Moore-Penrose pseudoinverse. After this update formula_26 is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).\n\nMOD has proved to be a very efficient method for low-dimensional input data formula_27 requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.\n\nK-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means. It enforces that each element of the input data formula_28 is encoded by a linear combination of not more than formula_29 elements in a way identical to the MOD approach:\n\nformula_30\n\nThis algorithm's essence is to first fix the dictionary, find the best possible formula_31 under the above constraint (using Orthogonal Matching Pursuit) and then iteratively update the atoms of dictionary formula_10 in the following manner:\n\nformula_33\n\nThe next steps of the algorithm include rank-1 approximation of the residual matrix formula_34, updating formula_35 and enforcing the sparsity of formula_36 after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.\n\nOne can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem. The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set formula_9. The step that occurs at i-th iteration is described by this expression:\n\nformula_38, where formula_39 is a random subset of formula_40 and formula_41 is a gradient step.\n\nAn algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function. Consider the following Lagrangian:\n\nformula_42, where formula_43 is a constraint on the norm of the atoms and formula_44 are the so-called dual variables forming the diagonal matrix formula_45.\n\nWe can then provide an analytical expression for the Lagrange dual after minimization over formula_10:\n\nformula_47.\n\nAfter applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient) we get the value of formula_10:\n\nformula_49\n\nSolving this problem is less computational hard because the amount of dual variables formula_50 is a lot of times much less than the amount of variables in the primal problem.\n\nIn this approach, the optimization problem is formulated as:\n\nformula_51, where formula_52is the permitted error in the reconstruction LASSO.\n\nIt finds an estimate of formula_53by minimizing the least square error subject to a \"L\"-norm constraint in the solution vector, formulated as:\n\nformula_54, where formula_55controls the trade-off between sparsity and the reconstruction error. This gives the global optimal solution. See also Online dictionary learning for Sparse coding\n\nParametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones. This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include: \n- Translation-invariant dictionaries. These dictionaries are composed by the translations of the atoms originating from the dictionary constructed for a finite-size signal patch. This allows the resulting dictionary to provide a representation for the arbitrary-sized signal.\n- Multiscale dictionaries. This method focuses on constructing a dictionary that is composed of differently scaled dictionaries to improve sparsity.\n- Sparse dictionaries. This method focuses on not only providing a sparse representation but also constructing a sparse dictionary which is enforced by the expression formula_56 where formula_57 is some pre-defined analytical dictionary with desirable properties such as fast computation and formula_58 is a sparse matrix. Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches.\n\nMany common approaches to sparse dictionary learning rely on the fact that the whole input data formula_59 (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points formula_60 becoming available.\n\nA dictionary can be learned in an online manner the following way:\n1. For formula_61\n2. Draw a new sample formula_62\n3. Find a sparse coding using LARS: formula_63\n4. Update dictionary using block-coordinate approach: formula_64\n\nThis method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).\n\nThe dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.\n\nIt also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.\n\nSparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis and unsupervised clustering. In evaluations with the Bag-of-Words model, sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.\n\nDictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), functional MRI (fMRI), and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.\n\n", "related": "\n- Sparse approximation\n- Sparse PCA\n- Matrix factorization\n- K-SVD\n- Neural sparse coding\n"}
{"id": "49316492", "url": "https://en.wikipedia.org/wiki?curid=49316492", "title": "Darkforest", "text": "Darkforest\n\nDarkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.\n\nDarkforest is of similar strength to programs like CrazyStone and Zen. It has been tested against a professional human player at the 2016 UEC cup. Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques.\n\nDarkforest is named after Liu Cixin's science fiction novel \"The Dark Forest\".\n\nCompeting with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep Convolutional Neural Network designed for long-term predictions, Darkforest has been able to substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.\n\nAgainst human players, Darkfores2 achieves a stable \"3d ranking\" on KGS Go Server, which roughly corresponds to an advanced amateur human player. However, after adding Monte Carlo Tree Search to Darkfores2 to create a much stronger player named darkfmcts3, it can achieve a \"5d ranking\" on the KGS Go Server.\n\ndarkfmcts3 is on par with state-of-the-art Go AIs such as Zen, DolBaram and Crazy Stone but lags behind AlphaGo. It won 3rd place in January 2016 KGS Bot Tournament against other Go AIs.\n\nAfter Google's AlphaGo won against Fan Hui in 2015, Facebook made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.\n\nDarkforest uses a neural network to sort through the 10 board positions, and find the most powerful next move. However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so Darkfores2 combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of Darkforest, with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games. This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in Darkfores2 by running the processes in parallel with frequent communication between the two.\n\nGo is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do. In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that \"felt human\".\n\nIt has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays tenuki (\"move elsewhere\") pointlessly when local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, it plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.\n\nThe family of Darkforest computer go programs is based on convolution neural networks. The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search. Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a Monte Carlo tree search.\n\nDarkfmcts3 relies on a convolution neural networks that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players liberties are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. Ko (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. formula_1, where formula_2 is a real number at a position), and each player's territory (binary, based on which player a location is closer to).\n\nDarkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a rectified linear unit, a popular activation function for deep neural networks. A key innovation of Darkfmct3 compared to previous approaches is that it uses only one softmax function to predict the next move, which enables the approach to reduce the overall number of parameters. Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla stochastic gradient descent.\n\nDarkfmct3 synchronously couples a convolutional neural network with a Monte Carlo tree search. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.\n\nDarkfores2 beats Darkforest, its neural network-only predecessor, around 90% of the time, and Pachi, one of the best search-based engines, around 95% of the time. On the Kyu rating system, Darkforest holds a 1-2d level. Darkfores2 achieves a stable 3d level on KGS Go Server as a ranked bot. With the added Monte Carlo tree search, Darkfmcts3 with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n\n", "related": "\n- Go and mathematics\n\n- Source code on Github\n"}
{"id": "5008963", "url": "https://en.wikipedia.org/wiki?curid=5008963", "title": "Inauthentic text", "text": "Inauthentic text\n\nAn inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.\n\nSometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low.\n\nWith the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase \"Colorless green ideas sleep furiously\" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.\n\nThe first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score.\n\n", "related": "\n- Scraper site\n- Spamdexing\n\n- An Inauthentic Paper Detector from Indiana University School of Informatics\n"}
{"id": "50211107", "url": "https://en.wikipedia.org/wiki?curid=50211107", "title": "Bayesian structural time series", "text": "Bayesian structural time series\n\nBayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data.\n\nThe model has also promising application in the field of analytical marketing. In particular, it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators. Difference-in-differences models and interrupted time series designs are alternatives to this approach. \"In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls.\"\n\nThe model consists of three main components:\n1. Kalman filter. The technique for time series decomposition. In this step, a researcher can add different state variables: trend, seasonality, regression, and others.\n2. Spike-and-slab method. In this step, the most important regression predictors are selected.\n3. Bayesian model averaging. Combining the results and prediction calculation.\nThe model could be used to discover the causations with its counterfactual prediction and the observed data.\n\nA possible drawback of the model can be its relatively complicated mathematical underpinning and difficult implementation as a computer program. However, the programming language R has ready-to-use packages for calculating the BSTS model, which do not require strong mathematical background from a researcher.\n\n", "related": "\n- Bayesian inference using Gibbs sampling\n- Correlation does not imply causation\n\n- Scott, S. L., & Varian, H. R. 2014a. Bayesian variable selection for nowcasting economic time series. \"Economic Analysis of the Digital Economy.\"\n- Scott, S. L., & Varian, H. R. 2014b. Predicting the present with bayesian structural time series. \"International Journal of Mathematical Modelling and Numerical Optimisation.\"\n- Varian, H. R. 2014. Big Data: New Tricks for Econometrics. \"Journal of Economic Perspectives\"\n- Brodersen, K. H., Gallusser, F., Koehler, J., Remy, N., & Scott, S. L. 2015. Inferring causal impact using Bayesian structural time-series models. \"The Annals of Applied Statistics.\"\n- R package \"bsts\".\n- R package \"CausalImpact\".\n- O’Hara, R. B., & Sillanpää, M. J. 2009. A review of Bayesian variable selection methods: what, how and which. \"Bayesian analysis.\"\n- Hoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T. 1999. Bayesian model averaging: a tutorial. \"Statistical science.\"\n"}
{"id": "50222574", "url": "https://en.wikipedia.org/wiki?curid=50222574", "title": "Semantic folding", "text": "Semantic folding\n\nSemantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.\n\nSemantic folding theory draws inspiration from Douglas R. Hofstadter's \"Analogy as the Core of Cognition\" which suggests that the brain makes sense of the world by identifying and applying analogies. The theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a similarity measure and offers, as a solution, the sparse binary vector employing a two-dimensional topographic semantic space as a distributional reference frame. The theory builds on the computational theory of the human cortex known as hierarchical temporal memory (HTM), and positions itself as a complementary theory for the representation of language semantics.\n\nA particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level.\n\nAnalogous to the structure of the neocortex, Semantic Folding theory posits the implementation of a semantic space as a two-dimensional grid. This grid is populated by context-vectors in such a way as to place similar context-vectors closer to each other, for instance, by using competitive learning principles. This vector space model is presented in the theory as an equivalence to the well known word space model described in the Information Retrieval literature.\n\nGiven a semantic space (implemented as described above) a word-vector can be obtained for any given word Y by employing the following algorithm:\n\nFor each position X in the semantic map (where X represents cartesian coordinates)\n\nThe result of this process will be a word-vector containing all the contexts in which the word Y appears and will therefore be representative of the semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse distributed representation (SDR) format [Schütze, 1993] & [Sahlgreen, 2006]. Some properties of word-SDRs that are of particular interest with respect to computational semantics are:\n- high noise resistance: As a result of similar contexts being placed closer together in the underlying map, word-SDRs are highly tolerant of false or shifted \"bits\".\n- boolean logic: It is possible to manipulate word-SDRs in a meaningful way using boolean (OR, AND, exclusive-OR) and/or arithmetical (SUBtract) functions .\n- sub-sampling: Word-SDRs can be sub-sampled to a high degree without any appreciable loss of semantic information.\n- topological two-dimensional representation: The SDR representation maintains the topological distribution of the underlying map therefore words with similar meanings will have similar word-vectors. This suggests that a variety of measures can be applied to the calculation of semantic similarity, from a simple overlap of vector elements, to a range of distance measures such as: Euclidean distance, Hamming distance, Jaccard distance, cosine similarity, Levenshtein distance, Sørensen-Dice index, etc.\n\nSemantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: Vocabulary mismatch (the fact that the same meaning can be expressed in many ways) and ambiguity of natural language (the fact that the same term can have several meanings).\n\nThe application of semantic spaces in natural language processing (NLP) aims at overcoming limitations of rule-based or model-based approaches operating on the keyword level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning. Rule-based and machine learning-based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.\n\nResearch in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: latent semantic analysis from Microsoft and Hyperspace Analogue to Language from the University of California. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the accuracy of modelling associative relations between words (e.g. \"spider-web\", \"lighter-cigarette\", as opposed to synonymous relations such as \"whale-dolphin\", \"astronaut-driver\") was achieved by explicit semantic analysis (ESA) in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 dimensions (where each dimension represents an Article in Wikipedia). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.\n\nMore recently, advances in neural networking techniques in combination with other new approaches (tensors) led to a host of new recent developments: Word2vec from Google and GloVe from Stanford University.\n\nSemantic folding represents a novel, biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with 16,000 dimensions (a semantic fingerprint) in a 2D semantic map (the semantic universe). Sparse binary representation are advantageous in terms of computational efficiency, and allow for the storage of very large numbers of possible patterns.\n\nThe topological distribution over a two-dimensional grid (outlined above) lends itself to a bitmap type visualization of the semantics of any word or text, where each active semantic feature can be displayed as e.g. a pixel. As can be seen in the images shown here, this representation allows for a direct visual comparison of the semantics of two (or more) linguistic items.\n\nImage 1 clearly demonstrates that the two disparate terms \"dog\" and \"car\" have, as expected, very obviously different semantics.\n\nImage 2 shows that only one of the meaning contexts of \"jaguar\", that of \"Jaguar\" the car, overlaps with the meaning of Porsche (indicating partial similarity). Other meaning contexts of \"jaguar\" e.g. \"jaguar\" the animal clearly have different non-overlapping contexts.\nThe visualization of semantic similarity using Semantic Folding bears a strong resemblance to the fMRI images produced in a research study conducted by A.G. Huth et al., where it is claimed that words are grouped in the brain by meaning.\n", "related": "NONE"}
{"id": "50227596", "url": "https://en.wikipedia.org/wiki?curid=50227596", "title": "Spike-and-slab regression", "text": "Spike-and-slab regression\n\nIn statistics, spike-and-slab regression is a Bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations.\n\nInitially, the idea of the spike-and-slab model was proposed by Mitchell & Beauchamp (1988). The approach was further significantly developed by Madigan & Raftery (1994) and George & McCulloch (1997). The final adjustments to the model were done by Ishwaran & Rao (2005).\n\nSuppose we have \"P\" possible predictors in some model. Vector \"γ\" has a length equal to \"P\" and consists of zeros and ones. This vector indicates whether a particular variable is included in the regression or not. If no specific prior information on initial inclusion probabilities of particular variables is available, a Bernoulli prior distribution is a common default choice. Conditional on a predictor being in the regression, we identify a prior distribution for the model coefficient, which corresponds to that variable (\"β\"). A common choice on that step is to use a Normal prior with mean equal to zero and a large variance calculated based on formula_1 (where formula_2 is a design matrix of explanatory variables of the model).\n\nA draw of \"γ\" from its prior distribution is a list of the variables included in the regression. Conditional on this set of selected variables, we take a draw from the prior distribution of the regression coefficients (if \"γ\" = 1 then \"β\" ≠ 0 and if \"γ\" = 0 then \"β\" = 0). \"βγ\" denotes the subset of \"β\" for which \"γ\" = 1. In the next step, we calculate a posterior probability distribution for both inclusion and coefficients by applying a standard statistical procedure. All steps of the described algorithm are repeated thousands of times using Markov chain Monte Carlo (MCMC) technique. As a result, we obtain a posterior distribution of \"γ\" (variable inclusion in the model), \"β\" (regression coefficient values) and the corresponding prediction of \"y\".\n\nThe model got its name (spike-and-slab) due to the shape of the two prior distributions. The \"spike\" is the probability of a particular coefficient in the model to be zero. The \"slab\" is the prior distribution for the regression coefficient values.\n\nAn advantage of Bayesian variable selection techniques is that they are able to make use of prior knowledge about the model. In the absence of such knowledge, some reasonable default values can be used; to quote Scott and Varian (2013): \"For the analyst who prefers simplicity at the cost of some reasonable assumptions, useful prior information can be reduced to an expected model size, an expected \"R\", and a sample size \"ν\" determining the weight given to the guess at \"R\".\" Some researchers suggest the following default values: \"R\" = 0.5, \"ν\" = 0.01, and = 0.5 (parameter of a prior Bernoulli distribution).\n\nA possible drawback of the Spike-and-Slab model can be its mathematical complexity (in comparison to linear regression). A deep understanding of this model requires sound knowledge in stochastic processes. On the other hand, some modern statistical software (e.g. R) have ready-to-use solutions for calculating various Bayesian variable selection models. In this case, it would be enough for a researcher to know the idea of the method, required model parameters and input variables. The analysis of the model outcomes (distribution of \"γ\", \"β\", and corresponding predictions of \"y\") can be more challenging in comparison to linear regression case. The spike-and-slab model produces inclusion probabilities for each of possible predictors. This can cause difficulties when comparing results to the studies with simple regression (usually only regression coefficients with corresponding statistics are available).\n\n", "related": "\n- Bayesian inference using Gibbs sampling\n- Bayesian structural time series\n"}
{"id": "50336055", "url": "https://en.wikipedia.org/wiki?curid=50336055", "title": "Glossary of artificial intelligence", "text": "Glossary of artificial intelligence\n\nThis glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.\n\n \n\n \n\n", "related": "\n- Artificial intelligence\n"}
{"id": "30909817", "url": "https://en.wikipedia.org/wiki?curid=30909817", "title": "Multilinear subspace learning", "text": "Multilinear subspace learning\n\nMultilinear subspace learning is an approach to dimensionality reduction. \nDimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor. Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\n\nThe mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as the sensor provides them; as matrices or higher order tensors, their representations are computed by performing N multiple linear projections.\n\nMultilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).\n\nWith the advances in data acquisition and storage technology, big data (or massive data sets) are being generated on a daily basis in a wide range of emerging applications. Most of these big data are multidimensional. Moreover, they are usually very-high-dimensional, with a large amount of redundancy, and only occupying a part of the input space. Therefore, dimensionality reduction is frequently employed to map high-dimensional data to a low-dimensional space while retaining as much information as possible.\n\nLinear subspace learning algorithms are traditional dimensionality reduction techniques that represent input data as vectors and solve for an optimal linear mapping to a lower-dimensional space. Unfortunately, they often become inadequate when dealing with massive multidimensional data. They result in very-high-dimensional vectors, lead to the estimation of a large number of parameters.\n\nMultilinear Subspace Learning employ different types of data tensor analysis tools for dimensionality reduction. Multilinear Subspace learning can be applied to observations whose measurements were vectorized and organized into a data tensor, or whose measurements are treated as a matrix and concatenated into a tensor.\n\nHistorically, multilinear principal component analysis has been referred to as \"M-mode PCA\", a terminology which was coined by Peter Kroonenberg. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between multilinear tensor decompositions that computed 2nd order statistics associated with each data tensor mode(axis)s, and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis. MPCA is an extension of PCA.\n\nMultilinear independent component analysis is an extension of ICA.\n\n- Multilinear extension of LDA\n- TTP-based: Discriminant Analysis with Tensor Representation (DATER)\n- TTP-based: General tensor discriminant analysis (GTDA)\n- TVP-based: Uncorrelated Multilinear Discriminant Analysis (UMLDA)\n\n- Multilinear extension of CCA\n- TTP-based: Tensor Canonical Correlation Analysis (TCCA)\n- TVP-based: Multilinear Canonical Correlation Analysis (MCCA)\n- TVP-based: Bayesian Multilinear Canonical Correlation Analysis (BMTF)\n\n- A TTP is a direct projection of a high-dimensional tensor to a low-dimensional tensor of the same order, using \"N\" projection matrices for an \"N\"th-order tensor. It can be performed in \"N\" steps with each step performing a tensor-matrix multiplication (product). The \"N\" steps are exchangeable. This projection is an extension of the higher-order singular value decomposition (HOSVD) to subspace learning. Hence, its origin is traced back to the Tucker decomposition in 1960s.\n\n- A TVP is a direct projection of a high-dimensional tensor to a low-dimensional vector, which is also referred to as the rank-one projections. As TVP projects a tensor to a vector, it can be viewed as multiple projections from a tensor to a scalar. Thus, the TVP of a tensor to a \"P\"-dimensional vector consists of \"P\" projections from the tensor to a scalar. The projection from a tensor to a scalar is an elementary multilinear projection (EMP). In EMP, a tensor is projected to a point through \"N\" unit projection vectors. It is the projection of a tensor on a single line (resulting a scalar), with one projection vector in each mode. Thus, the TVP of a tensor object to a vector in a \"P\"-dimensional vector space consists of \"P\" EMPs. This projection is an extension of the canonical decomposition, also known as the parallel factors (PARAFAC) decomposition.\n\nThere are \"N\" sets of parameters to be solved, one in each mode. The solution to one set often depends on the other sets (except when \"N=1\", the linear case). Therefore, the suboptimal iterative procedure in is followed.\n\n1. Initialization of the projections in each mode\n2. For each mode, fixing the projection in all the other mode, and solve for the projection in the current mode.\n3. Do the mode-wise optimization for a few iterations or until convergence.\n\nThis is originated from the alternating least square method for multi-way data analysis.\n\nThe advantages of MSL over traditional linear subspace modeling, in common domains where the representation is naturally somewhat tensorial, are:\n\n- MSL preserves the structure and correlation that the original data had before projection, by operating on a natural tensorial representation of the multidimensional data.\n- MSL can learn more compact representations than its linear counterpart; in other words, it needs to estimate a much smaller number of parameters. Thus, MSL can handle big tensor data more efficiently, by performing computations on a representation with many fewer dimensions. This leads to lower demand on computational resources.\n\nHowever, MSL algorithms are iterative and are not guaranteed to converge; where an MSL algorithm does converge, it may do so at a local optimum. (In contrast, traditional linear subspace modeling techniques often produce an exact closed-form solution.) MSL convergence problems can often be mitigated by choosing an appropriate subspace dimensionality, and by appropriate strategies for initialization, for termination, and for choosing the order in which projections are solved.\n\n- Survey: A survey of multilinear subspace learning for tensor data (open access version).\n- Lecture: Video lecture on UMPCA at the 25th International Conference on Machine Learning (ICML 2008).\n\n- MATLAB Tensor Toolbox by Sandia National Laboratories.\n- The MPCA algorithm written in Matlab (MPCA+LDA included).\n- The UMPCA algorithm written in Matlab (data included).\n- The UMLDA algorithm written in Matlab (data included).\n\n- 3D gait data (third-order tensors): 128x88x20(21.2M); 64x44x20(9.9M); 32x22x10(3.2M);\n\n", "related": "\n- CP decomposition\n- Dimension reduction\n- Multilinear algebra\n- Multilinear Principal Component Analysis\n- Tensor\n- Tensor decomposition\n- Tensor software\n- Tucker decomposition\n"}
{"id": "49786340", "url": "https://en.wikipedia.org/wiki?curid=49786340", "title": "Movidius", "text": "Movidius\n\nMovidius is a company based in San Mateo, California that designs specialised low-power processor chips for computer vision. The company was acquired by Intel in September 2016.\n\nMovidius was co-founded in Dublin in 2005 by Sean Mitchell and David Moloney. Between 2006 and 2016, it raised nearly $90 million in capital funding. In May, 2013 the company appointed Remi El-Ouazzane as CEO. In January, 2016 the company announced a partnership with Google. Movidius has been active in the Google Project Tango project. Movidius announced a planned acquisition by Intel in September 2016.\n\nThe company's Myriad 2 chip is an always-on manycore vision processing unit that can function on power-constrained devices. The \"Fathom\" is a USB stick containing a Myriad 2 processor, allowing a vision accelerator to be added to devices using ARM processors including PCs, drones, robots, IoT devices and video surveillance for tasks such as identifying people or objects. It can run at between 80 and 150 GFLOPS on little more than 1W of power.\n\nIntel's Myriad X VPU (vision processing unit) is the third generation and most advanced VPU from Movidius, an Intel company. Intel's Myriad X VPU is the first of its class to feature the Neural Compute Engine—a dedicated hardware accelerator for deep neural network deep-learning inferences. The Neural Compute Engine in conjunction with the 16 SHAVE cores and an ultra-high throughput intelligent memory fabric makes Myriad X a strong option for on-device deep neural networks and computer vision applications. Intel's Myriad X VPU has received additional upgrades to imaging and vision engines including additional programmable SHAVE cores, upgraded and expanded vision accelerators, and a new native 4K image processor pipeline with support for up to 8 HD sensors connecting directly to the VPU. As with Myriad 2, the Myriad X VPU is programmable via the Myriad Development Kit (MDK) which includes all necessary development tools, frameworks and APIs to implement custom vision, imaging and deep neural network workloads on the chip.\n\nThe Intel Movidius Neural Compute Stick (NCS) is a tiny fanless deep-learning device that can be used to learn AI programming at the edge. NCS is powered by the same low-power, high-performance Intel Movidius Vision Processing Unit that can be found in millions of smart security cameras, gesture-controlled drones, industrial machine vision equipment, and more. Supported frameworks are TensorFlow and Caffe. \n\nOn 14 November 2018, the company announced the latest version of NCS, marketed as \"Neural Compute Stick 2\" at the AI DevCon event in Beijing.\n\nGoogle Clips camera uses Myriad 2 VPU.\nThe Intel RealSense Tracking Camera T265 is another product that uses the Myriad 2.\n\n", "related": "\n- Vision processing unit\n- MPSoC\n- Coprocessor\n- Convolutional neural network\n"}
{"id": "50828755", "url": "https://en.wikipedia.org/wiki?curid=50828755", "title": "Timeline of machine learning", "text": "Timeline of machine learning\n\nThis page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.\n\n", "related": "\n- History of artificial intelligence\n- Machine learning\n- Timeline of artificial intelligence\n- Timeline of machine translation\n"}
{"id": "995455", "url": "https://en.wikipedia.org/wiki?curid=995455", "title": "Savi Technology", "text": "Savi Technology\n\nSavi Technology was founded in 1989 and is based in Alexandria, Virginia.\n\nSavi delivers live streaming facts and insights about the location, condition, and security of in-transit goods. Using big data analytics, Savi equips shippers, carriers, 3PLs, and governments to optimize supply chain logistics before, during and after transit, reducing costs and inventory while improving service. Savi is trusted to run the world’s largest and most complex asset tracking and monitoring network serving the US DoD, Allied military and more than 1000 commercial companies around the globe.\n\nSavi Technology, an innovator in big data/machine learning analytic solutions, supply-chain-management software and sensor technology, offers sensor analytics solutions for logistics and supply chain operations for shippers, governments, third-party logistics providers and technology providers. Savi solutions track shipment locations in real time and apply analytics to accurately predict the arrival of goods. The company's in-transit visibility platform, Savi Visibility™ provides 100% visibility on all multimodal in-transit shipments. The company's analytics solution, Savi Insight™ ingests, cleans, normalizes and analyzes massive types of real-time data streams, IoT data, sensor data, telematics and carrier feeds, social and weather data as well as milestone based data, such as (EDI) enterprise data information, (ERP), and historical information, applies proprietary algorithms and machine learning to transform supply chains by delivering logistics teams with actionable operational insights. The company offers a variety of hardware solutions including tags (also called sensors) that enable governments and organizations to access real-time information on the location, condition, and security status of assets and shipments; mobile IoT sensors, fixed and mobile readers; active radio-frequency identification devices and sensors; and portable deployment kits (PDKs). In addition, Savi provides professional services, including program management, systems integration, system and network design, support, and hosting. It serves the U.S. Department of Defense, the U.S. and allied militaries, civilian governmental organizations, and commercial companies, as well as transportation, third-party logistics (3PL), pharmaceuticals, retail, life sciences, and manufacturing industries worldwide.\n\n- Bloomberg\n- The Washington Post\n", "related": "NONE"}
{"id": "2934910", "url": "https://en.wikipedia.org/wiki?curid=2934910", "title": "Cognitive robotics", "text": "Cognitive robotics\n\nCognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.\n\nWhile traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics.\n\nCognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.\n\nA preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to \"expect\" a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.\n\nOnce a robot can coordinate its motors to produce a desired result, the technique of \"learning by imitation\" may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.\n\nA more complex learning approach is \"autonomous knowledge acquisition\": the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.\n\nA somewhat more directed mode of exploration can be achieved by \"curiosity\" algorithms, such as Intelligent Adaptive Curiosity or Category-Based Intrinsic Motivation. These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.\n\nSome researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships.\n\nSome of the fundamental questions to still be answered in cognitive robotics are:\n- How much human programming should or can be involved to support the learning processes?\n- How can one quantify progress? Some of the adopted ways is the reward and punishment. But what kind of reward and what kind of punishment? In humans, when teaching a child for example, the reward would be candy or some encouragement, and the punishment can take many forms. But what is an effective way with robots?\n\nCognitive Robotics book by Hooman Samani, takes a multidisciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects.\n\n", "related": "\n- Artificial intelligence\n- Intelligent agent\n- Cognitive science\n- Cybernetics\n- Developmental robotics\n- Embodied cognitive science\n- Epigenetic robotics\n- Evolutionary robotics\n- Hybrid intelligent system\n- Intelligent control\n\n- The Symbolic and Subsymbolic Robotic Intelligence Control System (SS-RICS)\n- Intelligent Systems Group - University of Utrecht\n- The Cognitive Robotics Group - University of Toronto\n- The IDSIA Robotics Lab and Cognitive Robotics Lab of Juergen Schmidhuber\n- What Does the Future Hold for Cognitive Robots? - Idaho National Laboratory\n- Cognitive Robotics at the Naval Research Laboratory\n- Cognitive robotics at ENSTA autonomous embodied systems, evolving in complex and non-constraint environments, using mainly vision as sensor.\n- The Center for Intelligent Systems - Vanderbilt University\n- Institute for Cognition and Robotics (CoR-Lab) at Bielefeld University\n- SocioCognitive Robotics at Delft University of Technology\n- Autonomous Systems Laboratory at Universidad Politecnica de Madrid\n- Knowledge Technology at Universität Hamburg\n- The Cognitive Robotics Association, founded in 1998, directed by Gerhard Lakemeyer, University of Aachen, organizes every two year the Cognitive Robotics Workshop and it is generously supported by the AI journal\n\n- iCub\n- RoboBusiness: Robots that Dream of Being Better\n- www.Conscious-Robots.com\n- The cognitive Robotics Association\n"}
{"id": "51112472", "url": "https://en.wikipedia.org/wiki?curid=51112472", "title": "Dataiku", "text": "Dataiku\n\nDataiku is a computer software company headquartered in New York City. The company develops collaborative data science software marketed for big data.\n\nThe company was founded in Paris in 2013 by 4 co-founders. Two of them met while working at French search engine company Exalead, including chief executive Florian Douetteau, and Clément Sténac.\n\nDataiku opened an office in New York City in 2015 which became the company headquarters. They opened an office in London in the summer of 2016, and announced an office in Sydney in February 2019.\n\nIn 2017, Dataiku entered the Gartner Magic Quadrant for Data Science Platforms as a \"visionary\", moved up to the \"challenger\" quadrant in 2019 and then to \"leader\" in the 2020 edition.\n\nThe software Dataiku Data Science Studio (DSS) was announced in 2014, supporting predictive modelling to build business applications. Later versions of DSS added other features.\n\nDataiku offers a free edition and enterprise versions with additional features, such as multi-user collaboration or real-time scoring.\n\nFor its first two years, the company relied on its own capital. In January 2015, Dataiku raised $3.6 million from Serena Capital and Alven Capital, two French technology venture capital funds. This was followed by $14 million raised with FirstMark Capital, a New York City-based venture capital firm in October 2016. In September 2017 the company raised a $28 million Series B investment from Battery Ventures, as well as historic investors. In December 2018 Dataiku raised $101 million for data science platform. Among investors were Iconiq Capital, Alven Capital, Battery Ventures, Dawn Capital and FirstMark Capital. In December 2019, CapitalG purchased some of the shares previously owned by Serena Capital in a secondary round that valued Dataiku at $1.4 billion.\n", "related": "NONE"}
{"id": "50773876", "url": "https://en.wikipedia.org/wiki?curid=50773876", "title": "Algorithm selection", "text": "Algorithm selection\n\nAlgorithm selection (sometimes also called per-instance algorithm selection or offline algorithm selection) is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, algorithms have different performances. That is, while one algorithm performs well on some instances, it performs poorly on others and vice versa for another algorithm. If we can identify when to use which algorithm, we can get the best of both worlds and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms.\n\nGiven a portfolio formula_1 of algorithms formula_2, a set of instances formula_3 and a cost metric formula_4, the algorithm selection problem consists of finding a mapping formula_5 from instances formula_6 to algorithms formula_1 such that the cost formula_8 across all instances is optimized.\n\nA well-known application of algorithm selection is the Boolean satisfiability problem. Here, the portfolio of algorithms is a set of (complementary) SAT solvers, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other formula_9-hard problems (such as mixed integer programming, CSP, AI planning, TSP, MAXSAT, QBF and answer set programming). Competition-winning systems in SAT are SATzilla, 3S and CSHC\n\nIn machine learning, algorithm selection is better known as meta-learning. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set.\n\nThe algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features formula_10, algorithm selection can be seen as a multi-class classification problem by learning a mapping formula_11 for a given instance formula_12.\n\nInstance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas, or number of samples, features, class balance for ML data sets to get an impression about their characteristics.\n\nWe distinguish between two kinds of features: \n1. Static features are in most cases some counts and statistics (e.g., clauses-to-variables ratio in SAT). These features ranges from very cheap features (e.g. number of variables) to very complex features (e.g., statistics about variable-clause graphs).\n2. Probing features (sometimes also called landmarking features) are computed by running some analysis of algorithm behavior on an instance (e.g., accuracy of a cheap decision tree algorithm on an ML data set, or running for a short time a stochastic local search solver on a Boolean formula). These feature often cost more than simple static features.\n\nDepending on the used performance metric formula_13, feature computation can be associated with costs.\nFor example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system.\nSAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for CNF formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds).\n\nIt is important to take the overhead of feature computation into account in practice in such scenarios; otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with prefect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted.\n\nOne of the first successful algorithm selection approaches predicted the performance of each algorithm formula_14 and selecting the algorithm with the best predicted performance formula_15 for a new instance formula_12.\n\nA common assumption is that the given set of instances formula_6 can be clustered into homogeneous subsets \nand for each of these subsets, there is one well-performing algorithm for all instances in there.\nSo, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster.\nA new instance is assigned to a cluster and the associated algorithm selected.\n\nA more modern approach is cost-sensitive hierarchical clustering using supervised learning to identify the homogeneous instance subsets.\n\nA common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms) \nand choose the class that was predicted most often by the pairwise models.\nWe can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms.\nThis is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference.\nTherefore, each instance formula_12 for training a classification model formula_19 vs formula_20 is associated with a cost formula_21.\n\nThe algorithm selection problem can be effectively applied under the following assumptions:\n- The portfolio formula_1 of algorithms is complementary with respect to the instance set formula_6, i.e., there is no single algorithm formula_2 that dominates the performance of all other algorithms on formula_6 (see figures to the right for examples on complementary analysis).\n- In some application, the computation of instance features is associated with a cost. For example, if the cost metric is running time, we have also to consider the time to compute the instance features. In such cases, the cost to compute features should not be larger than the performance gain through algorithm selection.\n\nAlgorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied.\nApplication domains include:\n\n- hard combinatorial problems: SAT, Mixed Integer Programming, CSP, AI Planning, TSP, MAXSAT, QBF and Answer Set Programming\n- combinatorial auctions\n- in machine learning, the problem is known as meta-learning\n- software design\n- black-box optimization\n- multi-agent systems\n- numerical optimization\n- linear algebra, differential equations\n- evolutionary algorithms\n- vehicle routing problem\n- power systems\n\nFor an extensive list of literature about algorithm selection, we refer to a literature overview.\n\nOnline algorithm selection in Hyper-heuristic refers to switching between different algorithms during the solving process. In contrast, (offline) algorithm selection is an one-shot game where we select an algorithm for a given instance only once.\n\nAn extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely.\n\nGiven the increasing importance of parallel computation,\nan extension of algorithm selection for parallel computation is parallel portfolio selection,\nin which we select a subset of the algorithms to simultaneously run in a parallel portfolio.\n\n- Algorithm Selection Library (ASlib)\n- Algorithm selection literature\n", "related": "NONE"}
{"id": "3920550", "url": "https://en.wikipedia.org/wiki?curid=3920550", "title": "Transfer learning", "text": "Transfer learning\n\nTransfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.\n\nAndrew Ng said in his NIPS 2016 tutorial that TL will be next driver of ML commercial success after supervised learning to highlight the importance of TL.\n\nIn 1993, Lorien Pratt published a paper on transfer in machine learning, formulating the discriminability-based transfer (DBT) algorithm.\n\nIn 1997, the journal \"Machine Learning\" published a special issue devoted to transfer learning, and by 1998, the field had advanced to include multi-task learning, along with a more formal analysis of its theoretical foundations. \"Learning to Learn\", edited by Pratt and Sebastian Thrun, is a 1998 review of the subject.\n\nTransfer learning has also been applied in cognitive science, with the journal \"Connection Science\"\npublishing a special issue on reuse of neural networks through transfer in 1996.\n\nThe definition of transfer learning is given in terms of domain and task. The domain formula_1 consists of: a feature space formula_2 and a marginal probability distribution formula_3, where formula_4. Given a specific domain, formula_5, a task consists of two components: a label space formula_6 and an objective predictive function formula_7(denoted by formula_8), which is learned from the training data consisting of pairs, which consist of pairs formula_9, where formula_10 and formula_11. The function formula_7 can be used to predict the corresponding label,formula_13, of a new instance formula_14.\n\nGiven a source domain formula_15 and learning task formula_16, a target domain formula_17and learning task formula_18, transfer learning aims to help improve the learning of the target predictive function formula_19 in formula_17 using the knowledge in formula_15 and formula_16, where formula_23, or formula_24.\n\nAlgorithms are available for transfer learning in Markov logic networks and Bayesian networks. Transfer learning has also been applied to cancer subtype\ndiscovery, building utilization, general game playing, text classification, digit recognition and spam filtering.\n\n", "related": "\n- Crossover (genetic algorithm)\n- Domain adaptation\n- General game playing\n- Multi-task learning\n- Multitask optimization\n"}
{"id": "460689", "url": "https://en.wikipedia.org/wiki?curid=460689", "title": "Evolutionary programming", "text": "Evolutionary programming\n\nEvolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\n\nIt was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite-state machines as predictors and evolved them.\nCurrently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.\n\nIts main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.\n\n", "related": "\n- Artificial intelligence\n- Genetic algorithm\n- Genetic operator\n\n- Fogel, L.J., Owens, A.J., Walsh, M.J. (1966), \"Artificial Intelligence through Simulated Evolution\", John Wiley.\n- Fogel, L.J. (1999), \"Intelligence through Simulated Evolution : Forty Years of Evolutionary Programming\", John Wiley.\n- Eiben, A.E., Smith, J.E. (2003), \"Introduction to Evolutionary Computing\", Springer.\n\n- The Hitch-Hiker's Guide to Evolutionary Computation: What's Evolutionary Programming (EP)?\n- Evolutionary Programming by Jason Brownlee (PhD)\n"}
{"id": "40254", "url": "https://en.wikipedia.org/wiki?curid=40254", "title": "Genetic algorithm", "text": "Genetic algorithm\n\nIn computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. John Holland introduced genetic algorithms in 1960 based on the concept of Darwin’s theory of evolution; his student David E. Goldberg further extended GA in 1989.\n\nIn a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.\n\nThe evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a \"generation\". In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.\n\nA typical genetic algorithm requires:\n\n1. a genetic representation of the solution domain,\n2. a fitness function to evaluate the solution domain.\n\nA standard representation of each candidate solution is as an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.\n\nOnce the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.\n\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space). Occasionally, the solutions may be \"seeded\" in areas where optimal solutions are likely to be found.\n\nDuring each successive generation, a portion of the existing population is selected to breed a new generation. Individual solutions are selected through a \"fitness-based\" process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.\n\nThe fitness function is defined over the genetic representation and measures the \"quality\" of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The \"fitness\" of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.\n\nIn some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.\n\nThe next step is to generate a second generation population of solutions from those selected through a combination of genetic operators: crossover (also called recombination), and mutation.\n\nFor each new solution to be produced, a pair of \"parent\" solutions is selected for breeding from the pool selected previously. By producing a \"child\" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its \"parents\". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.\nAlthough reproduction methods that are based on the use of two parents are more \"biology inspired\", some research suggests that more than two \"parents\" generate higher quality chromosomes.\n\nThese processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.\n\nOpinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.\n\nAlthough crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.\n\nIt is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed.\n\nIn addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The \"speciation\" heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.\n\nThis generational process is repeated until a termination condition has been reached. Common terminating conditions are:\n\n- A solution is found that satisfies minimum criteria\n- Fixed number of generations reached\n- Allocated budget (computation time/money) reached\n- The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results\n- Manual inspection\n- Combinations of the above\n\nGenetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:\n\n1. A description of a heuristic that performs adaptation by identifying and recombining \"building blocks\", i.e. low order, low defining-length schemata with above average fitness.\n2. A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.\n\nGoldberg describes the heuristic as follows:\n\nDespite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.\n\nThere are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:\n\n- Repeated fitness function evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive fitness function evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods cannot deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an approximated fitness that is computationally efficient. It is apparent that amalgamation of approximate models may be one of the most promising approaches to convincingly use GA to solve complex real life problems.\n- Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or a plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.\n- The \"better\" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.\n- In many problems, GAs have a tendency to converge towards local optima or even arbitrary points rather than the global optimum of the problem. This means that it does not \"know how\" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the fitness landscape: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions, although the No Free Lunch theorem proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a \"niche penalty\", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and genetic programming) because crossing over a homogeneous population does not yield new solutions. In evolution strategies and evolutionary programming, diversity is not essential because of a greater reliance on mutation.\n- Operating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called \"triggered hypermutation\"), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called \"random immigrants\"). Again, evolution strategies and evolutionary programming can be implemented with a so-called \"comma strategy\" in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.\n- GAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like decision problems), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.\n- For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence (e.g.: ant colony optimization, particle swarm optimization) and methods based on integer linear programming. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\nThe simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.\n\nWhen bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so-called \"Hamming walls\", in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.\n\nOther approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a \"virtual alphabet\" (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.\n\nAn expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome. This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.\n\nA practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as \"elitist selection\" and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.\n\nParallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.\nOther variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.\n\nGenetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of \"pc\" and \"pm\", AGAs utilize the population information in each generation and adaptively adjust the \"pc\" and \"pm\" in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm), the adjustment of \"pc\" and \"pm\" depends on the fitness values of the solutions. In \"CAGA\" (clustering-based adaptive genetic algorithm), through the use of clustering analysis to judge the optimization states of the population, the adjustment of \"pc\" and \"pm\" depends on these optimization states.\nIt can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA while overcoming the lack of robustness of hill climbing.\n\nThis means that the rules of genetic variation may have a different meaning in the natural case. For instance – provided that steps are stored in consecutive order – crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency.\n\nA variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.\n\nA number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA, GEMGA and LLGA.\n\nProblems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.\n\nAs a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).\n\nExamples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector, antennae designed to pick up radio signals in space, walking methods for computer figures, optimal design of aerodynamic bodies in complex flowfields \n\nIn his \"Algorithm Design Manual\", Skiena advises against genetic algorithms for any task:\n\nIn 1950, Alan Turing proposed a \"learning machine\" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).\n\nAlthough Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book \"Adaptation in Natural and Artificial Systems\" (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.\n\nIn the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. \nIn 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995. Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version. Since the 1990s, MATLAB has built in three derivative-free optimization heuristic algorithms (simulated annealing, particle swarm optimization, genetic algorithm) and two direct search algorithms (simplex search, pattern search).\n\nGenetic algorithms are a sub-field:\n- Evolutionary algorithms\n- Evolutionary computing\n- Metaheuristics\n- Stochastic optimization\n- Optimization\n\nEvolutionary algorithms is a sub-field of evolutionary computing.\n\n- Evolution strategies (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain. They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy (CMA-ES).\n- Evolutionary programming (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.\n- Estimation of Distribution Algorithm (EDA) substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as Probabilistic Graphical Models, from which new solutions can be sampled or generated from guided-crossover.\n- Gene expression programming (GEP) also uses populations of computer programs. These complex computer programs are encoded in simpler linear chromosomes of fixed length, which are afterwards expressed as expression trees. Expression trees or computer programs evolve because the chromosomes undergo mutation and recombination in a manner similar to the canonical GA. But thanks to the special organization of GEP chromosomes, these genetic modifications always result in valid computer programs.\n- Genetic programming (GP) is a related technique popularized by John Koza in which computer programs, rather than function parameters, are optimized. Genetic programming often uses tree-based internal data structures to represent the computer programs for adaptation instead of the list structures typical of genetic algorithms.\n- Grouping genetic algorithm (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items. The idea behind this GA evolution proposed by Emanuel Falkenauer is that solving some complex problems, a.k.a. \"clustering\" or \"partitioning\" problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include bin packing, line balancing, clustering with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.\n- Interactive evolutionary algorithms are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.\n\nSwarm intelligence is a sub-field of evolutionary computing.\n\n- Ant colony optimization (ACO) uses many ants (or agents) equipped with a pheromone model to traverse the solution space and find locally productive areas. Although considered an Estimation of distribution algorithm,\n- Particle swarm optimization (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables.\n\nEvolutionary computation is a sub-field of the metaheuristic methods.\n\n- Electimize algorithm is an evolutionary algorithm that simulates the phenomenon of electron flow and electrical conductivity. Some current research showed Electimize to be more efficient in solving NP-hard optimization problems than traditional evolutionary algorithms. The algorithm provides higher capacity in searching the solution space extensively, and identifying global optimal alternatives. Unlike other evolutionary algorithms, Electimize evaluates the quality of the values in the solution string independently.\n- Memetic algorithm (MA), often called \"hybrid genetic algorithm\" among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from memes, which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.\n- Bacteriologic algorithms (BA) inspired by evolutionary ecology and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, there is not one individual that fits the whole environment. So, one needs to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining.\n- Cultural algorithm (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.\n- Differential search algorithm (DS) inspired by migration of superorganisms.\n- Gaussian adaptation (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information. Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain \"ambition\" to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder (average information) of the Gaussian simultaneously keeping the mean fitness constant.\n\nMetaheuristic methods broadly fall within stochastic optimisation methods.\n\n- Simulated annealing (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.\n- Tabu search (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.\n- Extremal optimization (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). The governing principle behind this algorithm is that of \"emergent\" improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.\n\n- The cross-entropy (CE) method generates candidate solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.\n- Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and metaheuristics.\n\n", "related": "\n- List of genetic algorithm applications\n- Genetic algorithms in signal processing (a.k.a. particle filters)\n- Propagation of schema\n- Universal Darwinism\n- Metaheuristics\n- Learning classifier system\n- Rule-based machine learning\n\n- Rechenberg, Ingo (1994): Evolutionsstrategie '94, Stuttgart: Fromman-Holzboog.\n- Schwefel, Hans-Paul (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).\n\n- Provides a list of resources in the genetic algorithms field\n\n- Genetic Algorithms - Computer programs that \"evolve\" in ways that resemble natural selection can solve complex problems even their creators do not fully understand An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma\n- An online interactive Genetic Algorithm tutorial for a reader to practise or learn how a GA works: Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.\n- A Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University An excellent tutorial with lots of theory\n- \"Essentials of Metaheuristics\", 2009 (225 p). Free open text by Sean Luke.\n- Global Optimization Algorithms – Theory and Application\n- Genetic Algorithms in Python Tutorial with the intuition behind GAs and Python implementation.\n- Genetic Algorithms evolves to solve the prisoner's dilemma. Written by Robert Axelrod.\n"}
{"id": "52242050", "url": "https://en.wikipedia.org/wiki?curid=52242050", "title": "Multiplicative weight update method", "text": "Multiplicative weight update method\n\nThe multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.\n\n\"Multiplicative weights\" implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered.\n\nThe earliest known version of this technique was in an algorithm named \"fictitious play\" which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of \"fictitious play\" to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Papert's earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension.\n\nIn operation research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nIn computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.\n\nA binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.\n\nGiven a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistakes will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most mistakes.\n\nUnlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same \"expert advice\" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.\nThe very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm.\n\nIf formula_10, the weight of the expert's advice will remain the same. When formula_11 increases, the weight of the expert's advice will decrease. Note that some researchers fix formula_12 in weighted majority algorithm.\n\nAfter formula_6 steps, let formula_14 be the number of mistakes of expert i and formula_15 be the number of mistakes our algorithm has made. Then we have the following bound for every formula_16:\n\nIn particular, this holds for i which is the best expert. Since the best expert will have the least formula_14, it will give the best bound on the number of mistakes made by the algorithm as a whole.\n\nGiven the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:\n\npredict \n\nwhere \n\nThe number of mistakes made by the randomized weighted majority algorithm is bounded as: \n\nwhere formula_22 and formula_23.\n\nNote that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.\nIn this randomized algorithm, formula_24 if formula_25. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define formula_12 in weighted majority algorithm and allow formula_27 in randomized weighted majority algorithm.\n\nThe multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.\n\nSuppose we were given the distribution formula_28 on experts. Let formula_29 = payoff matrix of a finite two-player zero-sum game, with formula_30 rows.\n\nWhen the row player formula_31 uses plan formula_16 and the column player formula_33 uses plan formula_34, the payoff of player formula_33 is formula_36≔formula_37, assuming formula_38.\n\nIf player formula_31 chooses action formula_16 from a distribution formula_28 over the rows, then the expected result for player formula_33 selecting action formula_34 is formula_44.\n\nTo maximize formula_45, player formula_33 is should choose plan formula_34. Similarly, the expected payoff for player formula_48 is formula_49. Choosing plan formula_16 would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:\n\nwhere P and i changes over the distributions over rows, Q and j changes over the columns.\n\nThen, let formula_52 denote the common value of above quantities, also named as the \"value of the game\". Let formula_53 be an error parameter. To solve the zero-sum game bounded by additive error of formula_54,\n\nSo there is an algorithm solving zero-sum game up to an additive factor of δ using O(/formula_57) calls to ORACLE, with an additional processing time of O(n) per call\n\nBailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it.\n\nIn machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.\n\nBased on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program.\n\nGiven formula_58 labeled examples formula_59 where formula_60 are feature vectors, and formula_61 are their labels.\n\nThe aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that formula_62 for all formula_34. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine formula_64 to be formula_65, the problem reduces to finding a solution to the following LP:\n\nThis is general form of LP.\n\nThe hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.\nIt is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.\n\nAssume the learning rate formula_69 and for formula_70, formula_71 is picked by Hedge. Then for all experts formula_16,\n\nInitialization: Fix an formula_69. For each expert, associate the weight formula_75 ≔1\nFor t=1,2,…,T:\n\nThis algorithm maintains a set of weights formula_80 over the training examples. On every iteration formula_3, a distribution formula_71 is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis formula_83 that (hopefully) has small error with respect to the distribution. Using the new hypothesis formula_83, AdaBoost generates the next weight vector formula_85. The process repeats. After T such iterations, the final hypothesis formula_86 is the output. The hypothesis formula_86 combines the outputs of the T weak hypotheses using a weighted majority vote.\n\nGiven a formula_109 matrix formula_29 and formula_111, is there a formula_112 such that formula_113?\n\nUsing the oracle algorithm in solving zero-sum problem, with an error parameter formula_115, the output would either be a point formula_112 such that formula_117 or a proof that formula_112 does not exist, i.e., there is no solution to this linear system of inequalities.\n\nGiven vector formula_119, solves the following relaxed problem\n\nIf there exists a x satisfying (1), then x satisfies (2) for all formula_121. The contrapositive of this statement is also true.\nSuppose if oracle returns a feasible solution for a formula_122, the solution formula_112 it returns has bounded width formula_124.\nSo if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of formula_125. The algorithm makes at most formula_126 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.\n\nMultiplicative weights update is the discrete-time variant of the replicator equation (replicator dynamics), which is a commonly used model in evolutionary game theory. It converges to Nash equilibrium when applied to a congestion game.\n\nIn operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry, such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension.\n", "related": "NONE"}
{"id": "938663", "url": "https://en.wikipedia.org/wiki?curid=938663", "title": "Multi-task learning", "text": "Multi-task learning\n\nMulti-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called \"hints\".\n\nIn a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.\n\nIn the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.\n\nMulti-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.\n\nWithin the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.. For example, the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains.\n\nOne can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.\n\nRelated to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.\n\nTraditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.\n\nThe MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.\n\nSuppose the training data set is formula_1, with formula_2, formula_3, where indexes task, and formula_4. Let formula_5. In this setting there is a consistent input and output space and the same loss function formula_6 for each task: . This results in the regularized machine learning problem: \nwhere formula_7 is a vector valued reproducing kernel Hilbert space with functions formula_8 having components formula_9.\n\nThe reproducing kernel for the space formula_7 of functions formula_11 is a symmetric matrix-valued function formula_12 , such that formula_13 and the following reproducing property holds: \nThe form of the kernel induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a \"separable kernel,\" which factors into separate kernels on the input space and on the tasks formula_14. In this case the kernel relating scalar components formula_15 and formula_16 is given by formula_17. For vector valued functions formula_18 we can write formula_19, where is a scalar reproducing kernel, and is a symmetric positive semi-definite formula_20 matrix. Henceforth denote formula_21 .\n\nThis factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by . Methods for non-separable kernels is an current field of research.\n\nFor the separable case, the representation theorem is reduced to formula_22. The model output on the training data is then , where is the formula_23 empirical kernel matrix with entries formula_24, and is the formula_25 matrix of rows formula_26.\n\nWith the separable kernel, equation can be rewritten as\n\nwhere is a (weighted) average of applied entry-wise to and . (The weight is zero if formula_27 is a missing observation).\n\nNote the second term in can be derived as follows:\n\nThere are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.\n\nVia the regularizer formulation, one can represent a variety of task structures easily. \n- Letting formula_29 (where formula_30 is the \"T\"x\"T\" identity matrix, and formula_31 is the \"T\"x\"T\" matrix of ones) is equivalent to letting control the variance formula_32 of tasks from their mean formula_33. For example, blood levels of some biomarker may be taken on patients at formula_34 time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients.\n- Letting formula_35 , where formula_36 is equivalent to letting formula_37 control the variance measured with respect to a group mean: formula_38. (Here formula_39 the cardinality of group r, and formula_40 is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group.\n- Letting formula_41, where formula_42 is the Laplacian for the graph with adjacency matrix \"M\" giving pairwise similarities of tasks. This is equivalent to giving a larger penalty to the distance separating tasks \"t\" and \"s\" when they are more similar (according to the weight formula_43,) i.e. formula_44 regularizes formula_45.\n- All of the above choices of A also induce the additional regularization term formula_46 which penalizes complexity in f more broadly.\n\nLearning problem can be generalized to admit learning task matrix A as follows:\nChoice of formula_47 must be designed to learn matrices \"A\" of a given type. See \"Special cases\" below.\n\nRestricting to the case of convex losses and coercive penalties Ciliberto \"et al.\" have shown that although is not convex jointly in \"C\" and \"A,\" a related problem is jointly convex.\n\nSpecifically on the convex set formula_48, the equivalent problem\n\nis convex with the same minimum value. And if formula_49 is a minimizer for then formula_50 is a minimizer for .\n\nThe perturbation via the barrier formula_51 forces the objective functions to be equal to formula_52 on the boundary of formula_53 .\n\nSpectral penalties - Dinnuzo \"et al\" suggested setting \"F\" as the Frobenius norm formula_56. They optimized directly using block coordinate descent, not accounting for difficulties at the boundary of formula_57.\n\nClustered tasks learning - Jacob \"et al\" suggested to learn \"A\" in the setting where \"T\" tasks are organized in \"R\" disjoint clusters. In this case let formula_58 be the matrix with formula_59. Setting formula_60, and formula_61, the task matrix formula_62 can be parameterized as a function of formula_63: formula_64 , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation formula_65. In this formulation, formula_66.\n\nNon-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.\n\nNon-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.\n\nUsing the principles of MTL, techniques for collaborative spam filtering that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local classifier to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.\n\nUsing boosted decision trees, one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.\n\nThe Multi-Task Learning via StructurAl Regularization (MALSAR) Matlab package implements the following multi-task learning algorithms:\n- Mean-Regularized Multi-Task Learning\n- Multi-Task Learning with Joint Feature Selection\n- Robust Multi-Task Feature Learning\n- Trace-Norm Regularized Multi-Task Learning\n- Alternating Structural Optimization\n- Incoherent Low-Rank and Sparse Learning\n- Robust Low-Rank Multi-Task Learning\n- Clustered Multi-Task Learning\n- Multi-Task Learning with Graph Structures\n\n", "related": "\n- Artificial intelligence\n- Artificial neural network\n- Automated machine learning (AutoML)\n- Evolutionary computation\n- General game playing\n- Human-based genetic algorithm\n- Kernel methods for vector output\n- Multitask optimization\n- Robot learning\n- Transfer learning\n\n- The Biosignals Intelligence Group at UIUC\n- Washington University at St. Louis Depart. of Computer Science\n\n- The Multi-Task Learning via Structural Regularization Package\n- Online Multi-Task Learning Toolkit (OMT) A general-purpose online multi-task learning toolkit based on conditional random field models and stochastic gradient descent training (C#, .NET)\n"}
{"id": "52642349", "url": "https://en.wikipedia.org/wiki?curid=52642349", "title": "AIVA", "text": "AIVA\n\nAIVA (Artificial Intelligence Virtual Artist) is an electronic composer recognized by the SACEM.\n\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM).\nBy reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of detecting regularities in music and on this base composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures. Since January 2019, the company offers a commercial product, Music Engine, capable of generating short (up to 3 minutes) compositions in various styles (rock, pop, jazz, fantasy, shanty, tango, 20th century cinematic, modern cinematic, and Chinese).\n\nAIVA was presented at TED by Pierre Barreau.\n\nAIVA is a published composer; its first studio album \"Genesis\" was released in November 2016. Second album \"Among the Stars\" in 2018.\n\n- 2016 CD album « Genesis » Hv-Com – LEPM 048427. Track listing \"Genesis\":\n- 2018 CD album « Among the Stars » Hv-Com – LEPM 048708\nAvignon Symphonic Orchestra [ORAP] also performed Aiva's compositions in April 2017.\n\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\n\n", "related": "\n- Applications of artificial intelligence\n- Computer music\n- Music and artificial intelligence\n"}
{"id": "41732818", "url": "https://en.wikipedia.org/wiki?curid=41732818", "title": "Qloo", "text": "Qloo\n\nQloo (pronounced \"clue\") is a company that uses artificial intelligence (AI) to understand taste and cultural correlations. It provides companies with an application programming interface (API). It received funding from Leonardo DiCaprio, Elton John, Barry Sternlicht, Pierre Lagrange and others.\n\nQloo establishes consumer preference correlations via machine learning across data spanning cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The recommender system uses AI to predict correlations for further applications.\n\nQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Qloo was tested on a private website in April 2012. \nIn 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, and venture capital firm Kindler Capital.\nQloo had a public beta release in November 2012 after its initial funding.\n\nIn 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009.\nOn November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.\n\nIn 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and Elton John.\n\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo and BMW.\n\nIn 2019, the company announced that it had acquired cultural recommendation service TasteDive, with Alex Elias becoming chairman of TasteDive. In September 2019, Qloo was named among the Top 14 Artificial Intelligence APIs by ProgrammableWeb.\n\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including: film, music, television, dining, nightlife, fashion, books and travel. Each category contains subcategories.\n\nQloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions.\nQloo has partnerships with companies such as Expedia and iTunes.\n", "related": "NONE"}
{"id": "53279262", "url": "https://en.wikipedia.org/wiki?curid=53279262", "title": "Instance selection", "text": "Instance selection\n\nInstance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.\n\nAlgorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.\n\nThe literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select: algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite DROP3, ICF and LSBo. On the other hand, within the category of algorithms that select internal instances, it is possible to mention ENN and LSSm. In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have a negative impact on the data mining task. They can be used by other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by DROP3 as the first step, and the LSSm algorithm is used by LSBo.\n\nThere is also another group of algorithms that adopt different selection criteria. For example, the algorithms LDIS, CDIS and XLDIS select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as DROP3 and ICF.\n\nBesides that, there is a third category of algorithms that, instead of selecting actual instances of the dataset, select prototypes (that can be synthetic instances). In this category it is possible to include PSSA, PSDSP and PSSP. The three algorithms adopt the notion of spatial partition (a hyperrectangle) for identifying similar instances and extract prototypes for each set of similar instances. In general, these approaches can also be modified for selecting actual instances of the datasets. The algorithm ISDSP adopts a similar approach for selecting actual instances (instead of prototypes).\n", "related": "NONE"}
{"id": "53802271", "url": "https://en.wikipedia.org/wiki?curid=53802271", "title": "Machine learning control", "text": "Machine learning control\n\nMachine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\nwhich solves optimal control problems with methods of machine learning.\nKey applications are complex nonlinear systems\nfor which linear control theory methods are not applicable.\n\nFour types of problems are commonly encountered.\n- Control parameter identification: MLC translates to a parameter identification if the structure of the control law is given but the parameters are unknown. One example is the genetic algorithm for optimizing coefficients of a PID controller or discrete-time optimal control.\n- Control design as regression problem of the first kind: MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known full state feedback. A neural network is commonly used technique for this task.\n- Control design as regression problem of the second kind: MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, nor the control law structure, nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful regression technique for this purpose.\n- Reinforcement learning control: The control law may be continually updated over measured performance changes (rewards) using reinforcement learning.\n\nMLC comprises, for instance, neural network control, \ngenetic algorithm based control, \ngenetic programming control,\nreinforcement learning control, \nand has methodological overlaps with other data-driven control,\nlike artificial intelligence and robot control.\n\nMLC has been successfully applied\nto many nonlinear control problems,\nexploring unknown and often unexpected actuation mechanisms.\nExample applications include\n\n- Attitude control of satellites.\n- Building thermal control.\n- Feedback turbulence control.\n- Remotely operated under water vehicle.\n- Many more engineering MLC application are summarized in the review article of PJ Fleming & RC Purshouse (2002).\n\nAs for all general nonlinear methods,\nMLC comes with no guaranteed convergence, \noptimality or robustness for a range of operating conditions.\n\n- Dimitris C Dracopoulos (August 1997) \"Evolutionary Learning Algorithms for Neural Adaptive Control\", Springer. .\n- Thomas Duriez, Steven L. Brunton & Bernd R. Noack (November 2016) \"Machine Learning Control - Taming Nonlinear Dynamics and Turbulence\", Springer. .\n", "related": "NONE"}
{"id": "53970843", "url": "https://en.wikipedia.org/wiki?curid=53970843", "title": "Machine learning in bioinformatics", "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data.\n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six biological domains: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction.\n\nMachine learning methods for analysis of neuroimaging data are used to help diagnose stroke. Three-dimensional CNN and SVM methods are often used. \nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nAnother application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.\n", "related": "NONE"}
{"id": "54033657", "url": "https://en.wikipedia.org/wiki?curid=54033657", "title": "Labeled data", "text": "Labeled data\n\nLabeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.\n\nLabels can be obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., \"Does this photo contain a horse or a cow?\"), and are significantly more expensive to obtain than the raw unlabeled data.\n\nAfter obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.\n", "related": "NONE"}
{"id": "53631046", "url": "https://en.wikipedia.org/wiki?curid=53631046", "title": "Caffe (software)", "text": "Caffe (software)\n\nCAFFE (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\n\nYangqing Jia created the caffe project during his PhD at UC Berkeley. Now there are many contributors to the project, and it is hosted at GitHub.\n\nCaffe supports many different types of deep learning architectures geared towards image classification and image segmentation. It supports CNN, RCNN, LSTM and fully connected neural network designs. Caffe supports GPU- and CPU-based acceleration computational kernel libraries such as NVIDIA cuDNN and Intel MKL.\n\nCaffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. Yahoo! has also integrated caffe with Apache Spark to create CaffeOnSpark, a distributed deep learning framework.\n\nIn April 2017, Facebook announced Caffe2, which included new features such as Recurrent Neural Networks.\nAt the end of March 2018, Caffe2 was merged into PyTorch.\n\n", "related": "\n- Comparison of deep learning software\n"}
{"id": "44577560", "url": "https://en.wikipedia.org/wiki?curid=44577560", "title": "Occam learning", "text": "Occam learning\n\nIn computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n\nOccam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.\n\nOccam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, \"parsimony\" (of the output hypothesis) implies \"predictive power\".\n\nThe succinctness of a concept formula_1 in concept class formula_2 can be expressed by the length formula_3 of the shortest bit string that can represent formula_1 in formula_2. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.\n\nLet formula_2 and formula_7 be concept classes containing target concepts and hypotheses respectively. Then, for constants formula_8 and formula_9, a learning algorithm formula_10 is an formula_11-Occam algorithm for formula_2 using formula_7 iff, given a set formula_14 of formula_15 samples labeled according to a concept formula_16, formula_10 outputs a hypothesis formula_18 such that\n- formula_19 is consistent with formula_1 on formula_21 (that is, formula_22), and\n- formula_23\nwhere formula_24 is the maximum length of any sample formula_25. An Occam algorithm is called \"efficient\" if it runs in time polynomial in formula_24, formula_15, and formula_28 We say a concept class formula_2 is \"Occam learnable\" with respect to a hypothesis class formula_7 if there exists an efficient Occam algorithm for formula_2 using formula_32\n\nOccam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows:\n\nLet formula_10 be an efficient formula_11-Occam algorithm for formula_2 using formula_7. Then there exists a constant formula_37 such that for any formula_38, for any distribution formula_39, given formula_40 samples drawn from formula_39 and labelled according to a concept formula_42 of length formula_24 bits each, the algorithm formula_10 will output a hypothesis formula_45 such that formula_46 with probability at least formula_47 .Here, formula_48 is with respect to the concept formula_1 and distribution formula_39. This implies that the algorithm formula_10 is also a PAC learner for the concept class formula_2 using hypothesis class formula_7. A slightly more general formulation is as follows:\n\nLet formula_38. Let formula_10 be an algorithm such that, given formula_15 samples drawn from a fixed but unknown distribution formula_57 and labeled according to a concept formula_42 of length formula_24 bits each, outputs a hypothesis formula_60 that is consistent with the labeled samples. Then, there exists a constant formula_61 such that if formula_62, then formula_10 is guaranteed to output a hypothesis formula_60 such that formula_46 with probability at least formula_47.\n\nWhile the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about \"necessity.\" Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is \"polynomially closed under exception lists,\" PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.\n\nA concept class formula_2 is polynomially closed under exception lists if there exists a polynomial-time algorithm formula_68 such that, when given the representation of a concept formula_42 and a finite list formula_70 of \"exceptions\", outputs a representation of a concept formula_71 such that the concepts formula_1 and formula_73 agree except on the set formula_70.\n\nWe first prove the Cardinality version. Call a hypothesis formula_75 \"bad\" if formula_76, where again formula_48 is with respect to the true concept formula_1 and the underlying distribution formula_57. The probability that a set of samples formula_21 is consistent with formula_19 is at most formula_82, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in formula_83 is at most formula_84, which is less than formula_85 if formula_86. This concludes the proof of the second theorem above.\n\nUsing the second theorem, we can prove the first theorem. Since we have a formula_11-Occam algorithm, this means that any hypothesis output by formula_10 can be represented by at most formula_89 bits, and thus formula_90. This is less than formula_91 if we set formula_92 for some constant formula_37. Thus, by the Cardinality version Theorem, formula_10 will output a consistent hypothesis formula_19 with probability at least formula_96. This concludes the proof of the first theorem above.\n\nThough Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists.\n\nOccam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples.\n\n", "related": "\n- Structural Risk Minimization\n- Computational learning theory\n"}
{"id": "54625345", "url": "https://en.wikipedia.org/wiki?curid=54625345", "title": "Right to explanation", "text": "Right to explanation\n\nIn the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to \"an\" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\n\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate.\n\nCredit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),\nTitle 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):\n\nThe official interpretation of this section details what types of statements are acceptable.\n\nCredit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:\n\nThe European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: \"[the data subject should have] the right ... to obtain an explanation of the decision reached\". In full:\n\nHowever, the extent to which the regulations themselves provide a \"right to explanation\" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both \"solely\" based on automated processing, and have legal or similarly significant effects — which significantly limits the range of automated systems and decisions to which the right would apply. In particular, the right is unlikely to apply in many of the cases of algorithmic controversy that have been picked up in the media.\n\nA second potential source of such a right has been pointed to in Article 15, the \"right of access by the data subject\". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to \"meaningful information about the logic involved\" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.\n\nIn France the 2016 \"Loi pour une République numérique\" (Digital Republic Act or \"loi numérique\") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is \"a decision taken on the basis of an algorithmic treatment\", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:\n1. the degree and the mode of contribution of the algorithmic processing to the decision- making;\n2. the data processed and its source;\n3. the treatment parameters, and where appropriate, their weighting, applied to the situation of the person concerned;\n4. the operations carried out by the treatment.\nScholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions \"solely\" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a \"right to an explanation\" has been sought within, find their origins in French law in the late 1970s.\n\nSome argue that a \"right to explanation\" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.\n\nMore fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.\n\nSimilarly, human decisions often cannot be easily explained: they may be based on intuition or a \"gut feeling\" that is hard to put into words. Some would argue that machines should not be required to meet a higher standard than humans.\n\n", "related": "\n- Explainable artificial intelligence\n- Regulation of algorithms\n\n- Artificial Intelligence Owes You an Explanation, by John Frank Weaver, \"Slate\", May 8, 2017\n"}
{"id": "55075082", "url": "https://en.wikipedia.org/wiki?curid=55075082", "title": "BigDL", "text": "BigDL\n\nBigDL is a distributed deep learning framework for Apache Spark, created by Jason Dai at Intel.\n\nIt is hosted at GitHub.\n\n", "related": "\n- Comparison of deep learning software\n"}
{"id": "55375136", "url": "https://en.wikipedia.org/wiki?curid=55375136", "title": "Highway network", "text": "Highway network\n\nIn machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").\n\nHighway networks have been used as part of text sequence labeling and speech recognition tasks.\n\nThe model has two gates in addition to the H(W, x) gate: the transform gate T(W, x) and the carry gate C(W, x). Those two last gates are non-linear transfer functions (by convention Sigmoid function). The H(W, x) function can be any desired transfer function.\n\nThe carry gate is defined as C(W, x) = 1 - T(W, x). While the transform gate is just a gate with a sigmoid transfer function.\n\nThe structure of a hidden layer follows the equation:\n\nformula_1\nThe advantage of a Highway Network over the common deep neural networks is that solves or prevents partially the Vanishing gradient problem, thus leading to easier to optimize neural networks.\n", "related": "NONE"}
{"id": "54994687", "url": "https://en.wikipedia.org/wiki?curid=54994687", "title": "Documenting Hate", "text": "Documenting Hate\n\nDocumenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. , over 100 news organizations had joined the project.\n\nDocumenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the United States presidential election of 2016. The project was launched on 17 January 2017, after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.\n\nOn 18 August 2017, ProPublica and Google announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses machine learning and natural language processing techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the data visualization studio Pitch Interactive.\n\n, thousands of incidents had been reported via Documenting Hate. , over 100 news organizations had joined the project, including the \"Boston Globe\", the \"New York Times\", \"Vox\", and the Georgetown University \"Hoya\".\n\nA policy analyst for the Center for Data Innovation (an affiliate of the Information Technology and Innovation Foundation), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.\n\n", "related": "\n- Unite the Right rally\n\n- Documenting Hate on ProPublica (www.documentinghate.com redirects to this ProPublica page)\n- Documenting Hate News Index\n- Google News Lab\n- Google Cloud Natural Language API\n- Pitch Interactive\n"}
{"id": "1363880", "url": "https://en.wikipedia.org/wiki?curid=1363880", "title": "Random forest", "text": "Random forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered \"Random Forests\" as a trademark (, owned by Minitab, Inc.). The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nThe general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions. A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.\nThe early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node. Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Dietterich.\n\nThe introduction of random forests proper was first made in a paper\nby Leo Breiman. This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging. In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\n1. Using out-of-bag error as an estimate of the generalization error.\n2. Measuring variable importance through permutation.\n\nThe report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie \"et al.\", \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".\n\nIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (\"B\" times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nAfter training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on :\n\nor by taking the majority vote in the case of classification trees.\n\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on :\n\nThe number of samples/trees, , is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees can be found using cross-validation, or by observing the \"out-of-bag error\": the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\nThe above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.\n\nTypically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend (rounded down) with a minimum node size of 5 as the default.. In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters.\n\nAdding one further step of randomization yields \"extremely randomized trees\", or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally \"optimal\" cut-point for each feature under consideration (based on, e.g., information gain or the Gini impurity), a \"random\" cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are formula_3 for classification and formula_4 for regression, where formula_4 is the number of features in the model. \n\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest.\n\nThe first step in measuring the variable importance in a data set formula_6 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n\nTo measure the importance of the formula_7-th feature after training, the values of the formula_7-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the formula_7-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n\nFeatures which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu \"et al.\"\n\nThis method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased treescan be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\nA relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called \"weighted neighborhoods schemes\". These are models built from a training set formula_10 that make predictions formula_11 for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :\n\nHere, formula_13 is the non-negative weight of the 'th training point relative to the new point in the same tree. For any particular , the weights for points formula_14 must sum to one. Weight functions are given as follows:\n\n- In -NN, the weights are formula_15 if is one of the points closest to , and zero otherwise.\n- In a tree, formula_16 if is one of the points in the same leaf as , and zero otherwise.\n\nSince a forest averages the predictions of a set of trees with individual weight functions formula_17, its predictions are\n\nThis shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points formula_14 sharing the same leaf in any tree formula_7. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.\n\nIn machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level formula_21 is built, where formula_22 is a parameter of the algorithm.\n\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\nGiven a training sample formula_23 of formula_24-valued independent random variables distributed as the independent prototype pair formula_25, where formula_26. We aim at predicting the response formula_27, associated with the random variable formula_28, by estimating the regression function formula_29. A random regression forest is an ensemble of formula_30 randomized regression trees. Denote formula_31 the predicted value at point formula_32 by the formula_7-th tree, where formula_34 are independent random variables, distributed as a generic random variable formula_35, independent of the sample formula_36. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate formula_37.\nFor regression trees, we have formula_38, where formula_39 is the cell containing formula_32, designed with randomness formula_41 and dataset formula_36, and formula_43.\n\nThus random forest estimates satisfy, for all formula_44, formula_45. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\nwhich is equal to the mean of the formula_47's falling in the cells containing formula_32 in the forest. If we define the connection function of the formula_30 finite forest as formula_50, i.e. the proportion of cells shared between formula_32 and formula_52, then almost surely we have formula_53, which defines the KeRF.\n\nThe construction of Centered KeRF of level formula_21 is the same as for centered forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\nAssume that there exist sequences formula_59 such that, almost surely,\nThen almost surely,\nWhen the number of trees formula_30 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\nAssume that there exist sequences formula_63 such that, almost surely\n- formula_64\n- formula_65\n- formula_66\nThen almost surely,\nAssume that formula_68, where formula_69 is a centered Gaussian noise, independent of formula_28, with finite variance formula_71. Moreover, formula_28 is uniformly distributed on formula_73 and formula_74 is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\nProviding formula_75 and formula_76, there exists a constant formula_77 such that, for all formula_78,\nformula_79.\n\nProviding formula_75 and formula_76, there exists a constant formula_82 such that,\nformula_83.\n\nThe algorithm is often used in scientific works because of its advantages. For example, it can be used for quality assessment of Wikipedia articles.\n\n- The Original RF by Breiman and Cutler written in Fortran 77.\n- ALGLIB contains a modification of the random forest in C#, C++, Pascal, VBA.\n- party Implementation based on the conditional inference trees in R.\n- randomForest for classification and regression in R.\n- Python implementation with examples in scikit-learn.\n- Orange data mining suite includes random forest learner and can visualize the trained forest.\n- Matlab implementation.\n- SQP software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.\n- Weka RandomForest in Java library and GUI.\n- ranger A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for R.\n\n", "related": "\n- Boosting\n- Decision tree learning\n- Ensemble learning\n- Gradient boosting\n- Non-parametric statistics\n- Randomized algorithm\n\n\n- Random Forests classifier description (Leo Breiman's site)\n- Liaw, Andy & Wiener, Matthew \"Classification and Regression by randomForest\" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R)\n"}
{"id": "55817338", "url": "https://en.wikipedia.org/wiki?curid=55817338", "title": "Algorithmic bias", "text": "Algorithmic bias\n\nAlgorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation.\n\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.\n\nAlgorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias stem from the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n\nAlgorithms are difficult to define, but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output. For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more.\n\nContemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality. The term \"algorithmic bias\" describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as \"biased\". This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).\n\nBias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria. Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded. Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers. Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.\n\nBeyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores). Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as in flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. Algorithms may also display an \"uncertainty bias\", offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.\n\nThe earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book \"Computer Power and Human Reason\", Artificial Intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.\n\nWeizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law,\" that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including his or her biases and expectations. While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decisionmaking processes\" as data is being selected.\n\nFinally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results. Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.\n\nAn early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions.\n\nThough well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten. In theory, these biases may create new patterns of behavior, or \"scripts,\" in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.\n\nThe decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,\" such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity.\n\nBecause of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.\n\nConcerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,\nand Transparency in Machine Learning. Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences. In recent years, the study of the Fairness, Accountability,\nand Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAT*.\nHowever, FAT has come under serious criticism itself due to dubious funding.\n\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines. Encoding pre-existing bias into software can preserve social and institutional bias, and without correction, could be replicated in all future uses of that algorithm.\n\nAn example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.\n\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.\n\nA \"decontextualized algorithm\" uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.\n\nLastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury. Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.\n\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms. This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion. Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.\n\nIn 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.\n\nAdditional emergent biases include:\n\nUnpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data. In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.\n\nEmergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand. These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.\n\nApart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of UK immigration law.\n\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.\n\nRecommender systems such as those used to recommend online videos or news articles can create feedback loops. When users click on content that is suggested by algorithms, it influences the next set of suggestions. Over time this may lead to users entering a Filter Bubble and being unaware of important or useful content.\n\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.\n\nIn a 1998 paper describing Google, it was shown that the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\" This bias would be an \"invisible\" manipulation of the user.\n\nA series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated.\n\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew,\" but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.\n\nIn 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.\n\nWeb search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites. Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. In fact, current machine translation systems fail to reproduce the real world distribution of female workers. \n\nIn 2018, Amazon.com turned off a system it developed to screen job applications when they realized it was biased against women.\n\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making. Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data.\n\nOne example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminals's father was a consideration in those risk assessment scores. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.\n\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points. Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.\n\nBiometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.\n\nOne study that set out to examine “Risk, Race, & Recidivism: Predictive Bias and Disparate Impact” alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.\n\nIn 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.\n\nA study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on \"creditworthiness\" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.\n\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks,\" whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.\n\nAlthough algorithms are a great way to flag hate speech, there has been a racial bias noticed from the algorithms. Algorithms were found to be 1 and a half more likely to flag information if it was posted by an African American person and 2.2 times likely to flag information as hate speech if written in Ebonics. The study was completed at the University of Washington in the year of 2019. The researchers looked at flagged hate speech on Twitter. There are slurs and epithets that communities have re-appropriated, and people within that community aren't using offensively, such as \"queer\" and the \"N-word\", but algorithms don't understand the context in which these words are being used and will flag the content anyway. \n\nSurveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. A 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites. Additional studies of facial recognition software have found the opposite to be true when trained on non-criminal databases, with the software being the least accurate in identifying darker-skinned females.\n\nIn 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in \"The Atlantic\", arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its \"adult content\" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel \"Brokeback Mountain\".\n\nIn 2019, it was found that on Facebook, searches for \"photos of my female friends\" yielded suggestions such as \"in bikinis\" or \"at the beach\". In contrast, searches for \"photos of my male friends\" yielded no results.\n\nFacial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, a instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.\n\nThere has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images. The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being \"outed\" against their will.\n\nWhile users generate results that are \"completed\" automatically, Google has failed to remove sexist and racist autocompletion text. In \"Algorithms of Oppression: How Search Engines Reinforce Racism,\" Safiya Noble notes an example of the search for \"black girls,\" which was reported to result in pornographic images. Due to Google's algorithm, it is unable to erase pages unless they qualify as unlawful.\n\nSeveral problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.\n\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.\n\nSocial scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.\n\nAn example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.\n\nAdditional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms. One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.\n\nCommercial algorithms are proprietary, and may be treated as trade secrets. Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output. Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.\n\nA significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n\nSome practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation, for example building systems to infer ethnicity from names, however this can introduce other forms of bias if not undertaken with care. Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.\n\nAlgorithmic bias does not only include protected categories, but can also concerns characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult. Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.\n\nA study of 84 policy guidelines on ethical AI found that fairness and \"mitigation of unwanted bias\" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts. \n\nThere have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent field focuses on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion).\n\nCurrently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.\n\nEthics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the \"right to understanding\" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for \"Explainable AI\" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.\n\nFrom a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias. This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes. Others propose the need for clear liability insurance mechanisms.\n\nAmid concerns that the design of AI systems is primarily the domain of white, male engineers, a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems. For example, just 12% of machine learning engineers are women, with black AI leaders pointing to a \"diversity crisis\" in the field. Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms. \n\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.\n\nThe GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting that ... the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.\n\nThe United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\". Intended only as guidance, the report did not create any legal precedent.\n\nIn 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required \"the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.\" The task force is required to present findings and recommendations for further regulatory action in 2019.\n\nOn July 31, 2018, a draft of the Personal Data Bill was presented. The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for \"...harm resulting from any processing or any kind of processing undertaken by the fiduciary\". It defines \"any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal\" or \"any discriminatory treatment\" as a source of harm that could arise from improper use of data. It also makes special provisions for people of \"Intersex status”.\n\n- Fairness (machine learning)\n", "related": "NONE"}
{"id": "55843837", "url": "https://en.wikipedia.org/wiki?curid=55843837", "title": "Automated machine learning", "text": "Automated machine learning\n\nAutomated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring to become an expert in this field first. \n\nAutomating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \n\nIn a typical machine learning application, practitioners have a dataset consisting of input data points to train on. The raw data itself may not be in a form such that all algorithms may be applicable to it out of the box. An expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their machine learning model. Clearly all of those steps induce their own challenges, accumulating to a significant hurdle to get started with machine learning.\n\nA downside are the additional parameters of AutoML tools, which may need some expertise to be set themselves. Although those hyperparameters exist, AutoML simplifies the application of machine learning for non-experts dramatically.\n\nAutomated machine learning can target various stages of the machine learning process. Essentially the targets can be grouped into the fields data preparation, feature engineering, model selection, selection of evaluation metrics, and hyperparameter optimization.\n- Automated data preparation and ingestion (from raw data and miscellaneous formats)\n- Automated column type detection; e.g., boolean, discrete numerical, continuous numerical, or text\n- Automated column intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\n- Automated task detection; e.g., binary classification, regression, clustering, or ranking\n- Automated feature engineering\n- Feature selection\n- Feature extraction\n- Meta learning and transfer learning\n- Detection and handling of skewed data and/or missing values\n- Automated model selection\n- Hyperparameter optimization of the learning algorithm and featurization\n- Automated pipeline selection under time, memory, and complexity constraints\n- Automated selection of evaluation metrics / validation procedures\n- Automated problem checking\n- Leakage detection\n- Misconfiguration detection\n- Automated analysis of results obtained\n- User interfaces and visualizations for automated machine learning\n\n", "related": "\n- Neural architecture search\n- Hyperparameter optimization\n- Model selection\n- Neuroevolution\n- Self-tuning\n"}
{"id": "30992863", "url": "https://en.wikipedia.org/wiki?curid=30992863", "title": "Proaftn", "text": "Proaftn\n\nProaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.\nThe method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set.\n\nTo resolve the classification problems, Proaftn proceeds by the following stages:\n\nStage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:\n\n- Step 1. Structuring: The prototypes and their parameters (thresholds, weights, etc.) are established using the available knowledge given by the expert.\n- Step 2. Validation: We use one of the two following techniques in order to validate or adjust the parameters obtained in the first step through the assignment examples known as a training set.\nDirect technique: It consists in adjusting the parameters through the training set and with the expert intervention.\n\nIndirect technique: It consists in fitting the parameters without the expert intervention as used in machine learning approaches. \nIn multicriteria classification problem, the indirect technique is known as \"preference disaggregation analysis\". This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.\nFurthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn.\n\nStage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.\n\n- Site dedicated to the sorting problematic of MCDA\n", "related": "NONE"}
{"id": "49082762", "url": "https://en.wikipedia.org/wiki?curid=49082762", "title": "List of datasets for machine-learning research", "text": "List of datasets for machine-learning research\n\nThese datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.\n\nDatasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n\nIn computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.\nDatasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.\n\nDatasets of sounds and sound features.\n\nDatasets containing electric signal information requiring some sort of Signal processing for further analysis.\n\nDatasets from physical systems.\n\nDatasets from biological systems.\n\nThis section includes datasets that deals with structured data.\n\nDatasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n\nAs datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.\n\n- OpenML: Web platform with Python, R, Java, and other APIs for downloading hundreds of machine learning datasets, evaluating algorithms on datasets, and benchmarking algorithm performance against dozens of other algorithms.\n- PMLB: A large, curated repository of benchmark datasets for evaluating supervised machine learning algorithms. Provides classification and regression datasets in a standardized format that are accessible through a Python API.\n\n", "related": "\n- Comparison of deep learning software\n- List of manual image annotation tools\n- List of biological databases\n"}
{"id": "53587467", "url": "https://en.wikipedia.org/wiki?curid=53587467", "title": "Outline of machine learning", "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n- An academic discipline\n- A branch of science\n- An applied science\n-  A subfield of computer science\n-   A branch of artificial intelligence\n-   A subfield of soft computing\n\nSubfields of machine learning\n- Computational learning theory – studying the design and analysis of machine learning algorithms.\n- Grammar induction\n- Meta learning\n\nCross-disciplinary fields involving machine learning\n- Adversarial machine learning\n- Predictive analytics\n- Quantum machine learning\n- Robot learning\n- Developmental robotics\n\nApplications of machine learning\n- Bioinformatics\n- Biomedical informatics\n- Computer vision\n- Customer relationship management –\n- Data mining\n- Email filtering\n- Inverted pendulum – balance and equilibrium system.\n- Natural language processing (NLP)\n- Automatic summarization\n- Automatic taxonomy construction\n- Dialog system\n- Grammar checker\n- Language recognition\n-  Handwriting recognition\n-  Optical character recognition\n-  Speech recognition\n- Machine translation\n- Question answering\n- Speech synthesis\n- Text mining\n-  Term frequency–inverse document frequency (tf–idf)\n- Text simplification\n- Pattern recognition\n- Facial recognition system\n- Handwriting recognition\n- Image recognition\n- Optical character recognition\n- Speech recognition\n- Recommendation system\n- Collaborative filtering\n- Content-based filtering\n- Hybrid recommender systems (Collaborative and content-based filtering)\n- Search engine\n- Search engine optimization\n- Social Engineering\n\nMachine learning hardware\n- Graphics processing unit\n- Tensor processing unit\n- Vision processing unit\n\nMachine learning tools   (list)\n- Comparison of deep learning software\n- Comparison of deep learning software/Resources\n\nMachine learning framework\n\nProprietary machine learning frameworks\n- Amazon Machine Learning\n- Microsoft Azure Machine Learning Studio\n- DistBelief – replaced by TensorFlow\n\nOpen source machine learning frameworks\n- Apache Singa\n- Apache MXNet\n- Caffe\n- PyTorch\n- mlpack\n- TensorFlow\n- Torch\n- CNTK\n- Accord.Net\n\nMachine learning library   \n- Deeplearning4j\n- Theano\n- Scikit-learn\n- Keras\n\nMachine learning algorithm\n\n- Almeida–Pineda recurrent backpropagation\n- ALOPEX\n- Backpropagation\n- Bootstrap aggregating\n- CN2 algorithm\n- Constructing skill trees\n- Dehaene–Changeux model\n- Diffusion map\n- Dominance-based rough set approach\n- Dynamic time warping\n- Error-driven learning\n- Evolutionary multimodal optimization\n- Expectation–maximization algorithm\n- FastICA\n- Forward–backward algorithm\n- GeneRec\n- Genetic Algorithm for Rule Set Production\n- Growing self-organizing map\n- Hyper basis function network\n- IDistance\n- K-nearest neighbors algorithm\n- Kernel methods for vector output\n- Kernel principal component analysis\n- Leabra\n- Linde–Buzo–Gray algorithm\n- Local outlier factor\n- Logic learning machine\n- LogitBoost\n- Manifold alignment\n- Markov chain Monte Carlo (MCMC)\n- Minimum redundancy feature selection\n- Mixture of experts\n- Multiple kernel learning\n- Non-negative matrix factorization\n- Online machine learning\n- Out-of-bag error\n- Prefrontal cortex basal ganglia working memory\n- PVLV\n- Q-learning\n- Quadratic unconstrained binary optimization\n- Query-level feature\n- Quickprop\n- Radial basis function network\n- Randomized weighted majority algorithm\n- Reinforcement learning\n- Repeated incremental pruning to produce error reduction (RIPPER)\n- Rprop\n- Rule-based machine learning\n- Skill chaining\n- Sparse PCA\n- State–action–reward–state–action\n- Stochastic gradient descent\n- Structured kNN\n- T-distributed stochastic neighbor embedding\n- Temporal difference learning\n- Wake-sleep algorithm\n- Weighted majority algorithm (machine learning)\n\nMachine learning method   (list)\n- Instance-based algorithm\n- K-nearest neighbors algorithm (KNN)\n- Learning vector quantization (LVQ)\n- Self-organizing map (SOM)\n- Regression analysis\n- Logistic regression\n- Ordinary least squares regression (OLSR)\n- Linear regression\n- Stepwise regression\n- Multivariate adaptive regression splines (MARS)\n- Regularization algorithm\n- Ridge regression\n- Least Absolute Shrinkage and Selection Operator (LASSO)\n- Elastic net\n- Least-angle regression (LARS)\n- Classifiers\n- Probabilistic classifier\n-  Naive Bayes classifier\n- Binary classifier\n- Linear classifier\n- Hierarchical classifier\n\nDimensionality reduction\n- Canonical correlation analysis (CCA)\n- Factor analysis\n- Feature extraction\n- Feature selection\n- Independent component analysis (ICA)\n- Linear discriminant analysis (LDA)\n- Multidimensional scaling (MDS)\n- Non-negative matrix factorization (NMF)\n- Partial least squares regression (PLSR)\n- Principal component analysis (PCA)\n- Principal component regression (PCR)\n- Projection pursuit\n- Sammon mapping\n- t-distributed stochastic neighbor embedding (t-SNE)\n\nEnsemble learning\n- AdaBoost\n- Boosting\n- Bootstrap aggregating (Bagging)\n- Ensemble averaging – process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n- Gradient boosted decision tree (GBDT)\n- Gradient boosting machine (GBM)\n- Random Forest\n- Stacked Generalization (blending)\n\nMeta learning\n- Inductive bias\n- Metadata\n\nReinforcement learning\n- Q-learning\n- State–action–reward–state–action (SARSA)\n- Temporal difference learning (TD)\n- Learning Automata\n\nSupervised learning\n- AODE\n- Artificial neural network\n- Association rule learning algorithms\n- Apriori algorithm\n- Eclat algorithm\n- Case-based reasoning\n- Gaussian process regression\n- Gene expression programming\n- Group method of data handling (GMDH)\n- Inductive logic programming\n- Instance-based learning\n- Lazy learning\n- Learning Automata\n- Learning Vector Quantization\n- Logistic Model Tree\n- Minimum message length (decision trees, decision graphs, etc.)\n- Nearest Neighbor Algorithm\n- Analogical modeling\n- Probably approximately correct learning (PAC) learning\n- Ripple down rules, a knowledge acquisition methodology\n- Symbolic machine learning algorithms\n- Support vector machines\n- Random Forests\n- Ensembles of classifiers\n- Bootstrap aggregating (bagging)\n- Boosting (meta-algorithm)\n- Ordinal classification\n- Information fuzzy networks (IFN)\n- Conditional Random Field\n- ANOVA\n- Quadratic classifiers\n- k-nearest neighbor\n- Boosting\n- SPRINT\n- Bayesian networks\n- Naive Bayes\n- Hidden Markov models\n- Hierarchical hidden Markov model\n\nBayesian statistics\n- Bayesian knowledge base\n- Naive Bayes\n- Gaussian Naive Bayes\n- Multinomial Naive Bayes\n- Averaged One-Dependence Estimators (AODE)\n- Bayesian Belief Network (BBN)\n- Bayesian Network (BN)\n\nDecision tree algorithm\n- Decision tree\n- Classification and regression tree (CART)\n- Iterative Dichotomiser 3 (ID3)\n- C4.5 algorithm\n- C5.0 algorithm\n- Chi-squared Automatic Interaction Detection (CHAID)\n- Decision stump\n- Conditional decision tree\n- ID3 algorithm\n- Random forest\n- SLIQ\n\nLinear classifier\n- Fisher's linear discriminant\n- Linear regression\n- Logistic regression\n- Multinomial logistic regression\n- Naive Bayes classifier\n- Perceptron\n- Support vector machine\n\nUnsupervised learning\n- Expectation-maximization algorithm\n- Vector Quantization\n- Generative topographic map\n- Information bottleneck method\n\nArtificial neural network\n- Feedforward neural network\n- Extreme learning machine\n- Convolutional neural network\n- Recurrent neural network\n- Long short-term memory (LSTM)\n- Logic learning machine\n- Self-organizing map\n\nAssociation rule learning\n- Apriori algorithm\n- Eclat algorithm\n- FP-growth algorithm\n\nHierarchical clustering\n- Single-linkage clustering\n- Conceptual clustering\n\nCluster analysis\n- BIRCH\n- DBSCAN\n- Expectation-maximization (EM)\n- Fuzzy clustering\n- Hierarchical Clustering\n- K-means clustering\n- K-medians\n- Mean-shift\n- OPTICS algorithm\n\nAnomaly detection\n- k-nearest neighbors classification (\"k\"-NN)\n- Local outlier factor\n\nSemi-supervised learning\n- Active learning – special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.\n- Generative models\n- Low-density separation\n- Graph-based methods\n- Co-training\n- Transduction\n\nDeep learning\n- Deep belief networks\n- Deep Boltzmann machines\n- Deep Convolutional neural networks\n- Deep Recurrent neural networks\n- Hierarchical temporal memory\n- Generative Adversarial Networks\n- Deep Boltzmann Machine (DBM)\n- Stacked Auto-Encoders\n\n- Anomaly detection\n- Association rules\n- Bias-variance dilemma\n- Classification\n- Multi-label classification\n- Clustering\n- Data Pre-processing\n- Empirical risk minimization\n- Feature engineering\n- Feature learning\n- Learning to rank\n- Occam learning\n- Online machine learning\n- PAC learning\n- Regression\n- Reinforcement Learning\n- Semi-supervised learning\n- Statistical learning\n- Structured prediction\n- Graphical models\n-  Bayesian network\n-  Conditional random field (CRF)\n-  Hidden Markov model (HMM)\n- Unsupervised learning\n- VC theory\n\n- List of artificial intelligence projects\n- List of datasets for machine learning research\n\nHistory of machine learning\n- Timeline of machine learning\n\nMachine learning projects\n- DeepMind\n- Google Brain\n\nMachine learning organizations\n- Knowledge Engineering and Machine Learning Group\n\n- Artificial Intelligence and Security (AISec) (co-located workshop with CCS)\n- Conference on Neural Information Processing Systems (NIPS)\n- ECML PKDD\n- International Conference on Machine Learning (ICML)\n- ML4ALL (Machine Learning For All)\n\nBooks about machine learning\n\n- \"Machine Learning\"\n- \"Journal of Machine Learning Research\" (JMLR)\n- \"Neural Computation\"\n\n- Alberto Broggi\n- Andrei Knyazev\n- Andrew McCallum\n- Andrew Ng\n- Anuraag Jain\n- Armin B. Cremers\n- Ayanna Howard\n- Barney Pell\n- Ben Goertzel\n- Ben Taskar\n- Bernhard Schölkopf\n- Brian D. Ripley\n- Christopher G. Atkeson\n- Corinna Cortes\n- Demis Hassabis\n- Douglas Lenat\n- Eric Xing\n- Ernst Dickmanns\n- Geoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms\n- Hans-Peter Kriegel\n- Hartmut Neven\n- Heikki Mannila\n- Ian Goodfellow – Father of Generative & adversarial networks\n- Jacek M. Zurada\n- Jaime Carbonell\n- Jeremy Slovak\n- Jerome H. Friedman\n- John D. Lafferty\n- John Platt – invented SMO and Platt scaling\n- Julie Beth Lovins\n- Jürgen Schmidhuber\n- Karl Steinbuch\n- Katia Sycara\n- Leo Breiman – invented bagging and random forests\n- Lise Getoor\n- Luca Maria Gambardella\n- Léon Bottou\n- Marcus Hutter\n- Mehryar Mohri\n- Michael Collins\n- Michael I. Jordan\n- Michael L. Littman\n- Nando de Freitas\n- Ofer Dekel\n- Oren Etzioni\n- Pedro Domingos\n- Peter Flach\n- Pierre Baldi\n- Pushmeet Kohli\n- Ray Kurzweil\n- Rayid Ghani\n- Ross Quinlan\n- Salvatore J. Stolfo\n- Sebastian Thrun\n- Selmer Bringsjord\n- Sepp Hochreiter\n- Shane Legg\n- Stephen Muggleton\n- Steve Omohundro\n- Tom M. Mitchell\n- Trevor Hastie\n- Vasant Honavar\n- Vladimir Vapnik – co-inventor of the SVM and VC theory\n- Yann LeCun – invented convolutional neural networks\n- Yasuo Matsuyama\n- Yoshua Bengio\n- Zoubin Ghahramani\n\n", "related": "\n- Outline of artificial intelligence\n- Outline of computer vision\n- Outline of robotics\n\n- Accuracy paradox\n- Action model learning\n- Activation function\n- Activity recognition\n- ADALINE\n- Adaptive neuro fuzzy inference system\n- Adaptive resonance theory\n- Additive smoothing\n- Adjusted mutual information\n- AIVA\n- AIXI\n- AlchemyAPI\n- AlexNet\n- Algorithm selection\n- Algorithmic inference\n- Algorithmic learning theory\n- AlphaGo\n- AlphaGo Zero\n- Alternating decision tree\n- Apprenticeship learning\n- Causal Markov condition\n- Competitive learning\n- Concept learning\n- Decision tree learning\n- Distribution learning theory\n- Eager learning\n- End-to-end reinforcement learning\n- Error tolerance (PAC learning)\n- Explanation-based learning\n- Feature\n- GloVe\n- Hyperparameter\n- IBM Machine Learning Hub\n- Inferential theory of learning\n- Learning automata\n- Learning classifier system\n- Learning rule\n- Learning with errors\n- M-Theory (learning framework)\n- Machine learning control\n- Machine learning in bioinformatics\n- Margin\n- Markov chain geostatistics\n- Markov chain Monte Carlo (MCMC)\n- Markov information source\n- Markov logic network\n- Markov model\n- Markov random field\n- Markovian discrimination\n- Maximum-entropy Markov model\n- Multi-armed bandit\n- Multi-task learning\n- Multilinear subspace learning\n- Multimodal learning\n- Multiple instance learning\n- Multiple-instance learning\n- Never-Ending Language Learning\n- Offline learning\n- Parity learning\n- Population-based incremental learning\n- Predictive learning\n- Preference learning\n- Proactive learning\n- Proximal gradient methods for learning\n- Semantic analysis\n- Similarity learning\n- Sparse dictionary learning\n- Stability (learning theory)\n- Statistical learning theory\n- Statistical relational learning\n- Tanagra\n- Transfer learning\n- Variable-order Markov model\n- Version space learning\n- Waffles\n- Weka\n- Loss function\n- Loss functions for classification\n- Mean squared error (MSE)\n- Mean squared prediction error (MSPE)\n- Taguchi loss function\n- Low-energy adaptive clustering hierarchy\n\n- Anne O'Tate\n- Ant colony optimization algorithms\n- Anthony Levandowski\n- Anti-unification (computer science)\n- Apache Flume\n- Apache Giraph\n- Apache Mahout\n- Apache SINGA\n- Apache Spark\n- Apache SystemML\n- Aphelion (software)\n- Arabic Speech Corpus\n- Archetypal analysis\n- Arthur Zimek\n- Artificial ants\n- Artificial bee colony algorithm\n- Artificial development\n- Artificial immune system\n- Astrostatistics\n- Averaged one-dependence estimators\n- Bag-of-words model\n- Balanced clustering\n- Ball tree\n- Base rate\n- Bat algorithm\n- Baum–Welch algorithm\n- Bayesian hierarchical modeling\n- Bayesian interpretation of kernel regularization\n- Bayesian optimization\n- Bayesian structural time series\n- Bees algorithm\n- Behavioral clustering\n- Bernoulli scheme\n- Bias–variance tradeoff\n- Biclustering\n- BigML\n- Binary classification\n- Bing Predicts\n- Bio-inspired computing\n- Biogeography-based optimization\n- Biplot\n- Bondy's theorem\n- Bongard problem\n- Bradley–Terry model\n- BrownBoost\n- Brown clustering\n- Burst error\n- CBCL (MIT)\n- CIML community portal\n- CMA-ES\n- CURE data clustering algorithm\n- Cache language model\n- Calibration (statistics)\n- Canonical correspondence analysis\n- Canopy clustering algorithm\n- Cascading classifiers\n- Category utility\n- CellCognition\n- Cellular evolutionary algorithm\n- Chi-square automatic interaction detection\n- Chromosome (genetic algorithm)\n- Classifier chains\n- Cleverbot\n- Clonal selection algorithm\n- Cluster-weighted modeling\n- Clustering high-dimensional data\n- Clustering illusion\n- CoBoosting\n- Cobweb (clustering)\n- Cognitive computer\n- Cognitive robotics\n- Collostructional analysis\n- Common-method variance\n- Complete-linkage clustering\n- Computer-automated design\n- Concept class\n- Concept drift\n- Conference on Artificial General Intelligence\n- Conference on Knowledge Discovery and Data Mining\n- Confirmatory factor analysis\n- Confusion matrix\n- Congruence coefficient\n- Connect (computer system)\n- Consensus clustering\n- Constrained clustering\n- Constrained conditional model\n- Constructive cooperative coevolution\n- Correlation clustering\n- Correspondence analysis\n- Cortica\n- Coupled pattern learner\n- Cross-entropy method\n- Cross-validation (statistics)\n- Crossover (genetic algorithm)\n- Cuckoo search\n- Cultural algorithm\n- Cultural consensus theory\n- Curse of dimensionality\n- DADiSP\n- DARPA LAGR Program\n- Darkforest\n- Dartmouth workshop\n- DarwinTunes\n- Data Mining Extensions\n- Data exploration\n- Data pre-processing\n- Data stream clustering\n- Dataiku\n- Davies–Bouldin index\n- Decision boundary\n- Decision list\n- Decision tree model\n- Deductive classifier\n- DeepArt\n- DeepDream\n- Deep Web Technologies\n- Defining length\n- Dendrogram\n- Dependability state model\n- Detailed balance\n- Determining the number of clusters in a data set\n- Detrended correspondence analysis\n- Developmental robotics\n- Diffbot\n- Differential evolution\n- Discrete phase-type distribution\n- Discriminative model\n- Dissociated press\n- Distributed R\n- Dlib\n- Document classification\n- Documenting Hate\n- Domain adaptation\n- Doubly stochastic model\n- Dual-phase evolution\n- Dunn index\n- Dynamic Bayesian network\n- Dynamic Markov compression\n- Dynamic topic model\n- Dynamic unobserved effects model\n- EDLUT\n- ELKI\n- Edge recombination operator\n- Effective fitness\n- Elastic map\n- Elastic matching\n- Elbow method (clustering)\n- Emergent (software)\n- Encog\n- Entropy rate\n- Erkki Oja\n- Eurisko\n- European Conference on Artificial Intelligence\n- Evaluation of binary classifiers\n- Evolution strategy\n- Evolution window\n- Evolutionary Algorithm for Landmark Detection\n- Evolutionary algorithm\n- Evolutionary art\n- Evolutionary music\n- Evolutionary programming\n- Evolvability (computer science)\n- Evolved antenna\n- Evolver (software)\n- Evolving classification function\n- Expectation propagation\n- Exploratory factor analysis\n- F1 score\n- FLAME clustering\n- Factor analysis of mixed data\n- Factor graph\n- Factor regression model\n- Factored language model\n- Farthest-first traversal\n- Fast-and-frugal trees\n- Feature Selection Toolbox\n- Feature hashing\n- Feature scaling\n- Feature vector\n- Firefly algorithm\n- First-difference estimator\n- First-order inductive learner\n- Fish School Search\n- Fisher kernel\n- Fitness approximation\n- Fitness function\n- Fitness proportionate selection\n- Fluentd\n- Folding@home\n- Formal concept analysis\n- Forward algorithm\n- Fowlkes–Mallows index\n- Frederick Jelinek\n- Frrole\n- Functional principal component analysis\n- GATTO\n- GLIMMER\n- Gary Bryce Fogel\n- Gaussian adaptation\n- Gaussian process\n- Gaussian process emulator\n- Gene prediction\n- General Architecture for Text Engineering\n- Generalization error\n- Generalized canonical correlation\n- Generalized filtering\n- Generalized iterative scaling\n- Generalized multidimensional scaling\n- Generative adversarial network\n- Generative model\n- Genetic algorithm\n- Genetic algorithm scheduling\n- Genetic algorithms in economics\n- Genetic fuzzy systems\n- Genetic memory (computer science)\n- Genetic operator\n- Genetic programming\n- Genetic representation\n- Geographical cluster\n- Gesture Description Language\n- Geworkbench\n- Glossary of artificial intelligence\n- Glottochronology\n- Golem (ILP)\n- Google matrix\n- Grafting (decision trees)\n- Gramian matrix\n- Grammatical evolution\n- Granular computing\n- GraphLab\n- Graph kernel\n- Gremlin (programming language)\n- Growth function\n- HUMANT (HUManoid ANT) algorithm\n- Hammersley–Clifford theorem\n- Harmony search\n- Hebbian theory\n- Hidden Markov random field\n- Hidden semi-Markov model\n- Hierarchical hidden Markov model\n- Higher-order factor analysis\n- Highway network\n- Hinge loss\n- Holland's schema theorem\n- Hopkins statistic\n- Hoshen–Kopelman algorithm\n- Huber loss\n- IRCF360\n- Ian Goodfellow\n- Ilastik\n- Ilya Sutskever\n- Immunocomputing\n- Imperialist competitive algorithm\n- Inauthentic text\n- Incremental decision tree\n- Induction of regular languages\n- Inductive bias\n- Inductive probability\n- Inductive programming\n- Influence diagram\n- Information Harvesting\n- Information fuzzy networks\n- Information gain in decision trees\n- Information gain ratio\n- Inheritance (genetic algorithm)\n- Instance selection\n- Intel RealSense\n- Interacting particle system\n- Interactive machine translation\n- International Joint Conference on Artificial Intelligence\n- International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics\n- International Semantic Web Conference\n- Iris flower data set\n- Island algorithm\n- Isotropic position\n- Item response theory\n- Iterative Viterbi decoding\n- JOONE\n- Jabberwacky\n- Jaccard index\n- Jackknife variance estimates for random forest\n- Java Grammatical Evolution\n- Joseph Nechvatal\n- Jubatus\n- Julia (programming language)\n- Junction tree algorithm\n- K-SVD\n- K-means++\n- K-medians clustering\n- K-medoids\n- KNIME\n- KXEN Inc.\n- K q-flats\n- Kaggle\n- Kalman filter\n- Katz's back-off model\n- Kernel adaptive filter\n- Kernel density estimation\n- Kernel eigenvoice\n- Kernel embedding of distributions\n- Kernel method\n- Kernel perceptron\n- Kernel random forest\n- Kinect\n- Klaus-Robert Müller\n- Kneser–Ney smoothing\n- Knowledge Vault\n- Knowledge integration\n- LIBSVM\n- LPBoost\n- Labeled data\n- LanguageWare\n- Language Acquisition Device (computer)\n- Language identification in the limit\n- Language model\n- Large margin nearest neighbor\n- Latent Dirichlet allocation\n- Latent class model\n- Latent semantic analysis\n- Latent variable\n- Latent variable model\n- Lattice Miner\n- Layered hidden Markov model\n- Learnable function class\n- Least squares support vector machine\n- Leave-one-out error\n- Leslie P. Kaelbling\n- Linear genetic programming\n- Linear predictor function\n- Linear separability\n- Lingyun Gu\n- Linkurious\n- Lior Ron (business executive)\n- List of genetic algorithm applications\n- List of metaphor-based metaheuristics\n- List of text mining software\n- Local case-control sampling\n- Local independence\n- Local tangent space alignment\n- Locality-sensitive hashing\n- Log-linear model\n- Logistic model tree\n- Low-rank approximation\n- Low-rank matrix approximations\n- MATLAB\n- MIMIC (immunology)\n- MXNet\n- Mallet (software project)\n- Manifold regularization\n- Margin-infused relaxed algorithm\n- Margin classifier\n- Mark V. Shaney\n- Massive Online Analysis\n- Matrix regularization\n- Matthews correlation coefficient\n- Mean shift\n- Mean squared error\n- Mean squared prediction error\n- Measurement invariance\n- Medoid\n- MeeMix\n- Melomics\n- Memetic algorithm\n- Meta-optimization\n- Mexican International Conference on Artificial Intelligence\n- Michael Kearns (computer scientist)\n- MinHash\n- Mixture model\n- Mlpy\n- Models of DNA evolution\n- Moral graph\n- Mountain car problem\n- Movidius\n- Multi-armed bandit\n- Multi-label classification\n- Multi expression programming\n- Multiclass classification\n- Multidimensional analysis\n- Multifactor dimensionality reduction\n- Multilinear principal component analysis\n- Multiple correspondence analysis\n- Multiple discriminant analysis\n- Multiple factor analysis\n- Multiple sequence alignment\n- Multiplicative weight update method\n- Multispectral pattern recognition\n- Mutation (genetic algorithm)\n- MysteryVibe\n- N-gram\n- NOMINATE (scaling method)\n- Native-language identification\n- Natural Language Toolkit\n- Natural evolution strategy\n- Nearest-neighbor chain algorithm\n- Nearest centroid classifier\n- Nearest neighbor search\n- Neighbor joining\n- Nest Labs\n- NetMiner\n- NetOwl\n- Neural Designer\n- Neural Engineering Object\n- Neural Lab\n- Neural modeling fields\n- Neural network software\n- NeuroSolutions\n- Neuro Laboratory\n- Neuroevolution\n- Neuroph\n- Niki.ai\n- Noisy channel model\n- Noisy text analytics\n- Nonlinear dimensionality reduction\n- Novelty detection\n- Nuisance variable\n- Numenta\n- One-class classification\n- Onnx\n- OpenNLP\n- Optimal discriminant analysis\n- Oracle Data Mining\n- Orange (software)\n- Ordination (statistics)\n- Overfitting\n- PROGOL\n- PSIPRED\n- Pachinko allocation\n- PageRank\n- Parallel metaheuristic\n- Parity benchmark\n- Part-of-speech tagging\n- Particle swarm optimization\n- Path dependence\n- Pattern language (formal languages)\n- Peltarion Synapse\n- Perplexity\n- Persian Speech Corpus\n- Picas (app)\n- Pietro Perona\n- Pipeline Pilot\n- Piranha (software)\n- Pitman–Yor process\n- Plate notation\n- Polynomial kernel\n- Pop music automation\n- Population process\n- Portable Format for Analytics\n- Predictive Model Markup Language\n- Predictive state representation\n- Preference regression\n- Premature convergence\n- Principal geodesic analysis\n- Prior knowledge for pattern recognition\n- Prisma (app)\n- Probabilistic Action Cores\n- Probabilistic context-free grammar\n- Probabilistic latent semantic analysis\n- Probabilistic soft logic\n- Probability matching\n- Probit model\n- Product of experts\n- Programming with Big Data in R\n- Proper generalized decomposition\n- Pruning (decision trees)\n- Pushpak Bhattacharyya\n- Q methodology\n- Qloo\n- Quality control and genetic algorithms\n- Quantum Artificial Intelligence Lab\n- Queueing theory\n- Quick, Draw!\n- R (programming language)\n- Rada Mihalcea\n- Rademacher complexity\n- Radial basis function kernel\n- Rand index\n- Random indexing\n- Random projection\n- Random subspace method\n- Ranking SVM\n- RapidMiner\n- Rattle GUI\n- Raymond Cattell\n- Reasoning system\n- Regularization perspectives on support vector machines\n- Relational data mining\n- Relationship square\n- Relevance vector machine\n- Relief (feature selection)\n- Renjin\n- Repertory grid\n- Representer theorem\n- Reward-based selection\n- Richard Zemel\n- Right to explanation\n- RoboEarth\n- Robust principal component analysis\n- RuleML Symposium\n- Rule induction\n- Rules extraction system family\n- SAS (software)\n- SNNS\n- SPSS Modeler\n- SUBCLU\n- Sample complexity\n- Sample exclusion dimension\n- Santa Fe Trail problem\n- Savi Technology\n- Schema (genetic algorithms)\n- Search-based software engineering\n- Selection (genetic algorithm)\n- Self-Service Semantic Suite\n- Semantic folding\n- Semantic mapping (statistics)\n- Semidefinite embedding\n- Sense Networks\n- Sensorium Project\n- Sequence labeling\n- Sequential minimal optimization\n- Shattered set\n- Shogun (toolbox)\n- Silhouette (clustering)\n- SimHash\n- SimRank\n- Similarity measure\n- Simple matching coefficient\n- Simultaneous localization and mapping\n- Sinkov statistic\n- Sliced inverse regression\n- Snakes and Ladders\n- Soft independent modelling of class analogies\n- Soft output Viterbi algorithm\n- Solomonoff's theory of inductive inference\n- SolveIT Software\n- Spectral clustering\n- Spike-and-slab variable selection\n- Statistical machine translation\n- Statistical parsing\n- Statistical semantics\n- Stefano Soatto\n- Stephen Wolfram\n- Stochastic block model\n- Stochastic cellular automaton\n- Stochastic diffusion search\n- Stochastic grammar\n- Stochastic matrix\n- Stochastic universal sampling\n- Stress majorization\n- String kernel\n- Structural equation modeling\n- Structural risk minimization\n- Structured sparsity regularization\n- Structured support vector machine\n- Subclass reachability\n- Sufficient dimension reduction\n- Sukhotin's algorithm\n- Sum of absolute differences\n- Sum of absolute transformed differences\n- Swarm intelligence\n- Switching Kalman filter\n- Symbolic regression\n- Synchronous context-free grammar\n- Syntactic pattern recognition\n- TD-Gammon\n- TIMIT\n- Teaching dimension\n- Teuvo Kohonen\n- Textual case-based reasoning\n- Theory of conjoint measurement\n- Thomas G. Dietterich\n- Thurstonian model\n- Topic model\n- Tournament selection\n- Training, test, and validation sets\n- Transiogram\n- Trax Image Recognition\n- Trigram tagger\n- Truncation selection\n- Tucker decomposition\n- UIMA\n- UPGMA\n- Ugly duckling theorem\n- Uncertain data\n- Uniform convergence in probability\n- Unique negative dimension\n- Universal portfolio algorithm\n- User behavior analytics\n- VC dimension\n- VIGRA\n- Validation set\n- Vapnik–Chervonenkis theory\n- Variable-order Bayesian network\n- Variable kernel density estimation\n- Variable rules analysis\n- Variational message passing\n- Varimax rotation\n- Vector quantization\n- Vicarious (company)\n- Viterbi algorithm\n- Vowpal Wabbit\n- WACA clustering algorithm\n- WPGMA\n- Ward's method\n- Weasel program\n- Whitening transformation\n- Winnow (algorithm)\n- Win–stay, lose–switch\n- Witness set\n- Wolfram Language\n- Wolfram Mathematica\n- Writer invariant\n- Xgboost\n- Yooreeka\n- Zeroth (software)\n\n- Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). \"The Elements of Statistical Learning\", Springer. .\n- Pedro Domingos (September 2015), The Master Algorithm, Basic Books,\n- Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). \"Foundations of Machine Learning\", The MIT Press. .\n- Ian H. Witten and Eibe Frank (2011). \"Data Mining: Practical machine learning tools and techniques\" Morgan Kaufmann, 664pp., .\n- David J. C. MacKay. \"Information Theory, Inference, and Learning Algorithms\" Cambridge: Cambridge University Press, 2003.\n- Richard O. Duda, Peter E. Hart, David G. Stork (2001) \"Pattern classification\" (2nd edition), Wiley, New York, .\n- Christopher Bishop (1995). \"Neural Networks for Pattern Recognition\", Oxford University Press. .\n- Vladimir Vapnik (1998). \"Statistical Learning Theory\". Wiley-Interscience, .\n- Ray Solomonoff, \"An Inductive Inference Machine\", IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.\n- Ray Solomonoff, \"An Inductive Inference Machine\" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.\n\n- Data Science: Data to Insights from MIT (machine learning)\n- Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University's actual course taught by Ng, see.stanford.edu/Course/CS229 available for free].\n- mloss is an academic database of open-source machine learning software.\n"}
{"id": "54361643", "url": "https://en.wikipedia.org/wiki?curid=54361643", "title": "Hyperparameter optimization", "text": "Hyperparameter optimization\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.\n\nThe traditional way of performing hyperparameter optimization has been \"grid search\", or a \"parameter sweep\", which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set\nor evaluation on a held-out validation set.\n\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.\n\nFor example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant \"C\" and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of \"reasonable\" values for each, say\n\nGrid search then trains an SVM with each pair (\"C\", γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\n\nGrid search suffers from the curse of dimensionality, but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other.\n\nRandom Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.\n\nBayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.\n\nFor specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.\n\nA different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation. \n\nEvolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:\n\n1. Create an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100+)\n2. Evaluate the hyperparameters tuples and acquire their fitness function (e.g., 10-fold cross-validation accuracy of the machine learning algorithm with those hyperparameters)\n3. Rank the hyperparameter tuples by their relative fitness\n4. Replace the worst-performing hyperparameter tuples with new hyperparameter tuples generated through crossover and mutation\n5. Repeat steps 2-4 until satisfactory algorithm performance is reached or algorithm performance is no longer improving\n\nEvolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, deep neural network architecture search, as well as training of the weights in deep neural networks.\n\nPopulation Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. Poorly performing models are iteratively replaced with models that adopt modified hyperparameter values from a better performer. The modification allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures.\n\nRBF and spectral approaches have also been developed.\n\n- Katib is a Kubernetes-native system which includes grid search.\n- scikit-learn is a Python package which includes grid search.\n- Tune is a Python library for distributed hyperparameter tuning and supports grid search.\n- Talos includes grid search for Keras.\n- H2O AutoML provides grid search over algorithms in the H2O open source machine learning library.\n\n- hyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include random search.\n- Katib is a Kubernetes-native system which includes random search.\n- scikit-learn is a Python package which includes random search.\n- Tune is a Python library for distributed hyperparameter tuning and supports random search over arbitrary parameter distributions.\n- Talos includes a customizable random search for Keras.\n\n- Auto-sklearn is a Bayesian hyperparameter optimization layer on top of scikit-learn.\n- Ax is a Python-based experimentation platform that supports Bayesian optimization and bandit optimization as exploration strategies.\n- BOCS is a Matlab package which uses semidefinite programming for minimizing a black-box function over discrete inputs. A Python 3 implementation is also included.\n- HpBandSter is a Python package which combines Bayesian optimization with bandit-based methods.\n- Katib is a Kubernetes-native system which includes bayesian optimization.\n- mlrMBO, also with mlr, is an R package for model-based/Bayesian optimization of black-box functions.\n- scikit-optimize is a Python package or sequential model-based optimization with a scipy.optimize interface.\n- SMAC SMAC is a Python/Java library implementing Bayesian optimization.\n- tuneRanger is an R package for tuning random forests using model-based optimization.\n- optuna is a Python package for black box optimization, compatible with arbitrary functions that need to be optimized.\n\n- FAR-HO is a Python package containing Tensorflow implementations and wrappers for gradient-based hyperparamteter optimization with forward and reverse mode algorithmic differentiation.\n- XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, and Julia.\n\n- deap is a Python framework for general evolutionary computation which is flexible and integrates with parallelization packages like scoop and pyspark, and other Python frameworks like sklearn via sklearn-deap.\n- devol is a Python package that performs Deep Neural Network architecture search using genetic programming.\n- nevergrad is a Python package which includes population control methods and particle swarm optimization.\n- Tune is a Python library for distributed hyperparameter tuning and leverages nevergrad for evolutionary algorithm support.\n\n- dlib is a C++ package with a Python API which has a parameter-free optimizer based on LIPO and trust region optimizers working in tandem.\n- Tune is a Python library for hyperparameter tuning execution and integrates with/scales many existing hyperparameter optimization libraries such as hyperopt, nevergrad, and scikit-optimize.\n- Harmonica is a Python package for spectral hyperparameter optimization.\n- hyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include Tree of Parzen Estimators based distributed hyperparameter optimization.\n- Katib is a Kubernetes-native system which includes grid, random search, bayesian optimization, hyperband, and NAS based on reinforcement learning.\n- nevergrad is a Python package for gradient-free optimization using techniques such as differential evolution, sequential quadratic programming, fastGA, covariance matrix adaptation, population control methods, and particle swarm optimization.\n- nni is a Python package which includes hyperparameter tuning for neural networks in local and distributed environments. Its techniques include TPE, random, anneal, evolution, SMAC, batch, grid, and hyperband.\n- parameter-sherpa is a similar Python package which includes several techniques grid search, Bayesian and genetic Optimization\n- pycma is a Python implementation of Covariance Matrix Adaptation Evolution Strategy.\n- rbfopt is a Python package that uses a radial basis function model\n\n- Amazon Sagemaker uses Gaussian processes to tune hyperparameters.\n- BigML OptiML supports mixed search domains\n- Google HyperTune supports mixed search domains\n- Indie Solver supports multiobjective, multifidelity and constraint optimization\n- Mind Foundry OPTaaS supports mixed search domains, multiobjective, constraints, parallel optimization and surrogate models.\n- SigOpt supports mixed search domains, multiobjective, multisolution, multifidelity, constraint (linear and black-box), and parallel optimization.\n\n", "related": "\n- Automated machine learning\n- Neural architecture search\n- Meta-optimization\n- Model selection\n- Self-tuning\n- XGBoost\n"}
{"id": "56142183", "url": "https://en.wikipedia.org/wiki?curid=56142183", "title": "Paraphrasing (computational linguistics)", "text": "Paraphrasing (computational linguistics)\n\nParaphrase or Paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection. Paraphrasing is also useful in the evaluation of machine translation, as well as semantic parsing and generation of new samples to expand existing corpora.\n\nBarzilay and Lee proposed a method to generate paraphrases through the usage of monolingual parallel corpora, namely news articles covering the same event on the same day. Training consists of using multi-sequence alignment to generate sentence-level paraphrases from an unannotated corpus. This is done by\n\n- finding recurring patterns in each individual corpus, i.e. \" (injured/wounded) people, seriously\" where are variables\n- finding pairings between such patterns the represent paraphrases, i.e. \" (injured/wounded) people, seriously\" and \" were (wounded/hurt) by , among them were in serious condition\"\n\nThis is achieved by first clustering similar sentences together using n-gram overlap. Recurring patterns are found within clusters by using multi-sequence alignment. Then the position of argument words are determined by finding areas of high variability within each clusters, aka between words shared by more than 50% of a cluster's sentences. Pairings between patterns are then found by comparing similar variable words between different corpora. Finally new paraphrases can be generated by choosing a matching cluster for a source sentence, then substituting the source sentence's argument into any number of patterns in the cluster.\n\nParaphrase can also be generated through the use of phrase-based translation as proposed by Bannard and Callison-Burch. The chief concept consists of aligning phrases in a pivot language to produce potential paraphrases in the original language. For example, the phrase \"under control\" in an English sentence is aligned with the phrase \"unter kontrolle\" in its German counterpart. The phrase \"unter kontrolle\" is then found in another German sentence with the aligned English phrase being \"in check\", a paraphrase of \"under control\".\n\nThe probability distribution can be modeled as formula_1, the probability phrase formula_2 is a paraphrase of formula_3, which is equivalent to formula_4 summed over all formula_5, a potential phrase translation in the pivot language. Additionally, the sentence formula_3 is added as a prior to add context to the paraphrase. Thus the optimal paraphrase, formula_7 can be modeled as:\n\nformula_9 and formula_10 can be approximated by simply taking their frequencies. Adding formula_11 as a prior is modeled by calculating the probability of forming the formula_11 when formula_3 is substituted with \n\nThere has been success in using long short-term memory (LSTM) models to generate paraphrases. In short, the model consists of an encoder and decoder component, both implemented using variations of a stacked residual LSTM. First, the encoding LSTM takes a one-hot encoding of all the words in a sentence as input and produces a final hidden vector, which can be viewed as a representation of the input sentence. The decoding LSTM then takes the hidden vector as input and generates new sentence, terminating in an end-of-sentence token. The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent. New paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder.\n\nParaphrase recognition has been attempted by Socher et al through the use of recursive autoencoders. The main concept is to produce a vector representation of a sentence along with its components through recursively using an autoencoder. The vector representations of paraphrases should have similar vector representations; they are processed, then fed as input into a neural network for classification.\n\nGiven a sentence formula_14 with formula_15 words, the autoencoder is designed to take 2 formula_16-dimensional word embeddings as input and produce an formula_16-dimensional vector as output. The same autoencoder is applied to every pair of words in formula_11 to produce formula_19 vectors. The autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced. Given an odd number of inputs, the first vector is forwarded as is to the next level of recursion. The autoencoder is then trained to reproduce every vector in the full recursion tree including the initial word embeddings.\n\nGiven two sentences formula_20 and formula_21 of length 4 and 3 respectively, the autoencoders would produce 7 and 5 vector representations including the initial word embeddings. The euclidean distance is then taken between every combination of vectors in formula_20 and formula_21 to produce a similarity matrix formula_24. formula_11 is then subject to a dynamic min-pooling layer to produce a fixed size formula_26 matrix. Since formula_11 are not uniform in size among all potential sentences, formula_11 is split into formula_29 roughly even sections. The output is then normalized to have mean 0 and standard deviation 1 and is fed into a fully connected layer with a softmax output. The dynamic pooling to softmax model is trained using pairs of known paraphrases.\n\nSkip-thought vectors are an attempt to create a vector representation of the semantic meaning of a sentence in a similar fashion as the skip gram model. Skip-thought vectors are produced through the use of a skip-thought model which consists of three key components, an encoder and two decoders. Given a corpus of documents, the skip-thought model is trained to take a sentence as input and encode it into a skip-thought vector. The skip-thought vector is used as input for both decoders, one of which attempts to reproduce the previous sentence and the other the following sentence in its entirety. The encoder and decoder can be implemented through the use of a recursive neural network (RNN) or an LSTM.\n\nSince paraphrases carry the same semantic meaning between one another, they should have similar skip-thought vectors. Thus a simple logistic regression can be trained to a good performance with the absolute difference and component-wise product of two skip-thought vectors as input.\n\nThere are multiple methods that can be used to evaluate paraphrases. Since paraphrase recognition can be posed as a classification problem, most standard evaluations metrics such as accuracy, f1 score, or an ROC curve do relatively well. However, there is difficulty calculating f1-scores due to trouble produce a complete list of paraphrases for a given phrase along with the fact that good paraphrases are dependent upon context. A metric designed to counter these problems is ParaMetric. ParaMetric aims to calculate the precision and recall of an automatic paraphrase system by comparing the automatic alignment of paraphrases to a manual alignment of similar phrases. Since ParaMetric is simply rating the quality of phrase alignment, it can be used to rate paraphrase generation systems as well assuming it uses phrase alignment as part of its generation process. A noted drawback to ParaMetric is the large and exhaustive set of manual alignments that must be initially created before a rating can be produced.\n\nThe evaluation of paraphrase generation has similar difficulties as the evaluation of machine translation. Often the quality of a paraphrase is dependent upon its context, whether it is being used as a summary, and how it is generated among other factors. Additionally, a good paraphrase usually is lexically dissimilar from its source phrase. The simplest method used to evaluate paraphrase generation would be through the use of human judges. Unfortunately, evaluation through human judges tends to be time consuming. Automated approaches to evaluation prove to be challenging as it is essentially a problem as difficult as paraphrase recognition. While originally used to evaluate machine translations, bilingual evaluation understudy (BLEU) has been used successfully to evaluate paraphrase generation models as well. However, paraphrases often have several lexically different but equally valid solutions which hurts BLEU and other similar evaluation metrics.\n\nMetrics specifically designed to evaluate paraphrase generation include paraphrase in n-gram change (PINC) and paraphrase evaluation metric (PEM) along with the aforementioned ParaMetric. PINC is designed to be used in conjunction with BLEU and help cover its inadequacies. Since BLEU has difficulty measuring lexical dissimilarity, PINC is a measurement of the lack of n-gram overlap between a source sentence and a candidate paraphrase. It is essentially the Jaccard distance between the sentence excluding n-grams that appear in the source sentence to maintain some semantic equivalence. PEM, on the other hand, attempts to evaluate the \"adequacy, fluency, and lexical dissimilarity\" of paraphrases by returning a single value heuristic calculated using N-grams overlap in a pivot language. However, a large drawback to PEM is that must be trained using a large, in-domain parallel corpora as well as human judges. In other words, it is tantamount to training a paraphrase recognition system in order to evaluate a paraphrase generation system.\n\n", "related": "\n- Round-trip translation\n- Text simplification\n- Text normalization\n\n11. Online Paraphrasing tool for rewording articles. Paraphrasing tool\n\n- Microsoft Research Paraphrase Corpus - a dataset consisting of 5800 pairs of sentences extracted from news articles annotated to note whether a pair captures semantic equivalence\n- Paraphrase Database (PPDB) - A searchable database containing millions of paraphrases in 16 different languages\n"}
{"id": "31978226", "url": "https://en.wikipedia.org/wiki?curid=31978226", "title": "Life-time of correlation", "text": "Life-time of correlation\n\nThe life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross correlation in stochastic processes.\n\nThe correlation coefficient \"ρ\", expressed as an autocorrelation function or cross-correlation function, depends on the lag-time between the times being considered. Typically such functions, \"ρ\"(\"t\"), decay to zero with increasing lag-time, but they can assume values across all levels of correlations: strong and weak, and positive and negative as in the table.\n\nThe life-time of a correlation is defined as the length of time when the correlation coefficient is at the strong level. The durability of correlation is determined by signal (the strong level of correlation is separated from weak and negative levels). The mean life-time of correlation could measure how the durability of correlation depends on the window width size (the window is the length of time series used to calculate correlation).\n", "related": "NONE"}
{"id": "57261507", "url": "https://en.wikipedia.org/wiki?curid=57261507", "title": "RAMnets", "text": "RAMnets\n\nRAMnets is one of the oldest practical neurally inspired classification algorithm is still one of the best. The RAMnets is also known as a type of \"\"n\"-tuple recognition method\" or \"weightless neural network\".\n\nConsider (let us say \"N\") sets of n distinct bit locations are selected randomly. These are the \"n\"-tuples. The restriction of a pattern to an n-tuple can be regarded as an \"n\"-bit number which, together with the identity of the \"n\"-tuple, constitutes a `feature' of the pattern. The standard \"n\"-tuple recognizer operates simply as follows:\n\n\"A pattern is classified as belonging to the class for which it has the most features in common with at least one training pattern of that class.\nThis is the formula_1= 0 case of a more general rule whereby the class assigned to unclassified pattern u is\n\nwhere D is the set of training patterns in class c, formula_3= x for formula_4 ,formula_5 for formula_6,formula_7 is the Kronecker delta(formula_7=1 if i=j and 0 otherwise.)and formula_9is the i feature of the pattern u:\n\nHere u is the k bit of u and formula_11is the j bit location of the i n-tuple.\n\nWith C classes to distinguish, the system can be implemented as a network of NC nodes, each of which is a random access memory (RAM); hence the term \"RAMnet.\" The memory content formula_12 at address formula_13 of the i node allocated to class c is set to\n\nIn the usual formula_16 = 1 case, the 1-bit content of formula_12 is set if any pattern of D has feature formula_18 and unset otherwise. Recognition is accomplished by summing the contents of the nodes of each class at the addresses given by the features of the unclassified pattern. That is, pattern u is assigned to class\n\nThe RAMnets formed the basis of a commercial product known as WiSARD (Wilkie, Stonham and Aleksander Recognition Device) was the first artificial neural network machine to be patented.\n\nA RAM-discriminator consists of a set of one-bit word RAMs with inputs and a summing device (Σ). Any such RAM-discriminator can receive a binary pattern of X⋅n bits as input. The RAM input lines are connected to the input pattern by means of a biunivocal pseudo-random mapping. The summing device enables this network of RAMs to exhibit – just like other ANN models based on synaptic weights – generalization and noise tolerance.\n\nIn order to train the discriminator one has to set all RAM memory locations to 0 and choose a training set formed by binary patterns of X⋅n bits. For each training pattern, a 1 is stored in the memory location of each RAM addressed by this input pattern. Once the training of patterns is completed, RAM memory contents will be set to a certain number of 0’s and 1’s.\n\nThe information stored by the RAM during the training phase is used to deal with previous unseen patterns. When one of these is given as input, the RAM memory contents addressed by the input pattern are read and summed by Σ. The number thus obtained, which is called the discriminator response, is equal to the number of RAMs that output 1. r reaches the maximum if the input belongs to the training set. is equal to 0 if no \"n\"-bit component of the input pattern appears in the training set (not a single RAM outputs 1). Intermediate values of r express a kind of “similarity measure” of the input pattern with respect to the patterns in the training set.\n\nA system formed by various RAM-discriminators is called WiSARD. Each RAM-discriminator is trained on a particular class of patterns, and classification by the multi-discriminator system is performed in the following way. When a pattern is given as input, each RAM-discriminator gives a response to that input. The various responses are evaluated by an algorithm which compares them and computes the relative confidence of the highest response (e.g., the difference d between the highest response and the second highest response, divided by the highest response). A schematic representation of a RAM-discriminator and a 10 RAM-discriminator WiSARD is shown in Figure 1.\n\n", "related": "\n- Artificial Neural Network\n- Kronecker delta\n- Pattern Recognition\n- Unsupervised learning\n- Erlang distribution\n- Machine learning\n- Erlang (unit)\n\n- Michal Morciniec and Richard Rohwer(1995) \"The n-tuple Classifier: Too Good to Ignore\"\n- (This book focuses on unsupervised learning in neural networks)\n- A brief introduction to Weightless NeuralSystems (2009)\n\n1. An introductory tutorial to classifiers (introducing the basic terms, with numeric example)\n"}
{"id": "57687371", "url": "https://en.wikipedia.org/wiki?curid=57687371", "title": "Multimodal sentiment analysis", "text": "Multimodal sentiment analysis\n\nMultimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others.\n\nSimilar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis.\n\nFeature engineering, which involves the selection of features that are fed into machine learning algorithms, plays a key role in the sentiment classification performance. In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed.\n\nSimilar to the conventional text-based sentiment analysis, some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n-grams, which are basically a sequence of words in a given textual document. These features are applied using bag-of-words or bag-of-concepts feature representations, in which words or concepts are represented as vectors in a suitable space.\n\nSentiment and emotion characteristics are prominent in different phonetic and prosodic properties contained in audio features. Some of the most important audio features employed in multimodal sentiment analysis are mel-frequency cepstrum (MFCC), spectral centroid, spectral flux, beat histogram, beat sum, strongest beat, pause duration, and pitch. OpenSMILE and Praat are popular open-source toolkits for extracting such audio features.\n\nOne of the main advantages of analyzing videos with respect to texts alone, is the presence of rich sentiment cues in visual data. Visual features include facial expressions, which are of paramount importance in capturing sentiments and emotions, as they are a main channel of forming a person's present state of mind. Specifically, smile, is considered to be one of the most predictive visual cues in multimodal sentiment analysis. OpenFace is an open-source facial analysis toolkit available for extracting and understanding such visual features.\n\nUnlike the traditional text-based sentiment analysis, multimodal sentiment analysis undergo a fusion process in which data from different modalities (text, audio, or visual) are fused and analyzed together. The existing approaches in multimodal sentiment analysis data fusion can be grouped into three main categories: feature-level, decision-level, and hybrid fusion, and the performance of the sentiment classification depends on which type of fusion technique is employed.\n\nFeature-level fusion (sometimes known as early fusion) gathers all the features from each modality (text, audio, or visual) and joins them together into a single feature vector, which is eventually fed into a classification algorithm. One of the difficulties in implementing this technique is the integration of the heterogeneous features.\n\nDecision-level fusion (sometimes known as late fusion), feeds data from each modality (text, audio, or visual) independently into its own classification algorithm, and obtains the final sentiment classification results by fusing each result into a single decision vector. One of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data, and each modality can utilize its most appropriate classification algorithm.\n\nHybrid fusion is a combination of feature-level and decision-level fusion techniques, which exploits complementary information from both methods during the classification process. It usually involves a two-step procedure wherein feature-level fusion is initially performed between two modalities, and decision-level fusion is then applied as a second step, to fuse the initial results from the feature-level fusion, with the remaining modality.\n\nSimilar to text-based sentiment analysis, multimodal sentiment analysis can be applied in the development of different forms of recommender systems such as in the analysis of user-generated videos of movie reviews and general product reviews, to predict the sentiments of customers, and subsequently create product or service recommendations. Multimodal sentiment analysis also plays an important role in the advancement of virtual assistants through the application of natural language processing (NLP) and machine learning techniques. In the healthcare domain, multimodal sentiment analysis can be utilized to detect certain medical conditions such as stress, anxiety, or depression. Multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs, which is considered as a complicated and challenging domain, as sentiments expressed by reporters tend to be less obvious or neutral.\n", "related": "NONE"}
{"id": "8190902", "url": "https://en.wikipedia.org/wiki?curid=8190902", "title": "Anomaly detection", "text": "Anomaly detection\n\nIn data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.\n\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not \"rare\" objects, but unexpected \"bursts\" in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.\n\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given \"normal\" training data set, and then test the likelihood of a test instance to be generated by the learnt model.\n\nAnomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting ecosystem disturbances. It is often used in preprocessing to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.\n\nSeveral anomaly detection techniques have been proposed in literature. Some of the popular techniques are:\n\n- Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept).\n- Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data.\n- One-class support vector machines.\n- Replicator neural networks., autoencoders, long short-term memory neural networks\n- Bayesian Networks.\n- Hidden Markov models (HMMs).\n- Cluster analysis-based outlier detection.\n- Deviations from association rules and frequent itemsets.\n- Fuzzy logic-based outlier detection.\n- Ensemble techniques, using feature bagging, score normalization and different sources of diversity.\n\nThe performance of different methods depends a lot on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.\n\nAnomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. The counterpart of anomaly detection in intrusion detection is misuse detection.\n\n- ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.\n\n- Anomaly detection benchmark data repository of the Ludwig-Maximilians-Universität München; Mirror at University of São Paulo.\n- ODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.\n\n", "related": "\n- Change detection\n- Statistical process control\n- Novelty detection\n- Hierarchical temporal memory\n"}
{"id": "12656117", "url": "https://en.wikipedia.org/wiki?curid=12656117", "title": "Novelty detection", "text": "Novelty detection\n\nNovelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing. The principle is long known in neurophysiology, with roots in the orienting response research by E. N. Sokholov in the 1950s. The reverse phenomenon is habituation, i.e., the phenomenon that known patterns yield a less marked response. Early neural modeling attempts were by Yehuda Salu. An increasing body of knowledge has been collected concerning the corresponding mechanisms in the brain. In technology, the principle became important for radar detection methods during the Cold War, where unusual aircraft-reflection patterns could indicate an attack by a new type of aircraft. Today, the phenomenon plays an important role in machine learning and data science, where the corresponding methods are known as anomaly detection or outlier detection. An extensive methodological overview is given by Markou and Singh.\n\n", "related": "\n- Change detection\n- Outlier\n"}
{"id": "8880387", "url": "https://en.wikipedia.org/wiki?curid=8880387", "title": "Programming by example", "text": "Programming by example\n\nIn computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples.\n\nPbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language. Many PbE systems have been developed as research prototypes, but few have found widespread real-world application. More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol: Seahawk and Gbrowse moby. \n\nAlso the programming by demonstration (PbD) term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task. The usual distinction in literature between these terms is that in PbE the user gives a prototypical product of the computer execution, such as a row in the desired results of a query; while in PbD the user performs a sequence of actions that the computer must repeat, generalizing it to be used in different data sets. For final users, to automate a workflow in a complex tool (e.g. Photoshop), the most simple case of PbD is the macro recorder. \n\n", "related": "\n- Query by Example\n- Automated machine learning\n- Example-based machine translation\n- Inductive programming\n- Lapis (text editor), which allows simultaneous editing of similar items in multiple selections created by example\n- Programming by demonstration\n- Test-driven development\n\n- Henry Lieberman's page on Programming by Example\n- Online copy of Watch What I Do, Allen Cypher's book on Programming by Demonstration\n- Online copy of Your Wish is My Command, Henry Lieberman's sequel to Watch What I Do\n- A Visual Language for Data Mapping, John Carlson's description of an Integrated Development Environment (IDE) that used Programming by Example (desktop objects) for data mapping, and an iconic language for recording operations\n"}
{"id": "8529968", "url": "https://en.wikipedia.org/wiki?curid=8529968", "title": "Bayesian regret", "text": "Bayesian regret\n\nIn game theory, Bayesian regret is the average difference between the utility of a strategy and an ideal utility where desired outcomes are maximized. \n\nThe term \"Bayesian\" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.\n\nIn social choice theory, Bayesian regret is the average difference in social utility between the chosen candidate and the best candidate. It is only measurable if it is possible to know the voters' true numerical utility for each candidate – that is, in Monte Carlo simulations of virtual elections. \n\nIf Bayesian regret can be \"measured only if it is possible to know the voters' true numerical utility for each candidate\", it would seem to be irrelevant to two voting methods that instead grade the utility of each candidate: Majority Judgment (MJ) invites citizens to grade the suitability of each candidate for a single office: Excellent (ideal), Very Good, Good, Acceptable, Poor, or Reject (entirely unsuitable). The winner is the one who receives the highest median-grade. Since MJ allows each citizen honestly and fully to express their judgment of each candidate, it would seem to give each voter every appropriate reason to be pleased with the election. At least the voters who constitute the relevant absolute majority should be pleased. Similarly, Evaluative Proportional Representation (EPR) in Section 5.5.5 in Proportional Representation allows all the members of a legislature to be elected at the same time. Each elected member has a different weighted vote during its deliberations. At the same time, each citizen is assured that their honest vote proportionately increases the voting power of the member who received either their highest grade, remaining highest grade, or proxy vote. No vote is needlessly wasted quantitatively or qualitatively. Again, each citizen is given every appropriate reason to be please with an EPR election --no legitimate regret.\n\nThe term Bayesian is somewhat a misnomer, really meaning only \"average probabilistic\"; there is no standard or objective way to create distributions of voters and candidates.\n\nThe Bayesian regret concept was recognized as useful (and used) for comparing single-winner voting systems by Bordley and Merrill, and it also was invented independently by R. J. Weber. Bordley attributed it (and the whole idea of the usefulness of \"social\" utility, that is, summed over all people in the population) to John Harsanyi in 1955.\n\nThis term has been used to compare a random buy-and-hold strategy to professional traders' records. This same concept has received numerous different names, as the New York Times notes: \n\n\"In 1957, for example, a statistician named James Hanna called his theorem Bayesian Regret. He had been preceded by David Blackwell, also a statistician, who called his theorem Controlled Random Walks. Other, later papers had titles like 'On Pseudo Games', 'How to Play an Unknown Game', 'Universal Coding' and 'Universal Portfolios'\".\n\n", "related": "\n- Loss function\n- Regret (decision theory)\n- Social utility efficiency\n\n- Robert F. Bordley: \"A pragmatic method for evaluating election schemes through simulation\", \"Amer. Polit. Sci. Rev.\" 77 (1983) 123–141.\n- Samuel Merrill: Making multicandidate elections more democratic, Princeton Univ. Press 1988.\n- Samuel Merrill: \"A comparison of efficiency of multicandidate electoral systems\", \"Amer. J. Polit. Sci.\" 28, 1 (1984) 23–48.\n"}
{"id": "1514392", "url": "https://en.wikipedia.org/wiki?curid=1514392", "title": "Training, validation, and test sets", "text": "Training, validation, and test sets\n\nIn machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.\n\nThe data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in different stages of the creation of the model.\n\nThe model is initially fit on a training dataset, that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a neural net or a naive Bayes classifier) is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), which is commonly denoted as the \"target\" (or \"label\"). The current model is run with the training dataset and produces a result, which is then compared with the \"target\", for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\n\nSuccessively, the fitted model is used to predict the responses for the observations in a second dataset called the validation dataset. The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters (e.g. the number of hidden units in a neural network). Validation datasets can be used for regularization by early stopping: stop training when the error on the validation dataset increases, as this is a sign of overfitting to the training dataset.\nThis simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.\n\nFinally, the test dataset is a dataset used to provide an unbiased evaluation of a \"final\" model fit on the training dataset. If the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.\n\nA training dataset is a dataset of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a classifier.\n\nMost approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify and exploit apparent relationships in the training data that do not hold in general.\n\nA test dataset is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal overfitting has taken place (see figure below). A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.\n\nA test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier.\n\nA validation dataset is a dataset of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the \"dev set\". In artificial neural networks, a hyperparameter is, for example, the number of hidden units. It, as well as the testing set (as mentioned above), should follow the same probability distribution as the training dataset.\n\nIn order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation dataset in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training dataset is used to train the candidate algorithms, the validation dataset is used to compare their performances and decide which one to take and, finally, the test dataset is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation dataset functions as a hybrid: it is training data used for testing, but neither as part of the low-level training nor as part of the final testing.\n\nThe basic process of using a validation dataset for model selection (as part of training dataset, validation dataset, and test dataset) is:\nAn application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).\n\nMost simply, part of the original dataset can be set aside and used as a test set: this is known as the holdout method.\n\nThe terms test set and validation set are sometimes used in a way that flips their meaning in both industry and academia. In the erroneous usage, \"test set\" becomes the development set, and \"validation set\" is the independent set used to evaluate the performance of a fully specified classifier. The literature on machine learning often reverses the meaning of “validation” and “test” sets. This is the most blatant example of the terminological confusion that pervades artificial intelligence research.\n\nA dataset can be repeatedly split into a training dataset and a validation dataset: this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal datasets and using them as training/validation, and then validation/training, or repeatedly selecting a random subset as a validation dataset. To validate the model performance, sometimes an additional test dataset that was held out from cross-validation is used.\n\nAnother example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition), which splits a complete multi-class problem into a set of smaller classification problems. It serves for learning more accurate concepts due to simpler classification boundaries in subtasks and individual feature selection procedures for subtasks. When doing classification decomposition, the central choice is the order of combination of smaller classification steps, called the classification path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example, on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.\n\nCommercial tools, such as Diffgram and Supervisely, are available to create training data.\n\n", "related": "\n- Statistical classification\n- List of datasets for machine learning research\n\n- FAQ: What are the population, sample, training set, design set, validation set, and test set?\n- What is the Difference Between Test and Validation Datasets?\n- What is training, validation, and testing data-sets scenario in machine learning?\n- Is there a rule-of-thumb for how to divide a dataset into training and validation sets?\n"}
{"id": "57841422", "url": "https://en.wikipedia.org/wiki?curid=57841422", "title": "Appen (company)", "text": "Appen (company)\n\nAppen Limited (formerly known as Appen Butler Hill) is a publicly traded company listed on the Australian Securities Exchange (ASX) under the code APX.\n\nAppen provides or improves data used for the development of machine learning and artificial intelligence products. Data types include speech and natural language data, image and video data, text and alphanumeric data and relevance data to improve search and social media engines. Appen's customers use machine learning for a variety of use cases including automatic speech recognition (ASR), computer vision, increasing conversions in eCommerce, delivering more meaningful and personalized advertising, enhancing social media feeds or improving customer service capabilities with tools like chatbots and virtual assistants.\n\nFor machines to demonstrate artificial intelligence, they need to be programmed with human-quality training data that helps them learn. Appen uses crowdsourcing to collect and improve data and has access to a skilled crowd of over than 1 million part-time contractors who collect, annotate, evaluate, label, rate, test, translate and transcribe speech, image, text and video data to turn it into effective machine learning training data for a variety of use cases.\n\nThe company's global headquarters is in Chatswood, New South Wales which is 10 kilometers north of the central business district of Sydney, Australia. The United States headquarters is in Kirkland Washington which is a suburb of Seattle and there are also US offices in San Francisco, California and Detroit, Michigan. Appen also has offices in Beijing, China, Cavite, Philippines, and Exeter, England.\n\nAt the end of 2017, revenues were AUD 166.6 million and the company had more than 350 full-time employees and over 1,000,000 approved flexible workers in the Appen crowd. Tasks are performed in more than 180 languages and 130 countries.\n\nMost of the company's revenues are earned offshore and clients include eight of the top ten largest technology companies.\n\nAppen was founded in Sydney in 1996 by linguist Dr. Julie Vonwiller. She was joined by her husband Chris Vonwiller who left his job at Telstra in 2000 to join Appen full-time and is currently Non-Executive Chairman of Appen.\n\nIn 2011, Appen merged with the Butler Hill Group, which was based in Ridgefield, Connecticut and Seattle, Washington and originally founded by Lisa Braden-Harder in 1993. Lisa was a member of the pioneering team in grammar checking technology at the IBM T.J. Watson Research Center before the Butler Hill Group and stayed on as CEO until 2015. After the merger, the combined business became Appen Butler Hill and expanded its business scope to include language resources, search and text.\n\nIn 2012, Appen acquired Wikman Remer, a firm based in San Rafael, California, which developed tools and platforms for employee engagement, online moderation and curation.\n\nAppen Butler Hill was re-branded as Appen in 2013, and it went public on the ASX on January 7, 2015.\n\nIn July 2015 Mark Brayan joined Appen as CEO and continues to hold that position today.\n\nIn October 2016 Appen acquired a UK based transcription services company called Mendip Media Group (MMG)\n\nAppen also acquired Leapforce in November 2017 for U.S. $80M, adding additional capabilities in search relevance and growing its crowd to over 1,000,000 flexible workers.\n\n- Appen acquired data annotation company called Leapforce in 2017.\n- Appen acquired Figure Eight in 2019.\n\n- Australian Growth Technology Company Award 2017\n- Flex Jobs Top 100 Company With Remote Jobs in 2014, 2015, 2016, 2017 and 2018 including the #1 ranking in 2017 and the #2 ranking in 2018\n- Deloitte Asia Pacific Technology Fast 500 2017\n- Four-time winner of the Deloitte Technology Fast 50 Australia award and also named number 6 in the Deloitte Leadership Awards for companies having revenues in excess of $50 million in 2013\n- 2014 BRW Momentum Awards for Best Mid-market Business $50–100MM Category Finalist\n- Inaugural winner in 2008 of the Prime Minister's Exporter of the Year Award and Australian Export Category Winner, Information & Communication Technology Award\n- Ranked 8th largest language service provider globally by Common Sense Advisory (CSA Research) in Who's Who in Language Services and Technology: 2019 Rankings.\n- Inaugural winner in 2008 of the Prime Minister's Exporter of the Year Award and Australian Export Category Winner, Information and Communication Technology Award.\n\n- Official Website\n", "related": "NONE"}
{"id": "58714104", "url": "https://en.wikipedia.org/wiki?curid=58714104", "title": "OpenAI Five", "text": "OpenAI Five\n\nOpenAI Five is the name of a machine learning project that performs as a team of video game bots playing against human players in the competitive five-on-five video game \"Dota 2\". The system was developed by OpenAI, an American artificial intelligence (AI) research and development company founded with the mission to develop safe AI in a way that benefits humanity. OpenAI Five's first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against a professional player of the game known as Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\n\nThe company uses \"Dota 2\" as an experiment for general-purpose applied machine learning to capture the unpredictability and continuous nature of the real world. The team stated that the complex nature of the game and its strong reliance on having to work together as a team to win was a major reason it was specifically chosen. The algorithms used for the project have also been applied to other systems, such as controlling a robotic hand. The project has also been compared to a number of other similar cases of AI playing against and defeating humans, such as Watson on the television game show \"Jeopardy!\", Deep Blue in chess, and AlphaGo in the board game Go.\n\nDevelopment on the algorithms used for the bots began in November 2016. OpenAI decided to use \"Dota 2\", a competitive five-on-five video game, as a base due to it being popular on the live streaming platform Twitch, having native support for Linux, and had an application programming interface (API) available. Before becoming a team of five, the first public demonstration occurred at The International 2017 in August, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player of the game, lost against an OpenAI bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks \"like being a surgeon\". OpenAI calls the system \"reinforcement learning\", as the bots learn over time by playing against itself hundreds a times a day for months, in which they are rewarded for actions such as killing an enemy and destroying towers.\n\nBy June 2018, the ability of the bots expanded to play together as a full team of five and were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two games against professional teams, one against the Brazilian-based paiN Gaming and the other against an all-star team of former Chinese players. Although the bots lost both matches, OpenAI still considered it a successful venture, stating that playing against some of the best players in \"Dota 2\" allowed them to analyze and adjust their algorithms for future games. The bots' final public demonstration occurred in April 2019, where they won a best-of-three series against The International 2018 champions OG at a live event in San Francisco. A four-day online event to play against the bots, open to the public, occurred the same month. There, the bots played in 42,729 total games, winning all but 4,075 of them.\n\nEach OpenAI Five network contains a single layer with a 1024-unit LSTM that observes the current game state extracted from the Dota developer’s API. The neural network conducts actions via numerous possible action heads (no human data involved), and every head has meaning. For instance, the number of ticks to delay an action, what action to select – the X or Y coordinate of this action in a grid around the unit. In addition, action heads are computed independently. The AI system observes the world as a list of 20,000 numbers and takes an action by conducting a list of eight enumeration values. Also, it selects different actions and targets to understand how to encode every action and observe the world.\n\nOpenAI Five has been developed as a general-purpose reinforcement learning training system on the \"Rapid\" infrastructure. Rapid consists of two layers: it spins up thousands of machines and helps them ‘talk’ to each other and a second layer runs software. By 2018, OpenAI Five had played around 180 years worth of games in reinforcement learning running on 256 GPUs and 128,000 CPU cores, using a newly developed policy gradient method dubbed \"Proximal Policy Optimization.\"\n\nPrior to OpenAI Five, other AI versus human experiments and systems have been successfully used before, such as \"Jeopardy!\" with Watson, chess with Deep Blue, and Go with AlphaGo. In comparison with other games that have used AI systems to play against human players, \"Dota 2\" differs as explained below:\n\nLong run view: The bots run at 30 frames per second for an average match time of 45 minutes, which results in 80,000 ticks per game. OpenAI Five observes every fourth frame, generating 20,000 moves. By comparison, chess usually ends before 40 moves, while Go ends before 150 moves.\n\nPartially observed state of the game: Players and their allies can only see the map directly around them. The rest of it is covered in a fog of war which hides enemies units and their movements. Thus, playing \"Dota 2\" requires making inferences based on this incomplete data, as well as predicting what their opponent could be doing at the same time. By comparison, Chess and Go are \"full-information games\", as they do not hide elements from the opposing player.\n\nContinuous action space: Each playable character in a \"Dota 2\" game, known as a hero, can take dozens of actions that target either another unit or a position. The OpenAI Five developers allow the space into 170,000 possible actions per hero. Without counting the perpetual aspects of the game, there are an average of ~1,000 valid actions each tick. By comparison, the average number of actions in chess is 35 and 250 in Go.\n\nContinuous observation space: \"Dota 2\" is played on a large map with ten heroes, five on each team, along with dozens of buildings and non-player character (NPC) units. The OpenAI system observes the state of a game through developers’ bot API, as 20,000 numbers that constitute all information a human is allowed to get access to. A chess board is represented as about 70 lists, whereas a Go board has about 400 enumerations.\n\nOpenAI Five have received acknowledgement from the AI, tech, and video game community at large. Microsoft founder Bill Gates called it a \"big deal\", as their victories \"required teamwork and collaboration\". Chess player Garry Kasparov, who lost against the Deep Blue AI in 1997, stated that despite their losing performance at The International 2018, the bots would eventually \"get there, and sooner than expected\".\n\nAndreas Theodorou, an AI researcher at the University of Bath who uses computer games to study collaboration, says OpenAI Five was a \"big step forward\" in the AI industry, although noting that perhaps the most significant achievement was their use of transparent visualizations. In a conversation with MIT Technology Review, AI experts also considered OpenAI Five system as a significant achievement, as they noted that \"Dota 2\" was an \"extremely complicated game\", so even beating non-professional players was impressive. Inspired by the success of OpenAI five, other AI companies have begun to develop systems that will be able to compete in similar complex video games that require strategic thinking, team play, and reasoning, such as \"StarCraft\".\n\n", "related": "NONE"}
{"id": "58175832", "url": "https://en.wikipedia.org/wiki?curid=58175832", "title": "Multitask optimization", "text": "Multitask optimization\n\nMulti-task optimization is a paradigm in the optimization literature that focuses on solving multiple self-contained tasks simultaneously. The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics. \n\nThe key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate the search on the other. \n\nThe success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler to more complex tasks. In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems.\n\nThere are two common approaches for multi-task optimization: Bayesian optimization and evolutionary computation.\n\nMulti-task Bayesian optimization is a modern model-based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms. The method builds a multi-task Gaussian\nprocess model on the data originating from different searches progressing in tandem. The captured inter-task dependencies are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces.\n\nEvolutionary multi-tasking has been explored as a means of exploiting the implicit parallelism of population-based search algorithms to simultaneously progress multiple distinct optimization tasks. By mapping all tasks to a unified search space, the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer. This is induced when solutions associated with different tasks crossover. Recently, modes of knowledge transfer that are different from direct solution crossover have been explored.\n\nAlgorithms for multi-task optimization span a wide array of real-world applications. Recent studies highlight the potential for speed-ups in the optimization of engineering design parameters by conducting related designs jointly in a multi-task manner. In machine learning, the transfer of optimized features across related data sets can enhance the efficiency of the training process as well as improve the generalization capability of learned models. In addition, the concept of multi-tasking has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning.\n\nApplications have also been reported in cloud computing, with future developments geared towards cloud-based on-demand optimization services that can cater to multiple customers simultaneously.\n\n", "related": "\n- Multi-objective optimization\n- Multi-task learning\n- Multicriteria classification\n- Multiple-criteria decision analysis\n"}
{"id": "58655546", "url": "https://en.wikipedia.org/wiki?curid=58655546", "title": "Associative classifier", "text": "Associative classifier\n\nAn associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value. The term associative classification was coined by Bing Liu et al., in which the authors defined a model made of rules \"whose right-hand side are restricted to the classification class attribute\".\n\nThe model generated by an AC and used to label new records consists of association rules, where the consequent corresponds to the class label. As such, they can also be seen as a list of \"if-then\" clauses: if the record matches some criteria (expressed in the left side of the rule, also called antecedent), it is then labeled accordingly to the class on the right side of the rule (or consequent).\n\nMost ACs read the list of rules in order, and apply the first matching rule to label the new record .\n\nThe rules of an AC inherit some of the metrics of association rules, like the support or the confidence. Metrics can be used to order or filter the rules in the model and to evaluate their quality.\n\nThe first proposal of a classification model made of association rules was CBA, although other authors had previously proposed the mining of association rules for classification. Other authors have since then proposed multiple changes to the initial model, like the addition of a redundant rule pruning phase or the exploitation of Emerging Patterns.\n\nNotable implementations include:\n\n- CMAR\n- CPAR\n- L³\n- CAEP\n- GARC\n- ADT.\n", "related": "NONE"}
{"id": "406624", "url": "https://en.wikipedia.org/wiki?curid=406624", "title": "Time series", "text": "Time series\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n\nTime series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n\nTime series \"analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series \"forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Interrupted time series analysis is the analysis of interventions on a single time series.\n\nTime series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)\n\nTime series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).\n\nMethods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.\n\nAdditionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.\n\nMethods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.\n\nA time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). A data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.\n\nThere are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc.\n\nIn the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection and estimation. In the context of data mining, pattern recognition and machine learning time series analysis can be used for clustering, classification, query by content, anomaly detection as well as forecasting.\n\nThe clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.\n\nOther techniques include:\n\n- Autocorrelation analysis to examine serial dependence\n- Spectral analysis to examine cyclic behavior which need not be related to seasonality. For example, sun spot activity varies over 11 year cycles. Other common examples include celestial phenomena, weather patterns, neural activity, commodity prices, and economic activity.\n- Separation into components representing trend, seasonality, slow and fast variation, and cyclical irregularity: see trend estimation and decomposition of time series\n\nCurve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a \"smooth\" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.\n\nThe construction of economic time series involves the estimation of some components for some dates by interpolation between values (\"benchmarks\") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (\"reading between the lines\"). Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates. Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set. Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.\n\nExtrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.\n\nIn general, a function approximation problem asks us to select a function among a well-defined class that closely matches (\"approximates\") a target function in a task-specific way.\nOne can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n\nSecond, the target function, call it \"g\", may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (\"x\", \"g\"(\"x\")) is provided. Depending on the structure of the domain and codomain of \"g\", several techniques for approximating \"g\" may be applicable. For example, if \"g\" is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of \"g\" is a finite set, one is dealing with a classification problem instead. A related problem of \"online\" time series approximation is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.\n\nTo some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.\n- Fully formed statistical models for stochastic simulation purposes, so as to generate alternative versions of the time series, representing what might happen over non-specific time-periods in the future\n- Simple or fully formed statistical models to describe the likely outcome of the time series in the immediate future, given knowledge of the most recent outcomes (forecasting).\n- Forecasting on time series is usually done using automated statistical software packages and programming languages, such as Apache Spark, Julia, Python, R, SAS, SPSS and many others.\n- Forecasting on large scale data is done using Spark which has spark-ts as a third party package.\n\nAssigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language.\n\nThis approach is based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation, the development of which was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. Kálmán, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. See Kalman filter, Estimation theory, and Digital signal processing\n\nSplitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.\n\nModels for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the \"autoregressive\" (AR) models, the \"integrated\" (I) models, and the \"moving average\" (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial \"V\" for \"vector\", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some \"forcing\" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final \"X\" for \"exogenous\".\n\nNon-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel)\n\nAmong other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.\n\nIn recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.\n\nA Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.\n\nA number of different notations are in use for time-series analysis. A common notation specifying a time series \"X\" that is indexed by the natural numbers is written\n\nAnother common notation is\nwhere \"T\" is the index set.\n\nThere are two sets of conditions under which much of the theory is built:\n- Stationary process\n- Ergodic process\n\nHowever, ideas of stationarity must be expanded to consider two important ideas: strict stationarity and second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.\n\nIn addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time–frequency representation of a time-series or signal.\n\nTools for investigating time-series data include:\n\n- Consideration of the autocorrelation function and the spectral density function (also cross-correlation functions and cross-spectral density functions)\n- Scaled cross- and auto-correlation functions to remove contributions of slow components\n- Performing a Fourier transform to investigate the series in the frequency domain\n- Use of a filter to remove unwanted noise\n- Principal component analysis (or empirical orthogonal function analysis)\n- Singular spectrum analysis\n- \"Structural\" models:\n- General State Space Models\n- Unobserved Components Models\n- Machine Learning\n- Artificial neural networks\n- Support vector machine\n- Fuzzy logic\n- Gaussian process\n- Hidden Markov model\n- Queueing theory analysis\n- Control chart\n- Shewhart individuals control chart\n- CUSUM chart\n- EWMA chart\n- Detrended fluctuation analysis\n- Dynamic time warping\n- Cross-correlation\n- Dynamic Bayesian network\n- Time-frequency analysis techniques:\n- Fast Fourier transform\n- Continuous wavelet transform\n- Short-time Fourier transform\n- Chirplet transform\n- Fractional Fourier transform\n- Chaotic analysis\n- Correlation dimension\n- Recurrence plots\n- Recurrence quantification analysis\n- Lyapunov exponents\n- Entropy encoding\n\nTime series metrics or features that can be used for time series classification or regression analysis:\n\n- Univariate linear measures\n- Moment (mathematics)\n- Spectral band power\n- Spectral edge frequency\n- Accumulated Energy (signal processing)\n- Characteristics of the autocorrelation function\n- Hjorth parameters\n- FFT parameters\n- Autoregressive model parameters\n- Mann–Kendall test\n- Univariate non-linear measures\n- Measures based on the correlation sum\n- Correlation dimension\n- Correlation integral\n- Correlation density\n- Correlation entropy\n- Approximate entropy\n- Sample entropy\n- Wavelet entropy\n- Rényi entropy\n- Higher-order methods\n- Marginal predictability\n- Dynamical similarity index\n- State space dissimilarity measures\n- Lyapunov exponent\n- Permutation methods\n- Local flow\n- Other univariate measures\n- Algorithmic complexity\n- Kolmogorov complexity estimates\n- Hidden Markov Model states\n- Rough path signature\n- Surrogate time series and surrogate correction\n- Loss of recurrence (degree of non-stationarity)\n- Bivariate linear measures\n- Maximum linear cross-correlation\n- Linear Coherence (signal processing)\n- Bivariate non-linear measures\n- Non-linear interdependence\n- Dynamical Entrainment (physics)\n- Measures for Phase synchronization\n- Measures for Phase locking\n- Similarity measures:\n- Cross-correlation\n- Dynamic Time Warping\n- Hidden Markov Models\n- Edit distance\n- Total correlation\n- Newey–West estimator\n- Prais–Winsten transformation\n- Data as Vectors in a Metrizable Space\n-  Minkowski distance\n-  Mahalanobis distance\n- Data as time series with envelopes\n-  Global standard deviation\n-  Local standard deviation\n-  Windowed standard deviation\n- Data interpreted as stochastic series\n-  Pearson product-moment correlation coefficient\n-  Spearman's rank correlation coefficient\n- Data interpreted as a probability distribution function\n-  Kolmogorov–Smirnov test\n-  Cramér–von Mises criterion\n\nTime series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)\n\n- Braided graphs\n- Line charts\n- Slope graphs\n\n- Horizon graphs\n- Reduced line chart (small multiples)\n- Silhouette graph\n- Circular silhouette graph\n\n- Durbin J., Koopman S.J. (2001), \"Time Series Analysis by State Space Methods\", Oxford University Press.\n- Priestley, M. B. (1981), \"Spectral Analysis and Time Series\", Academic Press.\n- Shumway R. H., Stoffer D. S. (2017), \"Time Series Analysis and its Applications: With R Examples (ed. 4)\", Springer,\n- Weigend A. S., Gershenfeld N. A. (Eds.) (1994), \"Time Series Prediction: Forecasting the Future and Understanding the Past\". Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis (Santa Fe, May 1992), Addison-Wesley.\n- Wiener, N. (1949), \"Extrapolation, Interpolation, and Smoothing of Stationary Time Series\", MIT Press.\n- Woodward, W. A., Gray, H. L. & Elliott, A. C. (2012), \"Applied Time Series Analysis\", CRC Press.\n\n- Introduction to Time series Analysis (Engineering Statistics Handbook) — A practical guide to Time series analysis.\n", "related": "NONE"}
{"id": "40409788", "url": "https://en.wikipedia.org/wiki?curid=40409788", "title": "Convolutional neural network", "text": "Convolutional neural network\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.\n\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\n\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n\nThe name “convolutional neural\nnetwork” indicates that the network employs a mathematical operation called\nconvolution. Convolution is a specialized kind of linear operation. Convolutional\nnetworks are simply neural networks that use convolution in place of general matrix\nmultiplication in at least one of their layers.\n\nA convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that \"convolve\" with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.\n\nThough the layers are colloquially referred to as convolutions, this is only by convention. Mathematically, it is technically a \"sliding dot product\" or cross-correlation. This has significance for the indices in the matrix, in that it affects how weight is determined at a specific index point.\n\nWhen programming a CNN, the input is a tensor with shape (number of images) x (image width) x (image height) x (image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map width) x (feature map height) x (feature map channels). A convolutional layer within a neural network should have the following attributes:\n\n- Convolutional kernels defined by a width and height (hyper-parameters).\n- The number of input channels and output channels (hyper-parameter).\n- The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map.\n\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for \"each\" neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.\n\nConvolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer. In addition, pooling may compute a max or an average. \"Max pooling\" uses the maximum value from each of a cluster of neurons at the prior layer. \"Average pooling\" uses the average value from each of a cluster of neurons at the prior layer.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from \"every\" element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its \"receptive field\". So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.\n\nEach neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.\n\nThe vector of weights and the bias are called \"filters\" and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces memory footprint because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.\n\nCNN design follows vision processing in living organisms.\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n\nTheir 1968 paper identified two basic visual cell types in the brain:\n\n- simple cells, whose output is maximized by straight edges having particular orientations within their receptive field\n- complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\n\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe \"neocognitron\" was introduced by Kunihiko Fukushima in 1980.\nIt was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.\n\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\n\nThe neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.\n\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance. It did so by utilizing weight sharing in combination with Backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights, instead of a local one.\n\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution. Since these TDNNs operated on spectrograms the resulting phoneme recognition system was invariant to both, shifts in time and in frequency. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.\n\nTDNNs now achieve the best performance in far distance speech recognition.\n\nIn 1990 Yamaguchi et al. introduced the concept of max pooling. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n\nA system to recognize hand-written ZIP Code numbers involved convolutions in which the kernel coefficients had been laboriously hand designed.\n\nYann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\n\nThis approach became a foundation of modern computer vision.\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks () digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n\nSimilarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.\n\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.\n\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.\n\nIn 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results. In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions. In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).\n\nSubsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.\n\nCompared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.\nA notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).\nCHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n\nIn the past, traditional multilayer perceptron (MLP) models have been used for image recognition. However, due to the full connectivity between nodes, they suffered from the curse of dimensionality, and did not scale well with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.\n\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n\nConvolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n- 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\n- Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\n- Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting feature map to be equivariant under changes in the locations of input features in the visual field, i.e. they grant translational equivariance.\n- Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\n\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\n\n- The \"depth\" of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\n- \"Stride\" controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. When the stride is 2 then the filters jump 2 pixels at a time as they slide around. Similarly, for any integer formula_1 a stride of \"S\" causes the filter to be translated by \"S\" units at a time per output. In practice, stride lengths of formula_2 are rare. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions when stride length is increased.\n- Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.\n\nThe spatial size of the output volume can be computed as a function of the input volume size formula_3, the kernel field size of the convolutional layer neurons formula_4, the stride with which they are applied formula_5, and the amount of zero padding formula_6 used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by\n\nformula_7\n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be formula_8 when the stride is formula_9 ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a \"depth slice\", the neurons in each depth slice are constrained to use the same weights and bias.\n\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which \"max pooling\" is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.\n\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n\nThe pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\nformula_10\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.\n\nDue to the aggressive reduction in the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.\n\nPooling is an important component of convolutional neural networks for object detection based on Fast R-CNN architecture.\n\nReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function formula_11. It effectively removes negative values from an activation map by setting them to zero. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n\nOther functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent formula_12, formula_13, and the sigmoid function formula_14. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\nThe \"loss layer\" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used.\n\nSoftmax loss is used for predicting a single class of \"K\" mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting \"K\" independent probability values in formula_15. Euclidean loss is used for regressing to real-valued labels formula_16.\n\nCNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n\nSince feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values \"v\" with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\nCommon filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n\nThe challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.\n\nTypical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either \"dropped out\" of the net with probability formula_17 or kept with probability formula_18, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n\nIn the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n\nAt testing time after training has finished, we would ideally like to find a sample average of all possible formula_19 dropped-out networks; unfortunately this is unfeasible for large values of formula_20. However, we can find an approximation by using the full network with each node's output weighted by a factor of formula_18, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates formula_19 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability formula_17. Each unit thus receives input from a random subset of units in the previous layer.\n\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n\nIn stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\n\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\nSince the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n\nL1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector formula_24 of every neuron to satisfy formula_25. Typical values of formula_26 are order of 3–4. Some papers report improvements when using this form of regularization.\n\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\n\nCurrently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\n\nThus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\nCNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called \nAlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\n\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\n\nIn 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.\n\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\n\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\nCNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.\n\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\n\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\n\nFor many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.\n\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs.\n\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n- Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\n- Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\n- Dlib: A toolkit for making real world machine learning and data analysis applications in C++.\n- Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\n- TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\n- Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\n- Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.\n\n- Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.\n\n", "related": "\n- Convolution\n- Deep learning\n- Natural-language processing\n- Neocognitron\n- Scale-invariant feature transform\n- Time delay neural network\n- Vision processing unit\n\n- CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision\n- An Intuitive Explanation of Convolutional Neural Networks — A beginner level introduction to what Convolutional Neural Networks are and how they work\n- Convolutional Neural Networks for Image Classification — Literature Survey\n"}
{"id": "43385931", "url": "https://en.wikipedia.org/wiki?curid=43385931", "title": "Data exploration", "text": "Data exploration\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\n\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\n\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using spreadsheets or similar tools to view the raw data.\n\nAll of these activities are aimed at creating a mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\n\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data (data cleansing), correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\n\nData exploration can also refer to the ad hoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data.\n\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\n\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As its most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Many common patterns include regression and classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\n\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\n\n- Trifacta – a data preparation and analysis platform\n- Paxata – self-service data preparation software\n- Alteryx – data blending and advanced data analytics software\n- Microsoft Power BI - interactive visualization and data analysis tool\n- OpenRefine - a standalone open source desktop application for data clean-up and data transformation\n- Tableau software – interactive data visualization software\n\n", "related": "\n- Exploratory data analysis\n- Machine learning\n- Data profiling\n- Data visualization\n"}
{"id": "59729969", "url": "https://en.wikipedia.org/wiki?curid=59729969", "title": "Waifu2x", "text": "Waifu2x\n\nwaifu2x is an image scaling and noise reduction program for anime-style art, but also supports photos.\n\nwaifu2x was inspired by Super-Resolution Convolutional Neural Network (SRCNN). It uses Nvidia CUDA for computing, although alternative implementations that allow for OpenCL and Vulkan have been created.\n\"waifu\" is anime slang for a character to whom one is attracted.\n\"2x\" means two-times magnification. \n\n", "related": "\n- Comparison gallery of image scaling algorithms\n\n"}
{"id": "59968610", "url": "https://en.wikipedia.org/wiki?curid=59968610", "title": "Learning curve (machine learning)", "text": "Learning curve (machine learning)\n\nIn machine learning, a learning curve (or training curve) shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much a machine learning model benefits from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, it will not benefit much from more training data.\n\nThe machine learning curve is useful for many purposes including comparing different algorithms, choosing model parameters during design, adjusting optimization to improve convergence, and determining the amount of data used for training.\n\nIn the machine learning domain, there are two connotations of learning curves differing in the x-axis of the curves, with experience of the model graphed either as the number of training examples used for learning or the number of iterations used in training the model.\n\n", "related": "\n- Overfitting\n- Bias–variance tradeoff\n- Model selection\n- Cross-validation (statistics)\n- Validity (statistics)\n- Verification and validation\n"}
{"id": "59969558", "url": "https://en.wikipedia.org/wiki?curid=59969558", "title": "Learning rate", "text": "Learning rate\n\nIn machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". The learning rate is often denoted by the character η or α.\n\nIn setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the direction toward the minimum is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum. \n\nIn order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. In these quasi-Newton methods, step size becomes an important component of the optimization algorithm.\n\nA learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: decay and momentum . There are many different learning rate schedules but the most common are time-based, step-based and exponential.\n\nDecay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minima, and is controlled by a hyperparameter.\n\nMomentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps. Momentum is controlled by a hyper parameter analogous to a ball's mass which must be chosen manually—too high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras.\n\nTime-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:\n\nformula_1\n\nwhere formula_2 is the learning rate, formula_3 is a decay parameter and formula_4 is the iteration step.\n\nStep-based learning schedules changes the learning rate according to some pre defined steps. The decay application formula is here defined as:\n\nformula_5\n\nwhere formula_6 is the learning rate at iteration formula_4, formula_8 is the initial learning rate, formula_3 is how much the learning rate should change at each drop (0.5 corresponds to a halving) and formula_10 corresponds to the droprate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations). The \"floor\" function here drops the value of its input to 0 for all values smaller than 1.\n\nExponential learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:\n\nformula_11\n\nwhere formula_3 is a decay parameter.\n\nThe issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam which are generally built into deep learning libraries such as Keras.\n\n", "related": "\n- Hyperparameter (machine learning)\n- Hyperparameter optimization\n- Stochastic gradient descent\n- Variable metric methods\n- Overfitting\n- Backpropagation\n- AutoML\n- Model selection\n- Self-tuning\n"}
{"id": "59973182", "url": "https://en.wikipedia.org/wiki?curid=59973182", "title": "Nature Machine Intelligence", "text": "Nature Machine Intelligence\n\nNature Machine Intelligence is a scientific journal dedicated to covering machine learning and artificial intelligence. It was created by Nature Research in response to the machine learning explosion of the 2010s. It launched in January 2019, and its opening was met with controversy and boycotts within the machine learning research community due to opposition to Nature publishing the journal as closed access.\n", "related": "NONE"}
{"id": "54550729", "url": "https://en.wikipedia.org/wiki?curid=54550729", "title": "Connectionist temporal classification", "text": "Connectionist temporal classification\n\nConnectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in 2006.\n\nThe input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. The difficulty of training comes from there being many more observations than there are labels. For example in speech audio there can be multiple time slices which correspond to a single phoneme. Since we don't know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task, but there is an efficient forward–backward algorithm for that.\n\nCTC scores can then be used with the back-propagation algorithm to update the neural network weights.\n\nAlternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM).\n", "related": "NONE"}
{"id": "37815827", "url": "https://en.wikipedia.org/wiki?curid=37815827", "title": "Astrostatistics", "text": "Astrostatistics\n\nAstrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference.\n\nPractitioners are represented by the International Astrostatistics Association affiliated with the International Statistical Institute, the International Astronomical Union Working Group in Astrostatistics and Astroinformatics, the American Astronomical Society Working Group in Astroinformatics and Astrostatistics, the American Statistical Association Interest Group in Astrostatistics, and the Cosmostatistics Initiative. All of these organizations participate in the Astrostatistics and Astroinformatics Portal Web site.\n", "related": "NONE"}
{"id": "60968880", "url": "https://en.wikipedia.org/wiki?curid=60968880", "title": "Weak supervision", "text": "Weak supervision\n\nWeak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.\n\nMachine learning models and techniques are increasingly accessible to researchers and developers; the real-world usefulness of these models, however, depends on access to high-quality labeled training data. This need for labeled training data often proves to be a significant obstacle to the application of machine learning models within an organization or industry. This bottleneck effect manifests itself in various ways, including the following examples:\n\nInsufficient quantity of labeled data\n\nWhen machine learning techniques are initially used in new applications or industries, there is often not enough training data available to apply traditional processes. Some industries have the benefit of decades' worth of training data readily available; those that do not are at a significant disadvantage. In such cases, obtaining training data may be impractical, expensive, or impossible without waiting years for its accumulation.\n\nInsufficient subject-matter expertise to label data\n\nWhen labeling training data requires specific relevant expertise, creation of a usable training data set can quickly become prohibitively expensive. This issue is likely to occur, for example, in biomedical or security-related applications of machine learning.\n\nInsufficient time to label and prepare data\n\nMost of the time required to implement machine learning is spent in preparing data sets. When an industry or research field deals with problems that are, by nature, rapidly evolving, it can be impossible to collect and prepare data quickly enough for results to be useful in real-world applications. This issue could occur, for example, in fraud detection or cybersecurity applications.\n\nOther areas of machine learning exist that are likewise motivated by the demand for increased quantity and quality of labeled training data but employ different high-level techniques to approach this demand. These other approaches include active learning, semi-supervised learning, and transfer learning.\n\nWeak labels are intended to decrease the cost and increase the efficiency of human efforts expended in hand-labeling data. They can take many forms, including the following:\n\n- Imprecise or inexact labels: developers may use higher-level, less precise input from subject-matter experts to create heuristic rules, define expected distributions, or impose other constraints on the training data.\n- Inaccurate labels: developers may use inexpensive, lower-quality input through means such as crowdsourcing to obtain labels that are numerous, but not expected to be perfectly correct.\n- Existing resources: developers may take advantage of existing resources (such as knowledge bases, alternative data sets, or pre-trained models) to create labels that are helpful, though not perfectly suited for the given task.\n\nApplications of weak supervision are numerous and varied within the machine learning research community.\n\nStanford University researchers created Snorkel, an open-source system for quickly assembling training data through weak supervision. Snorkel employs the central principles of the data programming paradigm, in which developers create labeling functions, which are then used to programmatically label data, and employs supervised learning techniques to assess the accuracy of those labeling functions. In this way, potentially low-quality inputs can be used to create high-quality models.\n\nIn a joint work with Google, Stanford researchers showed that existing organizational knowledge resources could be converted into weak supervision sources and used to significantly decrease development costs and time.\n\nIn 2019, Massachusetts Institute of Technology and Google researchers released cleanlab, the first standardized Python package for machine learning and deep learning with noisy labels. Cleanlab implements confident learning, a framework of theory and algorithms for dealing with uncertainty in dataset labels, to (1) find label errors in datasets, (2) characterize label noise, and (3) standardize and simplify research in weak supervision and learning with noisy labels.\n\nResearchers at University of Massachusetts Amherst propose augmenting traditional active learning approaches by soliciting labels on features rather than instances within a data set.\n\nResearchers at Johns Hopkins University propose reducing the cost of labeling data sets by having annotators provide rationales supporting each of their data annotations, then using those rationales to train both discriminative and generative models for labeling additional data.\n\nResearchers at University of Alberta propose a method that applies traditional active learning approaches to enhance the quality of the imperfect labels provided by weak supervision.\n", "related": "NONE"}
{"id": "60992857", "url": "https://en.wikipedia.org/wiki?curid=60992857", "title": "Federated learning", "text": "Federated learning\n\nFederated learning (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples. This approach stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed.\n\nFederated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus addressing critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, or pharmaceutics.\n\nFederated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights of a deep neural network) between these local models at some frequency to generate a global model.\n\nFederated learning algorithms may use a central server that orchestrates the different steps of the algorithm and acts as a reference clock, or they may be peer-to-peer, where no such central server exists. In the non peer-to-peer case, a federated learning process can be broken down in multiple rounds, each consisting of 4 general steps.\nThe main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are identically distributed and roughly have the same size. None of these hypotheses are made for federated learning; instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude.\n\nTo ensure good task performance of a final, central machine learning model, federated learning relies on an iterative process broken up into an atomic set of client-server interactions known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model.\n\nIn the methodology below, we use a central server for this aggregation, while local nodes perform local training depending on the central server's orders. However, other strategies lead to the same results without central servers, in a peer-to-peer approach, using gossip methodologies.\n\nA statistical model (e.g., linear regression, neural network, boosting) is chosen to be trained on local nodes and initialized. Nodes are activated and wait for the central server to give calculation tasks.\n\nFor multiple iterations of so-called federated learning rounds, the following steps are performed:\n\nA fraction of local nodes are selected to start training on local data. They all acquire the same current statistical model from the central server. Other nodes wait for the next federated round.\n\nThe central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g. for some batch updates of gradient descent).\n\nEach node returns the locally learned incremental model updates to the central server. The central server aggregates all results and stores the new model. It also handles failures (e.g., connection lost with a node while training). The system returns to the selection phase.\n\nWhen a pre-specified termination criterion (e.g. maximal number of rounds or local accuracies higher than some target) has been met, the central server orders the end of the iterative training process. The central server contains a robust model which was trained on multiple heterogeneous data sources.\n\nThe way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches: for instance no central orchestrating server, or stochastic communication.\n\nIn particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to a several randomly-selected others, which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost.\n\nOnce the topology of the node network is chosen, one can control different parameters of the federated learning process (in opposition to the machine learning model's own hyperparameters) to optimize learning :\n\n- Number of federated learning rounds : T\n- Total number of nodes used in the process : K\n- Fraction of nodes used at each iteration for each node : C\n- Local batch size used at each learning iteration : B\n\nOther model-dependent parameters can also be tinkered with, such as :\n\n- Number of iterations for local training before pooling : N\n- Local learning rate : η\n\nThose parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, bandwidth). For instance, stochastically choosing a limited fraction C of nodes for each iteration diminishes computing cost and may prevent overfitting, in the same way that stochastic gradient descent can reduce overfitting.\n\nIn this section, we follow the exposition of Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan and al. 2017.\n\nTo describe the federated strategies, let us introduce some notations:\n\n- K : total number of clients;\n- k : index of clients;\n- n : number of data samples available during training for client k;\n- w : model's weight vector on client k, at the federated round t;\n- l(w, b) : loss function for weights w and batch b;\n- E : number of local epochs;\n\nDeep learning training mainly relies on variants of stochastic gradient descent, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.\n\nFederated stochastic gradient descent is the direct transposition of this algorithm to the federated setting, but by using a random fraction C of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.\n\nFederative averaging (FedAvg) is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged model's performance.\n\nFederated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoid data communication, which can require significant resources before starting centralized machine learning.\n\nFederated learning raises several statistical challenges :\n\n- Heterogeneity between the different local datasets: each node may have some bias with respect to the general population, and the size of the datasets may vary significantly;\n- Temporal heterogeneity: each local dataset's distribution may vary with time;\n- Interoperability of each node's dataset is a prerequisite;\n- Each node's dataset may require regular curations.\n- Hiding training data might allow attackers to inject backdoors into the global model .\n- Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation\n\nThe main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.\n\nWith federated learning, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy. Despite such protective measures, these parameters mays still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using differential privacy or secure aggregation.\n\nThe generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the federated learning methodology can be adapted to generate two models at once in a multi-task learning framework.\n\nIn the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will remain on each local node and only be trained on the local node's dataset.\n\nWestern legal frameworks emphasize more and more on data protection and data traceability. White House 2012 Report recommended the application of a data minimization principle, which is mentioned in European GDPR. In some cases, it is impossible to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases federated learning brings solutions to train a global model while respecting security constraints.\n\nFederated learning has started to emerge as an important research topic in 2015 and 2016, with the first publications on federative averaging in telecommunication settings. Recent publications have emphasized the development of resource allocation strategies, especially to reduce communication requirements between nodes with gossip algorithms. In addition, recent publications continue to work on the federated algorithms robustness to differential privacy attacks.\n\nFederated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with other (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node.\n\nOne of the first use cases of federated learning was implemented by Google for predictive keyboards. Under high regulatory pressure, it showed impossible to upload every user's text message to train the predictive algorithm for word guessing. Besides, such a process would hijack too much of the user's data. Despite the sometimes limited memory and computing power of smartphones, Google has made a compelling use case out of its G-board, as presented during the Google IO 2019 event.\n\nPharmaceutical research is pivoting towards a new paradigm : real world data use for generating drug leads and synthetic control arms. Generating knowledge on complex biological problems require to gather a lot of data from diverse medical institutions, which are eager to maintain control of their sensitive patient data. Federated learning, especially assisted by high traceability technologies (distributive ledgers) enable researchers to train predictive models on many sensitive data in a transparent way without uploading them. In 2019, French start-up Owkin is pioneering the development of biomedical machine learning models based on such algorithms to capture heterogeneous data from both pharmaceutical companies and medical institutions.\n\nSelf driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes.\n\nSome libraries have been made to facilitate the development of federated learning systems, including: \n\n- PySyft - a library for implementing federated learning, multi-party computation, and differential privacy within PyTorch and TensorFlow from the open-source community OpenMined\n- TensorFlow Federated - a platform within TensorFlow with high-level and low-level interfaces for federated learning\n\n- \"Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016\" at eur-lex.europa.eu. Retrieved October 18, 2019.\n", "related": "NONE"}
{"id": "61213249", "url": "https://en.wikipedia.org/wiki?curid=61213249", "title": "Distill (journal)", "text": "Distill (journal)\n\nDistill, is a peer-reviewed scientific journal covering machine learning. Articles may contain interactive graphics and so-called explorable explanations. The journal was established in March 2017 by Google, OpenAI, DeepMind, and Y Combinator Research. The editors-in-chief are Shan Carter (Google Brain), Chris Olah (OpenAI), and Arvind Satyanarayan (MIT Computer Science and Artificial Intelligence Laboratory). The journal is indexed in Ei Compendex. Its launch was criticized as overly hyped by \"The Scholarly Kitchen\", which also noted that most authors were Google employees.\n", "related": "NONE"}
{"id": "61373032", "url": "https://en.wikipedia.org/wiki?curid=61373032", "title": "Machine learning in physics", "text": "Machine learning in physics\n\nApplying classical methods of machine learning to the study of quantum systems (sometimes called \"quantum machine learning\") is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other examples include learning Hamiltonians, learning quantum phase transitions, and automatically generating new quantum experiments. Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials or directly solving the Schrödinger equation with a variational method.\n\nThe ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, Bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification, Hamiltonian learning, and the characterization of an unknown unitary transformation. Other problems that have been addressed with this approach are given in the following list:\n\n- Identifying an accurate model for the dynamics of a quantum system, through the reconstruction of the Hamiltonian;\n- Extracting information on unknown states;\n- Learning unknown unitary transformations and measurements;\n- Engineering of quantum gates from qubit networks with pairwise interactions, using time dependent or independent Hamiltonians.\n- Improving the extraction accuracy of physical observables from absorption images of ultracold atoms (degenerate Fermi gas), by the generation of an ideal reference frame.\n\nQuantum machine learning can also be applied to dramatically accelerate the prediction of quantum properties of molecules and materials. This can be helpful for the computational design of new molecules or materials. Some examples include\n\n- Interpolating interatomic potentials;\n- Inferring molecular atomization energies throughout chemical compound space;\n- Accurate potential energy surfaces with restricted Boltzmann machines;\n- Automatic generation of new quantum experiments;\n- Solving the many-body, static and time-dependent Schrödinger equation;\n- Identifying phase transitions from entanglement spectra;\n- Generating adaptive feedback schemes for quantum metrology and quantum tomography.\n\nVariational circuits are a family of algorithms which utilize training based on circuit parameters and an objective function. Variational circuits are generally composed of a classical device communicating input parameters (random or pre-trained parameters) into a quantum device, along with a classical Mathematical optimization function. These circuits are very heavily dependent on the architecture of the proposed quantum device because parameter adjustments are adjusted based solely on the classical components within the device. Though the application is considerably infantile in the field of quantum machine learning, it has incredibly high promise for more efficiently generating efficient optimization functions.\n\nMachine learning techniques can be used to find a better manifold of integration for path integrals in order to avoid the sign problem.\n\n", "related": "\n- Quantum computing\n- Quantum machine learning\n- Quantum algorithm for linear systems of equations\n- Quantum annealing\n- Quantum neural network\n"}
{"id": "60951296", "url": "https://en.wikipedia.org/wiki?curid=60951296", "title": "Machine learning in video games", "text": "Machine learning in video games\n\nIn video games, various artificial intelligence techniques have been used in a variety of ways, ranging from non-player character (NPC) control to procedural content generation (PCG). Machine learning is a subset of artificial intelligence that focuses on using algorithms and statistical models to make machines act without specific programming. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems.\n\nInformation on machine learning techniques in the field of games is mostly known to public through research projects as most gaming companies choose not to publish specific information about their intellectual property. The most publicly known application of machine learning in games is likely the use of deep learning agents that compete with professional human players in complex strategy games. There has been a significant application of machine learning on games such as Atari/ALE, \"Doom\", \"Minecraft\", \"StarCraft\", and car racing. Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning.\n\nDeep learning is a subset of machine learning which focuses heavily on the use of artificial neural networks (ANN) that learn to solve complex tasks. Deep learning uses multiple layers of ANN and other techniques to progressively extract information from an input. Due to this complex layered approach, deep learning models often require powerful machines to train and run on.\n\nConvolutional neural networks (CNN) are specialized ANNs that are often used to analyze image data. These types of networks are able to learn translation invariant patterns, which are patterns that are not dependent on location. CNNs are able to learn these patterns in a hierarchy, meaning that earlier convolutional layers will learn smaller local patterns while later layers will learn larger patterns based on the previous patterns. A CNN's ability to learn visual data has made it a commonly used tool for deep learning in games.\n\nRecurrent neural networks are a type of ANN that are designed to process sequences of data in order, one part at a time rather than all at once. An RNN runs over each part of a sequence, using the current part of the sequence along with memory of previous parts of the current sequence to produce an output. These types of ANN are highly effective at tasks such as speech recognition and other problems that depend heavily on temporal order. There are several types of RNNs with different internal configurations; the basic implementation suffers from a lack of long term memory due to the vanishing gradient problem, thus it is rarely used over newer implementations.\n\nA long short-term memory (LSTM) network is a specific implementation of a RNN that is designed to deal with the vanishing gradient problem seen in simple RNNs, which would lead to them gradually \"forgetting\" about previous parts of an inputted sequence when calculating the output of a current part. LSTMs solve this problem with the addition of an elaborate system that uses an additional input/output to keep track of long term data. LSTMs have achieved very strong results across various fields, and were used by several monumental deep learning agents in games.\n\nReinforcement learning is the process of training an agent using rewards and/or punishments. The way an agent is rewarded or punished depends heavily on the problem; such as giving an agent a positive reward for winning a game or a negative one for losing. Reinforcement learning is used heavily in the field of machine learning and can be seen in methods such as Q-learning, policy search, Deep Q-networks and others. It has seen strong performance in both the field of games and robotics.\n\nNeuroevolution involves the use of both neural networks and evolutionary algorithms. Instead of using gradient descent like most neural networks, neuroevolution models make use of evolutionary algorithms to update neurons in the network. Researchers claim that this process is less likely to get stuck in a local minimum and is potentially faster than state of the art deep learning techniques.\n\nMachine learning agents have been used to take the place of a human player rather than function as NPCs, which are deliberately added into video games as part of designed gameplay. Deep learning agents have achieved impressive results when used in competition with both humans and other artificial intelligence agents.\n\nChess is a turn-based strategy game that is considered a difficult AI problem due to the computational complexity of its board space. Similar strategy games are often solved with some form of a Minimax Tree Search. These types of AI agents have been known to beat professional human players, such as the historic 1997 Deep Blue versus Garry Kasparov match. Since then, machine learning agents have shown ever greater success than previous AI agents.\n\nGo is another turn-based strategy game which is considered an even more difficult AI problem than chess. The state space of is Go is around 10^170 possible board states compared to the 10^120 board states for Chess. Prior to recent deep learning models, AI Go agents were only able to play at the level of a human amateur.\n\nGoogle's 2015 AlphaGo was the first AI agent to beat a professional Go player. AlphaGo used a deep learning model to train the weights of a Monte Carlo tree search (MCTS). The deep learning model consisted of 2 ANN, a policy network to predict the probabilities of potential moves by opponents, and a value network to predict the win chance of a given state. The deep learning model allows the agent to explore potential game states more efficiently than a vanilla MCTS. The network were initially trained on games of humans players and then were further trained by games against itself.\n\nAlphaGo Zero, another implementation of AlphaGo, was able to train entirely by playing against itself. It was able to quickly train up to the capabilities of the previous agent.\n\n\"StarCraft\" and its sequel \"\" are real-time strategy (RTS) video games that have become popular environments for AI research. Blizzard and DeepMind have worked together to release a public \"StarCraft 2\" environment for AI research to be done on. Various deep learning methods have been tested on both games, though most agents usually have trouble outperforming the default AI with cheats enable or skilled players of the game.\n\nAlphastar was the first AI agent to beat professional \"StarCraft 2\" players without any in-game advantages. The deep learning network of the agent initially received input from a simplified zoomed out version of the gamestate, but was later updated to play using a camera like other human players. The developers have not publicly released the code or architecture of their model, but have listed several state of the art machine learning techniques such as relational deep reinforcement learning, long short-term memory, auto-regressive policy heads, pointer networks, and centralized value baseline. Alphastar was initially trained with supervised learning, it watched replays of many human games in order to learn basic strategies. It then trained against different versions of itself and was improved through reinforcement learning. The final version was hugely successful, but only trained to play on a specific map in a protoss mirror matchup.\n\n\"Dota 2\" is a multiplayer online battle arena (MOBA) game. Like other complex games, traditional AI agents have not been able to compete on the same level as professional human player. The only widely published information on AI agents attempted on \"Dota 2\" is OpenAI's deep learning Five agent.\n\nOpenAI Five utilized separate LSTM networks to learn each hero. It trained using a reinforcement learning technique known as Proximal Policy Learning running on a system containing 256 GPUs and 128,000 CPU cores. Five trained for months, accumulating 180 years of game experience each day, before facing off with professional players. It was eventually able to beat the 2018 \"Dota 2\" esports champion team in a 2019 series of games.\n\n\"Planetary Annihilation\" is a real-time strategy game which focuses on massive scale war. The developers use ANNs in their default AI agent.\n\nThere have been attempts to make machine learning agents that are able to play more than one game. These \"general\" gaming agents are trained to understand games based on shared properties between them.\n\nAlphaZero is a modified version of AlphaGo Zero which is able to play Shogi, chess, and Go. The modified agent starts without only information basic rules of the game, and is also trained entirely through self-learning. DeepMind was able to train this generalized agent to be competitive with previous versions of itself on Go, as well as top agents in the other two games.\n\nMachine learning agents are often not covered in many game design courses. Previous use of machine learning agents in games may not have been very practical, as even the 2015 version of AlphaGo took hundreds of CPUs and GPUs to train to a strong level. This potentially limits the creation of highly effective deep learning agents to large corporations or extremely wealthy individuals. The extensive training time of neural network based approaches can also take weeks on these powerful machines.\n\nThe problem of effectively training ANN based models extends beyond powerful hardware environments; finding a good way to represent data and learn meaningful things from it is also often a difficult problem. ANN models often overfit to very specific data and perform poorly in more generalized cases. AlphaStar shows this weakness, despite being able to beat professional players, it is only able to do so on a single map when playing a mirror protoss matchup. OpenAI Five also shows this weakness, it was only able to beat professional player when facing a very limited hero pool out of the entire game. This example show how difficult it can be to train a deep learning agent to perform in more generalized situations.\n\nMachine learning agents have shown great success in a variety of different games. However, agents that are too competent also risk making games too difficult for new or casual players. Research has shown that challenge that is too far above a player's skill level will ruin lower player enjoyment. These highly trained agents are likely only desirable against very skilled human players who have many of hours of experience in a given game. Given these factors, highly effective deep learning agents are likely only a desired choice in games that have a large competitive scene, where they can function as an alternative practice option to a skilled human player.\n\nComputer vision focuses on training computers to gain a high-level understanding of digital images or videos. Many computer vision techniques also incorporate forms of machine learning, and have been applied on various video games. This application of computer vision focuses on interpreting game events using visual data. In some cases, artificial intelligence agents have used model-free techniques to learn to play games without any direct connection to internal game logic, solely using video data as input.\n\nAndrej Karpathy has demonstrated that relatively trivial neural network with just one hidden layer is capable of being trained to play \"Pong\" based on screen data alone.\n\nIn 2013, a team at DeepMind demonstrated the use of deep Q-learning to play a variety of Atari video games — \"Beamrider\", \"Breakout\", \"Enduro\", \"Pong\", \"Q*bert\", \"Seaquest\", and \"Space Invaders\" — from screen data.\n\n\"Doom\" (1993) is a first-person shooter (FPS) game. Student researchers from Carnegie Mellon University used computer vision techniques to create an agent that could play the game using only image pixel input from the game. The students used convolutional neural network (CNN) layers to interpret incoming image data and output valid information to a recurrent neural network which was responsible for outputting game moves.\n\nOther uses of vision-based deep learning techniques for playing games have included playing \"Super Mario Bros.\" only using image input, using deep Q-learning for training.\n\nMachine learning has seen research for use in content recommendation and generation. Procedural content generation is the process of creating data algorithmically rather than manually. This type of content is used to add replayability to games without relying on constant additions by human developers. PCG has been used in various games for different types of content generation, examples of which include weapons in \"Borderlands 2\", all world layouts in Minecraft and entire universes in \"No Man's Sky\". Common approaches to PCG include techniques that involve grammars, search-based algorithms, and logic programming. These approaches require humans to manually define the range of content possible, meaning that a human developer decides what features make up a valid piece of generated content. Machine learning is theoretically capable of learning these features when given examples to train off of, thus greatly reducing the complicated step of developers specifying the details of content design. Machine learning techniques used for content generation include Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN), Generative Adversarial networks (GAN), and K-means clustering. Not all of these techniques make use of ANNs, but the rapid development of deep learning has greatly increased the potential of techniques that do.\n\n\"Galactic Arms Race\" is a space shooter video game that uses neuroevolution powered PCG to generate unique weapons for the player. This game was a finalist in the 2010 Indie Game Challenge and its related research paper won the Best Paper Award at the 2009 IEEE Conference on Computational Intelligence and Games. The developers use a form of neuroevolution called cgNEAT to generate new content based on each player's personal preferences.\n\nEach generated item is represented by a special ANN known as a Compositional Pattern Producing Network (CPPNs). During the evolutionary phase of the game cgNEAT calculates the fitness of current items based on player usage and other gameplay metrics, this fitness score is then used decide which CPPNs will reproduce to create a new item. The ending result is the generation of new weapon effects based on the player's preference.\n\n\"Super Mario Bros.\" has been used by several researchers to simulate PCG level creation. Various attempts having used different methods. A version in 2014 used n-grams to generate levels similar to the ones it trained on, which was later improved by making use of MCTS to guide generation. These generations were often not optimal when taking gameplay metrics such as player movement into account, a separate research project in 2017 tried to resolve this problem by generating levels based on player movement using Markov Chains. These projects were not subjected to human testing and may not meet human playability standards.\n\nPCG level creation for \"The Legend of Zelda\" has been attempted by researchers at the University of California, Santa Cruz. This attempt made use of a Bayesian Network to learn high level knowledge from existing levels, while Principal Component Analysis (PCA) was used to represent the different low level features of these levels. The researchers used PCA to compare generated levels to human made levels and found that they were considered very similar. This test did not include playability or human testing of the generated levels.\n\nMusic is often seen in video games and can be a crucial element for influencing the mood of different situations and story points. Machine learning has seen use in the experimental field of music generation; it is uniquely suited to processing raw unstructured data and forming high level representations that could be applied to the diverse field of music. Most attempted methods have involved the use of ANN in some form. Methods include the use of basic feedforward neural networks, autoencoders, restricted boltzmann machines, recurrent neural networks, convolutional neural networks, generative adversarial networks (GANs), and compound architectures that use multiple methods.\n\nThe 2014 research paper on \"Variational Recurrent Auto-Encoders\" attempted to generate music based on songs from 8 different video games. This project is one of the few conducted purely on video game music. The neural network in the project was able to generate data that was very similar to the data of the games it trained off of. The generated data did not translate into good quality music.\n", "related": "NONE"}
{"id": "55631919", "url": "https://en.wikipedia.org/wiki?curid=55631919", "title": "DoNotPay", "text": "DoNotPay\n\nDoNotPay is a legal services chatbot founded by Joshua Browder, a British-American entrepreneur. The chatbot was originally built to contest parking tickets, but has expanded to include other services as well. As a \"robot lawyer,\" DoNotPay is a downloadable mobile app that makes use of artificial intelligence to provide legal services to all users free of charge. It is currently available in the United Kingdom and United States (all 50 states).\n\nDoNotPay has been featured by the BBC, NPR, NBC, \"Bloomberg\", \"Washington Times\", and many other major news outlets.\n\nDoNotPay had started off as an app for contesting parking tickets, but has since expanded to include features that help users with many different types of legal issues, ranging from consumer protection to immigration rights and other social issues. The \"robot lawyer\" makes use of automation to provide free legal consultation for the public. The application is supported by IBM's Watson computer.\n\nAs of October 2018, the app only allows for appealing small claims with a maximum limit of $25,000, but Browder plans to expand into more legal areas and add many more features in the near future. Browder claims that one of his major goals for DoNotPay is to eventually allow all members of society to have access to the same levels of legal representation. The app also allows users to file small claims with utility providers and other companies.\n\nIn 2015, DoNotPay was founded by Browder when he was 17 years old. Originally, Browder had created an app that allowed users in the United Kingdom to protest their parking tickets. Coverage for DoNotPay was then subsequently expanded to the United States, covering all 50 states.\n\nImmediately after its launch, Browder's DoNotPay application quickly became widely used by thousands of users and gained significant international media coverage. In 2016, \"The Guardian\" reported that the chatbot had successfully contested more than 250,000 parking tickets in London and New York and won 160,000 of them, all free of charge, claiming a success rate of over 60 percent.\n\nIn 2017, Browder launched 1,000 more bots to help with filling out transaction legal forms in the US and UK. DoNotPay has also expanded to include features that help users obtain refunds on flight tickets and hotel bookings, cancel free trials, sue people, and even offer legal services relating to social issues such as asylum applications and housing for the homeless.\n\nIn 2018, DoNotPay acquired Visabot, a chatbot that helps provide automated services to users seeking to obtain U.S. visas and green cards. Around the same time, DoNotPay also launched a service that helped users seek claims from Equifax during the aftermath of its security breach, a feature that has since been integrated into the DoNotPay app.\n\nAs of 2019, DoNotPay provides specialized advice for appealing parking tickets in locations such as New York City, Cambridge, Massachusetts, Chicago, Milwaukee, Sacramento, and UCSD.\n\nIn 2019, the DoNotPay application has even advised students at Stanford University, Browder's alma mater, to waive their Student Activities Fees. More recently, DoNotPay launched Free Trial Card, which gives users a virtual credit card number that can be used to sign up for free online trials such as Netflix and Spotify. As soon as the free trial period ends, the card automatically declines any charges, thus ending free trials without having to give up the cardholder's personal payment information.\n\nIn 2019, Browder obtained $4.6 million in funding from Silicon Valley investors such as Andreessen Horowitz and Founders Fund, who were early funders of Facebook.\n\n", "related": "\n- Artificial intelligence and law\n- Computational law\n- Legal expert systems\n- Legal informatics\n- Legal technology\n- Robot lawyer\n"}
{"id": "62295363", "url": "https://en.wikipedia.org/wiki?curid=62295363", "title": "Knowledge distillation", "text": "Knowledge distillation\n\nIn machine learning, knowledge distillation is the process of transferring knowledge from a large model to a smaller one. While large models (such as very deep neural networks or ensembles of many models) are able to better learn how to generalise from the data compared to smaller ones, they have the obvious drawback of being computationally expensive to evaluate, therefore limiting their applications to sufficiently powerful hardware. Knowledge distillation allows to transfer the knowledge learnt by a large model to a smaller one, that would not be able to easily learn it directly from the data itself, producing a new model that is faster to evaluate, and therefore deployable on less powerful hardware (such as a mobile device), while at the same time experiencing only a small loss of classification performance compared to the original, large model.\n\nKnowledge distillation has been successfully used in several applications of machine learning such as object detection, acoustic models, and natural language processing.\n\nWhat it is desired from a good model is good generalization capability, therefore transferring the knowledge from a large to a small model needs to somehow teach to the latter how to generalise in the same way as the former. If both models are trained on the same data, the small model will be worse at generalisation because of its inferior expressive power. However, some information of how a model generalise is encoded in the pseudo-probabilities assigned to its output: when a model correctly predicts a class, it assigns a large value to the output variable corresponding to such class, and smaller values to the other output variables. The distribution of values among the outputs for a record provides information on how the model tends to generalise, therefore the goal of successfully transferring knowledge can be achieved by training only the large model on the data, exploiting its better ability to learn generalisation, and then distilling such ability into the smaller model, that would not be able to learn it on its own, by training it to learn the soft output of the large model.\n\nModel compression, a methodology to compress the knowledge of multiple models into a single neural network, was introduced in 2006. Compression was achieved by training a smaller model on large amounts of pseudo-data labelled by a higher-performing ensemble, optimising to match the logit of the compressed model to the logit of the ensemble. Knowledge distillation is a generalisation of such approach, introduced by Geoffrey Hinton et al. in 2015, in a preprint that formulated the concept and showed some results achieved in the task of image classification.\n\nGiven a large model as a function of the vector variable formula_1, trained for a specific classification task, typically the final layer of the network is a softmax in the form\nwhere formula_3 is a parameter called \"temperature\", that for a standard softmax is normally set to 1. The softmax operator converts the logit values formula_4 to pseudo-probabilities, and higher values of temperature have the effect of generating a softer distribution of pseudo-probabilities among the output classes. Knowledge distillation consists of training a smaller network, called \"distilled model\", on a dataset called transfer set (different than the dataset used to train the large model) using as loss function the cross entropy between the output of the distilled model formula_5 and the output formula_6 produced by the large model on the same record (or the average of the individual outputs, if the large model is an ensemble), using a high value of softmax temperature formula_3 for both models\nIn this context, a high temperature increases the entropy of the output, and therefore provides more information to learn for the distilled model compared to hard targets, at the same time reducing the variance of the gradient between different records and therefore allowing higher learning rates.\n\nIf ground truth is available for the transfer set, the process can be strengthened by adding to the loss the cross entropy between the output of the distilled model (computed with formula_9) and the known label formula_10\nwhere the component of the loss with respect to the large model is weighted by a factor of formula_12 since, as the temperature increases, the gradient of the loss with respect to the model weights scales by a factor of formula_13.\n\nUnder the assumption that the logits have zero mean, it is possible to show that model compression is a special case of knowledge distillation. The gradient of the knowledge distillation loss formula_14 with respect to the logit of the distilled model formula_15 is given by\nwhere formula_17 are the logits of the large model. For large values of formula_3 this can be approximated as\nand under the zero-mean hypothesis formula_20 it becomes formula_21, which is the derivative of formula_22, i.e. the loss is equivalent to matching the logits of the two models, as done in model compression.\n\n\n- Distilling the knowledge in a neural network – Google AI\n- Knowledge distillation\n", "related": "NONE"}
{"id": "62285602", "url": "https://en.wikipedia.org/wiki?curid=62285602", "title": "Multi-agent learning", "text": "Multi-agent learning\n\nMulti-agent learning is the use of machine learning in a multi-agent system. Typically, agents improve their decisions via experience. In particular, an agent has to learn how to coordinate with the other agents.\n\nAccording to an article by Shoham et al. in 2007, it is difficult to pinpoint all relevant articles in the domain. There are some inherent difficulties about multi-agent deep reinforcement learning. The environment is not stationnary anymore, thus the Markov property is violated: transitions and rewards does not only depend on the current state of an agent.\n", "related": "NONE"}
{"id": "62683332", "url": "https://en.wikipedia.org/wiki?curid=62683332", "title": "Fairness (machine learning)", "text": "Fairness (machine learning)\n\nIn machine learning, a given algorithm is said to be fair, or to have fairness if its results are independent of some variables we consider to be sensitive and not related with it (f.e.: gender, ethnicity, sexual orientation, etc.).\n\nResearch about fairness in machine learning is a relatively recent topic. Most of the articles about it have been written in the last three years. Some of the most important facts in this topic are the following:\n- In 2018, IBM introduces AI Fairness 360, a Python library with several algorithms to reduce bias in a program, increasing its fairness.\n- Facebook made public, in 2018, their use of a tool, Fairness Flow, to detect bias in their AI. However, said tool code is not accessible, and it is not known if it really corrects this bias.\n- In 2019, Google publishes a set of tools in Github to study the effects of fairness in the long run.\n\nThe algorithms used for assuring fairness are still being improved. However, the main progress in this area is that some big corporations are realising the importance the reduction of algorithm bias will have on the society. \n\nIn classification problems, an algorithm learns a function to predict a discrete characteristic formula_1, the target variable, from known characteristics formula_2. We model formula_3 as a discrete random variable which encodes some characteristics contained or implicitly encoded in formula_2 that we consider as sensitive characteristics (gender, ethnicity, sexual orientation, etc.). We finally denote by formula_5 the prediction of the classifier.\nNow let us define three main criteria to evaluate if a given classifier is fair, that is, if its predictions are not influenced by some of these sensitive variables.\n\nWe say the random variables formula_6 satisfy independence if the sensitive characteristics formula_3 are statistically independent to the prediction formula_5, and we write formula_9.\n\nWe can also express this notion with the following formula:\nformula_10\nThis means that the probability of being classified by the algorithm in each of the groups is equal for two individuals with different sensitive characteristics.\n\nYet another equivalent expression for independence can be given using the concept of mutual information between random variables, defined as\nformula_11\nIn this formula, formula_12 of the random variable. Then formula_13 satisfy independence if formula_14.\n\nA possible relaxation of the indepence definition include introducing a positive slack formula_15 and is given by the formula:\nformula_16\n\nFinally, another possible relaxation is to require formula_17.\n\nWe say the random variables formula_18 satisfy separation if the sensitive characteristics formula_3 are statistically independent to the prediction formula_5 given the target value formula_1, and we write formula_22.\n\nWe can also express this notion with the following formula:\nformula_23\nThis means that the probability of being classified by the algorithm in each of the groups is equal for two individuals with different sensitive characteristics given that they actually belong in the same group (have the same target variable).\n\nAnother equivalent expression, in the case of a binary target rate, is that the true positive rate and the false positive rate are equal (and therefore the false negative rate and the true negative rate are equal) for every value of the sensitive characteristics:\nformula_24\nformula_25\n\nFinally, a possible relaxation of the given definitions is the difference between rates to be a positive number lower than a given slack formula_15, instead of equals to zero.\n\nWe say the random variables formula_18 satisfy sufficiency if the sensitive characteristics formula_3 are statistically independent to the target value formula_1 given the prediction formula_5, and we write formula_31.\n\nWe can also express this notion with the following formula:\nformula_32\nThis means that the probability of actually being in each of the groups is equal for two individuals with different sensitive characteristics given that they were predicted to belong to the same group.\n\nFinally, we sum up some of the main results that relate the three definitions given above:\n\n- If formula_3 and formula_1 are not statistically independent, then sufficiency and independence cannot both hold.\n- Assuming formula_1 is binary, if formula_3 and formula_1 are not statistically independent, and formula_5 and formula_1 are not statistically independent either, then independence and separation cannot both hold.\n- If formula_18 as a joint distribution has positive probability for all its possible values and formula_3 and formula_1 are not statistically independent, then separation and sufficiency cannot both hold.\n\nMost statistical measures of fairness rely on different metrics, so we will start by defining them. When working with a binary classifier, both the predicted and the actual classes can take two values: positive and negative. Now let us start explaining the different possible relations between predicted and actual outcome:\n- True positive (TP): The case where both the predicted and the actual outcome are in the positive class.\n- True negative (TN): The case where both the predicted and the actual outcome are in the negative class.\n- False positive (FP): A case predicted to be in the positive class when the actual outcome is in the negative one.\n- False negative (FN): A case predicted to be in the negative class when the actual outcome is in the positive one.\nThis relations can be easily represented with a confusion matrix, a table which describes the accuracy of a classification model. In this matrix, columns and rows represent instances of the predicted and the actual cases, respectively.\n\nBy using this relations, we can define multiple metrics which can be later used to measure the fairness of an algorithm:\n- Positive predicted value (PPV): the fraction of positive cases which were correctly predicted out of all the positive predictions. It is usually referred to as precision, and represents the probability of a positive prediction to be right. It is given by the following formula:\nformula_43\n- False discovery rate (FDR): the fraction of positive predictions which were actually negative out of all the positive predictions. It represents the probability of a positive prediction to be wrong, and it is given by the following formula:\nformula_44\n- Negative predicted value (NPV): the fraction of negative cases which were correctly predicted out of all the negative predictions. It represents the probability of a negative prediction to be right, and it is given by the following formula:\nformula_45\n- False omission rate (FOR): the fraction of negative predictions which were actually positive out of all the negative predictions. It represents the probability of a negative prediction to be wrong, and it is given by the following formula:\nformula_46\n- True positive rate (TPR): the fraction of positive cases which were correctly predicted out of all the positive cases. It is usually referred to as sensitivity or recall, and it represents the probability of the positive subjects to be classified correctly as such. It is given by the formula:\nformula_47\n- False negative rate (FNR): the fraction of positive cases which were incorrectly predicted to be negative out of all the positive cases. It represents the probability of the positive subjects to be classified incorrectly as negative ones, and it is given by the formula:\nformula_48\n- True negative rate (TNR): the fraction of negative cases which were correctly predicted out of all the negative cases. It represents the probability of the negative subjects to be classified correctly as such, and it is given by the formula:\nformula_49\n- False positive rate (FPR): the fraction of negative cases which were incorrectly predicted to be positive out of all the negative cases. It represents the probability of the negative subjects to be classified incorrectly as positive ones, and it is given by the formula:\nformula_50\n\nThe following criteria can be understood as measures of the three definitions given in the first section, or a relaxation of them. In the table to the right, we can see the relationships between them.\n\nTo define this measures specifically, we will divide them into three big groups as done in Verma et al.: definitions based on predicted outcome, on predicted and actual outcomes, and definitions based on predicted probabilities and actual outcome.\n\nWe will be working with a binary classifier and the following notation: formula_51 refers to the score given by the classifier, which is the probability of a certain subject to be in the positive or the negative class.formula_5 represents the final classification predicted by the algorithm, and its value is usually derived from formula_51, for example will be positive when formula_51 is above a certain threshold.formula_1 represents the actual outcome, that is, the real classification of the individual and, finally, formula_3 denotes the sensitive attributes of the subjects.\n\nThe definitions in this section focus on a predicted outcome formula_5 for various distributions of subjects. They are the simplest and most intuitive notions of fairness.\n\n- Group fairness, also referred to as statistical parity, demographic parity, acceptance rate and benchmarking. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal probability of being assigned to the positive predicted class. This is, if the following formula is satisfied:\nformula_58\n\n- Conditional statistical parity. Basically consists in the definition above, but restricted only to a subset of the attributes. With mathematical notation this would be:\nformula_59\n\nThis definitions not only consider de predicted outcome formula_5 but also compare it to the actual outcome formula_1.\n\n- Predictive parity, also referred to as outcome test. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV. This is, if the following formula is satisfied:\nformula_62\nformula_63\n\n- False positive error rate balance, also referred to as predictive equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have aqual FPR. This is, if the following formula is satisfied:\nformula_64\nformula_65\n\n- False negative error rate balance, also referred to as equal opportunity. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal FNR. This is, if the following formula is satisfied:\nformula_66\nformula_67\n\n- Equalized odds, also referred to as conditional procedure accuracy equality and disparate mistreatment. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal TPR and equal FPR, satisfying the formula:\nformula_68\n\n- Conditional use accuracy equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV and equal NPV, satisfying the formula:\nformula_69\n\n- Overall accuracy equality. A classifier satisfies this definition if the subject in the protected and unprotected groups have equal prediction accuracy, that is, the probability of a subject from one class to be assigned to it. This is, if it satisfies the following formula:\nformula_70\n\n- Treatment equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have an equal ratio of FN and FP, satisfying the formula:\nformula_71\n\nThese definitions are based in the actual outcome formula_1 and the predicted probability score formula_51.\n\n- Test-fairness, also known as calibration or matching conditional frequencies. A classifier satisfies this definition if individuals with the same predicted probability score formula_51 have the same probability to be classified in the positive class when they belong to either the protected or the unprotected group:\nformula_75\n\n- Well-calibration is an extension of the previous definition. It states that when individuals inside or outside the protected group have the same predicted probability score formula_51 they must have the same probability of being classified in the positive class, and this probability must be equal to formula_51:\nformula_78\n\n- Balance for positive class. A classifier satisfies this definition if the subjects constituting the positive class from both protected and unprotected groups have equal average predicted probability score formula_51. This means that the expected value of probability score for the protected and unprotected groups with positive actual outcome formula_1 is the same, satisfying the formula:\nformula_81\n\n- Balance for negative class. A classifier satisfies this definition if the subjects constituting the negative class from both protected and unprotected groups have equal average predicted probability score formula_51. This means that the expected value of probability score for the protected and unprotected groups with negative actual outcome formula_1 is the same, satisfying the formula:\nformula_84\n\nFairness can be applied to machine learning algorithms in three different ways: preprocessing the data used in the algorithm, optimization during the training, or post-processing the answers of the algorithm.\n\nUsually, the classifier is not the only problem, the dataset is also biased. The discrimination of a dataset formula_85 with respect to the group formula_86 can be defined as follows:\nformula_87\n\nThat is, an approximation to the difference between the probabilities of belonging in the positive class given that the subject has a protected characteristic different from formula_88 and equal to formula_88.\n\nAlgorithms correcting bias at preprocessing remove information concerning variables in the dataset which can result in unfair decisions of the AI, while trying to alter just the bare minimum of this data. This is not as easy as just removing the sensitive variable, because other attributes can be related to the protected one.\n\nA way to do this is by mapping each individual in the initial dataset into an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group, while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm.\nThis way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesn't belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classificator.\n\nAn example is explained in Zemel et al. where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all the information except those that can lead to biased decisions, and to obtain a prediction as accurate as possible.\n\nOn the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness.\n\nReweighing is an example of preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group.\n\nIf the dataset formula_85 was unbiased the sensitive variable formula_3 and the target variable formula_1 would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows:\nformula_93\n\nIn reality, however, the dataset is not unbiased and the variables are not statistically independent so the observed probability is:\nformula_94\n\nTo compensate for the bias, lower weights to favored objects and higher weights to unfavored objects will be assigned. For each formula_95 we get:\nformula_96\n\nWhen we have for each formula_2 a weight associated formula_98 we compute the weighted discrimination with respect to group formula_86 as follows:\nformula_100\n\nIt can be shown that after reweighting this weighted discrimination is 0.\n\nAnother approach is correcting the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm. These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group.\n\nThe main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed.\n\nThis technique obtains good results in improving fairness while keeping high accuracy and lets the programmer choose the fairness measures to improve. However, each machine learning task may need a different method to be applied and the code in the classifier needs to be modified, which is not always possible.\n\nWe train two classifiers at the same time through some gradient-based method (f.e.: gradient descent). The first one, the \"predictor\" tries to accomplish the task of predicting formula_1, the target variable, given formula_2, the input, by modifying its weights formula_103 to minimize some loss function formula_104. The second one, the \"adversary\" tries to accomplish the task of predicting formula_3, the sensitive variable, given formula_106 by modifying its weights formula_107 to minimize some loss function formula_108.\n\nAn important point here is that, in order to propagate correctly, formula_106 above must refer to the raw output of the classifier, not the discrete prediction; for example, with an artificial neural network and a classification problem, formula_106 could refer to the output of the softmax layer.\n\nThen we update formula_107 to minimize formula_112 at each training step according to the gradient formula_113 and we modify formula_103 according to the expression:\nformula_115\nwhere formula_116 is a tuneable hyperparameter that can vary at each time step.\nThe intuitive idea is that we want the \"predictor\" to try to minimize formula_117 (therefore the term formula_118) while, at the same time, maximize formula_112 (therefore the term formula_120), so that the \"adversary\" fails at predicting the sensitive variable from formula_106.\n\nThe term formula_122 prevents the \"predictor\" from moving in a direction that helps the \"adversary\" decrease its loss function.\n\nIt can be shown that training a \"predictor\" classification model with this algorithm improves demographic parity with respect to training it without the \"adversary\".\n\nThe final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive answer, while low scores are likely to get a negative answer, but we need to adjust the threshold to determine when to answer yes or no depending on our needs. Note that variations in the threshold affect the trade-off between true positive rate and true negative rate.\n\nIf the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but this type of classifiers tend to be biased, so we may need to set a different threshold for each protected group to achieve fairness. A way to do this is plotting the true positive rate against the false negative rate at various threshold settings (this is called ROC curve) and check which threshold satisfies that the rates are equal for the protected group and the rest of the individuals.\n\nThe advantages of postprocessing include that the technique can be applied after any classifiers, without modifying it, and has a good performance in fairness measures. The cons are the need to access to the protected attribute in test time and the lack of choice in the balance between accuracy and fairness.\n\nGiven a classifier let formula_123 be the probability computed by the classifiers as the probability that the instance formula_2 belongs to the positive class +. When formula_123 is close to 1 or to 0, the instance formula_2 is specified with high degree of certainty to belong to class + or - respectively. However, when formula_123 is closer to 0.5 the classification is more unclear.\n\nWe say formula_2 is a \"rejected instance\" if formula_129 with a certain formula_130 such that formula_131.\n\nThe algorithm of \"ROC\" consists on classifying the non-rejected instances following the rule above and the rejected instances as follows: if the instance is an example of a deprived group (formula_132) then label it as positive, otherwise, label it as negative.\n\nWe can optimize different measures of discrimination (link) as functions of formula_130 to find the optimal formula_130 for each problem and avoid becoming discriminatory against the privileged group.\n\n", "related": "\n- Algorithmic bias\n- Machine learning\n"}
{"id": "62817500", "url": "https://en.wikipedia.org/wiki?curid=62817500", "title": "Leakage (machine learning)", "text": "Leakage (machine learning)\n\nIn statistics and machine learning, leakage (also data leakage, or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause modeler to select a suboptimal model, which otherwise could be outperformed by a leakage-free model.\nLeakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model: features and training examples.\n\nColumn-wise leakage is caused by the inclusion of columns which are one of: a duplicate label, a proxy for the label, or the label itself, when training the model which are not available at prediction time (anachronisms). This can include leaks which partially give away the label. \n\nFor example, including a \"MonthySalary\" column when predicting \"YearlySalary\"; or \"MinutesLate\" when predicting \"IsLate\"; or more subtly \"NumOfLatePayments\" when predicting \"ShouldGiveLoan\".\n\nRow-wise leakage leakage is caused by improper sharing of information between rows of data.\n\nData leakage types:\n- Premature featurization; leaking from premature featurization before CV/TrainTest split (must fit MinMax/ngrams/etc on only the train split, then transform the test set)\n- Duplicate rows between train/validation/test (e.g. oversampling a dataset to pad its size before splitting; e.g. different rotations/augmentations of an single image; bootstrap sampling before splitting; or duplicating rows to up sample the minority class)\n- Non-i.i.d. data\n- Time leakage (e.g. splitting a time-series dataset randomly instead of newer data in test set using a TrainTest split or rolling-origin cross validation)\n- Group leakage -- not including a grouping split column (e.g. Andrew Ng's group had 100k x-rays of 30k patients, meaning ~3 images per patient. The paper used random splitting instead of ensuring that all images of a patient was in the same split. Hence the model partially memorized the patients instead of learning to recognize pneumonia in chest x-rays. Revised paper had a drop in scores.)\n\nFor time-dependent datasets, the structure of the system being studied evolves over time (i.e. it is \"non-stationary\"). This can introduce systematic differences between the training and validation sets. For example, if a model for predicting stock values is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year. \n\n", "related": "\n- AutoML\n- Cross-validation\n- Overfitting\n- Resampling (statistics)\n- Supervised learning\n- Training, validation, and test sets\n"}
{"id": "49239240", "url": "https://en.wikipedia.org/wiki?curid=49239240", "title": "VaultML", "text": "VaultML\n\nVault AI is an Israeli–based artificial intelligence company that lays claims to have created technologies that can \"read\" movie and TV screenplays in order to predict box office and investment performance. Part of the process reportedly entails analyzing 300,000 to 400,000 elements from the script, which could be anything from plot, character development, script structure, scene events. The founders are made up of high frequency trading veterans and state they use similar approaches to predicting film performance. Vault published its 2015 film predictions for over 20 movies in early 2015 and successfully predicted correctly many box office performances throughout that year. Vault's algorithms out earned the market on a return on investment basis.\n", "related": "NONE"}
{"id": "52003586", "url": "https://en.wikipedia.org/wiki?curid=52003586", "title": "End-to-end reinforcement learning", "text": "End-to-end reinforcement learning\n\nIn end-to-end reinforcement learning, the entire process from sensors to motors in a robot or agent (called the end-to-end process) involves a single, layered or recurrent neural network without modularization, and is trained by reinforcement learning (RL). The approach has been proposed for a long time, but was reenergized by the successful results in learning to play Atari video games (2013–15) and AlphaGo (2016) by Google DeepMind.\n\nRL traditionally required explicit design of state space and action space, while the mapping from state space to action space is learned. Therefore, RL has been limited to learning only for action, and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning. Neural networks have been often used in RL, to provide non-linear function approximation to avoid the curse of dimensionality. Recurrent neural networks have been also employed, mainly to avoid perceptual aliasing or partially observable Markov decision process (POMDP).\n\nEnd-to-end RL extends RL from learning only for actions to learning the entire process from sensors to motors including higher-level functions that are difficult to develop independently from other functions. Higher-level functions do not connect directly with either sensors or motors, and so even giving their inputs and outputs is difficult.\n\nThe approach originated in TD-Gammon (1992). In backgammon, the evaluation of the game situation during self-play was learned through TD(formula_1) using a layered neural network. Four inputs were used for the number of pieces of a given color at a given location on the board, totaling 198 input signals. With zero knowledge built in, the network learned to play the game at an intermediate level.\n\nShibata began working with this framework in 1997. They employed Q-learning and actor-critic for continuous motion tasks, and used a recurrent neural network for memory-required tasks. They applied this framework to some real robot tasks. They demonstrated learning of various functions.\n\nBeginning around 2013, Google DeepMind showed impressive learning results in video games and game of Go (AlphaGo). They used a deep convolutional neural network that showed superior results in image recognition. They used 4 frames of almost raw RGB pixels (84x84) as inputs. The network was trained based on RL with the reward representing the sign of the change in the game score. All 49 games were learned using the same network architecture and Q-learning with minimal prior knowledge, and outperformed competing methods on almost all the games and performed at a level that is comparable or superior to a professional human game tester. It is sometimes called Deep-Q network (DQN). In AlphaGo, deep neural networks are trained not only by reinforcement learning, but also by supervised learning and Monte Carlo tree search.\n\nShibata's group showed that various functions emerge in this framework, including: \n\n- Image recognition\n- Color constancy (optical illusion)\n- Sensor motion (active recognition)\n- Hand-eye coordination and hand reaching movement\n- Explanation of brain activities\n- Knowledge transfer\n- Memory\n- Selective attention\n- Prediction\n- Exploration\n\nCommunications were established in this framework. Modes include:\n\n- Dynamic communication (negotiation)\n- Binalization of signals\n- Grounded communication using a real robot and camera\n", "related": "NONE"}
{"id": "53769524", "url": "https://en.wikipedia.org/wiki?curid=53769524", "title": "Matroid (company)", "text": "Matroid (company)\n\nMatroid, Inc. is a Computer Vision company that offers a platform for creating computer vision models, called detectors, to search visual media for objects, persons, events, emotions, and actions. Matroid provides real-time notifications once the object of interest has been detected, as well as the ability to search past events.\n\nMatroid was founded in 2016 by Reza Zadeh, a Stanford professor. The company raised $3.5 million in Series A funding from New Enterprise Associates in 2016, and an additional $10 million from Intel in 2017.\n\nOnce a detector has been trained using the Matroid GUI, it automatically finds the objects of interest in real-time video and archived footage. Users can explore detection information via reports, notifications, or a calendar interface to view events and identify trends. Matroid’s functionality is also exposed via a developer API.\n\nSupported hardware platforms:\n\n- On-cloud: www.matroid.com, allows for scaling based on workload\n\n- On-prem: contains the same functionality of www.matroid.com in a secure, offline environment for applications where data privacy and security are key concerns\n- On-device: runs on embedded devices such as cameras, sensors, etc.\n\nThe company has a range of customers in Media, Security, Healthcare, Industrial IoT, AI chip, and other industries.\nMatroid annually holds a conference, Scaled Machine Learning, where technical speakers lead discussions about running and scaling machine learning algorithms, artificial intelligence, and computing platforms, such as GPUs, CPUs, TPUs, & the nascent AI chip industry.\n\nPast speakers include Turing Award Winners, creators of Keras, TensorFlow, PyTorch, Caffe, OpenAI, Kubernetes, Horovod, Allen Institute for AI, Apache Spark, Apache Arrow, MLPerf, Matroid, and others.\n\n2019 - Matroid was selected by Gartner, Inc. as a “Cool Vendor” for Cool Vendors in AI Core Technologies.\n\n2018 - Matroid announced a partnership with HP for their on-prem platform. Matroid certified a selection of HP Z computers as Computer-Vision-Ready (CV-Ready) for monitoring video streams. \n\n2018 - Oracle announced their software integration with Matroid to provide real-time and analytics based on people monitoring.\n\n2016 - Matroid was awarded a Best Paper Award at KDD 2016. \n\nTogether with Stanford Hospital and hospitals in Hong Kong, India, and Nepal, Matroid used computer vision in the field of Ophthalmology. The company created a model that learns to predict glaucoma from areas of the eye previously ignored during diagnosis, specifically the Lamina Cribrosa, as no established automated metrics existed for this region yet. Matroid is able to detect glaucoma on OCT scans of the eye, with an F1 score of 96% and similar AUC and accuracy.\n\nFusionNet was released as a leading neural networks architecture at the Princeton ModelNet competition.  It is a fusion of three convolutional neural networks, one trained on pixel representation and two networks trained on voxelized objects. It exploits the strength of each component network in order to improve the classification performance. Each component network of FusionNet considers multiple views or orientations of each object before classifying it. While it is intuitive that one can get more information from multiple views of the object than a single view, it is not trivial to put the information together in order to enhance the accuracy. Matroid used information from 20 views for pixel representation and 60 CAD object orientations for voxel representation before predicting the object class. FusionNet outperformed the current leading submission on the Princeton ModelNet leaderboard in both the 10 class and the 40 class datasets.\n\nMatroid released a book with co-author Bharath Ramsundar, TensorFlow for Deep Learning. It introduces the fundamentals of machine learning through TensorFlow and explains how to use TensorFlow to build systems capable of detecting objects in images, understanding human text, and predicting the properties of potential medicines.\n", "related": "NONE"}
{"id": "60929882", "url": "https://en.wikipedia.org/wiki?curid=60929882", "title": "Flux (machine-learning framework)", "text": "Flux (machine-learning framework)\n\nFlux is an open-source machine-learning library and ecosystem written in Julia. Its current stable release is v0.10.3. Flux uses Julia's language features, such as just-ahead-of-time compilation, to provide an interface to users. It has a layer-stacking-based interface for simpler models, and can be readily integrated with other Julia packages. For example, GPU support is supplied transparently by CuArrays.jl, due to Julia's multiple dispatch. This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.\n\nThis feature has been used, for example, to implement support for Neural Differential Equations, by fusing Flux and DifferentialEquations.jl into DiffEqFlux.jl.\n\nFlux supports recurrent and convolutional networks. It is also capable of Differentiable programming through its source-to-source Automatic differentiation package, Zygote.\n\nJulia is a popular machine-learning language in Github and Flux is appointed as its most highly regarded machine-learning repository. A demonstration compiling Julia code to run in Google's Tensor processing unit received praise from Google Brain AI lead Jeff Dean.\n\nFlux was employed to the first application of machine-learning to data encrypted with Homomorphic encryption without ever decrypting it. This kind of application is envisioned to be central for privacy to future API using machine-learning models.\n\nFlux.jl is an intermediate representation for running high level programs on CUDA hardware. It was the predecessor to CUDAnative.jl which is also a GPU programming language.\n\n", "related": "\n- Differentiable programming\n- Comparison of deep-learning software\n"}
