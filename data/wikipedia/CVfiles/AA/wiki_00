{"id": "346382", "url": "https://en.wikipedia.org/wiki?curid=346382", "title": "Image analysis", "text": "Image analysis\n\nImage analysis is the extraction of meaningful information from images; mainly from digital images by means of digital image processing techniques. Image analysis tasks can be as simple as reading bar coded tags or as sophisticated as identifying a person from their face.\n\nComputers are indispensable for the analysis of large amounts of data, for tasks that require complex computation, or for the extraction of quantitative information. On the other hand, the human visual cortex is an excellent image analysis apparatus, especially for extracting higher-level information, and for many applications — including medicine, security, and remote sensing — human analysts still cannot be replaced by computers. For this reason, many important image analysis tools such as edge detectors and neural networks are inspired by human visual perception models.\n\nDigital Image Analysis or Computer Image Analysis is when a computer or electrical device automatically studies an image to obtain useful information from it. Note that the device is often a computer but may also be an electrical circuit, a digital camera or a mobile phone. \nIt involves the fields of computer or machine vision, and medical imaging, and makes heavy use of pattern recognition, digital geometry, and signal processing. This field of computer science developed in the 1950s at academic institutions such as the MIT A.I. Lab, originally as a branch of artificial intelligence and robotics.\n\nIt is the quantitative or qualitative characterization of two-dimensional (2D) or three-dimensional (3D) digital images. 2D images are, for example, to be analyzed in computer vision, and 3D images in medical imaging. The field was established in the 1950s—1970s, for example with pioneering contributions by Azriel Rosenfeld, Herbert Freeman, Jack E. Bresenham, or King-Sun Fu.\n\nThere are many different techniques used in automatically analysing images. Each technique may be useful for a small range of tasks, however there still aren't any known methods of image analysis that are generic enough for wide ranges of tasks, compared to the abilities of a human's image analysing capabilities. Examples of image analysis techniques in different fields include:\n- 2D and 3D object recognition,\n- image segmentation,\n- motion detection e.g. Single particle tracking,\n- video tracking,\n- optical flow,\n- medical scan analysis,\n- 3D Pose Estimation,\n- automatic number plate recognition.\n\nThe applications of digital image analysis are continuously expanding through all areas of science and industry, including:\n- assay micro plate reading, such as detecting where a chemical was manufactured.\n- astronomy, such as calculating the size of a planet.\n- defense\n- error level analysis\n- filtering\n- machine vision, such as to automatically count items in a factory conveyor belt.\n- materials science, such as determining if a metal weld has cracks.\n- medicine, such as detecting cancer in a mammography scan.\n- metallography, such as determining the mineral content of a rock sample.\n- microscopy, such as counting the germs in a swab.\n- optical character recognition, such as automatic license plate detection.\n- remote sensing, such as detecting intruders in a house, and producing land cover/land use maps.\n- robotics, such as to avoid steering into an obstacle.\n- security, such as detecting a person's eye color or hair color.\n\n\"Object-Based Image Analysis\" (OBIA) employs two main processes, segmentation and classification. Traditional image segmentation is on a per-pixel basis. However, OBIA groups pixels into homogeneous objects. These objects can have different shapes and scale. Objects also have statistics associated with them which can be used to classify objects. Statistics can include geometry, context and texture of image objects. The analyst defines statistics in the classification process to generate for example land cover. The technique is implemented in software such as eCognition or the Orfeo toolbox.\n\nWhen applied to earth images, OBIA is known as \"Geographic Object-Based Image Analysis\" (GEOBIA), defined as \"a sub-discipline of geoinformation science devoted to (...) partitioning remote sensing (RS) imagery into meaningful image-objects, and assessing their characteristics through spatial, spectral and temporal scale\".\nThe international GEOBIA conference has been held biannually since 2006.\n\nObject-based image analysis is also applied in other fields, such as cell biology or medicine. It can for instance detect changes of cellular shapes in the process of cell differentiation.\n\nLand cover and land use change detection using remote sensing and geospatial data provides baseline information for assessing the climate change impacts on habitats and biodiversity, as well as natural resources, in the target areas.\n\n- Application of land cover mapping:\n- Local and regional planning\n- Disaster management\n- Vulnerability and Risk Assessments\n- Ecological management\n- Monitoring the effects of climate change\n- Wildlife management.\n- Alternative landscape futures and conservation\n- Environmental forecasting\n- Environmental impact assessment\n- Policy development\n\n", "related": "\n- Archeological imagery\n- Imaging technologies\n- Image processing\n- Military intelligence\n- Remote sensing\n\n- \"The Image Processing Handbook\" by John C. Russ, (2006)\n- \"Image Processing and Analysis - Variational, PDE, Wavelet, and Stochastic Methods\" by Tony F. Chan and Jianhong (Jackie) Shen, (2005)\n- \"Front-End Vision and Multi-Scale Image Analysis\" by Bart M. ter Haar Romeny, Paperback, (2003)\n- \"Practical Guide to Image Analysis\" by J.J. Friel, et al., ASM International, (2000).\n- \"Fundamentals of Image Processing\" by Ian T. Young, Jan J. Gerbrands, Lucas J. Van Vliet, Paperback, (1995)\n- \"Image Analysis and Metallography\" edited by P.J. Kenny, et al., International Metallographic Society and ASM International (1989).\n- \"Quantitative Image Analysis of Microstructures\" by H.E. Exner & H.P. Hougardy, DGM Informationsgesellschaft mbH, (1988).\n- \"Metallographic and Materialographic Specimen Preparation, Light Microscopy, Image Analysis and Hardness Testing\", Kay Geels in collaboration with Struers A/S, ASTM International 2006.\n"}
{"id": "1571780", "url": "https://en.wikipedia.org/wiki?curid=1571780", "title": "Condensation algorithm", "text": "Condensation algorithm\n\nThe condensation algorithm (Conditional Density Propagation) is a computer vision algorithm. The principal application is to detect and track the contour of objects moving in a cluttered environment. Object tracking is one of the more basic and difficult aspects of computer vision and is generally a prerequisite to object recognition. Being able to identify which pixels in an image make up the contour of an object is a non-trivial problem. Condensation is a probabilistic algorithm that attempts to solve this problem.\n\nThe algorithm itself is described in detail by Isard and Blake in a publication in the \"International Journal of Computer Vision\" in 1998. One of the most interesting facets of the algorithm is that it does not compute on every pixel of the image. Rather, pixels to process are chosen at random, and only a subset of the pixels end up being processed. Multiple hypotheses about what is moving are supported naturally by the probabilistic nature of the approach. The evaluation functions come largely from previous work in the area and include many standard statistical approaches. The original part of this work is the application of particle filter estimation techniques.\n\nThe algorithm’s creation was inspired by the inability of Kalman filtering to perform object tracking well in the presence of significant background clutter. The presence of clutter tends to produce probability distributions for the object state which are multi-modal and therefore poorly modeled by the Kalman filter. The condensation algorithm in its most general form requires no assumptions about the probability distributions of the object or measurements.\n\nThe condensation algorithm seeks to solve the problem of estimating the conformation of an object described by a vector formula_1 at time formula_2, given observations formula_3 of the detected features in the images up to and including the current time. The algorithm outputs an estimate to the state conditional probability density formula_4 by applying a nonlinear filter based on factored sampling and can be thought of as a development of a Monte-Carlo method. formula_4 is a representation of the probability of possible conformations for the objects based on previous conformations and measurements. The condensation algorithm is a generative model since it models the joint distribution of the object and the observer.\n\nThe conditional density of the object at the current time formula_4 is estimated as a weighted, time-indexed sample set formula_7 with weights formula_8. N is a parameter determining the number of sample sets chosen. A realization of formula_4 is obtained by sampling with replacement from the set formula_10 with probability equal to the corresponding element of formula_11.\n\nThe assumptions that object dynamics form a temporal Markov chain and that observations are independent of each other and the dynamics facilitate the implementation of the condensation algorithm. The first assumption allows the dynamics of the object to be entirely determined by the conditional density formula_12. The model of the system dynamics determined by formula_12 must also be selected for the algorithm, and generally includes both deterministic and stochastic dynamics.\n\nThe algorithm can be summarized by initialization at time formula_14 and three steps at each time \"t\":\n\nForm the initial sample set and weights by sampling according to the prior distribution. For example, specify as Gaussian and set the weights equal to each other.\n\n1. Sample with replacement formula_15 times from the set formula_16 with probability formula_17 to generate a realization of formula_4.\n2. Apply the learned dynamics formula_12 to each element of this new set, to generate a new set formula_20.\n3. To take into account the current observation formula_21, set formula_22 for each element formula_20.\n\nThis algorithm outputs the probability distribution formula_4 which can be directly used to calculate the mean position of the tracked object, as well as the other moments of the tracked object.\n\nCumulative weights can instead be used to achieve a more efficient sampling.\n\nSince object-tracking can be a real-time objective, consideration of algorithm efficiency becomes important. The condensation algorithm is relatively simple when compared to the computational intensity of the Ricatti equation required for Kalman filtering. The parameter formula_15, which determines the number of samples in the sample set, will clearly hold a trade-off in efficiency versus performance.\n\nOne way to increase efficiency of the algorithm is by selecting a low degree of freedom model for representing the shape of the object. The model used by Isard 1998 is a linear parameterization of B-splines in which the splines are limited to certain configurations. Suitable configurations were found by analytically determining combinations of contours from multiple views, of the object in different poses, and through principal component analysis (PCA) on the deforming object.\n\nIsard and Blake model the object dynamics formula_12 as a second order difference equation with deterministic and stochastic components: formula_27\n\nwhere formula_28 is the mean value of the state, and formula_29, formula_30 are matrices representing the deterministic and stochastic components of the dynamical model respectively. formula_29, formula_30, and formula_28 are estimated via Maximum Likelihood Estimation while the object performs typical movements.\n\nThe observation model formula_34 cannot be directly estimated from the data, requiring assumptions to be made in order to estimate it. Isard 1998 assumes that the clutter which may make the object not visible is a Poisson random process with spatial density formula_35 and that any true target measurement is unbiased and normally distributed with standard deviation formula_36.\n\nThe basic condensation algorithm is used to track a single object in time. It is possible to extend the condensation algorithm using a single probability distribution to describe the likely states of multiple objects to track multiple objects in a scene at the same time.\n\nSince clutter can cause the object probability distribution to split into multiple peaks, each peak represents a hypothesis about the object configuration. Smoothing is a statistical technique of conditioning the distribution based on both past and future measurements once the tracking is complete in order to reduce the effects of multiple peaks. Smoothing cannot be directly done in real-time since it requires information of future measurements.\n\nThe algorithm can be used for vision-based robot localization of mobile robots. Instead of tracking the position of an object in the scene, however, the position of the camera platform is tracked. This allows the camera platform to be globally localized given a visual map of the environment.\n\nExtensions of the condensation algorithm have also been used to recognize human gestures in image sequences. This application of the condensation algorithm impacts the range of human–computer interactions possible. It has been used to recognize simple gestures of a user at a whiteboard to control actions such as selecting regions of the boards to print or save them. Other extensions have also been used for tracking multiple cars in the same scene.\n\nThe condensation algorithm has also been used for face recognition in a video sequence.\n\nAn implementation of the condensation algorithm in C can be found on Michael Isard’s website.\n\nAn implementation in MATLAB can be found on the Mathworks File Exchange.\n\nAn example of implementation using the OpenCV library can be found on the OpenCV forums.\n\n", "related": "\n- Particle filter – Condensation is the application of Sampling Importance Resampling (SIR) estimation to contour tracking\n"}
{"id": "5491440", "url": "https://en.wikipedia.org/wiki?curid=5491440", "title": "Multi-scale approaches", "text": "Multi-scale approaches\n\nThe scale space representation of a signal obtained by Gaussian smoothing satisfies a number of special properties, scale-space axioms, which make it into a special form of multi-scale representation. There are, however, also other types of \"multi-scale approaches\" in the areas of computer vision, image processing and signal processing, in particular the notion of wavelets. The purpose of this article is to describe a few of these approaches:\n\nFor \"one-dimensional signals\", there exists quite a well-developed theory for continuous and discrete kernels that guarantee that new local extrema or zero-crossings cannot be created by a convolution operation. For \"continuous signals\", it holds that all scale-space kernels can be decomposed into the following sets of primitive smoothing kernels:\n- the \"Gaussian kernel\" :formula_1 where formula_2,\n- \"truncated exponential\" kernels (filters with one real pole in the \"s\"-plane):\n- translations,\n- rescalings.\n\nFor \"discrete signals\", we can, up to trivial translations and rescalings, decompose any discrete scale-space kernel into the following primitive operations:\n- the \"discrete Gaussian kernel\"\n- \"generalized binomial kernels\" corresponding to linear smoothing of the form\n- \"first-order recursive filters\" corresponding to linear smoothing of the form\n- the one-sided \"Poisson kernel\"\n\nFrom this classification, it is apparent that we require a continuous semi-group structure, there are only three classes of scale-space kernels with a continuous scale parameter; the Gaussian kernel which forms the scale-space of continuous signals, the discrete Gaussian kernel which forms the scale-space of discrete signals and the time-causal Poisson kernel that forms a temporal scale-space over discrete time. If we on the other hand sacrifice the continuous semi-group structure, there are more options:\n\nFor discrete signals, the use of generalized binomial kernels provides a formal basis for defining the smoothing operation in a pyramid. For temporal data, the one-sided truncated exponential kernels and the first-order recursive filters provide a way to define \"time-causal scale-spaces\" that allow for efficient numerical implementation and respect causality over time without access to the future. The first-order recursive filters also provide a framework for defining recursive approximations to the Gaussian kernel that in a weaker sense preserve some of the scale-space properties.\n\n", "related": "\n- Scale space\n- Scale space implementation\n- Scale-space segmentation\n"}
{"id": "5717580", "url": "https://en.wikipedia.org/wiki?curid=5717580", "title": "Active appearance model", "text": "Active appearance model\n\nAn active appearance model (AAM) is a computer vision algorithm for matching a statistical model of object shape and appearance to a new image. They are built during a training phase. A set of images, together with coordinates of landmarks that appear in all of the images, is provided to the training supervisor.\n\nThe model was first introduced by Edwards, Cootes and Taylor in the context of face analysis at the 3rd International Conference on Face and Gesture Recognition, 1998. Cootes, Edwards and Taylor further described the approach as a general method in computer vision at the European Conference on Computer Vision in the same year. The approach is widely used for matching and tracking faces and for medical image interpretation.\n\nThe algorithm uses the difference between the current estimate of appearance and the target image to drive an optimization process.\nBy taking advantage of the least squares techniques, it can match to new images very swiftly.\n\nIt is related to the active shape model (ASM). One disadvantage of ASM is that it only uses shape constraints (together with some information about the image structure near the landmarks), and does not take advantage of all the available information – the texture across the target object. This can be modelled using an AAM.\n\n- T. F. Cootes, C. J. Taylor, D. H. Cooper, and J. Graham. Training models of shape from sets of examples. \"In Proceedings of BMVC'92, pages 266–275, 1992\"\n- S. C. Mitchell, J. G. Bosch, B. P. F. Lelieveldt, R. J. van der Geest, J. H. C. Reiber, and M. Sonka. 3-d active appearance models: Segmentation of cardiac MR and ultrasound images. \"IEEE Trans. Med. Imaging, 21(9):1167–1178, 2002\"\n- T.F. Cootes, G. J. Edwards, and C. J. Taylor. Active appearance models. ECCV, 2:484–498, 1998[https://www.cs.cmu.edu/~efros/courses/AP06/Papers/cootes-eccv-98.pdf [pdf<nowiki>]</nowiki>]\n\n- Professor Tim Cootes AAM Code Free Tools for experimenting with AAMs from Manchester University (for research use only).\n- Professor Tim Cootes AAM Page Co-creator of AAM page from Manchester University.\n- IMM AAM Code Dr Mikkel B. Stegmann's home page of AAM-API, C++ AAM implementation (non-commercial use only).\n- Matlab AAM Code Open-source Matlab implementation of the original AAM algorithm.\n- AAMtools An Active Appearance Modelling Toolbox in Matlab by Dr George Papandreou.\n- DeMoLib AAM Toolbox in C++ by Dr Jason Saragih and Dr Roland Goecke.\n", "related": "NONE"}
{"id": "5477059", "url": "https://en.wikipedia.org/wiki?curid=5477059", "title": "Scale space implementation", "text": "Scale space implementation\n\nThe linear scale-space representation of an \"N\"-dimensional continuous signal,\n\nis obtained by convolving \"f\" with an \"N\"-dimensional Gaussian kernel:\n\nIn other words:\n\nHowever, for implementation, this definition is impractical, since it is continuous. When applying the scale space concept to a discrete signal \"f\", different approaches can be taken. This article is a brief summary of some of the most frequently used methods.\n\nUsing the \"separability property\" of the Gaussian kernel\n\nthe \"N\"-dimensional convolution operation can be decomposed into a set of separable smoothing steps with a one-dimensional Gaussian kernel \"G\" along each dimension\n\nwhere\n\nand the standard deviation of the Gaussian σ is related to the scale parameter \"t\" according to \"t\" = σ.\n\nSeparability will be assumed in all that follows, even when the kernel is not exactly Gaussian, since separation of the dimensions is the most practical way to implement multidimensional smoothing, especially at larger scales. Therefore, the rest of the article focuses on the one-dimensional case.\n\nWhen implementing the one-dimensional smoothing step in practice, the presumably simplest approach is to convolve the discrete signal \"f\" with a \"sampled Gaussian kernel\":\n\nwhere\n\n(with \"t\" = σ) which in turn is truncated at the ends to give a filter with finite impulse response\n\nfor \"M\" chosen sufficiently large (see error function) such that\n\nA common choice is to set \"M\" to a constant \"C\" times the standard deviation of the Gaussian kernel\n\nwhere \"C\" is often chosen somewhere between 3 and 6.\n\nUsing the sampled Gaussian kernel can, however, lead to implementation problems, in particular when computing higher-order derivatives at finer scales by applying sampled derivatives of Gaussian kernels. When accuracy and robustness are primary design criteria, alternative implementation approaches should therefore be considered.\n\nFor small values of ε (10 to 10) the errors introduced by truncating the Gaussian are usually negligible. For larger values of ε, however, there are many better alternatives to a rectangular window function. For example, for a given number of points, a Hamming window, Blackman window, or Kaiser window will do less damage to the spectral and other properties of the Gaussian than a simple truncation will. Notwithstanding this, since the Gaussian kernel decreases rapidly at the tails, the main recommendation is still to use a sufficiently small value of ε such that the truncation effects are no longer important.\n\nA more refined approach is to convolve the original signal with the \"discrete Gaussian kernel\" \"T\"(\"n\", \"t\")\n\nwhere\n\nand formula_14 denotes the modified Bessel functions of integer order, \"n\". This is the discrete counterpart of the continuous Gaussian in that it is the solution to the discrete diffusion equation (discrete space, continuous time), just as the continuous Gaussian is the solution to the continuous diffusion equation.\n\nThis filter can be truncated in the spatial domain as for the sampled Gaussian\n\nor can be implemented in the Fourier domain using a closed-form expression for its discrete-time Fourier transform:\n\nWith this frequency-domain approach, the scale-space properties transfer \"exactly\" to the discrete domain, or with excellent approximation using periodic extension and a suitably long discrete Fourier transform to approximate the discrete-time Fourier transform of the signal being smoothed. Moreover, higher-order derivative approximations can be computed in a straightforward manner (and preserving scale-space properties) by applying small support central difference operators to the discrete scale space representation.\n\nAs with the sampled Gaussian, a plain truncation of the infinite impulse response will in most cases be a sufficient approximation for small values of ε, while for larger values of ε it is better to use either a decomposition of the discrete Gaussian into a cascade of generalized binomial filters or alternatively to construct a finite approximate kernel by multiplying by a window function. If ε has been chosen too large such that effects of the truncation error begin to appear (for example as spurious extrema or spurious responses to higher-order derivative operators), then the options are to decrease the value of ε such that a larger finite kernel is used, with cutoff where the support is very small, or to use a tapered window.\n\nSince computational efficiency is often important, low-order \"recursive filters\" are often used for scale-space smoothing. For example, Young and van Vliet use a third-order recursive filter with one real pole and a pair of complex poles, applied forward and backward to make a sixth-order symmetric approximation to the Gaussian with low computational complexity for any smoothing scale.\n\nBy relaxing a few of the axioms, Lindeberg concluded that good smoothing filters would be \"normalized Pólya frequency sequences\", a family of discrete kernels that includes all filters with real poles at 0 < \"Z\" < 1 and/or \"Z\" > 1, as well as with real zeros at \"Z\" < 0. For symmetry, which leads to approximate directional homogeneity, these filters must be further restricted to pairs of poles and zeros that lead to zero-phase filters.\n\nTo match the transfer function curvature at zero frequency of the discrete Gaussian, which ensures an approximate semi-group property of additive \"t\", two poles at\n\ncan be applied forward and backwards, for symmetry and stability. This filter is the simplest implementation of a normalized Pólya frequency sequence kernel that works for any smoothing scale, but it is not as excellent an approximation to the Gaussian as Young and van Vliet's filter, which is \"not\" normalized Pólya frequency sequence, due to its complex poles.\n\nThe transfer function, \"H\", of a symmetric pole-pair recursive filter is closely related to the discrete-time Fourier transform of the discrete Gaussian kernel via first-order approximation of the exponential:\n\nwhere the \"t\" parameter here is related to the stable pole position \"Z\" = \"p\" via:\n\nFurthermore, such filters with \"N\" pairs of poles, such as the two pole pairs illustrated in this section, are an even better approximation to the exponential:\n\nwhere the stable pole positions are adjusted by solving:\n\nThe impulse responses of these filters are not very close to gaussian unless more than two pole pairs are used. However, even with only one or two pole pairs per scale, a signal successively smoothed at increasing scales will be very close to a gaussian-smoothed signal. The semi-group property is poorly approximated when too few pole pairs are used.\n\nScale-space axioms that are still satisfied by these filters are:\n\n- \"linearity\"\n- \"shift invariance\" (integer shifts)\n- \"non-creation of local extrema\" (zero-crossings) in one dimension\n- \"non-enhancement of local extrema\" in any number of dimensions\n- \"positivity\"\n- \"normalization\"\n\nThe following are only approximately satisfied, the approximation being better for larger numbers of pole pairs:\n\n- existence of an \"infinitesimal generator\" \"A\" (the infinitesimal generator of the discrete Gaussian, or a filter approximating it, approximately maps a recursive filter response to one of infinitesimally larger \"t\")\n- the \"semi-group structure\" with the associated \"cascade smoothing property\" (this property is approximated by considering kernels to be equivalent when they have the same \"t\" value, even if they are not quite equal)\n- \"rotational symmetry\"\n- \"scale invariance\"\n\nThis recursive filter method and variations to compute both the Gaussian smoothing as well as Gaussian derivatives has been described by several authors. Tan \"et al.\" have analyzed and compared some of these approaches, and have pointed out that the Young and van Vliet filters are a cascade (multiplication) of forward and backward filters, while the Deriche and the Jin \"et al.\" filters are sums of forward and backward filters.\n\nAt fine scales, the recursive filtering approach as well as other separable approaches are not guaranteed to give the best possible approximation to rotational symmetry, so non-separable implementations for 2D images may be considered as an alternative.\n\nWhen computing several derivatives in the N-jet simultaneously, discrete scale-space smoothing with the discrete analogue of the Gaussian kernel, or with a recursive filter approximation, followed by small support difference operators, may be both faster and more accurate than computing recursive approximations of each derivative operator.\n\nFor small scales, a low-order FIR filter may be a better smoothing filter than a recursive filter. The symmetric 3-kernel , for \"t\" ≤ 0.5 smooths to a scale of \"t\" using a pair of real zeros at \"Z\" < 0, and approaches the discrete Gaussian in the limit of small \"t\". In fact, with infinitesimal \"t\", either this two-zero filter or the two-pole filter with poles at \"Z\" = \"t\"/2 and \"Z\" = 2/\"t\" can be used as the infinitesimal generator for the discrete Gaussian kernels described above.\n\nThe FIR filter's zeros can be combined with the recursive filter's poles to make a general high-quality smoothing filter. For example, if the smoothing process is to always apply a biquadratic (two-pole, two-zero) filter forward then backwards on each row of data (and on each column in the 2D case), the poles and zeros can each do a part of the smoothing. The zeros limit out at \"t\" = 0.5 per pair (zeros at \"Z\" = –1), so for large scales the poles do most of the work. At finer scales, the combination makes an excellent approximation to the discrete Gaussian if the poles and zeros each do about half the smoothing. The \"t\" values for each portion of the smoothing (poles, zeros, forward and backward multiple applications, etc.) are additive, in accordance with the approximate semi-group property.\n\nThe FIR filter transfer function is closely related to the discrete Gaussian's DTFT, just as was the recursive filter's. For a single pair of zeros, the transfer function is\n\nwhere the \"t\" parameter here is related to the zero positions \"Z\" = \"z\" via:\n\nand we require \"t\" ≤ 0.5 to keep the transfer function non-negative.\n\nFurthermore, such filters with \"N\" pairs of zeros, are an even better approximation to the exponential and extend to higher values of \"t\" :\n\nwhere the stable zero positions are adjusted by solving:\n\nThese FIR and pole-zero filters are valid scale-space kernels, satisfying the same axioms as the all-pole recursive filters.\n\nRegarding the topic of automatic scale selection based on normalized derivatives, pyramid approximations are frequently used to obtain real-time performance. The appropriateness of approximating scale-space operations within a pyramid originates from the fact that repeated cascade smoothing with generalized binomial kernels leads to equivalent smoothing kernels that under reasonable conditions approach the Gaussian. Furthermore, the binomial kernels (or more generally the class of generalized binomial kernels) can be shown to constitute the unique class of finite-support kernels that guarantee non-creation of local extrema or zero-crossings with increasing scale (see the article on multi-scale approaches for details). Special care may, however, need to be taken to avoid discretization artifacts.\n\nFor one-dimensional kernels, there is a well-developed theory of multi-scale approaches, concerning filters that do not create new local extrema or new zero-crossings with increasing scales. For continuous signals, filters with real poles in the \"s\"-plane are within this class, while for discrete signals the above-described recursive and FIR filters satisfy these criteria. Combined with the strict requirement of a continuous semi-group structure, the continuous Gaussian and the discrete Gaussian constitute the unique choice for continuous and discrete signals.\n\nThere are many other multi-scale signal processing, image processing and data compression techniques, using wavelets and a variety of other kernels, that do not exploit or require the same requirements as scale space descriptions do; that is, they do not depend on a coarser scale not generating a new extremum that was not present at a finer scale (in 1D) or non-enhancement of local extrema between adjacent scale levels (in any number of dimensions).\n\n", "related": "\n- Scale space\n- Pyramid (image processing)\n- Multi-scale approaches\n- Gaussian filter\n"}
{"id": "6838895", "url": "https://en.wikipedia.org/wiki?curid=6838895", "title": "N-jet", "text": "N-jet\n\nAn \"N\"-jet is the set of (partial) derivatives of a function formula_1 up to order \"N\".\n\nSpecifically, in the area of computer vision, the \"N\"-jet is usually computed from a scale space representation formula_2 of the input image formula_3, and the partial derivatives of formula_2 are used as a basis for expressing various types of visual modules. For example, algorithms for tasks such as feature detection, feature classification, stereo matching, tracking and object recognition can be expressed in terms of \"N\"-jets computed at one or several scales in scale space.\n\n", "related": "\n- Scale space implementation\n- Jet (mathematics)\n"}
{"id": "7095986", "url": "https://en.wikipedia.org/wiki?curid=7095986", "title": "Neighborhood operation", "text": "Neighborhood operation\n\nIn computer vision and image processing a neighborhood operation is a commonly used class of computations on image data which implies that it is processed according to the following pseudo code:\n\nThis general procedure can be applied to image data of arbitrary dimensionality. Also, the image data on which the operation is applied does not have to be defined in terms of intensity or color, it can be any type of information which is organized as a function of spatial (and possibly temporal) variables in .\nThe result of applying a neighborhood operation on an image is again something which can be interpreted as an image, it has the same dimension as the original data. The value at each image point, however, does not have to be directly related to intensity or color. Instead it is an element in the range of the function , which can be of arbitrary type.\n\nNormally the neighborhood is of fixed size and is a square (or a cube, depending on the dimensionality of the image data) centered on the point . Also the function is fixed, but may in some cases have parameters which can vary with , see below.\n\nIn the simplest case, the neighborhood may be only a single point. This type of operation is often referred to as a point-wise operation.\n\nThe most common examples of a neighborhood operation use a fixed function which in addition is linear, that is, the computation consists of a linear shift invariant operation. In this case, the neighborhood operation corresponds to the convolution operation. A typical example is convolution with a low-pass filter, where the result can be interpreted in terms of local averages of the image data around each image point. Other examples are computation of local derivatives of the image data.\n\nIt is also rather common to use a fixed but non-linear function . This includes median filtering, and computation of local variances. The Nagao-Matsuyama filter is an example of a complex local neighbourhood operation that uses variance as an indicator of the uniformity within a pixel group. The result is similar to a convolution with a low-pass filter with the added effect of preserving sharp edges. \n\nThere is also a class of neighborhood operations in which the function has additional parameters which can vary with :\n\nThis implies that the result is not shift invariant. Examples are adaptive Wiener filters.\n\nThe pseudo code given above suggests that a neighborhood operation is implemented in terms of an outer loop over all image points. However, since the results are independent, the image points can be visited in arbitrary order, or can even be processed in parallel. Furthermore, in the case of linear shift-invariant operations, the computation of at each point implies a summation of products between the image data and the filter coefficients. The implementation of this neighborhood operation can then be made by having the summation loop outside the loop over all image points.\n\nAn important issue related to neighborhood operation is how to deal with the fact that the neighborhood becomes more or less undefined for points close to the edge or border of the image data. Several strategies have been proposed:\n\n- Compute result only for points for which the corresponding neighborhood is well-defined. This implies that the output image will be somewhat smaller than the input image.\n- Zero padding: Extend the input image sufficiently by adding extra points outside the original image which are set to zero. The loops over the image points described above visit only the original image points.\n- Border extension: Extend the input image sufficiently by adding extra points outside the original image which are set to the image value at the closest image point. The loops over the image points described above visit only the original image points.\n- Mirror extension: Extend the image sufficiently much by mirroring the image at the image boundaries. This method is less sensitive to local variations at the image boundary than border extension.\n- Wrapping: The image is tiled, so that going off one edge wraps around to the opposite side of the image. This method assumes that the image is largely homogeneous, for example a stochastic image texture without large textons.\n", "related": "NONE"}
{"id": "4081616", "url": "https://en.wikipedia.org/wiki?curid=4081616", "title": "Relaxation labelling", "text": "Relaxation labelling\n\nRelaxation labelling is an image treatment methodology. Its goal is to associate a label to the pixels of a given image or nodes of a given graph.\n\n- (Full text: )\n- (Full text: )\n", "related": "NONE"}
{"id": "8755061", "url": "https://en.wikipedia.org/wiki?curid=8755061", "title": "Landmark point", "text": "Landmark point\n\nIn morphometrics, landmark point or shortly landmark is a point in a shape object in which correspondences between and within the populations of the object are preserved. In other disciplines, landmarks may be known as vertices, anchor points, control points, sites, profile points, 'sampling' points, nodes, markers, fiducial markers, etc. Landmarks can be defined either manually by experts or automatically by a computer program. There are three basic types of landmarks: anatomical landmarks, mathematical landmarks or pseudo-landmarks. \n\nAn anatomical landmark is a biologically-meaningful point in an organism. Usually experts define anatomical points to ensure their correspondences within the same species. Examples of anatomical landmark in shape of a skull are the eye corner, tip of the nose, jaw, etc. Anatomical landmarks determine homologous parts of an organism, which share a common ancestry.\n\nMathematical landmarks are points in a shape that are located according to some mathematical or geometrical property, for instance, a high curvature point or an extreme point. A computer program usually determines mathematical landmarks used for an automatic pattern recognition.\n\nPseudo-landmarks are constructed points located between anatomical or mathematical landmarks. A typical example is an equally spaced set of points between two anatomical landmarks to get more sample points from a shape. Pseudo-landmarks are useful during shape matching, when the matching process requires a large number of points.\n\n", "related": "\n- Statistical shape analysis\n- Fiducial marker\n"}
{"id": "8707155", "url": "https://en.wikipedia.org/wiki?curid=8707155", "title": "Statistical shape analysis", "text": "Statistical shape analysis\n\nStatistical shape analysis is an analysis of the geometrical properties of some given set of shapes by statistical methods. For instance, it could be used to quantify differences between male and female gorilla skull shapes, normal and pathological bone shapes, leaf outlines with and without herbivory by insects, etc. Important aspects of shape analysis are to obtain a measure of distance between shapes, to estimate mean shapes from (possibly random) samples, to estimate shape variability within samples, to perform clustering and to test for differences between shapes. One of the main methods used is principal component analysis (PCA). Statistical shape analysis has applications in various fields, including medical imaging, computer vision, computational anatomy, sensor measurement, and geographical profiling.\n\nIn the point distribution model, a shape is determined by a finite set of coordinate points, known as landmark points. These landmark points often correspond to important identifiable features such as the corners of the eyes. Once the points are collected some form of registration is undertaken. This can be a baseline methods used by Fred Bookstein for geometric morphometrics in anthropology. Or an approach like Procrustes analysis which finds an average shape.\n\nDavid George Kendall investigated the statistical distribution of the shape of triangles, and represented each triangle by a point on a sphere. He used this distribution on the sphere to investigate ley lines and whether three stones were more likely to be co-linear than might be expected. Statistical distribution like the Kent distribution can be used to analyse the distribution of such spaces.\n\nAlternatively, shapes can be represented by curves or surfaces representing their contours, by the spatial region they occupy.\n\nDifferences between shapes can be quantified by investigating deformations transforming one shape into another. In particular a diffeomorphism preserves smoothness in the deformation. This was pioneered in D'Arcy Thompson's On Growth and Form before the advent of computers. Deformations can be interpreted as resulting from a force applied to the shape. Mathematically, a deformation is defined as a mapping from a shape \"x\" to a shape \"y\" by a transformation function formula_1, i.e., formula_2. Given a notion of size of deformations, the distance between two shapes can be defined as the size of the smallest deformation between these shapes.\n\n", "related": "\n- Active shape model\n- Geometric data analysis\n- Shape analysis (disambiguation)\n- Procrustes analysis\n- Computational anatomy\n- Large Deformation Diffeomorphic Metric Mapping\n- Bayesian Estimation of Templates in Computational Anatomy\n- Bayesian model of computational anatomy\n"}
{"id": "3145356", "url": "https://en.wikipedia.org/wiki?curid=3145356", "title": "Pose (computer vision)", "text": "Pose (computer vision)\n\nIn computer vision and robotics, a typical task is to identify specific objects in an image and to determine each object's position and orientation relative to some coordinate system. This information can then be used, for example, to allow a robot to manipulate an object or to avoid moving into the object. The combination of \"position\" and \"orientation\" is referred to as the pose of an object, even though this concept is sometimes used only to describe the orientation. \"Exterior orientation\" and \"translation\" are also used as synonyms of pose.\n\nThe image data from which the pose of an object is determined can be either a single image, a stereo image pair, or an image sequence where, typically, the camera is moving with a known velocity. The objects which are considered can be rather general, including a living being or body parts, e.g., a head or hands. The methods which are used for determining the pose of an object, however, are usually specific for a class of objects and cannot generally be expected to work well for other types of objects.\n\nThe pose can be described by means of a rotation and translation transformation which brings the object from a reference pose to the observed pose. This rotation transformation can be represented in different ways, e.g., as a rotation matrix or a quaternion.\n\nThe specific task of determining the pose of an object in an image (or stereo images, image sequence) is referred to as \"pose estimation\". The pose estimation problem can be solved in different ways depending on the image sensor configuration, and choice of methodology. Three classes of methodologies can be distinguished:\n\n- Analytic or geometric methods: Given that the image sensor (camera) is calibrated and the mapping from 3D points in the scene and 2D points in the image is known. If also the geometry of the object is known, it means that the projected image of the object on the camera image is a well-known function of the object's pose. Once a set of control points on the object, typically corners or other feature points, has been identified, it is then possible to solve the pose transformation from a set of equations which relate the 3D coordinates of the points with their 2D image coordinates. Algorithms that determine the pose of a point cloud with respect to another point cloud are known as point set registration algorithms, if the correspondences between points are not already known.\n- Genetic algorithm methods: If the pose of an object does not have to be computed in real-time a genetic algorithm may be used. This approach is robust especially when the images are not perfectly calibrated. In this particular case, the pose represent the genetic representation and the error between the projection of the object control points with the image is the fitness function.\n- Learning-based methods: These methods use artificial learning-based system which learn the mapping from 2D image features to pose transformation. In short, this means that a sufficiently large set of images of the object, in different poses, must be presented to the system during a learning phase. Once the learning phase is completed, the system should be able to present an estimate of the object's pose given an image of the object.\n\n", "related": "\n- Gesture recognition\n- Homography (computer vision)\n- Camera calibration\n- Structure from motion\n- Essential matrix and Trifocal tensor (relative pose)\n"}
{"id": "7174467", "url": "https://en.wikipedia.org/wiki?curid=7174467", "title": "Connected-component labeling", "text": "Connected-component labeling\n\nConnected-component labeling (CCL), connected-component analysis (CCA), blob extraction, region labeling, blob discovery, or region extraction is an algorithmic application of graph theory, where subsets of connected components are uniquely labeled based on a given heuristic. Connected-component labeling is not to be confused with segmentation.\n\nConnected-component labeling is used in computer vision to detect connected regions in binary digital images, although color images and data with higher dimensionality can also be processed. When integrated into an image recognition system or human-computer interaction interface, connected component labeling can operate on a variety of information. Blob extraction is generally performed on the resulting binary image from a thresholding step, but it can be applicable to gray-scale and color images as well. Blobs may be counted, filtered, and tracked.\n\nBlob extraction is related to but distinct from blob detection. \n\nA graph, containing vertices and connecting edges, is constructed from relevant input data. The vertices contain information required by the comparison heuristic, while the edges indicate connected 'neighbors'. An algorithm traverses the graph, labeling the vertices based on the connectivity and relative values of their neighbors. Connectivity is determined by the medium; image graphs, for example, can be 4-connected neighborhood or 8-connected neighborhood.\n\nFollowing the labeling stage, the graph may be partitioned into subsets, after which the original information can be recovered and processed .\n\nThe usage of the term connected-components labeling (CCL) and its definition is quite consistent in the academic literature, whereas connected-components analysis (CCA) varies in terms of both terminology and problem definition.\n\nRosenfeld et al. define connected components labeling as the “[c]reation of a labeled image in which the positions associated with the same connected component of the binary input image have a unique label.” Shapiro et al. define CCL as an operator whose “input is a binary image and [...] output is a symbolic image in which the label assigned to each pixel is an integer uniquely identifying the connected component to which that pixel belongs.”\n\nThere is no consensus on the definition of CCA in the academic literature. It is often used interchangeably with CCL. A more extensive definition is given by Shapiro et al.: “Connected component analysis consists of connected component labeling of the black pixels followed by property measurement of the component regions and decision making.” The definition for connected-component analysis presented here is more general, taking the thoughts expressed in into account.\n\nThe algorithms discussed can be generalized to arbitrary dimensions, albeit with increased time and space complexity.\n\nThis is a fast and very simple method to implement and understand. It is based on graph traversal methods in graph theory. In short, once the first pixel of a connected component is found, all the connected pixels of that connected component are labelled before going onto the next pixel in the image. This algorithm is part of Vincent and Soille's watershed segmentation algorithm, other implementations also exist.\n\nIn order to do that a linked list is formed that will keep the indexes of the pixels that are connected to each other, steps (2) and (3) below. The method of defining the linked list specifies the use of a depth or a breadth first search. For this particular application, there is no difference which strategy to use. The simplest kind of a last in first out queue implemented as a singly linked list will result in a depth first search strategy.\n\nIt is assumed that the input image is a binary image, with pixels being either background or foreground and that the connected components in the foreground pixels are desired. The algorithm steps can be written as:\n\n1. Start from the first pixel in the image. Set current label to 1. Go to (2).\n2. If this pixel is a foreground pixel and it is not already labelled, give it the current label and add it as the first element in a queue, then go to (3). If it is a background pixel or it was already labelled, then repeat (2) for the next pixel in the image.\n3. Pop out an element from the queue, and look at its neighbours (based on any type of connectivity). If a neighbour is a foreground pixel and is not already labelled, give it the current label and add it to the queue. Repeat (3) until there are no more elements in the queue.\n4. Go to (2) for the next pixel in the image and increment current label by 1.\n\nNote that the pixels are labelled before being put into the queue. The queue will only keep a pixel to check its neighbours and add them to the queue if necessary. This algorithm only needs to check the neighbours of each foreground pixel once and doesn't check the neighbours of background pixels.\n\nRelatively simple to implement and understand, the two-pass algorithm, (also known as the Hoshen–Kopelman algorithm) iterates through 2-dimensional binary data. The algorithm makes two passes over the image. The first pass to assign temporary labels and record equivalences and the second pass to replace each temporary label by the smallest label of its equivalence class.\n\nThe input data can be modified \"in situ\" (which carries the risk of data corruption), or labeling information can be maintained in an additional data structure.\n\nConnectivity checks are carried out by checking neighbor pixels' labels (neighbor elements whose labels are not assigned yet are ignored), or say, the North-East, the North, the North-West and the West of the current pixel (assuming 8-connectivity). 4-connectivity uses only North and West neighbors of the current pixel. The following conditions are checked to determine the value of the label to be assigned to the current pixel (4-connectivity is assumed)\n\nConditions to check:\n\n1. Does the pixel to the left (West) have the same value as the current pixel?\n1. Yes – We are in the same region. Assign the same label to the current pixel\n2. No – Check next condition\n2. Do both pixels to the North and West of the current pixel have the same value as the current pixel but not the same label?\n1. Yes – We know that the North and West pixels belong to the same region and must be merged. Assign the current pixel the minimum of the North and West labels, and record their equivalence relationship\n2. No – Check next condition\n3. Does the pixel to the left (West) have a different value and the one to the North the same value as the current pixel?\n1. Yes – Assign the label of the North pixel to the current pixel\n2. No – Check next condition\n4. Do the pixel's North and West neighbors have different pixel values than current pixel?\n1. Yes – Create a new label id and assign it to the current pixel\n\nThe algorithm continues this way, and creates new region labels whenever necessary. The key to a fast algorithm, however, is how this merging is done. This algorithm uses the union-find data structure which provides excellent performance for keeping track of equivalence relationships. Union-find essentially stores labels which correspond to the same blob in a disjoint-set data structure, making it easy to remember the equivalence of two labels by the use of an interface method E.g.: findSet(l). findSet(l) returns the minimum label value that is equivalent to the function argument 'l'.\n\nOnce the initial labeling and equivalence recording is completed, the second pass merely replaces each pixel label with its equivalent disjoint-set representative element.\n\nA faster-scanning algorithm for connected-region extraction is presented below.\n\nOn the first pass:\n1. Iterate through each element of the data by column, then by row (Raster Scanning)\n2. If the element is not the background\n1. Get the neighboring elements of the current element\n2. If there are no neighbors, uniquely label the current element and continue\n3. Otherwise, find the neighbor with the smallest label and assign it to the current element\n4. Store the equivalence between neighboring labels\n\nOn the second pass:\n1. Iterate through each element of the data by column, then by row\n2. If the element is not the background\n1. Relabel the element with the lowest equivalent label\n\nHere, the background is a classification, specific to the data, used to distinguish salient elements from the foreground. If the background variable is omitted, then the two-pass algorithm will treat the background as another region.\n\n1. The array from which connected regions are to be extracted is given below (8-connectivity based).\n\nWe first assign different binary values to elements in the graph. The values \"0~1\" at the center of each of the elements in the following graph are the elements' values, whereas the \"1,2...,7\" values in the next two graphs are the elements' labels. The two concepts should not be confused.\n\n2. After the first pass, the following labels are generated:\n\nA total of 7 labels are generated in accordance with the conditions highlighted above.\n\nThe label equivalence relationships generated are,\n\n3. Array generated after the merging of labels is carried out. Here, the label value that was the smallest for a given region \"floods\" throughout the connected region and gives two distinct labels, and hence two distinct labels.\n\n4. Final result in color to clearly see two different regions that have been found in the array.\n\nThe pseudocode is:\n\nThe \"find\" and \"union\" algorithms are implemented as described in union find.\n\nCreate a region counter\n\nScan the image (in the following example, it is assumed that scanning is done from left to right and from top to bottom):\n\n- For every pixel check the \"north\" and \"west\" pixel (when considering 4-connectivity) or the \"northeast\", \"north\", \"northwest\", and \"west\" pixel for 8-connectivity for a given region criterion (i.e. intensity value of 1 in binary image, or similar intensity to connected pixels in gray-scale image).\n- If none of the neighbors fit the criterion then assign to region value of the region counter. Increment region counter.\n- If only one neighbor fits the criterion assign pixel to that region.\n- If multiple neighbors match and are all members of the same region, assign pixel to their region.\n- If multiple neighbors match and are members of different regions, assign pixel to one of the regions (it doesn't matter which one). Indicate that all of these regions are equivalent.\n- Scan image again, assigning all equivalent regions the same region value.\n\nSome of the steps present in the two-pass algorithm can be merged for efficiency, allowing for a single sweep through the image. Multi-pass algorithms also exist, some of which run in linear time relative to the number of image pixels.\n\nIn the early 1990s, there was considerable interest in parallelizing connected-component algorithms in image analysis applications, due to the bottleneck of sequentially processing each pixel.\n\nThe interest to the algorithm arises again with an extensive use of CUDA.\n\nAlgorithm:\n1. Connected-component matrix is initialized to size of image matrix.\n2. A mark is initialized and incremented for every detected object in the image.\n3. A counter is initialized to count the number of objects.\n4. A row-major scan is started for the entire image.\n5. If an object pixel is detected, then following steps are repeated while (Index !=0)\n1. Set the corresponding pixel to 0 in Image.\n2. A vector (Index) is updated with all the neighboring pixels of the currently set pixels.\n3. Unique pixels are retained and repeated pixels are removed.\n4. Set the pixels indicated by Index to mark in the connected-component matrix.\n6. Increment the marker for another object in the image.\n\nThe run time of the algorithm depends on the size of the image and the amount of foreground. The time complexity is comparable to the two pass algorithm if the foreground covers a significant part of the image. Otherwise the time complexity is lower. However, memory access is less structured than for the two-pass algorithm, which tends to increase the run time in practice.\n\nIn the last two decades many novel approaches on connected-component labeling have been proposed and almost none of them was compared on the same data. YACCLAB\n\nThe emergence of FPGAs with enough capacity to perform complex image processing tasks also led to high-performance architectures for connected-component labeling. Most of these architectures utilize the single pass variant of this algorithm, because of the limited memory resources available on an FPGA. These types of connected component labeling architectures are able to process several image pixels in parallel, thereby enabling a high throughput at low processing latency to be achieved.\n\n", "related": "\n- Feature extraction\n- Flood fill\n\n- Implementation in C#\n- about Extracting objects from image and Direct Connected Component Labeling Algorithm\n"}
{"id": "12265304", "url": "https://en.wikipedia.org/wiki?curid=12265304", "title": "Point distribution model", "text": "Point distribution model\n\nThe point distribution model is a model for representing the mean geometry of a shape and some statistical modes of geometric variation inferred from a training set of shapes.\n\nThe point distribution model concept has been developed by Cootes, Taylor \"et al.\" and became a standard in computer vision for the statistical study of shape and for segmentation of medical images where shape priors really help interpretation of noisy and low-contrasted pixels/voxels. The latter point leads to active shape models (ASM) and active appearance models (AAM).\n\nPoint distribution models rely on landmark points. A landmark is an annotating point posed by an anatomist onto a given locus for every shape instance across the training set population. For instance, the same landmark will designate the tip of the index finger in a training set of 2D hands outlines. Principal component analysis (PCA), for instance, is a relevant tool for studying correlations of movement between groups of landmarks among the training set population. Typically, it might detect that all the landmarks located along the same finger move exactly together across the training set examples showing different finger spacing for a flat-posed hands collection.\n\nFirst, a set of training images are manually landmarked with enough corresponding landmarks to sufficiently approximate the geometry of the original shapes. These landmarks are aligned using the generalized procrustes analysis, which minimizes the least squared error between the points.\n\nformula_1 aligned landmarks in two dimensions are given as\n\nIt's important to note that each landmark formula_3 should represent the same anatomical location. For example, landmark #3, formula_4 might represent the tip of the ring finger across all training images.\n\nNow the shape outlines are reduced to sequences of formula_1 landmarks, so that a given training shape is defined as the vector formula_6. Assuming the scattering is gaussian in this space, PCA is used to compute normalized eigenvectors and eigenvalues of the covariance matrix across all training shapes. The matrix of the top formula_7 eigenvectors is given as formula_8, and each eigenvector describes a principal mode of variation along the set.\n\nFinally, a linear combination of the eigenvectors is used to define a new shape formula_9, mathematically defined as:\n\nwhere formula_11 is defined as the mean shape across all training images, and formula_12 is a vector of scaling values for each principal component. Therefore, by modifying the variable formula_12 an infinite number of shapes can be defined. To ensure that the new shapes are all within the variation seen in the training set, it is common to only allow each element of formula_12 to be within formula_153 standard deviations, where the standard deviation of a given principal component is defined as the square root of its corresponding eigenvalue.\n\nPDM's can be extended to any arbitrary number of dimensions, but are typically used in 2D image and 3D volume applications (where each landmark point is formula_16 or formula_17).\n\nAn eigenvector, interpreted in euclidean space, can be seen as a sequence of formula_1 euclidean vectors associated to corresponding landmark and designating a compound move for the whole shape. Global nonlinear variation is usually well handled provided nonlinear variation is kept to a reasonable level. Typically, a twisting nematode worm is used as an example in the teaching of kernel PCA-based methods.\n\nDue to the PCA properties: eigenvectors are mutually orthogonal, form a basis of the training set cloud in the shape space, and cross at the 0 in this space, which represents the mean shape. Also, PCA is a traditional way of fitting a closed ellipsoid to a Gaussian cloud of points (whatever their dimension): this suggests the concept of bounded variation.\n\nThe idea behind PDM's is that eigenvectors can be linearly combined to create an infinity of new shape instances that will 'look like' the one in the training set. The coefficients are bounded alike the values of the corresponding eigenvalues, so as to ensure the generated 2n/3n-dimensional dot will remain into the hyper-ellipsoidal allowed domain—allowable shape domain (ASD).\n\n", "related": "\n- Procrustes analysis\n\n- Flexible Models for Computer Vision, Tim Cootes, Manchester University.\n- A practical introduction to PDM and ASMs.\n"}
{"id": "13038078", "url": "https://en.wikipedia.org/wiki?curid=13038078", "title": "Phase congruency", "text": "Phase congruency\n\nPhase congruency is a measure of feature significance in computer images, a method of edge detection that is particularly robust against changes in illumination and contrast.\n\nPhase congruency reflects the behaviour of the image in the frequency domain. It has been noted that edgelike features have many of their frequency components in the same phase. The concept is similar to coherence, except that it applies to functions of different wavelength.\n\nFor example, the Fourier decomposition of a square wave consists of sine functions, whose frequencies are odd multiples of the fundamental frequency. At the rising edges of the square wave, each sinusoidal component has a rising phase; the phases have maximal congruency at the edges. This corresponds to the human-perceived edges in an image where there are sharp changes between light and dark.\n\nPhase congruency compares the weighted alignment of the Fourier components of a signal formula_1 with the sum of the Fourier components.\n\nwhere formula_3 is the local or instantaneous phase as can be calculated using the Hilbert transform and formula_1 are the local amplitude, or energy, of the signal. When all the phases are aligned, this is equal to 1.\n\nThe square-wave example is naive in that most edge detection methods deal with it equally well. For example, the first derivative has a maximal magnitude at the edges. However, there are cases where the perceived edge does not have a sharp step or a large derivative. The method of phase congruency applies to many cases where other methods fail.\n\nA notable example is an image feature consisting of a single line, such as the letter \"l\". Many edge-detection algorithms will pick up two adjacent edges: the transitions from white to black, and black to white. On the other hand, the phase congruency map has a single line. A simple Fourier analogy of this case is a triangle wave. In each of its crests there is a congruency of crests from different sinusoidal functions.\n\nCalculating the phase congruency map of an image is very computationally intensive, and sensitive to image noise. Techniques of noise reduction are usually applied prior to the calculation.\n\n- Research page of Peter Kovesi, list of research papers, example images and implementations.\n- Lecture by Professor Michael Brady.\n", "related": "NONE"}
{"id": "12989981", "url": "https://en.wikipedia.org/wiki?curid=12989981", "title": "Intrinsic dimension", "text": "Intrinsic dimension\n\nIn the fields of pattern recognition and machine learning the intrinsic dimension for a data set can be thought of as the number of variables needed in a minimal representation of the data. Similarly, in signal processing of multidimensional signals, the intrinsic dimension of the signal describes how many variables are needed to generate a good approximation of the signal.\n\nWhen estimating intrinsic dimension however, a slightly broader definition based on manifold dimension is often used, where a representation in the intrinsic dimension does only need to exist locally. Such intrinsic dimension estimation methods can thus handle data sets with different intrinsic dimensions in different parts of the data set.\n\nThe intrinsic dimension can be used as a lower bound of what dimension it is possible to compress a data set into through dimension reduction, but it can also be used as a measure of the complexity of the data set or signal.\n\nFor a data set or signal of \"N\" variables, its intrinsic dimension \"M\" satisfies \"0 ≤ M ≤ N\".\n\nLet \"formula_1\" be a two-variable function (or signal) which is of the form\n\nformula_2\n\nfor some one-variable function \"g\" which is not constant. This means that \"f\" varies, in accordance to \"g\", with the first variable or along the first coordinate. On the other hand, \"f\" is constant with respect to the second variable or along the second coordinate. It is only necessary to know the value of one, namely the first, variable in order to determine the value of \"f\". Hence, it is a two-variable function but its intrinsic dimension is one.\n\nA slightly more complicated example isformula_3\"f\" is still intrinsic one-dimensional, which can be seen by making a variable transformation\n\nformula_4\n\nformula_5\n\nwhich gives\n\nformula_6\n\nSince the variation in \"f\" can be described by the single variable \"y\" its intrinsic dimension is one.\n\nFor the case that \"f\" is constant, its intrinsic dimension is zero since no variable is needed to describe variation. For the general case, when the intrinsic dimension of the two-variable function \"f\" is neither zero or one, it is two.\n\nIn the literature, functions which are of intrinsic dimension zero, one, or two are sometimes referred to as \"i0D\", \"i1D\" or \"i2D\", respectively.\n\nFor an \"N\"-variable function \"f\", the set of variables can be represented as an \"N\"-dimensional vector x:\n\nformula_7\n\nIf for some \"M\"-variable function \"g\" and \"M × N\" matrix A is it the case that\n\n- for all x; formula_8\n- \"M\" is the smallest number for which the above relation between \"f\" and \"g\" can be found,\n\nthen the intrinsic dimension of \"f\" is \"M\".\n\nThe intrinsic dimension is a characterization of \"f\", it is not an unambiguous characterization of \"g\" nor of A. That is, if the above relation is satisfied for some \"f\", \"g\", and A, it must also be satisfied for the same \"f\" and \"g′\" and A′ given by\n\nformula_9\n\nformula_10\n\nwhere B is a non-singular \"M × M\" matrix, since\n\nformula_11\n\nAn \"N\" variable function which has intrinsic dimension \"M < N\" has a characteristic Fourier transform. Intuitively, since this type of function is constant along one or several dimensions its Fourier transform must appear like an impulse (the Fourier transform of a constant) along the same dimension in the frequency domain.\n\nLet \"f\" be a two-variable function which is i1D. This means that there exists a normalized vector formula_12 and a one-variable function \"g\" such that\n\nformula_13\n\nfor all formula_14. If \"F\" is the Fourier transform of \"f\" (both are two-variable functions) it must be the case that\n\nformula_15\n\nHere \"G\" is the Fourier transform of \"g\" (both are one-variable functions), \"δ\" is the Dirac impulse function and m is a normalized vector in formula_16 perpendicular to n. This means that \"F\" vanishes everywhere except on a line which passes through the origin of the frequency domain and is parallel to m. Along this line \"F\" varies according to \"G\".\n\nLet \"f\" be an \"N\"-variable function which has intrinsic dimension \"M\", that is, there exists an \"M\"-variable function \"g\" and \"M × N\" matrix A such that\n\nformula_17\n\nIts Fourier transform \"F\" can then be described as follows:\n\n- \"F\" vanishes everywhere except for a subspace of dimension \"M\"\n- The subspace \"M\" is spanned by the rows of the matrix A\n- In the subspace, \"F\" varies according to \"G\" the Fourier transform of \"g\"\n\nThe type of intrinsic dimension described above assumes that a linear transformation is applied to the coordinates of the \"N\"-variable function \"f\" to produce the \"M\" variables which are necessary to represent every value of \"f\". This means that \"f\" is constant along lines, planes, or hyperplanes, depending on \"N\" and \"M\".\n\nIn a general case, \"f\" has intrinsic dimension \"M\" if there exist \"M\" functions \"a\", \"a\", ..., \"a\" and an \"M\"-variable function \"g\" such that\n\n- formula_18for all x\n- \"M\" is the smallest number of functions which allows the above transformation\n\nA simple example is transforming a 2-variable function \"f\" to polar coordinates:\n\nformula_6\n\n- formula_20, \"f\" is i1D and is constant along any circle centered at the origin\n- formula_21, \"f\" is i1D and is constant along all rays from the origin\n\nFor the general case, a simple description of either the point sets for which \"f\" is constant or its Fourier transform is usually not possible.\n\nDuring the 1950s so called \"scaling\" methods were developed in the social sciences to explore and summarize multidimensional data sets. After Shepard introduced non-metric multidimensional scaling in 1962 one of the major research areas within multi-dimensional scaling (MDS) was estimation of the intrinsic dimension. The topic was also studied in information theory, pioneered by Bennet in 1965 who coined the term \"intrinsic dimension\" and wrote a computer program to estimate it.\n\nDuring the 1970s intrinsic dimensionality estimation methods were constructed that did not depend on dimensionality reductions such as MDS: based on local eigenvalues., based on distance distributions, and based on other dimension-dependent geometric properties\n\nEstimating intrinsic dimension of sets and probability measures has also been extensively studied since around 1980 in the field of dynamical systems, where dimensions of (strange) attractors have been the subject of interest. For strange attractors there is no manifold assumption, and the dimension measured is some version of fractal dimension — which also can be non-integer. However, definitions of fractal dimension yield the manifold dimension for manifolds.\n\nIn the 2000s the \"curse of dimensionality\" has been exploited to estimate intrinsic dimension.\n\nThe case of a two-variable signal which is i1D appears frequently in computer vision and image processing and captures the idea of local image regions which contain lines or edges. The analysis of such regions has a long history, but it was not until a more formal and theoretical treatment of such operations began that the concept of intrinsic dimension was established, even though the name has varied.\n\nFor example, the concept which here is referred to as an \"image neighborhood of intrinsic dimension 1\" or \"i1D neighborhood\" is called \"1-dimensional\" by Knutsson (1982), \"linear symmetric\" by Bigün & Granlund (1987) and \"simple neighborhood\" in Granlund & Knutsson (1995).\n\n", "related": "\n- Dimension\n- Fractal dimension\n- Hausdorff dimension\n- Topological dimension\n\n"}
{"id": "13274389", "url": "https://en.wikipedia.org/wiki?curid=13274389", "title": "Articulated body pose estimation", "text": "Articulated body pose estimation\n\nArticulated body pose estimation in computer vision is the study of algorithms and systems that recover the pose of an articulated body, which consists of joints and rigid parts using image-based observations. It is one of the longest-lasting problems in computer vision because of the complexity of the models that relate observation with pose, and because of the variety of situations in which it would be useful.\n\nPerception of human beings in their neighboring environment is an important capability that robots must possess. If a person uses gestures to point to a particular object, then the interacting machine should be able to understand the situation in real world context. Thus pose estimation is an important and challenging problem in computer vision, and many algorithms have been deployed in solving this problem over the last two decades. Many solutions involve training complex models with large data sets.\n\nPose estimation is a difficult problem and an active subject of research because the human body has 244 degrees of freedom with 230 joints. Although not all movements between joints are evident, the human body is composed of 10 large parts with 20 degrees of freedom. Algorithms must account for large variability introduced by differences in appearance due to clothing, body shape, size, and hairstyles. Additionally, the results may be ambiguous due to partial occlusions from self-articulation, such as a person's hand covering their face, or occlusions from external objects. Finally, most algorithms estimate pose from monocular (two-dimensional) images, taken from a normal camera. Other issues include varying lighting and camera configurations. The difficulties are compounded if there are additional performance requirements. These images lack the three-dimensional information of an actual body pose, leading to further ambiguities. There is recent work in this area wherein images from RGBD cameras provide information about color and depth.\n\nThere is a need to develop accurate, tether-less, vision-based articulated body pose estimation systems to recover the pose of bodies, such as the human body, a hand, or non-human creatures. Such a system has several foreseeable applications, including the following: \n- Markerless motion capture for human-computer interfaces,\n- Physiotherapy,\n- Human image synthesis,\n- Ergonomics studies,\n- Robot control, and\n- Visual surveillance.\n\nThe typical articulated body pose estimation system involves a model-based approach, in which the pose estimation is achieved by maximizing/minimizing a similarity/dissimilarity between an observation (input) and a template model. Different kinds of sensors have been explored for use in making the observation, including the following:\n- Visible wavelength imagery,\n- Long-wave thermal infrared imagery,\n- Time-of-flight imagery, and\n- Laser range scanner imagery.\n\nThese sensors produce intermediate representations that are directly used by the model. The representations include the following:\n- Image appearance,\n- Voxel (volume element) reconstruction,\n- 3D point clouds, and sum of Gaussian kernels\n- 3D surface meshes.\n\nThe basic idea of part based model can be attributed to the human skeleton. Any object having the property of articulation can be broken down into smaller parts wherein each part can take different orientations, resulting in different articulations of the same object. Different scales and orientations of the main object can be articulated to scales and orientations of the corresponding parts. To formulate the model so that it can be represented in mathematical terms, the parts are connected to each other using springs. As such, the model is also known as a spring model. The degree of closeness between each part is accounted for by the compression and expansion of the springs. There is geometric constraint on the orientation of\nsprings. For example, limbs of legs cannot move 360 degrees. Hence parts cannot have that extreme orientation. This reduces the possible permutations.\n\nThe spring model forms a graph G(V,E) where V (nodes) corresponds to the parts and E (edges) represents springs connecting two neighboring parts. Each location in the image can be reached by the formula_1 and formula_2 coordinates of the pixel location. Let formula_3 be point at formula_4 location. Then the cost associated in joining the spring between formula_4 and the formula_6 point can be given by formula_7. Hence the\ntotal cost associated in placing formula_8 components at locations formula_9 is given by\n\nThe above equation simply represents the spring model used to describe body pose. To estimate pose from images, cost or energy function must be minimized. This energy function consists of two terms. The first is related to how each component matches the image data and the second deals with how much the\noriented (deformed) parts match, thus accounting for articulation along with object detection.\n\nThe part models, also known as pictorial structures, are of one the basic models on which other efficient models are built by slight modification. One such example is the flexible mixture model which reduces the database of hundreds or thousands of deformed parts by exploiting the notion of local rigidity.\n\nThe kinematic skeleton is constructed by a tree-structured chain, as illustrated in the Figure. Each rigid body segment has its local coordinate system that can be transformed to the world coordinate system via a 4×4 transformation matrix formula_11,\n\nwhere formula_13 denotes the local transformation from body segment formula_14 to its parent formula_15. Each joint in the body has 3 degrees of freedom (DoF) rotation. Given a transformation matrix formula_16 , the joint position at the T-pose can be transferred to its corresponding position in the world coordination. In many works, the 3D joint rotation is expressed as a normalized quaternion formula_17 due to its continuity that can facilitate gradient-based optimization in the parameter estimation.\n\nPersonal care robots may be deployed in future assisted living homes. For these robots, high-accuracy human detection and pose estimation is necessary to perform a variety of tasks, such as fall detection. Additionally, this application has a number of performance constraints. \n\nTraditionally, character animation has been a manual process. However, poses can be synced directly to a real-life actor through specialized pose estimation systems. Older systems relied on markers or specialized suits. Recent advances in pose estimation and motion capture have enabled markerless applications, sometimes in real time.\n\nCar accidents account for about two percent of deaths globally each year. As such, an intelligent system tracking driver pose may be useful for emergency alerts . Along the same lines, pedestrian detection algorithms have been used successfully in autonomous cars, enabling the car to make smarter decisions. \n\nCommercially, pose estimation has been used in the context of video games, popularized with the Microsoft Kinect sensor (a depth camera). These systems track the user to render their avatar in-game, in addition to performing tasks like gesture recognition to enable the user to interact with the game. As such, this application has a strict real-time requirement.\n\nPose estimation has been used to detect postural issues such as scoliosis by analyzing abnormalities in a patient's posture, physical therapy, and the study of the cognitive brain development of young children by monitoring motor functionality.\n\nOther applications include video surveillance, animal tracking and behavior understanding, sign language detection, advanced human–computer interaction, and markerless motion capturing.\n\nA commercially successful but specialized computer vision-based articulated body pose estimation technique is optical motion capture. This approach involves placing markers on the individual at strategic locations to capture the 6 degrees-of-freedom of each body part.\n\nA number of groups and companies are researching pose estimation, including groups at Brown University, Carnegie Mellon University, MPI Saarbruecken, Stanford University, the University of California, San Diego, the University of Toronto, the École Centrale Paris, ETH Zurich, National University of Sciences and Technology (NUST), and the University of California, Irvine.\n\nAt present, several companies are working on articulated body pose estimation.\n- Bodylabs: Bodylabs is a Manhattan-based software provider of human-aware artificial intelligence.\n\n- Michael J. Black, Professor at Brown University\n- Research Project Page of German Cheung at Carnegie Mellon University\n- Homepage of Dr.-Ing at MPI Saarbruecken\n- Markerless Motion Capture Project at Stanford\n- Computer Vision and Robotics Research Laboratory at the University of California, San Diego\n- Research Projects of David J. Fleet at the University of Toronto\n- Ronald Poppe at the University of Twente.\n- Professor Nikos Paragios at the Ecole Centrale de Paris\n- Articulated Pose Estimation with Flexible Mixtures of Parts Project at UC Irvine\n- http://screenrant.com/crazy3dtechnologyjamescameronavatarkofi3367/\n- 2D articulated human pose estimation software\n- Articulated Pose Estimation with Flexible Mixtures of Parts\n", "related": "NONE"}
{"id": "15281107", "url": "https://en.wikipedia.org/wiki?curid=15281107", "title": "Shape context", "text": "Shape context\n\nShape context is a feature descriptor used in object recognition. Serge Belongie and Jitendra Malik proposed the term in their paper \"Matching with Shape Contexts\" in 2000.\n\nThe shape context is intended to be a way of describing shapes that allows for measuring shape similarity and the recovering of point correspondences. The basic idea is to pick \"n\" points on the contours of a shape. For each point \"p\" on the shape, consider the \"n\" − 1 vectors obtained by connecting \"p\" to all other points. The set of all these vectors is a rich description of the shape localized at that point but is far too detailed. The key idea is that the distribution over relative positions is a robust, compact, and highly discriminative descriptor. So, for the point \"p\", the coarse histogram of the relative coordinates of the remaining \"n\" − 1 points,\n\nis defined to be the shape context of formula_2. The bins are normally taken to be uniform in log-polar space. The fact that the shape context is a rich and discriminative descriptor can be seen in the figure below, in which the shape contexts of two different versions of the letter \"A\" are shown.\n\n(a) and (b) are the sampled edge points of the two shapes. (c) is the diagram of the log-polar bins used to compute the shape context. (d) is the shape context for the point marked with a circle in (a), (e) is that for the point marked as a diamond in (b), and (f) is that for the triangle. As can be seen, since (d) and (e) are the shape contexts for two closely related points, they are quite similar, while the shape context in (f) is very different.\n\nNow in order for a feature descriptor to be useful, it needs to have certain invariances. In particular it needs to be invariant to translation, scale, small perturbations, and depending on application rotation. Translational invariance come naturally to shape context. Scale invariance is obtained by normalizing all radial distances by the mean distance formula_3 between all the point pairs in the shape although the median distance can also be used. Shape contexts are empirically demonstrated to be robust to deformations, noise, and outliers using synthetic point set matching experiments.\n\nOne can provide complete rotation invariance in shape contexts. One way is to measure angles at each point relative to the direction of the tangent at that point (since the points are chosen on edges). This results in a completely rotationally invariant descriptor. But of course this is not always desired since some local features lose their discriminative power if not measured relative to the same frame. Many applications in fact forbid rotation invariance e.g. distinguishing a \"6\" from a \"9\".\n\nA complete system that uses shape contexts for shape matching consists of the following steps (which will be covered in more detail in the Details of Implementation section):\n\n1. Randomly select a set of points that lie on the edges of a known shape and another set of points on an unknown shape.\n2. Compute the shape context of each point found in step 1.\n3. Match each point from the known shape to a point on an unknown shape. To minimize the cost of matching, first choose a transformation (e.g. affine, thin plate spline, etc.) that warps the edges of the known shape to the unknown (essentially aligning the two shapes). Then select the point on the unknown shape that most closely corresponds to each warped point on the known shape.\n4. Calculate the \"shape distance\" between each pair of points on the two shapes. Use a weighted sum of the shape context distance, the image appearance distance, and the bending energy (a measure of how much transformation is required to bring the two shapes into alignment).\n5. To identify the unknown shape, use a nearest-neighbor classifier to compare its shape distance to shape distances of known objects.\n\nThe approach assumes that the shape of an object is essentially captured by a finite subset of the points on the internal or external contours on the object. These can be simply obtained using the Canny edge detector and picking a random set of points from the edges. Note that these points need not and in general do not correspond to key-points such as maxima of curvature or inflection points. It is preferable to sample the shape with roughly uniform spacing, though it is not critical.\n\nThis step is described in detail in the Theory section.\n\nConsider two points \"p\" and \"q\" that have normalized \"K\"-bin histograms (i.e. shape contexts) \"g\"(\"k\") and \"h\"(\"k\"). As shape contexts are distributions represented as histograms, it is natural to use the \"χ\" test statistic as the \"shape context cost\" of matching the two points:\n\nThe values of this range from 0 to 1.\nIn addition to the shape context cost, an extra cost based on the appearance can be added. For instance, it could be a measure of tangent angle dissimilarity (particularly useful in digit recognition):\n\nThis is half the length of the chord in unit circle between the unit vectors with angles formula_6 and formula_7. Its values also range from 0 to 1. Now the total cost of matching the two points could be a weighted-sum of the two costs:\n\nNow for each point \"p\" on the first shape and a point \"q\" on the second shape, calculate the cost as described and call it \"C\". This is the cost matrix.\n\nNow, a one-to-one matching \"p\" that matches each point \"p\" on shape 1 and \"q\" on shape 2 that minimizes the total cost of matching,\n\nis needed. This can be done in formula_10 time using the Hungarian method, although there are more efficient algorithms.\nTo have robust handling of outliers, one can add \"dummy\" nodes that have a constant but reasonably large cost of matching to the cost matrix. This would cause the matching algorithm to match outliers to a \"dummy\" if there is no real match.\n\nGiven the set of correspondences between a finite set of points on the two shapes, a transformation formula_11 can be estimated to map any point from one shape to the other. There are several choices for this transformation, described below.\n\nThe affine model is a standard choice: formula_12. The least squares solution for the matrix formula_13 and the translational offset vector \"o\" is obtained by:\n\nWhere formula_15 with a similar expression for formula_16. formula_17 is the pseudoinverse of formula_16.\n\nThe thin plate spline (TPS) model is the most widely used model for transformations when working with shape contexts. A 2D transformation can be separated into two TPS function to model a coordinate transform:\n\nwhere each of the \"ƒ\" and \"ƒ\" have the form:\n\nand the kernel function formula_21 is defined by formula_22. The exact details of how to solve for the parameters can be found elsewhere but it essentially involves solving a linear system of equations. The bending energy (a measure of how much transformation is needed to align the points) will also be easily obtained.\n\nThe TPS formulation above has exact matching requirement for the pairs of points on the two shapes. For noisy data, it is best to relax this exact requirement. If we let formula_23 denote the target function values at corresponding locations formula_24 (Note that for formula_25, formula_23 would formula_27 the x-coordinate of the point corresponding to formula_2 and for formula_29 it would be the y-coordinate, formula_30), relaxing the requirement amounts to minimizing\n\nwhere formula_32 is the bending energy and formula_33 is called the regularization parameter. This \"ƒ\" that minimizes \"H\"[\"ƒ\"] can be found in a fairly straightforward way. If one uses normalize coordinates for formula_34, then scale invariance is kept. However, if one uses the original non-normalized coordinates, then the regularization parameter needs to be normalized.\n\nNote that in many cases, regardless of the transformation used, the initial estimate of the correspondences contains some errors which could reduce the quality of the transformation. If we iterate the steps of finding correspondences and estimating transformations (i.e. repeating steps 2–5 with the newly transformed shape) we can overcome this problem. Typically, three iterations are all that is needed to obtain reasonable results.\n\nNow, a shape distance between two shapes formula_35 and formula_16. This distance is going to be a weighted sum of three potential terms:\n\nShape context distance: this is the symmetric sum of shape context matching costs over best matching points:\n\nwhere \"T\"(·) is the estimated TPS transform that maps the points in \"Q\" to those in \"P\".\n\nAppearance cost: After establishing image correspondences and properly warping one image to match the other, one can define an appearance cost as the sum of squared brightness differences in Gaussian windows around corresponding image points:\n\nwhere formula_39 and formula_40 are the gray-level images (formula_40 is the image after warping) and formula_42 is a Gaussian windowing function.\n\nTransformation cost: The final cost formula_43 measures how much transformation is necessary to bring the two images into alignment. In the case of TPS, it is assigned to be the bending energy.\n\nNow that we have a way of calculating the distance between two shapes, we can use a nearest neighbor classifier (k-NN) with distance defined as the shape distance calculated here. The results of applying this to different situations is given in the following section.\n\nThe authors Serge Belongie and Jitendra Malik tested their approach on the MNIST database. Currently, more than 50 algorithms have been tested on the database. The database has a training set of 60,000 examples, and a test set of 10,000 examples. The error rate for this approach was 0.63% using 20,000 training examples and 3-NN. At the time of publication, this error rate was the lowest. Currently, the lowest error rate is 0.23%.\n\nThe authors experimented with the MPEG-7 shape silhouette database, performing Core Experiment CE-Shape-1 part B, which measures performance of similarity-based retrieval. The database has 70 shape categories and 20 images per shape category. Performance of a retrieval scheme is tested by using each image as a query and counting the number of correct images in the top 40 matches. For this experiment, the authors increased the number of points sampled from each shape. Also, since the shapes in the database sometimes were rotated or flipped, the authors took defined the distance between a reference shape and query shape to be minimum shape distance between the query shape and either the unchanged reference, the vertically flipped, or the reference horizontally flipped. With these changes, they obtained a retrieval rate of 76.45%, which by 2002 was the best.\nThe next experiment performed on shape contexts involved the 20 common household objects in the Columbia Object Image Library (COIL-20). Each object has 72 views in the database. In the experiment, the method was trained on a number of equally spaced views for each object and the remaining views were used for testing. A 1-NN classifier was used. The authors also developed an \"editing\" algorithm based on shape context similarity and k-medoid clustering that improved on their performance.\n\nShape contexts were used to retrieve the closest matching trademarks from a database to a query trademark (useful in detecting trademark infringement). No visually similar trademark was missed by the algorithm (verified manually by the authors).\n\n- Matching with Shape Contexts\n- MNIST database of handwritten digits\n- Columbia Object Image Library (COIL-20)\n- Caltech101 Database\n- Character recognition with Shape Context\n", "related": "NONE"}
{"id": "15375961", "url": "https://en.wikipedia.org/wiki?curid=15375961", "title": "Active vision", "text": "Active vision\n\nAn area of computer vision is active vision, sometimes also called \"active computer vision\". An active vision system is one that can manipulate the viewpoint of the camera(s) in order to investigate the environment and get better information from it.\n\nThe interest in active camera system started as early as two decades ago. Beginning in the late 1980s, Aloimonos et al. introduced the first general framework for active vision in order to improve the perceptual quality of tracking results. Active vision is particularly important to cope with problems like occlusions, limited field of view and limited resolution of the camera. Other advantages can be reducing the motion blur of a moving object and enhancing depth perception of an object by focusing two cameras on the same object or moving the cameras. \nActive control of the camera view point also helps in focusing computational resources on the relevant element of the scene. In this selective aspect, active vision can be seen as strictly related to (overt & covert) visual attention in biological organisms, which has been shown to enhance the perception of selected part of the visual field. This selective aspect of human (active) vision can be easily related to the foveal structure of the human eye, where in about 5% of the retina more than the 50% of the colour receptors are located.\n\nIt has also been suggested that visual attention and the selective aspect of active camera control can help in other tasks like learning more robust models of objects and environments with less labeled samples or autonomously\n\nAutonomous cameras are cameras that can direct themselves in their environment. There has been some recent work using this approach. In work from Denzler et al., the motion of a tracked object is modeled using a Kalman filter while the focal length that minimizes the uncertainty in the state estimations is the one that is used. A stereo set-up with two zoom cameras was used. A handful of papers have been written for zoom control and do not deal with total object-camera position estimation. An attempt to join estimation and control in the same framework can be found in the work of Bagdanov et al., where a Pan-Tilt-Zoom camera is used to track faces. Both the estimation and control models used are ad hoc, and the estimation approach is based on image features rather than 3D properties of the target being tracked.\n\nIn a master/slave configuration, a supervising static camera is used to monitor a wide field of view and to track every moving target of interest. The position of each of these targets over time is then provided to a foveal camera, which tries to observe the targets at a higher resolution. Both the static and the active cameras are calibrated to a common reference, so that data coming from one of them can be easily projected onto the other, in order to coordinate the control of the active sensors. Another possible use of the master/slave approach consists of a static (master) camera extracting visual features of an object of interest, while the active (slave) sensor uses these features to detect the desired object without the need of any training data.\n\nIn recent years there has been growing interest in building networks of active cameras and optional static cameras so that you can cover a large area while maintaining high resolution of multiple targets. This is ultimately a scaled-up version of either the master/slave approach or the autonomous camera approach. This approach can be highly effective, but also incredibly costly. Not only are multiple cameras involved but you also must have them communicate with each other which can be computationally expensive.\n\nControlled active vision can be defined as a controlled motion of a vision sensor can maximize the performance of any robotic algorithm that involves a moving vision sensor. It is a hybrid of control theory and conventional vision. An application of this framework is real-time robotic servoing around static or moving arbitrary 3-D objects. See Visual Servoing. Algorithms that incorporate the use of multiple windows and numerically stable confidence measures are combined with stochastic controllers in order to provide a satisfactory solution to the tracking problem introduced by combining computer vision and control. In the case where there is an inaccurate model of the environment, adaptive control techniques may be introduced. The above information and further mathematical representations of controlled active vision can be seen in the thesis of Nikolaos Papanikolopoulos.\n\nExamples of active vision systems usually involve a robot mounted camera, but other systems have employed human operator-mounted cameras (a.k.a. \"wearables\"). Applications include automatic surveillance, human robot interaction (video), SLAM, route planning, etc. In the DARPA Grand Challenge most of the teams used LIDAR combined with active vision systems\nto guide driverless vehicles across an off-road course.\n\nA good example of active vision can be seen in this YouTube video. It shows face tracking using active vision with a pan-tilt camera system. https://www.youtube.com/watch?v=N0FjDOTnmm0\n\nActive Vision is also important to understand how humans.\nand organism endowed with visual sensors, actually see the world considering the limits of their sensors, the richness and continuous variability of the visual signal and the effects of their actions and goals on their perception.\n\nThe controllable active vision framework can be used in a number of different ways. Some examples might be vehicle tracking, robotics applications, and interactive MRI segmentation.\n\nInteractive MRI segmentation uses controllable active vision by using a Lyapanov control design to establish a balance between the influence of a data-driven gradient flow and the human’s input over time. This smoothly couples automatic segmentation with interactivity. More information on this method can be found in. Segmentation in MRIs is a difficult subject, and it takes an expert to trace out the desired segments due to the MRI picking up all fluid and tissue. This could prove impractical because it would be a very lengthy process. Controllable active vision methods described in the cited paper could help improve the process while relying on the human less.\n\nVarious downloads of different implementations of active vision can be found from this link to the active vision lab at Oxford University. http://www.robots.ox.ac.uk/ActiveVision/Downloads/index.html\n\n- Active Vision Group at Oxford University.\n- Active Vision Laboratory at University of Edinburgh.\n- Active Vision Tracking System for MAV developed by University of New South Wales.\n", "related": "NONE"}
{"id": "15966023", "url": "https://en.wikipedia.org/wiki?curid=15966023", "title": "Pyramid (image processing)", "text": "Pyramid (image processing)\n\nPyramid, or pyramid representation, is a type of multi-scale signal representation developed by the computer vision, image processing and signal processing communities, in which a signal or an image is subject to repeated smoothing and subsampling. Pyramid representation is a predecessor to scale-space representation and multiresolution analysis.\n\nThere are two main types of pyramids: lowpass and bandpass.\n\nA lowpass pyramid is made by smoothing the image with an appropriate smoothing filter and then subsampling the smoothed image, usually by a factor of 2 along each coordinate direction. The resulting image is then subjected to the same procedure, and the cycle is repeated multiple times. Each cycle of this process results in a smaller image with increased smoothing, but with decreased spatial sampling density (that is, decreased image resolution). If illustrated graphically, the entire multi-scale representation will look like a pyramid, with the original image on the bottom and each cycle's resulting smaller image stacked one atop the other.\n\nA bandpass pyramid is made by forming the difference between images at adjacent levels in the pyramid and performing image interpolation between adjacent levels of resolution, to enable computation of pixelwise differences.\n\nA variety of different smoothing kernels have been proposed for generating pyramids. Among the suggestions that have been given, the \"binomial kernels\" arising from the binomial coefficients stand out as a particularly useful and theoretically well-founded class. Thus, given a two-dimensional image, we may apply the (normalized) binomial filter (1/4, 1/2, 1/4) typically twice or more along each spatial dimension and then subsample the image by a factor of two. This operation may then proceed as many times as desired, leading to a compact and efficient multi-scale representation. If motivated by specific requirements, intermediate scale levels may also be generated where the subsampling stage is sometimes left out, leading to an \"oversampled\" or \"hybrid pyramid\". With the increasing computational efficiency of CPUs available today, it is in some situations also feasible to use wider support Gaussian filters as smoothing kernels in the pyramid generation steps.\n\nIn a Gaussian pyramid, subsequent images are weighted down using a Gaussian average (Gaussian blur) and scaled down. Each pixel containing a local average corresponds to a neighborhood pixel on a lower level of the pyramid. This technique is used especially in texture synthesis.\n\nA Laplacian pyramid is very similar to a Gaussian pyramid but saves the difference image of the blurred versions between each levels. Only the smallest level is not a difference image to enable reconstruction of the high resolution image using the difference images on higher levels. This technique can be used in image compression.\n\nA steerable pyramid, developed by Simoncelli and others, is an implementation of a multi-scale, multi-orientation band-pass filter bank used for applications including image compression, texture synthesis, and object recognition. It can be thought of as an orientation selective version of a Laplacian pyramid, in which a bank of steerable filters are used at each level of the pyramid instead of a single Laplacian or Gaussian filter.\n\nIn the early days of computer vision, pyramids were used as the main type of multi-scale representation for computing multi-scale image features from real-world image data. More recent techniques include scale-space representation, which has been popular among some researchers due to its theoretical foundation, the ability to decouple the subsampling stage from the multi-scale representation, the more powerful tools for theoretical analysis as well as the ability to compute a representation at \"any\" desired scale, thus avoiding the algorithmic problems of relating image representations at different resolution. Nevertheless, pyramids are still frequently used for expressing computationally efficient approximations to scale-space representation.\n\nLaplacian image pyramids based on the bilateral filter provide a good framework for image detail enhancement and manipulation. The difference images between each layer are modified to exaggerate or reduce details at different scales in an image.\n\nSome image compression file formats use the Adam7 algorithm or some other interlacing technique.\nThese can be seen as a kind of image pyramid.\nBecause those file format store the \"large-scale\" features first, and fine-grain details later in the file,\na particular viewer displaying a small \"thumbnail\" or on a small screen can quickly download just enough of the image to display it in the available pixels—so one file can support many viewer resolutions, rather than having to store or generate a different file for each resolution.\n\n", "related": "\n- Mipmap\n- Scale space implementation\n- Level of detail\n- JPEG 2000#Multiple resolution representation\n\n- Gaussian-Laplacian Pyramid Image Coding - illustrates methods of Downsampling, Upsampling, and Gaussian convolution\n- The Gaussian Pyramid - provides a brief introduction for the procedure and cites several sources\n- Laplacian Irregular Graph Pyramid - Figure 1 on this page illustrates an example of the Gaussian Pyramid\n- The Laplacian Pyramid as a Compact Image Code on eBook Submission\n"}
{"id": "19132709", "url": "https://en.wikipedia.org/wiki?curid=19132709", "title": "Photometric stereo", "text": "Photometric stereo\n\nPhotometric stereo is a technique in computer vision for estimating the surface normals of objects by observing that object under different lighting conditions. It is based on the fact that the amount of light reflected by a surface is dependent on the orientation of the surface in relation to the light source and the observer. By measuring the amount of light reflected into a camera, the space of possible surface orientations is limited. Given enough light sources from different angles, the surface orientation may be constrained to a single orientation or even overconstrained.\n\nThe technique was originally introduced by Woodham in 1980. The special case where the data is a single image is known as shape from shading, and was analyzed by B. K. P. Horn in 1989. Photometric stereo has since been generalized to many other situations, including extended light sources and non-Lambertian surface finishes. Current research aims to make the method work in the presence of projected shadows, highlights, and non-uniform lighting.\n\nUnder Woodham's original assumptions — Lambertian reflectance, known point-like distant light sources, and uniform albedo — the problem can be solved by inverting the linear equation formula_1, where formula_2 is a (known) vector of formula_3 observed intensities, formula_4 is the (unknown) surface normal, and formula_5 is a (known) formula_6 matrix of normalized light directions.\n\nThis model can easily be extended to surfaces with non-uniform albedo, while keeping the problem linear. Taking an albedo reflectivity of formula_7, the formula for the reflected light intensity becomes:\n\nIf formula_5 is square (there are exactly 3 lights) and non-singular, it can be inverted, giving:\n\nSince the normal vector is known to have length 1, formula_7 must be the length of the vector formula_12, and formula_4 is the normalised direction of that vector.\nIf formula_5 is not square (there are more than 3 lights), a generalisation of the inverse can be obtained using the Moore-Penrose pseudoinverse, by simply multiplying both sides with formula_15 giving:\n\nAfter which the normal vector and albedo can be solved as described above.\n\nThe classical photometric stereo problem concerns itself only with Lambertian surfaces, with perfectly diffuse reflection. This is unrealistic for many types of materials, especially metals, glass and smooth plastics, and will lead to aberrations in the resulting normal vectors.\n\nMany methods have been developed to lift this assumption. In this section, a few of these are listed.\n\nHistorically, in computer graphics, the commonly used model to render surfaces started with Lambertian surfaces and progressed first to include simple specular reflections. Computer vision followed a similar course with photometric stereo. Specular reflections were among the first deviations from the Lambertian model. These are a few adaptations that have been developed.\n\n- Many techniques ultimately rely on modelling the reflectance function of the surface, that is, how much light is reflected in each direction. This reflectance function has to be invertible. The reflected light intensities towards the camera is measured, and the inverse reflectance function is fit onto the measured intensities, resulting in a unique solution for the normal vector.\n\nAccording to the Bidirectional reflectance distribution function (BRDF) model, a surface may distribute the amount of light it receives in any outward direction. This is the most general known model for opaque surfaces. Some techniques have been developed to model (almost) general BRDFs. In practice, all of these require many light sources to obtain reliable data. These are methods in which surfaces with general BRDFs can be measured.\n\n- Determine the explicit BRDF prior to scanning. To do this, a different surface is required that has the same or a very similar BRDF, of which the actual geometry (or at least the normal vectors for many points on the surface) is already known. The lights are then individually shone upon the known surface, and the amount of reflection into the camera is measured. Using this information, a look-up table can be created that maps reflected intensities for each light source to a list of possible normal vectors. This puts constraints on the possible normal vectors the surface may have, and reduces the photometric stereo problem to an interpolation between measurements. Typical known surfaces to calibrate the look-up table with are spheres for their wide variety of surface orientations.\n- Restricting the BRDF to be symmetrical. If the BRDF is symmetrical, the direction of the light can be restricted to a cone about the direction to the camera. Which cone this is depends on the BRDF itself, the normal vector of the surface, and the measured intensity. Given enough measured intensities and the resulting light directions, these cones can be approximated and therefore the normal vectors of the surface.\n\nSome progress has been made towards modelling an even more general surfaces, such as Spatially Varying Bidirectional Distribution Functions (SVBRDF), Bidirectional surface scattering reflectance distribution functions (BSSRDF), and accounting for interreflections. However, such methods are still fairly restrictive in photometric stereo. Better results have been achieved with structured light.\n\n", "related": "\n- Photometry\n- Stereo vision\n- 3D scanner\n"}
{"id": "11591518", "url": "https://en.wikipedia.org/wiki?curid=11591518", "title": "Visual descriptor", "text": "Visual descriptor\n\nIn computer vision, visual descriptors or image descriptors are descriptions of the visual features of the contents in images, videos, or algorithms or applications that produce such descriptions. They describe elementary characteristics such as the shape, the color, the texture or the motion, among others.\n\nAs a result of the new communication technologies and the massive use of Internet in our society, the amount of audio-visual information available in digital format is increasing considerably. Therefore, it has been necessary to design some systems that allow us to describe the content of several types of multimedia information in order to search and classify them.\n\nThe audio-visual descriptors are in charge of the contents description. These descriptors have a good knowledge of the objects and events found in a video, image or audio and they allow the quick and efficient searches of the audio-visual content.\n\nThis system can be compared to the search engines for textual contents. Although it is certain, that it is relatively easy to find text with a computer, is much more difficult to find concrete audio and video parts. For instance, imagine somebody searching a scene of a happy person. The happiness is a feeling and it is not evident its shape, color and texture description in images.\n\nThe description of the audio-visual content is not a superficial task and it is essential for the effective use of this type of archives. The standardization system that deals with audio-visual descriptors is the MPEG-7 (\"Motion Picture Expert Group - 7\").\n\nDescriptors are the first step to find out the connection between pixels contained in a digital image and what humans recall after having observed an image or a group of images after some minutes.\n\nVisual descriptors are divided in two main groups: \n1. General information descriptors: they contain low level descriptors which give a description about color, shape, regions, textures and motion.\n2. Specific domain information descriptors: they give information about objects and events in the scene. A concrete example would be face recognition.\n\nGeneral information descriptors consist of a set of descriptors that covers different basic and elementary features like: color, texture, shape, motion, location and others. This description is automatically generated by means of signal processing.\n\n- COLOR: the most basic quality of visual content. Five tools are defined to describe color. The three first tools represent the color distribution and the last ones describe the color relation between sequences or group of images:\n- \"Dominant Color Descriptor (DCD)\"\n- \"Scalable Color Descriptor (SCD)\"\n- \"Color Structure Descriptor (CSD)\"\n- \"Color Layout Descriptor (CLD)\"\n- \"Group of frame (GoF)\" or \"Group-of-pictures (GoP)\"\n- TEXTURE: also, an important quality in order to describe an image. The texture descriptors characterize image textures or regions. They observe the region homogeneity and the histograms of these region borders. The set of descriptors is formed by:\n- \"Homogeneous Texture Descriptor (HTD)\"\n- \"Texture Browsing Descriptor (TBD) \"\n- \"Edge Histogram Descriptor (EHD)\"\n- SHAPE: contains important semantic information due to human’s ability to recognize objects through their shape. However, this information can only be extracted by means of a segmentation similar to the one that the human visual system implements. Nowadays, such a segmentation system is not available yet, however there exists a serial of algorithms which are considered to be a good approximation. These descriptors describe regions, contours and shapes for 2D images and for 3D volumes. The shape descriptors are the following ones:\n- \"Region-based Shape Descriptor (RSD)\"\n- \"Contour-based Shape Descriptor (CSD)\"\n- \"3-D Shape Descriptor (3-D SD)\"\n- MOTION: defined by four different descriptors which describe motion in video sequence. Motion is related to the objects motion in the sequence and to the camera motion. This last information is provided by the capture device, whereas the rest is implemented by means of image processing. The descriptor set is the following one:\n- \"Motion Activity Descriptor (MAD)\"\n- \"Camera Motion Descriptor (CMD)\"\n- \"Motion Trajectory Descriptor (MTD)\"\n- \"Warping and Parametric Motion Descriptor (WMD and PMD)\"\n- LOCATION: elements location in the image is used to describe elements in the spatial domain. In addition, elements can also be located in the temporal domain:\n- \"Region Locator Descriptor (RLD)\"\n- \"Spatio Temporal Locator Descriptor (STLD)\"\n\nThese descriptors, which give information about objects and events in the scene, are not easily extractable, even more when the extraction is to be automatically done. Nevertheless, they can be manually processed.\n\nAs mentioned before, face recognition is a concrete example of an application that tries to automatically obtain this information.\n\nAmong all applications, the most important ones are:\n- Multimedia documents search engines and classifiers.\n- Digital library: visual descriptors allow a very detailed and concrete search of any video or image by means of different search parameters. For instance, the search of films where a known actor appears, the search of videos containing the Everest mountain, etc.\n- Personalized electronic news service.\n- Possibility of an automatic connection to a TV channel broadcasting a soccer match, for example, whenever a player approaches the goal area.\n- Control and filtering of concrete audio-visual contents, like violent or pornographic material. Also, authorization for some multimedia contents.\n\n", "related": "\n- MPEG-7\n- DSpace\n- Feature detection\n- Scale-invariant feature transform\n\nB.S. Manjunath (Editor), Philippe Salembier (Editor), and Thomas Sikora (Editor): \"Introduction to MPEG-7: Multimedia Content Description Interface\". Wiley & Sons, April 2002 - \n\n- Multimedia Content Analysis Using both Audio and Video Clues\n- Relating Visual and Semantic Image Descriptors\n- Fusing MPEG-7 visual descriptors for image classication\n- MPEG-7 Quick Reference\n"}
{"id": "21610688", "url": "https://en.wikipedia.org/wiki?curid=21610688", "title": "Joint compatibility branch and bound", "text": "Joint compatibility branch and bound\n\nJoint compatibility branch and bound (JCBB) is an algorithm in computer vision and robotics commonly used for data association in simultaneous localization and mapping. JCBB measures the joint compatibility of a set of pairings that successfully rejects spurious matchings and is hence known to be robust in complex environments.\n", "related": "NONE"}
{"id": "22714727", "url": "https://en.wikipedia.org/wiki?curid=22714727", "title": "Photo-consistency", "text": "Photo-consistency\n\nIn computer vision Photo-consistency determines whether a given voxel is occupied. A voxel is considered to be photo consistent when its color appears to be similar to all the cameras that can see it. Most voxel coloring or space carving techniques require using photo consistency as a check condition in Image-based modeling and rendering applications.\n\n3D Volumetric Reconstruction. \n- Image registration.\n- Multi-view reconstruction.\n", "related": "NONE"}
{"id": "10416897", "url": "https://en.wikipedia.org/wiki?curid=10416897", "title": "Image fusion", "text": "Image fusion\n\nThe image fusion process is defined as gathering all the important information from multiple images, and their inclusion into fewer images, usually a single one. This single image is more informative and accurate than any single source image, and it consists of all the necessary information. The purpose of image fusion is not only to reduce the amount of data but also to construct images that are more appropriate and understandable for the human and machine perception. In computer vision, multisensor image fusion is the process of combining relevant information from two or more images into a single image. The resulting image will be more informative than any of the input images.\n\nIn remote sensing applications, the increasing availability of space borne sensors gives a motivation for different image fusion algorithms.\nSeveral situations in image processing require high spatial and high spectral resolution in a single image. Most of the available equipment is not capable of providing such data convincingly. Image fusion techniques allow the integration of different information sources. The fused image can have complementary spatial and spectral resolution characteristics. However, the standard image fusion techniques can distort the spectral information of the multispectral data while merging.\n\nIn satellite imaging, two types of images are available. The panchromatic image acquired by satellites is transmitted with the maximum resolution available and the multispectral data are transmitted with coarser resolution. This will usually be two or four times lower. At the receiver station, the panchromatic image is merged with the multispectral data to convey more information.\n\nMany methods exist to perform image fusion. The very basic one is the high pass filtering technique. Later techniques are based on Discrete Wavelet Transform, uniform rational filter bank, and Laplacian pyramid.\n\nMulti-focus image fusion is used to collect useful and necessary information from input images with different focus depths in order to create an output image that ideally has all information from input images. In visual sensor network (VSN), sensors are cameras which record images and video sequences. In many applications of VSN, a camera can’t give a perfect illustration including all details of the scene. This is because of the limited depth of focus exists in the optical lens of cameras. Therefore, just the object located in the focal length of camera is focused and cleared and the other parts of image are blurred. VSN has an ability to capture images with different depth of focuses in the scene using several cameras. Due to the large amount of data generated by camera compared to other sensors such as pressure and temperature sensors and some limitation such as limited band width, energy consumption and processing time, it is essential to process the local input images to decrease the amount of transmission data. The aforementioned reasons emphasize the necessary of multi-focus images fusion. Multi-focus image fusion is a process which combines the input multi-focus images into a single image including all important information of the input images and it’s more accurate explanation of the scene than every single input image.\n\nMulti sensor data fusion has become a discipline which demands more general formal solutions to a number of application cases. Several situations in image processing require both high spatial and high spectral information in a single image. This is important in remote sensing. However, the instruments are not capable of providing such information either by design or because of observational constraints. One possible solution for this is data fusion.\n\nImage fusion methods can be broadly classified into two groups - spatial domain fusion and transform domain fusion.\n\nThe fusion methods such as averaging, Brovey method, principal component analysis (PCA) and IHS based methods fall under spatial domain approaches. Another important spatial domain fusion method is the high pass filtering based technique. Here the high frequency details are injected into upsampled version of MS images. The disadvantage of spatial domain approaches is that they produce spatial distortion in the fused image. Spectral distortion becomes a negative factor while we go for further processing, such as classification problem. Spatial distortion can be very well handled by frequency domain approaches on image fusion. The multiresolution analysis has become a very useful tool for analysing remote sensing images. The discrete wavelet transform has become a very useful tool for fusion. Some other fusion methods are also there, such as Laplacian pyramid based, curvelet transform based etc. These methods show a better performance in spatial and spectral quality of the fused image compared to other spatial methods of fusion.\n\nThe images used in image fusion should already be registered. Misregistration is a major source of error in image fusion. Some well-known image fusion methods are:\n- High pass filtering technique\n- IHS transform based image fusion\n- PCA based image fusion\n- Wavelet transform image fusion\n- Pair-wise spatial frequency matching\n\nImage fusion in remote sensing has several application domains. An important domain is the multi-resolution image fusion (commonly referred to pan-sharpening). In satellite imagery we can have two types of images\n- Panchromatic images - An image collected in the broad visual wavelength range but rendered in black and white.\n- Multispectral images - Images optically acquired in more than one spectral or wavelength interval. Each individual image is usually of the same physical area and scale but of a different spectral band.\n\nThe SPOT PAN satellite provides high resolution (10m pixel) panchromatic data. While the LANDSAT TM satellite provides low resolution (30m pixel) multispectral images. Image fusion attempts to merge these images and produce a single high resolution multispectral image.\n\nThe standard merging methods of image fusion are based on Red-Green-Blue (RGB) to Intensity-Hue-Saturation (IHS) transformation. The usual steps involved in satellite image fusion are as follows:\n1. Resize the low resolution multispectral images to the same size as the panchromatic image.\n2. Transform the R, G and B bands of the multispectral image into IHS components.\n3. Modify the panchromatic image with respect to the multispectral image. This is usually performed by histogram matching of the panchromatic image with Intensity component of the multispectral images as reference.\n4. Replace the intensity component by the panchromatic image and perform inverse transformation to obtain a high resolution multispectral image.\n\nPan-sharpening can be done with Photoshop. Other applications of image fusion in remote sensing are available.\n\nImage fusion has become a common term used within medical diagnostics and treatment. The term is used when multiple images of a patient are registered and overlaid or merged to provide additional information. Fused images may be created from multiple images from the same imaging modality, or by combining information from multiple modalities, such as magnetic resonance image (MRI), computed tomography (CT), positron emission tomography (PET), and single photon emission computed tomography (SPECT). In radiology and radiation oncology, these images serve different purposes. For example, CT images are used more often to ascertain differences in tissue density while MRI images are typically used to diagnose brain tumors.\n\nFor accurate diagnosis, radiologists must integrate information from multiple image formats. Fused, anatomically consistent images are especially beneficial in diagnosing and treating cancer. With the advent of these new technologies, radiation oncologists can take full advantage of intensity modulated radiation therapy (IMRT). Being able to overlay diagnostic images into radiation planning images results in more accurate IMRT target tumor volumes.\n\nComparative analysis of image fusion methods demonstrates that different metrics support different user needs, sensitive to different image fusion methods, and need to be tailored to the application. Categories of image fusion metrics are based on information theory features, structural similarity, or human perception.\n\n", "related": "\n- Sensor fusion\n- Data fusion\n\n- http://www.math.hcmuns.edu.vn/~ptbao/LVTN/2003/cameras/a161001433035.pdf Z. Wang, D. Ziou, C. Armenakis, D. Li, and Q. Li, “A comparative analysis of image fusion methods,” IEEE Trans. Geosci. Remote Sens., vol. 43, no. 6, pp. 81–84, Jun. 2005\n"}
{"id": "16928506", "url": "https://en.wikipedia.org/wiki?curid=16928506", "title": "Visual servoing", "text": "Visual servoing\n\nVisual servoing, also known as vision-based robot control and abbreviated VS, is a technique which uses feedback information extracted from a vision sensor (visual feedback) to control the motion of a robot. One of the earliest papers that talks about visual servoing was from the SRI International Labs in 1979.\n\nThere are two fundamental configurations of the robot end-effector (hand) and the camera:\n\n- Eye-in-hand, or end-point closed-loop control, where the camera is attached to the moving hand and observing the relative position of the target.\n- Eye-to-hand, or end-point open-loop control, where the camera is fixed in the world and observing the target and the motion of the hand.\n\nVisual Servoing control techniques are broadly classified into the following types:\n- Image-based (IBVS)\n- Position/pose-based (PBVS)\n- Hybrid approach\n\nIBVS was proposed by Weiss and Sanderson. The control law is based on the error between current and desired features on the image plane, and does not involve any estimate of the pose of the target. The features may be the coordinates of visual features, lines or moments of regions. IBVS has difficulties with motions very large rotations, which has come to be called camera retreat.\n\nPBVS is a model-based technique (with a single camera). This is because the pose of the object of interest is estimated with respect to the camera and then a command is issued to the robot controller, which in turn controls the robot. In this case the image features are extracted as well, but are additionally used to estimate 3D information (pose of the object in Cartesian space), hence it is servoing in 3D.\n\nHybrid approaches use some combination of the 2D and 3D servoing. There have been a few different approaches to hybrid servoing\n- 2-1/2-D Servoing\n- Motion partition-based\n- Partitioned DOF Based\n\nThe following description of the prior work is divided into 3 parts\n\n- Survey of existing visual servoing methods.\n- Various features used and their impacts on visual servoing.\n- Error and stability analysis of visual servoing schemes.\n\nVisual servo systems, also called servoing, have been around since the early 1980s \n, although the term visual servo itself was only coined in 1987.\nVisual Servoing is, in essence, a method for robot control where the sensor used is a camera (visual sensor). \nServoing consists primarily of two techniques,\none involves using information from the image to directly control the degrees of freedom (DOF) of the robot, thus referred to as Image Based Visual Servoing (IBVS).\nWhile the other involves the geometric interpretation of the information extracted from the camera, such as estimating the pose of the target and parameters of the camera (assuming some basic model of the target is known). Other servoing classifications exist based on the variations in each component of a servoing system \ne.g. the location of the camera, the two kinds are eye-in-hand and hand–eye configurations. \nBased on the control loop, the two kinds are end-point-open-loop and end-point-closed-loop. Based on whether the control is applied to the joints (or DOF)\ndirectly or as a position command to a robot controller the two types are\ndirect servoing and dynamic look-and-move.\nBeing one of the earliest works \nthe authors proposed a hierarchical\nvisual servo scheme applied to image-based servoing. The technique relies on\nthe assumption that a good set of features can be extracted from the object\nof interest (e.g. edges, corners and centroids) and used as a partial model\nalong with global models of the scene and robot. The control strategy is\napplied to a simulation of a two and three DOF robot arm.\n\nFeddema et al.\nintroduced the idea of generating task trajectory\nwith respect to the feature velocity. This is to ensure that the sensors are\nnot rendered ineffective (stopping the feedback) for any the robot motions.\nThe authors assume that the objects are known a priori (e.g. CAD model)\nand all the features can be extracted from the object.\nThe work by Espiau et al.\ndiscusses some of the basic questions in\nvisual servoing. The discussions concentrate on modeling of the interaction\nmatrix, camera, visual features (points, lines, etc..).\nIn \nan adaptive servoing system was proposed with a look-and-move\nservoing architecture. The method used optical flow along with SSD to\nprovide a confidence metric and a stochastic controller with Kalman filtering\nfor the control scheme. The system assumes (in the examples) that the plane\nof the camera and the plane of the features are parallel., discusses an approach of velocity control using the Jacobian relationship s˙ = Jv˙ . In addition the author uses Kalman filtering, assuming that\nthe extracted position of the target have inherent errors (sensor errors). A\nmodel of the target velocity is developed and used as a feed-forward input\nin the control loop. Also, mentions the importance of looking into kinematic\ndiscrepancy, dynamic effects, repeatability, settling time oscillations and lag\nin response.\n\nCorke poses a set of very critical questions on visual servoing and tries\nto elaborate on their implications. The paper primarily focuses the dynamics\nof visual servoing. The author tries to address problems like lag and stability,\nwhile also talking about feed-forward paths in the control loop. The paper\nalso, tries to seek justification for trajectory generation, methodology of axis\ncontrol and development of performance metrics.\n\nChaumette in provides good insight into the two major problems with\nIBVS. One, servoing to a local minima and second, reaching a Jacobian singularity. The author show that image points alone do not make good features\ndue to the occurrence of singularities. The paper continues, by discussing the\npossible additional checks to prevent singularities namely, condition numbers\nof J_s and Jˆ+_s, to check the null space of ˆ J_s and J^T_s . One main point that\nthe author highlights is the relation between local minima and unrealizable\nimage feature motions.\n\nOver the years many hybrid techniques have been developed. These\ninvolve computing partial/complete pose from Epipolar Geometry using multiple views or multiple cameras. The values are obtained by direct estimation or through a learning or a statistical scheme. While others have used\na switching approach that changes between image-based and position-based\nbased on a Lyapnov function.\nThe early hybrid techniques that used a combination of image-based and\npose-based (2D and 3D information) approaches for servoing required either\na full or partial model of the object in order to extract the pose information\nand used a variety of techniques to extract the motion information from the\nimage. used an affine motion model from the image motion in addition\nto a rough polyhedral CAD model to extract the object pose with respect to\nthe camera to be able to servo onto the object (on the lines of PBVS).\n\n2-1/2-D visual servoing developed by Malis et al. is a well known technique that breaks down the information required for servoing into an organized fashion which decouples rotations and translations. The papers\nassume that the desired pose is known a priori. The rotational information is\nobtained from partial pose estimation, a homography, (essentially 3D information) giving an axis of rotation and the angle (by computing the eigenvalues and eigenvectors of the homography). The translational information is\nobtained from the image directly by tracking a set of feature points. The only\nconditions being that the feature points being tracked never leave the field of\nview and that a depth estimate be predetermined by some off-line technique.\n2-1/2-D servoing has been shown to be more stable than the techniques that\npreceded it. Another interesting observation with this formulation is that\nthe authors claim that the visual Jacobian will have no singularities during\nthe motions.\nThe hybrid technique developed by Corke and Hutchinson, popularly called portioned approach partitions the visual (or image) Jacobian into\nmotions (both rotations and translations) relating X and Y axes and motions related to the Z axis. outlines the technique, to break out columns\nof the visual Jacobian that correspond to the Z axis translation and rotation\n(namely, the third and sixth columns). The partitioned approach is shown to\nhandle the Chaumette Conundrum discussed in. This technique requires\na good depth estimate in order to function properly.\nnamely main and secondary. The main task is keep the features of inter-\nest within the field of view. While the secondary task is to mark a fixation\npoint and use it as a reference to bring the camera to the desired pose. The\ntechnique does need a depth estimate from an off-line procedure. The paper\ndiscusses two examples for which depth estimates are obtained from robot\nodometry and by assuming that all features are on a plane. The secondary\ntask is achieved by using the notion of parallax. The features that are tracked\nare chosen by an initialization performed on the first frame, which are typi-\ncally points.\nmodeling and model-based tracking. Primary assumption made is that the\n3D model of the object is available. The authors highlights the notion that\nideal features should be chosen such that the DOF of motion can be decoupled\nby linear relation. The authors also introduce an estimate of the target\nvelocity into the interaction matrix to improve tracking performance. The\nresults are compared to well known servoing techniques even when occlusions\noccur.\n\nThis section discusses the work done in the field of visual servoing. We try\nto track the various techniques in the use of features. Most of the work\nhas used image points as visual features. The formulation of the interaction\nmatrix in assumes points in the image are used to represent the target.\nThere has some body of work that deviates from the use of points and use\nfeature regions, lines, image moments and moment invariants.\nIn, the authors discuss an affine based tracking of image features.\nThe image features are chosen based on a discrepancy measure, which is\nbased on the deformation that the features undergo. The features used were\ntexture patches. One of key points of the paper was that it highlighted the\nneed to look at features for improving visual servoing.\nIn the authors look into choice of image features (the same question\nwas also discussed in in the context of tracking). The effect of the choice\nof image features on the control law is discussed with respect to just the\ndepth axis. Authors consider the distance between feature points and the\narea of an object as features. These features are used in the control law with\nslightly different forms to highlight the effects on performance. It was noted\nthat better performance was achieved when the servo error was proportional\nto the change in depth axis.\nauthors provide a new formulation of the interaction matrix using the velocity\nof the moments in the image, albeit complicated. Even though the moments\nare used, the moments are of the small change in the location of contour\npoints with the use of Green’s theorem. The paper also tries to determine\nthe set of features (on a plane) to for a 6 DOF robot.\nIn discusses the use of image moments to formulate the visual Jacobian.\nThis formulation allows for decoupling of the DOF based on type of moments\nchosen. The simple case of this formulation is notionally similar to the 2-1/2-\nD servoing. The time variation of the moments (m˙ij) are determined using\nthe motion between two images and Greens Theorem. The relation between\nm˙ij and the velocity screw (v) is given as m˙_ij = L_m_ij v. This technique\navoids camera calibration by assuming that the objects are planar and using\na depth estimate. The technique works well in the planar case but tends to\nbe complicated in the general case. The basic idea is based on the work in [4]\nMoment Invariants have been used in. The key idea being to find\nthe feature vector that decouples all the DOF of motion. Some observations\nmade were that centralized moments are invariant for 2D translations. A\ncomplicated polynomial form is developed for 2D rotations. The technique\nfollows teaching-by-showing, hence requiring the values of desired depth and\narea of object (assuming that the plane of camera and object are parallel,\nand the object is planar). Other parts of the feature vector are invariants\nR3,R4. The authors claim that occlusions can be handled.\nence being that the authors use a technique similar to, where the task is\nbroken into two (in the case where the features are not parallel to the cam-\nera plane). A virtual rotation is performed to bring the featured parallel to\nthe camera plane. consolidates the work done by the authors on image\nmoments.\n\nEspiau in showed from purely experimental work that image based visual servoing (IBVS)\nis robust to calibration errors. The author used a camera with no explicit\ncalibration along with point matching and without pose estimation. The\npaper looks at the effect of errors and uncertainty on the terms in the interaction matrix from an experimental approach. The targets used were points\nand were assumed to be planar.\n\nA similar study was done in where the\nauthors carry out experimental evaluation of a few uncalibrated visual servo\nsystems that were popular in the 90’s. The major outcome was the experimental evidence of the effectiveness of visual servo control over conventional\ncontrol methods.\nKyrki et al. analyze servoing errors for position based and 2-1/2-D\nvisual servoing. The technique involves determining the error in extracting\nimage position and propagating it to pose estimation and servoing control.\nPoints from the image are mapped to points in the world a priori to obtain a mapping (which is basically the homography, although not explicitly stated\nin the paper). This mapping is broken down to pure rotations and translations. Pose estimation is performed using standard technique from Computer\nVision. Pixel errors are transformed to the pose. These are propagating to\nthe controller. An observation from the analysis shows that errors in the\nimage plane are proportional to the depth and error in the depth-axis is\nproportional to square of depth.\nMeasurement errors in visual servoing have been looked into extensively.\nMost error functions relate to two aspects of visual servoing. One being\nsteady state error (once servoed) and two on the stability of the control\nloop. Other servoing errors that have been of interest are those that arise\nfrom pose estimation and camera calibration. In, the authors extend the\nwork done in by considering global stability in the presence of intrinsic\nand extrinsic calibration errors. provides an approach to bound the task\nfunction tracking error. In, the authors use teaching-by-showing visual\nservoing technique. Where the desired pose is known a priori and the robot\nis moved from a given pose. The main aim of the paper is to determine the\nupper bound on the positioning error due to image noise using a convex-\noptimization technique.\nin depth estimates. The authors conclude the paper with the observation that\nfor unknown target geometry a more accurate depth estimate is required in\norder to limit the error.\nMany of the visual servoing techniques implicitly assume that\nonly one object is present in the image and the relevant feature for tracking\nalong with the area of the object are available. Most techniques require either\na partial pose estimate or a precise depth estimate of the current and desired\npose.\n\n- Matlab toolbox for visual servoing.\n- Java-based visual servoing simulator.\n- ViSP (ViSP states for \"Visual Servoing Platform\") is a modular software that allows fast development of visual servoing applications.\n\n", "related": "\n- Robotics\n- Robot\n- Computer Vision\n- Machine Vision\n- Robot control\n\n- S. A. Hutchinson, G. D. Hager, and P. I. Corke. A tutorial on visual servo control. IEEE Trans. Robot. Automat., 12(5):651—670, Oct. 1996.\n- F. Chaumette, S. Hutchinson. Visual Servo Control, Part I: Basic Approaches. IEEE Robotics and Automation Magazine, 13(4):82-90, December 2006.\n- F. Chaumette, S. Hutchinson. Visual Servo Control, Part II: Advanced Approaches. IEEE Robotics and Automation Magazine, 14(1):109-118, March 2007.\n- Notes from IROS 2004 tutorial on advanced visual servoing.\n- Springer Handbook of Robotics Chapter 24: Visual Servoing and Visual Tracking (François Chaumette, Seth Hutchinson)\n- UW-Madison, Robotics and Intelligent Systems Lab\n- INRIA Lagadic research group\n- Johns Hopkins University, LIMBS Laboratory\n- University of Siena, SIRSLab Vision & Robotics Group\n- Tohoku University, Intelligent Control Systems Laboratory\n- INRIA Arobas research group\n- LASMEA, Rosace group\n- UIUC, Beckman Institute\n"}
{"id": "16741381", "url": "https://en.wikipedia.org/wiki?curid=16741381", "title": "Boustrophedon cell decomposition", "text": "Boustrophedon cell decomposition\n\nThe boustrophedon cell decomposition (BCD) is a method used in artificial intelligence and robotics for configuration space solutions. Like other cellular decomposition methods, this method transforms the configuration space into cell regions that can be used for path planning.\n\nA strength of the boustrophedon cell decomposition is that it allows for more diverse, non-polygonal obstacles within a configuration space. The representation still depicts polygonal obstacles, but the representations are complex enough that they are very effective when describing things like rounded surfaces, jagged edges, etc.\n\nIt is a goal of the method to optimize a path that can be chosen by an intelligent system. While a BCD can represent the existence of objects in a physical space, it does very little to nothing in terms of recognizing the objects. This would be done using another method, one which most likely requires additional sensory data in order to be used.\n", "related": "NONE"}
{"id": "1290557", "url": "https://en.wikipedia.org/wiki?curid=1290557", "title": "Phase correlation", "text": "Phase correlation\n\nPhase correlation is an approach to estimate the relative translative offset between two similar images (digital image correlation) or other data sets. It is commonly used in image registration and relies on a frequency-domain representation of the data, usually calculated by fast Fourier transforms. The term is applied particularly to a subset of cross-correlation techniques that isolate the phase information from the Fourier-space representation of the cross-correlogram.\n\nThe following image demonstrates the usage of phase correlation to determine relative translative movement between two images corrupted by independent Gaussian noise. The image was translated by (30,33) pixels. Accordingly, one can clearly see a peak in the phase-correlation representation at approximately (30,33).\n\nGiven two input images formula_1 and formula_2:\n\nApply a window function (e.g., a Hamming window) on both images to reduce edge effects (this may be optional depending on the image characteristics). Then, calculate the discrete 2D Fourier transform of both images.\n\nCalculate the cross-power spectrum by taking the complex conjugate of the second result, multiplying the Fourier transforms together elementwise, and normalizing this product elementwise.\n\nWhere formula_5 is the Hadamard product (entry-wise product) and the absolute values are taken entry-wise as well. Written out entry-wise for element index formula_6:\n\nObtain the normalized cross-correlation by applying the inverse Fourier transform.\n\nDetermine the location of the peak in formula_9.\n\nCommonly, interpolation methods are used to estimate the peak location in the cross-correlogram to non-integer values, despite the fact that the data are discrete, and this procedure is often termed 'subpixel registration'. A large variety of subpixel interpolation methods are given in the technical literature. Common peak interpolation methods such as parabolic interpolation have been used, and the OpenCV computer vision package uses a centroid-based method, though these generally have inferior accuracy compared to more sophisticated methods.\n\nBecause the Fourier representation of the data has already been computed, it is especially convenient to use the Fourier shift theorem with real-valued (sub-integer) shifts for this purpose, which essentially interpolates using the sinusoidal basis functions of the Fourier transform. An especially popular FT-based estimator is given by Foroosh \"et al.\" In this method, the subpixel peak location is approximated by a simple formula involving peak pixel value and the values of its nearest neighbors, where formula_11 is the peak value and formula_12 is the nearest neighbor in the x direction (assuming, as in most approaches, that the integer shift has already been found and the comparand images differ only by a subpixel shift).\n\nThe Foroosh \"et al.\" method is quite fast compared to most methods, though it is not always the most accurate. Some methods shift the peak in Fourier space and apply non-linear optimization to maximize the correlogram peak, but these tend to be very slow since they must apply an inverse Fourier transform or its equivalent in the objective function.\n\nIt is also possible to infer the peak location from phase characteristics in Fourier space without the inverse transformation, as noted by Stone. These methods usually use a linear least squares (LLS) fit of the phase angles to a planar model. The long latency of the phase angle computation in these methods is a disadvantage, but the speed can sometimes be comparable to the Foroosh \"et al.\" method depending on the image size. They often compare favorably in speed to the multiple iterations of extremely slow objective functions in iterative non-linear methods.\n\nSince all subpixel shift computation methods are fundamentally interpolative, the performance of a particular method depends on how well the underlying data conform to the assumptions in the interpolator. This fact also may limit the usefulness of high numerical accuracy in an algorithm, since the uncertainty due to interpolation method choice may be larger than any numerical or approximation error in the particular method.\n\nSubpixel methods are also particularly sensitive to noise in the images, and the utility of a particular algorithm is distinguished not only by its speed and accuracy but its resilience to the particular types of noise in the application.\n\nThe method is based on the Fourier shift theorem.\nLet the two images formula_1 and formula_2 be circularly-shifted versions of each other:\n\n(where the images are formula_17 in size).\n\nThen, the discrete Fourier transforms of the images will be shifted relatively in phase:\n\nOne can then calculate the normalized cross-power spectrum to factor out the phase difference:\n\nsince the magnitude of an imaginary exponential always is one, and the phase of formula_20 always is zero.\n\nThe inverse Fourier transform of a complex exponential is a Kronecker delta, i.e. a single peak:\n\nThis result could have been obtained by calculating the cross correlation directly. The advantage of this method is that the discrete Fourier transform and its inverse can be performed using the fast Fourier transform, which is much faster than correlation for large images.\n\nUnlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects typical of medical or satellite images.\n\nThe method can be extended to determine rotation and scaling differences between two images by first converting the images to log-polar coordinates. Due to properties of the Fourier transform, the rotation and scaling parameters can be determined in a manner invariant to translation.\n\nIn practice, it is more likely that formula_2 will be a simple linear shift of formula_1, rather than a circular shift as required by the explanation above. In such cases, formula_9 will not be a simple delta function, which will reduce the performance of the method. In such cases, a window function (such as a Gaussian or Tukey window) should be employed during the Fourier transform to reduce edge effects, or the images should be zero padded so that the edge effects can be ignored. If the images consist of a flat background, with all detail situated away from the edges, then a linear shift will be equivalent to a circular shift, and the above derivation will hold exactly. The peak can be sharpened by using edge or vector correlation.\n\nFor periodic images (such as a chessboard), phase correlation may yield ambiguous results with several peaks in the resulting output.\n\nPhase correlation is the preferred method for television standards conversion, as it leaves the fewest artifacts.\n\nSection::::See also.\nGeneral\n- Cross correlation\n- Scaled Correlation\n\nTelevision\n- Television standards conversion\n- Reverse Standards Conversion\n\n- Using Matlab to perform normalized cross-correlation on images\n", "related": "NONE"}
{"id": "24286785", "url": "https://en.wikipedia.org/wiki?curid=24286785", "title": "3D data acquisition and object reconstruction", "text": "3D data acquisition and object reconstruction\n\n3D data acquisition and reconstruction is the generation of three-dimensional or spatiotemporal models from sensor data. The techniques and theories, generally speaking, work with most or all sensor types including optical, acoustic, laser scanning, radar, thermal, seismic.\n\nAcquisition can occur from a multitude of methods including 2D images, acquired sensor data and on site sensors.\n\n3D data acquisition and object reconstruction can be performed using stereo image pairs. Stereo photogrammetry or photogrammetry based on a block of overlapped images is the primary approach for 3D mapping and object reconstruction using 2D images. Close-range photogrammetry has also matured to the level where cameras or digital cameras can be used to capture the close-look images of objects, e.g., buildings, and reconstruct them using the very same theory as the aerial photogrammetry. An example of software which could do this is Vexcel FotoG 5. This software has now been replaced by Vexcel GeoSynth. Another similar software program is Microsoft Photosynth.\n\nA semi-automatic method for acquiring 3D topologically structured data from 2D aerial stereo images has been presented by Sisi Zlatanova. The process involves the manual digitizing of a number of points necessary for automatically reconstructing the 3D objects. Each reconstructed object is validated by superimposition of its wire frame graphics in the stereo model. The topologically structured 3D data is stored in a database and are also used for visualization of the objects. Notable software used for 3D data acquisition using 2D images include e.g. Agisoft Photoscan, RealityCapture, and ENSAIS Engineering College TIPHON (Traitement d'Image et PHOtogrammétrie Numérique).\n\nA method for semi-automatic building extraction together with a concept for storing building models alongside terrain and other topographic data in a topographical information system has been developed by Franz Rottensteiner. His approach was based on the integration of building parameter estimations into the photogrammetry process applying a hybrid modeling scheme. Buildings are decomposed into a set of simple primitives that are reconstructed individually and are then combined by Boolean operators. The internal data structure of both the primitives and the compound building models are based on the boundary representation methods\n\nMultiple images are used in Zeng's approach to surface reconstruction from multiple images. A central idea is to explore the integration of both 3D stereo data and 2D calibrated images. This approach is motivated by the fact that only robust and accurate feature points that survived the geometry scrutiny of multiple images are reconstructed in space. The density insufficiency and the inevitable holes in the stereo data should then be filled in by using information from multiple images. The idea is thus to first construct small surface patches from stereo points, then to progressively propagate only reliable patches in their neighborhood from images into the whole surface using a best-first strategy. The problem thus reduces to searching for an optimal local surface patch going through a given set of stereo points from images.\n\nMulti-spectral images are also used for 3D building detection. The first and last pulse data and the normalized difference vegetation index are used in the process.\n\nNew measurement techniques are also employed to obtain measurements of and between objects from single images by using the projection, or the shadow as well as their combination. This technology is gaining attention given its fast processing time, and far lower cost than stereo measurements.\n\nSemi-Automatic building extraction from LIDAR Data and High-Resolution Images is also a possibility. Again, this approach allows modelling without physically moving towards the location or object. From airborne LIDAR data, digital surface model (DSM) can be generated and then the objects higher than the ground are automatically detected from the DSM. Based on general knowledge about buildings, geometric characteristics such as size, height and shape information are then used to separate the buildings from other objects. The extracted building outlines are then simplified using an orthogonal algorithm to obtain better cartographic quality. Watershed analysis can be conducted to extract the ridgelines of building roofs. The ridgelines as well as slope information are used to classify the buildings per type. The buildings are then reconstructed using three parametric building models (flat, gabled, hipped).\n\nLIDAR and other terrestrial laser scanning technology offers the fastest, automated way to collect height or distance information. LIDAR or laser for height measurement of buildings is becoming very promising. Commercial applications of both airborne LIDAR and ground laser scanning technology have proven to be fast and accurate methods for building height extraction. The building extraction task is needed to determine building locations, ground elevation, orientations, building size, rooftop heights, etc. Most buildings are described to sufficient details in terms of general polyhedra, i.e., their boundaries can be represented by a set of planar surfaces and straight lines. Further processing such as expressing building footprints as polygons is used for data storing in GIS databases.\n\nUsing laser scans and images taken from ground level and a bird's-eye perspective, Fruh and Zakhor present an approach to automatically create textured 3D city models. This approach involves registering and merging the detailed facade models with a complementary airborne model. The airborne modeling process generates a half-meter resolution model with a bird's-eye view of the entire area, containing terrain profile and building tops. Ground-based modeling process results in a detailed model of the building facades. Using the DSM obtained from airborne laser scans, they localize the acquisition vehicle and register the ground-based facades to the airborne model by means of Monte Carlo localization (MCL). Finally, the two models are merged with different resolutions to obtain a 3D model.\n\nUsing an airborne laser altimeter, Haala, Brenner and Anders combined height data with the existing ground plans of buildings. The ground plans of buildings had already been acquired either in analog form by maps and plans or digitally in a 2D GIS. The project was done in order to enable an automatic data capture by the integration of these different types of information. Afterwards virtual reality city models are generated in the project by texture processing, e.g. by mapping of terrestrial images. The project demonstrated the feasibility of rapid acquisition of 3D urban GIS. Ground plans proved are another very important source of information for 3D building reconstruction. Compared to results of automatic procedures, these ground plans proved more reliable since they contain aggregated information which has been made explicit by human interpretation. For this reason, ground plans, can considerably reduce costs in a reconstruction project. An example of existing ground plan data usable in building reconstruction is the Digital Cadastral map, which provides information on the distribution of property, including the borders of all agricultural areas and the ground plans of existing buildings. Additionally information as street names and the usage of buildings (e.g. garage, residential building, office block, industrial building, church) is provided in the form of text symbols. At the moment the Digital Cadastral map is built up as a data base covering an area, mainly composed by digitizing preexisting maps or plans.\n\nSoftware used for airborne laser scanning includes OPALS (Orientation and Processing of Airborne Laser Scanning data).\n\n- Terrestrial laser scan devices (pulse or phase devices) + processing software generally start at a price of €150,000. Some less precise devices (as the Trimble VX) cost around €75,000.\n- Terrestrial LIDAR systems cost around €300,000.\n- Systems using regular still cameras mounted on RC helicopters (Photogrammetry) are also possible, and cost around €25,000. Systems that use still cameras with balloons are even cheaper (around €2,500), but require additional manual processing. As the manual processing takes around 1 month of labor for every day of taking pictures, this is still an expensive solution in the long run.\n- Obtaining satellite images is also an expensive endeavor. High resolution stereo images (0.5 m resolution) cost around €11,000. Image satellites include Quikbird, Ikonos. High resolution monoscopic images cost around €5,500. Somewhat lower resolution images (e.g. from the CORONA satellite; with a 2 m resolution) cost around €1.000 per 2 images. Note that Google Earth images are too low in resolution to make an accurate 3D model.\n\nAfter the data has been collected, the acquired (and sometimes already processed) data from images or sensors needs to be reconstructed. This may be done in the same program or in some cases, the 3D data needs to be exported and imported into another program for further refining, and/or to add additional data. Such additional data could be gps-location data, ... Also, after the reconstruction, the data might be directly implemented into a local (GIS) map or a worldwide map such as Google Earth.\n\nSeveral software packets are used in which the acquired (and sometimes already processed) data from images or sensors is imported. Notable software packets include:\n\n- 3DF Zephyr\n- Canoma\n- Leica Photogrammetry Suite\n- MeshLab\n- MountainsMap SEM (microscopy applications only)\n- PhotoModeler\n- SketchUp\n- tomviz\n\n", "related": "\n- Image reconstruction\n- Photogrammetry\n- Remote sensing\n"}
{"id": "2997502", "url": "https://en.wikipedia.org/wiki?curid=2997502", "title": "Orientation (computer vision)", "text": "Orientation (computer vision)\n\nIn computer vision and image processing a common assumption is that sufficiently small image regions can be characterized as locally one-dimensional, e.g., in terms of lines or edges. For natural images this assumption is usually correct except at specific points, e.g., corners or line junctions or crossings, or in regions of high frequency textures. However, what size the regions have to be in order to appear as one-dimensional varies both between images and within an image. Also, in practice a local region is never exactly one-dimensional but can be so to a sufficient degree of approximation.\n\nImage regions which are one-dimensional are also referred to as simple or intrinsic one-dimensional (i1D).\n\nGiven an image of dimension d (d = 2 for ordinary images), a mathematical representation of a local i1D image region is\n\nformula_1\n\nwhere formula_2 is the image intensity function which varies over a local image coordinate formula_3 (a d-dimensional vector), formula_4 is a one-variable function, and formula_5 is a unit vector.\n\nThe intensity function formula_2 is constant in all directions which are perpendicular to formula_5. Intuitively, the orientation of an i1D-region is therefore represented by the vector formula_5. However, for a given formula_2, formula_5 is not uniquely determined. If\n\nformula_11\n\nformula_12\n\nthen formula_2 can be written as\n\nformula_14\n\nwhich implies that formula_11 also is a valid representation of the local orientation.\n\nIn order to avoid this ambiguity in the representation of local orientation two representations have been proposed\n- The double angle representation\n- The tensor representation\n\nThe double angle representation is only valid for 2D images (d=2), but the tensor representation can be defined for arbitrary dimensions d of the image data.\n\nA line between two points p1 and p2 has no given direction, but has a well-defined orientation. However, if one of the points p1 is used as a reference or origin, then the other point p2 can be described in terms of a vector which points in the direction to p2. Intuitively, orientation can be thought of as a direction without sign. Formally, this relates to projective spaces where the orientation of a vector corresponds to the equivalence class of vectors which are scaled versions of the vector.\n\nFor an image edge, we may talk of its direction which can be defined in terms of the gradient, pointing in the direction of maximum image intensity increase (from dark to bright). This implies that two edges can have the same orientation but the corresponding image gradients point in opposite directions if the edges go in different directions.\n\nIn image processing, the computation of the local image gradient is a common operation, e.g., for edge detection. If formula_2 above is an edge, then its gradient is parallel to formula_5. As is already discussed above the gradient is not a unique representation of orientation. Also, in the case of a local region which is centered on a line, the image gradient is approximately zero. However, in this case the vector formula_5 is still well-defined except for its sign. Therefore, formula_5 is a more appropriate starting point for defining local orientation than the image gradient.\n\nA number of methods have been proposed for computing or estimating an orientation representation from image data. These include\n- Quadrature filter based methods\n- The structure tensor\n- Using a local polynomial approximation\n- The Energy tensor\n- The Boundary tensor\n\nThe first approach can be used both for the double angle representation (only 2D images) and the tensor representation, and the other methods compute a tensor representation of local orientation.\n\nGiven that a local image orientation representation has been computed for some image data, this formation can be used for solving the following tasks:\n- Estimation of line or edge consistency\n- Estimation of curvature information\n- Detection of corner points\n- Adaptive or anisotropic noise reduction\n- Motion estimation\n", "related": "NONE"}
{"id": "25860534", "url": "https://en.wikipedia.org/wiki?curid=25860534", "title": "3D pose estimation", "text": "3D pose estimation\n\n3D pose estimation is a process of predicting the transformation of an object from a user-defined reference pose, given an image or a 3D scan. It arises in computer vision or robotics where the pose or transformation of an object can be used for alignment of a Computer-Aided Design models, identification, grasping, or manipulation of the object.\nIt is possible to estimate the 3D rotation and translation of a 3D object from a single 2D photo, if an approximate 3D model of the object is known and the corresponding points in the 2D image are known. A common technique for solving this has recently been \"POSIT\", where the 3D pose is estimated directly from the 3D model points and the 2D image points, and corrects the errors iteratively until a good estimate is found from a single image. Most implementations of POSIT only work on non-coplanar points (in other words, it won't work with flat objects or planes).\n\nAnother approach is to register a 3D CAD model over the photograph of a known object by optimizing a suitable distance measure with respect to the pose parameters.\nThe distance measure is computed between the object in the photograph and the 3D CAD model projection at a given pose.\nPerspective projection or orthogonal projection is possible depending on the pose representation used.\nThis approach is appropriate for applications where a 3D CAD model of a known object (or object category) is available.\n\nGiven a 2D image of an object, and the camera that is calibrated with respect to a world coordinate system, it is also possible to find the pose which gives the 3D object in its object coordinate system. This works as follows.\n\nStarting with a 2D image, image points are extracted which correspond to corners in an image. The projection rays from the image points are reconstructed from the 2D points so that the 3D points, which must be incident with the reconstructed rays, can be determined.\n\nThe algorithm for determining pose estimation is based on the iterative closest point algorithm. The main idea is to determine the correspondences between 2D image features and points on the 3D model curve. \n\nThe above algorithm does not account for images containing an object that is partially occluded. The following algorithm assumes that all contours are rigidly coupled, meaning the pose of one contour defines the pose of another contour.\n\nSystems exist which use a database of an object at different rotations and translations to compare an input image against to estimate pose. These systems accuracy is limited to situations which are represented in their database of images, however the goal is to recognize a pose, rather than determine it.\n\n- posest, a GPL C/C++ library for 6DoF pose estimation from 3D-2D correspondences.\n- diffgeom2pose, fast Matlab solver for 6DoF pose estimation from only \"two\" 3D-2D correspondences of points with directions (vectors), or points at curves (point-tangents). The points can be SIFT attributed with feature directions.\n- MINUS: C++ package for (relative) pose estimation of three views. Includes cases of three corresponding points with lines at these points (as in feature positions and orientations, or curve points with tangents), and also for three corresponding points and one line correspondence.\n\n", "related": "\n- Gesture recognition\n- 3D object recognition\n- Articulated body pose estimation\n- Camera calibration\n- Homography (computer vision)\n- Trifocal tensor\n- Pose estimation\n\n- Rosenhahn, B. \"Foundations about 2D-3D Pose Estimation.\"\n- Rosenhahn, B. \"Pose Estimation of 3D Free-form Contours in Conformal Geometry.\"\n- Athitsos, V. \"Estimating 3D Hand Pose from a Cluttered Image.\"\n"}
{"id": "26592482", "url": "https://en.wikipedia.org/wiki?curid=26592482", "title": "ViBe", "text": "ViBe\n\nViBe is a background subtraction algorithm which has been presented at the IEEE ICASSP 2009 conference and was refined in later publications. More precisely, it is a software module for extracting background information from moving images. It has been developed by Oliver Barnich and Marc Van Droogenbroeck of the Montefiore Institute, University of Liège, Belgium.\n\nViBe is patented: the patent covers various aspects such as stochastic replacement, spatial diffusion, and non-chronological handling.\n\nViBe is written in the programming language C, and has been implemented on CPU, GPU and FPGA.\n\nMany advanced techniques are used to provide an estimate of the temporal probability density function (pdf) of a pixel x. ViBe's approach is different, as it imposes the influence of a value in the polychromatic space to be limited to the local neighborhood. In practice, ViBe does not estimate the pdf, but uses a set of previously observed sample values as a pixel model. To classify a value pt(x), it is compared to its closest values among the set of samples.\n\nViBe ensures a smooth exponentially decaying lifespan for the sample values that constitute the pixel models. This makes ViBe able to successfully deal with concomitant events with a single model of a reasonable size for each pixel. This is achieved by choosing, randomly, which sample to replace when updating a pixel model. Once the sample to be discarded has been chosen, the new value replaces the discarded sample. The pixel model that would result from the update of a given pixel model with a given pixel sample cannot be predicted since the value to be discarded is chosen at random.\n\nTo ensure the spatial consistency of the whole image model and handle practical situations such as small camera movements or slowly evolving background objects, ViBe uses a technique similar to that developed for the updating process in which it chooses at random and update a pixel model in the neighborhood of the current pixel. By denoting NG(x) and p(x) respectively the spatial neighborhood of a pixel x and its value, and assuming that it was decided to update the set of samples of x by inserting p(x), then ViBe also use this value p(x) to update the set of samples of one of the pixels in the neighborhood NG(x), chosen at random. As a result, ViBe is able to produce spatially coherent results directly without the use of any post-processing method.\n\nAlthough the model could easily recover from any type of initialization, for example by choosing a set of random values, it is convenient to get an accurate background estimate as soon as possible. Ideally a segmentation algorithm would like to be able to segment the video sequences starting from the second frame, the first frame being used to initialize the model. Since no temporal information is available prior to the second frame, ViBe populates the pixel models with values found in the spatial neighborhood of each pixel; more precisely, it initializes the background model with values taken randomly in each pixel neighborhood of the first frame. The background\nestimate is therefore valid starting from the second frame of a video sequence.\n", "related": "NONE"}
{"id": "27395982", "url": "https://en.wikipedia.org/wiki?curid=27395982", "title": "Automated Imaging Association", "text": "Automated Imaging Association\n\nAutomated Imaging Association (AIA) is the world's largest machine vision trade group. AIA has more than 350 members from 32 countries, including system integrators, camera, lighting and other vision components manufacturers, vision software providers, OEMs and distributors. The association's headquarters is located in Ann Arbor, Michigan. Now part of the A3; Association for Advancing Automation AIA joins RIA; Robotic Industries Association, MCMA, Motion Control & Motor Association and A3 Mexico to form one of the largest collaborative trade association. All organizations offer industry training, news and member benefits. \n\nThe Camera Link, Camera Link HS, GigE vision, USB3Vision and CoaXPress communication protocols are maintained and administered by the Automated Imaging Association (AIA).\n\nCamera Link, Camera Link HS, GigE vision, USB3Vision, CoaXPress are all available for public download on their Vision Online website. Manufacturers of vision products using the standard must license the standard.\n\nSony is among the multi-billion dollar member companies in the AIA. Cognex Corporation and National Instruments are also two big names in the machine vision industry that are members of the AIA. In 2010, 51% of the members are from North America, 30% are from Europe, 15% are from Eastern Asia, less than 1% are from South America, 2% are from Western Asia, less than 1% are from Southern Asia, 1% are from Southeastern Asia and less than 1% of the members are from Australia. \n", "related": "NONE"}
{"id": "27169328", "url": "https://en.wikipedia.org/wiki?curid=27169328", "title": "Randomized Hough transform", "text": "Randomized Hough transform\n\nHough transforms are techniques for object detection, a critical step in many implementations of computer vision, or data mining from images. Specifically, the Randomized Hough transform is a probabilistic variant to the classical Hough transform, and is commonly used to detect curves (straight line, circle, ellipse, etc.) The basic idea of Hough transform (HT) is to implement a voting procedure for all potential curves in the image, and at the termination of the algorithm, curves that do exist in the image will have relatively high voting scores. Randomized Hough transform (RHT) is different from HT in that it tries to avoid conducting the computationally expensive voting process for every nonzero pixel in the image by taking advantage of the geometric properties of analytical curves, and thus improve the time efficiency and reduce the storage requirement of the original algorithm. \n\nAlthough Hough transform (HT) has been widely used in curve detection, it has two major drawbacks: First, for each nonzero pixel in the image, the parameters for the existing curve and redundant ones are both accumulated during the voting procedure. Second, the accumulator array (or Hough space) is predefined in a heuristic way. The more accuracy needed, the higher parameter resolution should be defined. These two needs usually result in a large storage requirement and low speed for real applications. Therefore, RHT was brought up to tackle this problem.\n\nIn comparison with HT, RHT takes advantage of the fact that some analytical curves can be fully determined by a certain number of points on the curve. For example, a straight line can be determined by two points, and an ellipse (or a circle) can be determined by three points. The case of ellipse detection can be used to illustrate the basic idea of RHT. The whole process generally consists of three steps:\n1. Fit ellipses with randomly selected points.\n2. Update the accumulator array and corresponding scores.\n3. Output the ellipses with scores higher than some predefined threshold.\n\nOne general equation for defining ellipses is:\nformula_1\n\nwith restriction: formula_2\n\nHowever, an ellipse can be fully determined if one knows three points on it and the tangents in these points.\n\nRHT starts by randomly selecting three points on the ellipse. Let them be X, X and X. The first step is to find the tangents of these three points. They can be found by fitting a straight line using least squares technique for a small window of neighboring pixels. \n\nThe next step is to find the intersection points of the tangent lines. This can be easily done by solving the line equations found in the previous step. Then let the intersection points be T and T, the midpoints of line segments formula_3 and formula_4 be M and M. Then the center of the ellipse will lie in the intersection of formula_5 and formula_6. Again, the coordinates of the intersected point can be determined by solving line equations and the detailed process is skipped here for conciseness. \n\nLet the coordinates of ellipse center found in previous step be (x, y). Then the center can be translated to the origin with formula_7 and formula_8 so that the ellipse equation can be simplified to:\n\nformula_9\n\nNow we can solve for the rest of ellipse parameters: a, b and c by substituting the coordinates of X, X and X into the equation above.\n\nWith the ellipse parameters determined from previous stage, the accumulator array can be updated correspondingly. Different from classical Hough transform, RHT does not keep \"grid of buckets\" as the accumulator array. Rather, it first calculates the similarities between the newly detected ellipse and the ones already stored in accumulator array. Different metrics can be used to calculate the similarity. As long as the similarity exceeds some predefined threshold, replace the one in the accumulator with the average of both ellipses and add 1 to its score. Otherwise, initialize this ellipse to an empty position in the accumulator and assign a score of 1.\n\nOnce the score of one candidate ellipse exceeds the threshold, it is determined as existing in the image (in other words, this ellipse is detected), and should be removed from the image and accumulator array so that the algorithm can detect other potential ellipses faster. The algorithm terminates when the number of iterations reaches a maximum limit or all the ellipses have been detected.\n\nPseudo code for RHT:\n", "related": "NONE"}
{"id": "4163064", "url": "https://en.wikipedia.org/wiki?curid=4163064", "title": "Glossary of machine vision", "text": "Glossary of machine vision\n\nThe following are common definitions related to the machine vision field.\n\nGeneral related fields\n- Machine vision\n- Computer vision\n- Image processing\n- Signal processing\n\n- 1394. FireWire is Apple Inc.'s brand name for the IEEE 1394 interface. It is also known as i.Link (Sony’s name) or IEEE 1394 (although the 1394 standard also defines a backplane interface). It is a personal computer (and digital audio/digital video) serial bus interface standard, offering high-speed communications and isochronous real-time data services.\n- 1D. One-dimensional.\n- 2D computer graphics. The computer-based generation of digital images—mostly from two-dimensional models (such as 2D geometric models, text, and digital images) and by techniques specific to them.\n\n- 3D computer graphics. 3D computer graphics are different from 2D computer graphics in that a three-dimensional representation of geometric data is stored in the computer for the purposes of performing calculations and rendering 2D images. Such images may be for later display or for real-time viewing. Despite these differences, 3D computer graphics rely on many of the same algorithms as 2D computer vector graphics in the wire frame model and 2D computer raster graphics in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques.\n- 3D scanner. This is a device that analyzes a real-world object or environment to collect data on its shape and possibly color. The collected data can then be used to construct digital, three dimensional models useful for a wide variety of applications.\n\n- Aberration. Optically, defocus refers to a translation along the optical axis away from the plane or surface of best focus. In general, defocus reduces the sharpness and contrast of the image. What should be sharp, high-contrast edges in a scene become gradual transitions.\n- Aperture. In context of photography or machine vision, aperture refers to the diameter of the aperture stop of a photographic lens. The aperture stop can be adjusted to control the amount of light reaching the film or image sensor.\n- aspect ratio (image). The aspect ratio of an image is its displayed width divided by its height (usually expressed as \"\"x\":\"y\").\n- Angular resolution. Describes the resolving power of any image forming device such as an optical or radio telescope, a microscope, a camera, or an eye.\n- Automated optical inspection.\n\n- Barcode. A barcode (also bar code) is a machine-readable representation of information in a visual format on a surface.\n- Blob discovery. Inspecting an image for discrete blobs of connected pixels (e.g. a black hole in a grey object) as image landmarks. These blobs frequently represent optical targets for machining, robotic capture, or manufacturing failure.\n- Bitmap. A raster graphics image, digital image, or bitmap, is a data file or structure representing a generally rectangular grid of pixels, or points of color, on a computer monitor, paper, or other display device.\n\n- Camera. A camera is a device used to take pictures, either singly or in sequence. A camera that takes pictures singly is sometimes called a photo camera to distinguish it from a video camera.\n- Camera Link. Camera Link is a serial communication protocol designed for computer vision applications based on the National Semiconductor interface Channel-link. It was designed for the purpose of standardizing scientific and industrial video products including cameras, cables and frame grabbers. The standard is maintained and administered by the Automated Imaging Association or AIA, the global machine vision industry's trade group.\n- Charge-coupled device. A charge-coupled device (CCD) is a sensor for recording images, consisting of an integrated circuit containing an array of linked, or coupled, capacitors. CCD sensors and cameras tend to be more sensitive, less noisy, and more expensive than CMOS sensors and cameras.\n- CIE 1931 Color Space. In the study of the perception of color, one of the first mathematically defined color spaces was the CIE XYZ color space (also known as CIE 1931 color space), created by the International Commission on Illumination (CIE) in 1931.\n- CMOS. CMOS (\"see-moss\")stands for complementary metal-oxide semiconductor, is a major class of integrated circuits. CMOS imaging sensors for machine vision are cheaper than CCD sensors but more noisy.\n- CoaXPress. CoaXPress (CXP) is an asymmetric high speed serial communication standard over coaxial cable. CoaXPress combines high speed image data, low speed camera control and power over a single coaxial cable. The standard is maintained by JIIA, the Japan Industrial Imaging Association\n- Color. The perception of the frequency (or wavelength) of light, and can be compared to how pitch (or a musical note) is the perception of the frequency or wavelength of sound.\n- Color blindness. Also known as color vision deficiency, in humans is the inability to perceive differences between some or all colors that other people can distinguish\n- Color temperature. \"White light\" is commonly described by its color temperature. A traditional incandescent light source's color temperature is determined by comparing its hue with a theoretical, heated black-body radiator. The lamp's color temperature is the temperature in kelvins at which the heated black-body radiator matches the hue of the lamp.\n- Color vision. CV is the capacity of an organism or machine to distinguish objects based on the wavelengths (or frequencies) of the light they reflect or emit.\n- computer vision. The study and application of methods which allow computers to \"understand\" image content.\n- Contrast. In visual perception, contrast is the difference in visual properties that makes an object (or its representation in an image) distinguishable from other objects and the background.\n- C-Mount. Standardized adapter for optical lenses on CCD - cameras. C-Mount lenses have a back focal distance 17.5 mm vs. 12.5 mm for \"CS-mount\" lenses. A C-Mount lens can be used on a CS-Mount camera through the use of a 5 mm extension adapter. C-mount is a 1\" diameter, 32 threads per inch mounting thread (1\"-32UN-2A.)\n- CS-Mount. Same as C-Mount but the focal point is 5 mm shorter. A CS-Mount lens will not work on a C-Mount camera. CS-mount is a 1\" diameter, 32 threads per inch mounting thread.\n\n- Data matrix. A two dimensional Barcode.\n- Depth of field. In optics, particularly photography and machine vision, the depth of field (DOF) is the distance in front of and behind the subject which appears to be in focus.\n- Depth perception. DP is the visual ability to perceive the world in three dimensions. It is a trait common to many higher animals. Depth perception allows the beholder to accurately gauge the distance to an object.\n- Diaphragm. In optics, a diaphragm is a thin opaque structure with an opening (aperture) at its centre. The role of the diaphragm is to stop the passage of light, except for the light passing through the aperture.\n\n- Edge detection. ED marks the points in a digital image at which the luminous intensity changes sharply. It also marks the points of luminous intensity changes of an object or spatial-taxon silhouette.\n- Electromagnetic interference. Radio Frequency Interference (RFI) is electromagnetic radiation which is emitted by electrical circuits carrying rapidly changing signals, as a by-product of their normal operation, and which causes unwanted signals (interference or noise) to be induced in other circuits.\n\n- FireWire. FireWire (also known as i. Link or IEEE 1394) is a personal computer (and digital audio/video) serial bus interface standard, offering high-speed communications. It is often used as an interface for industrial cameras.\n- Fixed-pattern noise.\n- Flat-field correction.\n- Frame grabber. An electronic device that captures individual, digital still frames from an analog video signal or a digital video stream.\n- Fringe Projection Technique. 3D data acquisition technique employing projector displaying fringe pattern on a surface of measured piece, and one or more cameras recording image(s).\n- Field of view. The field of view (FOV) is the part which can be seen by the machine vision system at one moment. The field of view depends from the lens of the system and from the working distance between object and camera.\n- Focus. An image, or image point or region, is said to be in focus if light from object points is converged about as well as possible in the image; conversely, it is out of focus if light is not well converged. The border between these conditions is sometimes defined via a circle of confusion criterion.\n\n- Gamut. In color reproduction, including computer graphics and photography, the gamut, or color gamut , is a certain \"complete subset\" of colors.\n- Grayscale. A grayscale digital image is an image in which the value of each pixel is a single sample. Displayed images of this sort are typically composed of shades of gray, varying from black at the weakest intensity to white at the strongest, though in principle the samples could be displayed as shades of any color, or even coded with various colors for different intensities.\n- GUI. A graphical user interface (or GUI, sometimes pronounced \"gooey\") is a method of interacting with a computer through a metaphor of direct manipulation of graphical images and widgets in addition to text.\n\n- Histogram. In statistics, a histogram is a graphical display of tabulated frequencies. A histogram is the graphical version of a table which shows what proportion of cases fall into each of several or many specified categories. The histogram differs from a bar chart in that it is the \"area\" of the bar that denotes the value, not the height, a crucial distinction when the categories are not of uniform width (Lancaster, 1974). The categories are usually specified as non-overlapping intervals of some variable. The categories (bars) must be adjacent.\n- Histogram (Color). In computer graphics and photography, a color histogram is a representation of the distribution of colors in an image, derived by counting the number of pixels of each of given set of color ranges in a typically two-dimensional (2D) or three-dimensional (3D) color space. A histogram is a standard statistical description of a distribution in terms of occurrence frequencies of different event classes; for color, the event classes are regions in color space.\n- HSV color space. The HSV (Hue, Saturation, Value) model, also called HSB (Hue, Saturation, Brightness), defines a color space in terms of three constituent components:\n- Hue, the color type (such as red, blue, or yellow)\n- Saturation, the \"vibrancy\" of the color and colorimetric purity\n- Value, the brightness of the color\n\n- Image file formats. Image file formats provide a standardized method of organizing and storing image data. This article deals with digital image formats used to store photographic and other image information. Image files are made up of either pixel or vector (geometric) data, which is rasterized to pixels in the display process, with a few exceptions in vector graphic display. The pixels that make up an image are in the form of a grid of columns and rows. Each of the pixels in an image stores digital numbers representing brightness and color.\n- Image segmentation.\n\n- Infrared imaging. See Thermographic camera.\n- Incandescent light bulb. An incandescent light bulb generates light using a glowing filament heated to white-hot by an electric current.\n\n- JPEG. JPEG (pronounced jay-peg) is a most commonly used standard method of lossy compression for photographic images.\n\n- Kell factor. It is a parameter used to determine the effective resolution of a discrete display device.\n\n- Laser. In physics, a laser is a device that emits light through a specific mechanism for which the term laser is an acronym: light amplification by stimulated emission of radiation.\n- Lens. A lens is a device that causes light to either converge and concentrate or to diverge, usually formed from a piece of shaped glass. Lenses may be combined to form more complex optical systems as a Normal lens or a Telephoto lens.\n- Lens Controller. A lens controller is a device used to control a motorized (ZFI) lens. Lens controllers may be internal to a camera, a set of switches used manually, or a sophisticated device that allows control of a lens with a computer.\n- Lighting. Lighting refers to either artificial light sources such as lamps or to natural illumination.\n\n- Metrology. Metrology is the science of measurement. There are lots of applications for machine vision in metrology.\n- machine vision. MV is the application of computer vision to industry and manufacturing.\n- Motion perception. MP is the process of inferring the speed and direction of objects and surfaces that move in a visual scene given some visual input\n\n- Neural network. A NN is an interconnected group of artificial neurons that uses a mathematical or computational model for information processing based on a connectionist approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.\n- Normal lens. In machine vision a normal or entrocentric lens is a lens that generates images that are generally held to have a \"natural\" perspective compared with lenses with longer or shorter focal lengths. Lenses of shorter focal length are called wide-angle lenses, while longer focal length lenses are called telephoto lenses.\n\n- Optical character recognition. Usually abbreviated to OCR, involves computer software designed to translate images of typewritten text (usually captured by a scanner) into machine-editable text, or to translate pictures of characters into a standard encoding scheme representing them in (ASCII or Unicode).\n- Optical resolution. Describes the ability of a system to distinguish, detect, and/or record physical details by electromagnetic means. The system may be imaging (e.g., a camera) or non-imaging (e.g., a quad-cell laser detector).\n- Optical transfer function.\n\n- Pattern recognition. This is a field within the area of machine learning. Alternatively, it can be defined as the act of taking in raw data and taking an action based on the category of the data. It is a collection of methods for supervised learning.\n- Pixel. A pixel is one of the many tiny dots that make up the representation of a picture in a computer's memory or screen.\n- Pixelation. In computer graphics, pixelation is an effect caused by displaying a bitmap or a section of a bitmap at such a large size that individual pixels, small single-colored square display elements that comprise the bitmap, are visible.\n- Prime lens. Mechanical assembly of lenses whose focal length is fixed, as opposed to a zoom lens, which has a variable focal length.\n\n- Q-Factor (Optics). In optics, the \"Q\" factor of a resonant cavity is given by\nwhere formula_2 is the resonant frequency, formula_3 is the stored energy in the cavity, and formula_4 is the power dissipated. The optical \"Q\" is equal to the ratio of the resonant frequency to the bandwidth of the cavity resonance. The average lifetime of a resonant photon in the cavity is proportional to the cavity's \"Q\". If the \"Q\" factor of a laser's cavity is abruptly changed from a low value to a high one, the laser will emit a pulse of light that is much more intense than the laser's normal continuous output. This technique is known as Q-switching.\n\n- Region of interest. A Region of Interest, often abbreviated ROI, is a selected subset of samples within a dataset identified for a particular purpose.\n\n- RGB. The RGB color model utilizes the additive model in which red, green, and blue light are combined in various ways to create other colors.\n- ROI. See Region of Interest.\n- Foreground, figure and objects. See also spatial-taxon.\n\n- S-video. Separate video, abbreviated S-Video and also known as Y/C (or \"erroneously\", S-VHS and \"super video\") is an analog video signal that carries the video data as two separate signals (brightness and color), unlike composite video which carries the entire set of signals in one signal line. S-Video, as most commonly implemented, carries high-bandwidth 480i or 576i resolution video, i.e. standard definition video. It does not carry audio on the same cable.\n- Scheimpflug principle.\n- Shutter. A shutter is a device that allows light to pass for a determined period of time, for the purpose of exposing the image sensor to the right amount of light to create a permanent image of a view.\n- Shutter speed. In machine vision the shutter speed is the time for which the shutter is held open during the taking an image to allow light to reach the imaging sensor. In combination with variation of the lens aperture, this regulates how much light the imaging sensor in a digital camera will receive.\n- Smart camera. A smart camera is an integrated machine vision system which, in addition to image capture circuitry, includes a processor, which can extract information from images without need for an external processing unit, and interface devices used to make results available to other devices.\n- Spatial-Taxon. Spatial-taxons are information granules, composed of non-mutually exclusive pixel regions, within scene architecture. They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts.\n- Structured-light 3D scanner. The process of projecting a known pattern of illumination (often grids or horizontal bars) on to a scene. The way that these patterns appear to deform when striking surfaces allows vision systems to calculate the depth and surface information of the objects in the scene.\n- SVGA. Super Video Graphics Array, almost always abbreviated to Super VGA or just SVGA is a broad term that covers a wide range of computer display standards.\n\n- Telecentric lens. Compound lens with an unusual property concerning its geometry of image-forming rays. In machine vision systems telecentric lenses are usually employed in order to achieve dimensional and geometric invariance of images within a range of different distances from the lens and across the whole field of view.\n- Telephoto lens. Lens whose focal length is significantly longer than the focal length of a normal lens.\n- Thermography. Thermal imaging, a type of Infrared imaging.\n- TIFF. Tagged Image File Format (abbreviated TIFF) is a file format for mainly storing images, including photographs and line art.\n\n- USB. Universal Serial Bus (USB) provides a serial bus standard for connecting devices, usually to computers such as PCs, but is also becoming commonplace on cameras.\n\n- VESA. The Video Electronics Standards Association (VESA) is an international body, founded in the late 1980s by NEC Home Electronics and eight other video display adapter manufacturers. The initial goal was to produce a standard for 800×600 SVGA resolution video displays. Since then VESA has issued a number of standards, mostly relating to the function of video peripherals in IBM PC compatible computers.\n- VGA. Video Graphics Array (VGA) is a computer display standard first marketed in 1987 by IBM\n- Vision processing unit a class of microprocessors aimed at accelerating machine vision tasks.\n\n- Wide-angle lens. In photography and cinematography, a wide-angle lens is a lens whose focal length is shorter than the focal length of a normal lens.\n\n- X-rays. A form of electromagnetic radiation with a wavelength in the range of 10 to 0.01 nanometers, corresponding to frequencies in the range 30 to 3000 PHz (10 hertz). X-rays are primarily used for diagnostic medical and industrial imaging as well as crystallography. X-rays are a form of ionizing radiation and as such can be dangerous.\n\n- Y-cable. A Y-cable or Y cable is an electrical cable containing three ends of which one is a common end that in turn leads to a split into the remaining two ends, resembling the letter \"Y\". Y-cables are typically, but not necessarily, short (less than 12 inches), and often the ends connect to other cables. Uses may be as simple as splitting one audio or video channel into two, to more complex uses such as splicing signals from a high density computer connector to its appropriate peripheral .\n\n- Zoom lens. A mechanical assembly of lenses whose focal length can be changed, as opposed to a prime lens, which has a fixed focal length. See an animation of the zoom principle below.\n\n", "related": "\n- Glossary of artificial intelligence\n- Frame grabber\n- Google Goggles\n- Machine vision glossary\n- Morphological image processing\n- OpenCV\n- Smart camera\n"}
{"id": "2288302", "url": "https://en.wikipedia.org/wiki?curid=2288302", "title": "Active shape model", "text": "Active shape model\n\nActive shape models (ASMs) are statistical models of the shape of objects which iteratively deform to fit to an example of the object in a new image, developed by Tim Cootes and Chris Taylor in 1995. The shapes are constrained by the PDM (point distribution model) Statistical Shape Model to vary only in ways seen in a training set of labelled examples. \nThe shape of an object is represented by a set of points (controlled by the shape model). The ASM algorithm aims to match the model to a new image.\n\nThe ASM works by alternating the following steps:\n- Generate a suggested shape by looking in the image around each point for a better position for the point. This is commonly done using what is called a \"profile model\", which looks for strong edges or uses the Mahalanobis distance to match a model template for the point.\n- Conform the suggested shape to the point distribution model, commonly called a \"shape model\" in this context. The figure to the right shows an example.\n\nThe technique has been widely used to analyse images of faces, mechanical assemblies and medical images (in 2D and 3D).\n\nIt is closely related to the active appearance model. It is also known as a \"Smart Snakes\" method, since it is an analog to an active contour model which would respect explicit shape constraints.\n\n", "related": "\n- Procrustes analysis\n- Point distribution model\n\n- Matlab code open-source ASM implementation.\n- Description of AAMs from Manchester University.\n- Tim Cootes' home page (one of the original co-inventors of ASMs).\n- Source code for ASMs (the \"stasm\" library).\n- ASMlib-OpenCV, An open source C++/OpenCV implementation of ASM.\n"}
{"id": "474813", "url": "https://en.wikipedia.org/wiki?curid=474813", "title": "Color histogram", "text": "Color histogram\n\nIn image processing and photography, a color histogram is a representation of the distribution of colors in an image. For digital images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors.\n\nThe color histogram can be built for any kind of color space, although the term is more often used for three-dimensional spaces like RGB or HSV. For monochromatic images, the term intensity histogram may be used instead. For multi-spectral images, where each pixel is represented by an arbitrary number of measurements (for example, beyond the three measurements in RGB), the color histogram is \"N\"-dimensional, with N being the number of measurements taken. Each measurement has its own wavelength range of the light spectrum, some of which may be outside the visible spectrum.\n\nIf the set of possible color values is sufficiently small, each of those colors may be placed on a range by itself; then the histogram is merely the count of pixels that have each possible color. Most often, the space is divided into an appropriate number of ranges, often arranged as a regular grid, each containing many similar color values. The color histogram may also be represented and displayed as a smooth function defined over the color space that approximates the pixel counts.\n\nLike other kinds of histograms, the color histogram is a statistic that can be viewed as an approximation of an underlying continuous distribution of colors values.\n\nColor histograms are flexible constructs that can be built from images in various color spaces, whether RGB, rg chromaticity or any other color space of any dimension. A histogram of an image is produced first by discretization of the colors in the image into a number of bins, and counting the number of image pixels in each bin. For example, a Red–Blue chromaticity histogram can be formed by first normalizing color pixel values by dividing RGB values by R+G+B, then quantizing the normalized R and B coordinates into N bins each. A two-dimensional histogram of Red-Blue chromaticity divided into four bins (\"N\"=4) might yield a histogram that looks like this table:\n\nA histogram can be N-dimensional. Although harder to display, a three-dimensional color histogram for the above example could be thought of as four separate Red-Blue histograms, where each of the four histograms contains the Red-Blue values for a bin of green (0-63, 64-127, 128-191, and 192-255).\n\nThe histogram provides a compact summarization of the distribution of data in an image. The color histogram of an image is relatively invariant with translation and rotation about the viewing axis, and varies only slowly with the angle of view. By comparing histograms signatures of two images and matching the color content of one image with the other, the color histogram is particularly well suited for the problem of recognizing an object of unknown position and rotation within a scene. Importantly, translation of an RGB image into the illumination invariant rg-chromaticity space allows the histogram to operate well in varying light levels.\n\n1. What is a histogram?\n\nA histogram is a graphical representation of the number of pixels in an image. In a more simple way to explain, a histogram is a bar graph, whose X-axis represents the tonal scale(black at the left and white at the right), and Y-axis represents the number of pixels in an image in a certain area of the tonal scale. For example, the graph of a luminance histogram shows the number of pixels for each brightness level(from black to white), and when there are more pixels, the peak at the certain luminance level is higher.\n\n2. What is a color histogram?\n\nA color histogram of an image represents the distribution of the composition of colors in the image. It shows different types of colors appeared and the number of pixels in each type of the colors appeared. The relation between a color histogram and a luminance histogram is that a color histogram can be also expressed as “Three Luminance Histograms”, each of which shows the brightness distribution of each individual Red/Green/Blue color channel.\n\nA color histogram focuses only on the proportion of the number of different types of colors, regardless of the spatial location of the colors. The values of a color histogram are from statistics. They show the statistical distribution of colors and the essential tone of an image.\n\nIn general, as the color distributions of the foreground and background in an image are different, there might be a bimodal distribution in the histogram.\n\nFor the luminance histogram alone, there is no perfect histogram and in general, the histogram can tell whether it is over exposure or not, but there are times when you might think the image is over exposed by viewing the histogram; however, in reality it is not.\n\nThe formation of a color histogram is rather simple. From the definition above, we can simply count the number of pixels for each 256 scales in each of the 3 RGB channel, and plot them on 3 individual bar graphs.\n\nIn general, a color histogram is based on a certain color space, such as RGB or HSV. When we compute the pixels of different colors in an image, if the color space is large, then we can first divide the color space into certain numbers of small intervals. Each of the intervals is called a bin. This process is called color quantization. Then, by counting the number of pixels in each of the bins, we get the color histogram of the image.\n\nThe concrete steps of the principles can be viewed in Example 2.\n\nGiven the following image of a cat (an original version and a version that has been reduced to 256 colors for easy histogram purposes), the following data represents a color histogram in the RGB color space, using four bins. \n\nBin 0 corresponds to intensities 0-63 \n\nBin 1 is 64-127 \n\nBin 2 is 128-191 and Bin 3 is 192-255.\n\nApplication in camera:\n\nNowadays, some cameras have the ability of showing the 3 color histograms when we take photos. \n\nWe can examine clips (spikes on either the black or white side of the scale) in each of the 3 RGB color histograms. If we find one or more clipping on a channel of the 3 RGB channels, then this would result in a loss of detail for that color.\n\nTo illustrate this, consider this example:\n\n1. We know that each of the three R, G, B channels has a range of values from 0-255(8 bit). So consider a photo that has a luminance range of 0-255.\n\n2. Assume the photo we take is made of 4 blocks that are adjacent to each other and we set the luminance scale for each of the 4 blocks of original photo to be 10, 100, 205, 245. Thus, the image looks like the first figure on the right. \n\n3. Then, we over expose the photo a little, say, the luminance scale of each block is increased by 10. Thus, the luminance scale for each of the 4 blocks of new photo is 20, 110, 215, 255. Then, the image looks like the second figure on the right.\n\nThere is not much difference between figure 8 and figure 9, all we can see is that the whole image becomes brighter(the contrast for each of the blocks remain the same).\n\n4. Now, we over expose the original photo again, this time the luminance scale of each block is increased by 50. Thus, the luminance scale for each of the 4 blocks of new photo is 60, 150, 255, 255. The new image now looks like the third figure on the right.\n\nNote that the scale for last block is 255 instead of 295, for 255 is the top scale and thus the last block has clipped! When this happens, we lose the contrast of the last 2 blocks, and thus, we cannot recover the image no matter how we adjust it.\n\nTo conclude, when taking photos with a camera that displays histograms, always keep the brightest tone in the image below the largest scale 255 on the histogram in order to avoid losing details.\n\nThe main drawback of histograms for classification is that the representation is dependent of the color of the object being studied, ignoring its shape and texture. Color histograms can potentially be identical for two images with different object content which happens to share color information. Conversely, without spatial or shape information, similar objects of different color may be indistinguishable based solely on color histogram comparisons. There is no way to distinguish a red and white cup from a red and white plate. Put another way, histogram-based algorithms have no concept of a generic 'cup', and a model of a red and white cup is no use when given an otherwise identical blue and white cup. Another problem is that color histograms have high sensitivity to noisy interference such as lighting intensity changes and quantization errors. High dimensionality (bins) color histograms are also another issue. Some color histogram feature spaces often occupy more than one hundred dimensions.\n\nSome of the proposed solutions have been color histogram intersection, color constant indexing, cumulative color histogram, quadratic distance, and color correlograms. Although there are drawbacks of using histograms for indexing and classification, using color in a real-time system has several advantages. One is that color information is faster to compute compared to other invariants. It has been shown in some cases that color can be an efficient method for identifying objects of known location and appearance.\n\nFurther research into the relationship between color histogram data to the physical properties of the objects in an image has shown they can represent not only object color and illumination but relate to surface roughness and image geometry and provide an improved estimate of illumination and object color.\n\nUsually, Euclidean distance, histogram intersection, or cosine or quadratic distances are used for the calculation of image similarity ratings. Any of these values do not reflect the similarity rate of two images in itself; it is useful only when used in comparison to other similar values. This is the reason that all the practical implementations of content-based image retrieval must complete computation of all images from the database, and is the main disadvantage of these implementations.\n\nAnother approach to representative color image content is two-dimensional color histogram. A two-dimensional color histogram considers the relation between the pixel pair colors (not only the lighting component). A two-dimensional color histogram is a two-dimensional array. The size of each dimension is the number of colors that were used in the phase of color quantization. These arrays are treated as matrices, each element of which stores a normalized count of pixel pairs, with each color corresponding to the index of an element in each pixel neighborhood. For comparison of two-dimensional color histograms it is suggested calculating their correlation, because constructed as described above, is a random vector (in other words, a multi-dimensional random value). While creating a set of final images, the images should be arranged in decreasing order of the correlation coefficient.\n\nThe correlation coefficient may also be used for color histogram comparison. Retrieval results with correlation coefficient are better than with other metrics.\n\nThe idea of an intensity histogram can be generalized to continuous data,\nsay audio signals represented by real functions or images represented by functions with two-dimensional domain.\n\nLet formula_1 (see Lebesgue space), then the cumulative histogram operator formula_2 can be defined by:\nformula_4 is the Lebesgue measure of sets.\nformula_5 in turn is a real function.\nThe (non-cumulative) histogram is defined as its derivative.\n\n- 3D Color Inspector/Color Histogram, by Kai Uwe Barthel. (Free Java applet.)\n- Stanford Student Project on Image Based Retrieval - more in depth look at equations/application\n- MATLAB/Octave code for plotting Color Histograms and Color Clouds - The source code can be ported to other languages\n", "related": "NONE"}
{"id": "2132859", "url": "https://en.wikipedia.org/wiki?curid=2132859", "title": "Image moment", "text": "Image moment\n\nIn image processing, computer vision and related fields, an image moment is a certain particular weighted average (moment) of the image pixels' intensities, or a function of such moments, usually chosen to have some attractive property or interpretation.\n\nImage moments are useful to describe objects after segmentation. Simple properties of the image which are found \"via\" image moments include area (or total intensity), its centroid, and information about its orientation.\n\nFor a 2D continuous function \"f\"(\"x\",\"y\") the moment (sometimes called \"raw moment\") of order (\"p\" + \"q\") is defined as\n\nfor \"p\",\"q\" = 0,1,2...\nAdapting this to scalar (greyscale) image with pixel intensities \"I\"(\"x\",\"y\"), raw image moments \"M\" are calculated by\n\nIn some cases, this may be calculated by considering the image as a probability density function, \"i.e.\", by dividing the above by\n\nA uniqueness theorem (Hu [1962]) states that if \"f\"(\"x\",\"y\") \nis piecewise continuous and has nonzero values only in a finite part of the \"xy\" \nplane, moments of all orders exist, and the moment sequence (\"M\") is uniquely determined by \"f\"(\"x\",\"y\"). Conversely, (\"M\") uniquely determines \"f\"(\"x\",\"y\"). In practice, the image is summarized with functions of a few lower order moments.\n\nSimple image properties derived \"via\" raw moments include:\n- Area (for binary images) or sum of grey level (for greytone images):formula_4\n- Centroid: formula_5\n\nCentral moments are defined as\n\nwhere formula_7 and formula_8 are the components of the centroid.\n\nIf \"ƒ\"(\"x\", \"y\") is a digital image, then the previous equation becomes\n\nThe central moments of order up to 3 are:\n\nIt can be shown that:\n\nCentral moments are translational invariant.\n\nInformation about image orientation can be derived by first using the second order central moments to construct a covariance matrix.\n\nThe covariance matrix of the image formula_24 is now\n\nThe eigenvectors of this matrix correspond to the major and minor axes of the image intensity, so the orientation can thus be extracted from the angle of the eigenvector associated with the largest eigenvalue towards the axis closest to this eigenvector. It can be shown that this angle Θ is given by the following formula:\n\nThe above formula holds as long as:\n\nThe eigenvalues of the covariance matrix can easily be shown to be\n\nand are proportional to the squared length of the eigenvector axes. The relative difference in magnitude of the eigenvalues are thus an indication of the eccentricity of the image, or how elongated it is. The eccentricity is\n\nMoments are well-known for their application in image analysis, since they can be used to derive invariants with respect to specific transformation classes.\n\nThe term \"invariant moments\" is often abused in this context. However, while \"moment invariants\" are invariants that are formed from moments, the only moments that are invariants themselves are the central moments.\n\nNote that the invariants detailed below are exactly invariant only in the continuous domain. In a discrete domain, neither scaling nor rotation are well defined: a discrete image transformed in such a way is generally an approximation, and the transformation is not reversible. These invariants therefore are only approximately invariant when describing a shape in a discrete image.\n\nThe central moments \"μ\" of any order are, by construction, invariant with respect to translations.\n\nInvariants \"η\" with respect to both translation and scale can be constructed from central moments by dividing through a properly scaled zero-th central moment:\n\nwhere \"i\" + \"j\" ≥ 2.\nNote that translational invariance directly follows by only using central moments.\n\nAs shown in the work of Hu,\ninvariants with respect to translation, scale, and \"rotation\" can be constructed:\n\nformula_31\n\nformula_32\n\nformula_33\n\nformula_34\n\nformula_35\n\nformula_36\n\nformula_37\n\nThese are well-known as \"Hu moment invariants\".\n\nThe first one, \"I\", is analogous to the moment of inertia around the image's centroid, where the pixels' intensities are analogous to physical density. The last one, \"I\", is skew invariant, which enables it to distinguish mirror images of otherwise identical images.\n\nA general theory on deriving complete and independent sets of rotation moment invariants was proposed by J. Flusser. He showed that the traditional set of Hu moment invariants is neither independent nor complete. \"I\" is not very useful as it is dependent on the others. In the original Hu's set there is a missing third order independent moment invariant:\n\nLater, J. Flusser and T. Suk specialized the theory for N-rotationally symmetric shapes case.\n\nZhang et al. applied Hu moment invariants to solve the Pathological Brain Detection (PBD) problem.\nDoerr and Florence used information of the object orientation related to the second order central moments to effectively extract translation- and rotation-invariant object cross-sections from micro-X-ray tomography image data.\n\n- Analysis of Binary Images, University of Edinburgh\n- Statistical Moments, University of Edinburgh\n- Variant moments, Machine Perception and Computer Vision page (Matlab and Python source code)\n- Hu Moments introductory video on YouTube\n", "related": "NONE"}
{"id": "172088", "url": "https://en.wikipedia.org/wiki?curid=172088", "title": "Machine vision", "text": "Machine vision\n\nMachine vision (MV) is the technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision refers to many technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance.\n\nThe overall machine vision process includes planning the details of the requirements and project, and then creating a solution. During run-time, the process starts with imaging, followed by automated analysis of the image and extraction of the required information.\n\nDefinitions of the term \"Machine vision\" vary, but all include the technology and methods used to extract information from an image on an automated basis, as opposed to image processing, where the output is another image. The information extracted can be a simple good-part/bad-part signal, or more a complex set of data such as the identity, position and orientation of each object in an image. The information can be used for such applications as automatic inspection and robot and process guidance in industry, for security monitoring and vehicle guidance. This field encompasses a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision is practically the only term used for these functions in industrial automation applications; the term is less universal for these functions in other environments such as security and vehicle guidance. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of basic computer science; machine vision attempts to integrate existing technologies in new ways and apply them to solve real world problems in a way that meets the requirements of industrial automation and similar application areas. The term is also used in a broader sense by trade shows and trade groups such as the Automated Imaging Association and the European Machine Vision Association. This broader definition also encompasses products and applications most often associated with image processing. The primary uses for machine vision are automatic inspection and industrial robot/process guidance. See glossary of machine vision.\n\nThe primary uses for machine vision are imaging-based automatic inspection and sorting and robot guidance.; in this section the former is abbreviated as \"automatic inspection\". The overall process includes planning the details of the requirements and project, and then creating a solution. This section describes the technical process that occurs during the operation of the solution.\n\nThe first step in the automatic inspection sequence of operation is acquisition of an image, typically using cameras, lenses, and lighting that has been designed to provide the differentiation required by subsequent processing. MV software packages and programs developed in them then employ various digital image processing techniques to extract the required information, and often make decisions (such as pass/fail) based on the extracted information.\n\nThe components of an automatic inspection system usually include lighting, a camera or other imager, a processor, software, and output devices.\n\nThe imaging device (e.g. camera) can either be separate from the main image processing unit or combined with it in which case the combination is generally called a smart camera or smart sensor. Inclusion of the full processing function into the same enclosure as the camera is often referred to as embedded processing. When separated, the connection may be made to specialized intermediate hardware, a custom processing appliance, or a frame grabber within a computer using either an analog or standardized digital interface (Camera Link, CoaXPress). MV implementations also use digital cameras capable of direct connections (without a framegrabber) to a computer via FireWire, USB or Gigabit Ethernet interfaces.\n\nWhile conventional (2D visible light) imaging is most commonly used in MV, alternatives include multispectral imaging, hyperspectral imaging, imaging various infrared bands, line scan imaging, 3D imaging of surfaces and X-ray imaging. Key differentiations within MV 2D visible light imaging are monochromatic vs. color, frame rate, resolution, and whether or not the imaging process is simultaneous over the entire image, making it suitable for moving processes.\n\nThough the vast majority of machine vision applications are solved using two-dimensional imaging, machine vision applications utilizing 3D imaging are a growing niche within the industry. The most commonly used method for 3D imaging is scanning based triangulation which utilizes motion of the product or image during the imaging process. A laser is projected onto the surfaces of an object. In machine vision this is accomplished with a scanning motion, either by moving the workpiece, or by moving the camera & laser imaging system. The line is viewed by a camera from a different angle; the deviation of the line represents shape variations. Lines from multiple scans are assembled into a depth map or point cloud. Stereoscopic vision is used in special cases involving unique features present in both views of a pair of cameras. Other 3D methods used for machine vision are time of flight and grid based. One method is grid array based systems using pseudorandom structured light system as employed by the Microsoft Kinect system circa 2012.\n\nAfter an image is acquired, it is processed. Central processing functions are generally done by a CPU, a GPU, a FPGA or a combination of these. Deep learning training and inference impose higher processing performance requirements. Multiple stages of processing are generally used in a sequence that ends up as a desired result. A typical sequence might start with tools such as filters which modify the image, followed by extraction of objects, then extraction (e.g. measurements, reading of codes) of data from those objects, followed by communicating that data, or comparing it against target values to create and communicate \"pass/fail\" results. Machine vision image processing methods include;\n\n- Stitching/Registration: Combining of adjacent 2D or 3D images.\n- Filtering (e.g. morphological filtering)\n- Thresholding: Thresholding starts with setting or determining a gray value that will be useful for the following steps. The value is then used to separate portions of the image, and sometimes to transform each portion of the image to simply black and white based on whether it is below or above that grayscale value.\n- Pixel counting: counts the number of light or dark pixels\n- Segmentation: Partitioning a digital image into multiple segments to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.\n- Edge detection: finding object edges\n- Color Analysis: Identify parts, products and items using color, assess quality from color, and isolate features using color.\n- Blob detection and extraction: inspecting an image for discrete blobs of connected pixels (e.g. a black hole in a grey object) as image landmarks.\n- Neural net / deep learning / machine learning processing: weighted and self-training multi-variable decision making Circa 2019 there is a large expansion of this, using deep learning and machine learning to significantly expand machine vision capabilities.\n- Pattern recognition including template matching. Finding, matching, and/or counting specific patterns. This may include location of an object that may be rotated, partially hidden by another object, or varying in size.\n- Barcode, Data Matrix and \"2D barcode\" reading\n- Optical character recognition: automated reading of text such as serial numbers\n- Gauging/Metrology: measurement of object dimensions (e.g. in pixels, inches or millimeters)\n- Comparison against target values to determine a \"pass or fail\" or \"go/no go\" result. For example, with code or bar code verification, the read value is compared to the stored target value. For gauging, a measurement is compared against the proper value and tolerances. For verification of alpha-numberic codes, the OCR'd value is compared to the proper or target value. For inspection for blemishes, the measured size of the blemishes may be compared to the maximums allowed by quality standards.\n\nA common output from automatic inspection systems is pass/fail decisions. These decisions may in turn trigger mechanisms that reject failed items or sound an alarm. Other common outputs include object position and orientation information for robot guidance systems. Additionally, output types include numerical measurement data, data read from codes and characters, counts and classification of objects, displays of the process or results, stored images, alarms from automated space monitoring MV systems, and process control signals. This also includes user interfaces, interfaces for the integration of multi-component systems and automated data interchange.\n\nMachine vision commonly provides location and orientation information to a robot to allow the robot to properly grasp the product. This capability is also used to guide motion that is simpler than robots, such as a 1 or 2 axis motion controller. The overall process includes planning the details of the requirements and project, and then creating a solution. This section describes the technical process that occurs during the operation of the solution. Many of the process steps are the same as with automatic inspection except with a focus on providing position and orientation information as the end result.\n\nAs recently as 2006, one industry consultant reported that MV represented a $1.5 billion market in North America. However, the editor-in-chief of an MV trade magazine asserted that \"machine vision is not an industry per se\" but rather \"the integration of technologies and products that provide services or applications that benefit true industries such as automotive or consumer goods manufacturing, agriculture, and defense.\"\n\n", "related": "\n- Machine vision glossary\n- Feature detection (computer vision)\n- Foreground detection\n- Vision processing unit\n- Optical sorting\n"}
{"id": "8210422", "url": "https://en.wikipedia.org/wiki?curid=8210422", "title": "Stereo cameras", "text": "Stereo cameras\n\nThe stereo cameras approach is a method of distilling a noisy video signal into a coherent data set that a computer can begin to process into actionable symbolic objects, or abstractions. Stereo cameras is one of many approaches used in the broader fields of computer vision and machine vision.\n\nIn this approach, two cameras with a known physical relationship (i.e. a common field of view the cameras can see, and how far apart their focal points sit in physical space) are correlated via software. By finding mappings of common pixel values, and calculating how far apart these common areas reside in pixel space, a rough depth map can be created. This is very similar to how the human brain uses stereoscopic information from the eyes to gain depth cue information, i.e. how far apart any given object in the scene is from the viewer.\n\nThe camera attributes must be known, focal length and distance apart etc., and a calibration done. Once this is completed the systems can be used to sense the distances of objects by triangulation. Finding the same singular physical point in the two left and right images is known as the \"correspondence problem\". Correctly locating the point gives the computer the capability to calculate the distance that the robot or camera is from the object. On the BH2 Lunar Rover the cameras use five steps: a bayer array filter, photometric consistency dense matching algorithm, a Laplace of Gaussian (LoG) edge detection algorithm, a stereo matching algorithm and finally uniqueness constraint.\n\nThis type of stereoscopic image processing technique is used in applications such as 3D reconstruction, robotic control and sensing, crowd dynamics monitoring and off-planet terrestrial rovers; for example, in mobile robot navigation, people tracking, gesture recognition, targeting, 3D surface visualization, immersive and interactive gaming. Although the Xbox Kinect sensor is also able to create a depth map of an image, it uses an infrared camera for this purpose, and does not use the dual-camera technique.\n\nOther approaches to stereoscopic sensing include time of flight sensors and ultrasound.\n\n", "related": "\n- Stereo camera (about a camera with two separated views)\n"}
{"id": "474939", "url": "https://en.wikipedia.org/wiki?curid=474939", "title": "Geometric hashing", "text": "Geometric hashing\n\nIn computer science, geometric hashing is originally a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation (example below is based on similarity transformation), though extensions exist to some other object representations and transformations. In an off-line step, the objects are encoded by treating each pair of points as a geometric basis. The remaining points can be represented in an invariant fashion with respect to this basis using two parameters. For each point, its quantized transformed coordinates are stored in the hash table as a key, and indices of the basis points as a value. Then a new pair of basis points is selected, and the process is repeated. In the on-line (recognition) step, randomly selected pairs of data points are considered as candidate bases. For each candidate basis, the remaining data points are encoded according to the basis and possible correspondences from the object are found in the previously constructed table. The candidate basis is accepted if a sufficiently large number of the data points index a consistent object basis.\n\nGeometric hashing was originally suggested in computer vision for object recognition in 2D and 3D, but later was applied to different problems such as structural alignment of proteins.\n\nGeometric Hashing is a method used for object recognition. Let’s say that we want to check if a model image can be seen in an input image. This can be accomplished with geometric hashing. The method could be used to recognize one of the multiple objects in a base, in this case the hash table should store not only the pose information but also the index of object model in the base.\n\nFor simplicity, this example will not use too many point features and assume that their descriptors are given by their coordinates only (in practice local descriptors such as SIFT could be used for indexing).\n\n1. Find the model's feature points. Assume that 5 feature points are found in the model image with the coordinates formula_1formula_2formula_3formula_4formula_5, see the picture.\n2. Introduce a basis to describe the locations of the feature points. For 2D space and similarity transformation the basis is defined by a pair of points. The point of origin is placed in the middle of the segment connecting the two points (P2, P4 in our example), the formula_6 axis is directed towards one of them, the formula_7 is orthogonal and goes through the origin. The scale is selected such that absolute value of formula_6 for both basis points is 1.\n3. Describe feature locations with respect to that basis, i.e. compute the projections to the new coordinate axes. The coordinates should be discretised to make recognition robust to noise, we take the bin size 0.25. We thus get the coordinates formula_9formula_10formula_11formula_12formula_13\n4. Store the basis in a hash table indexed by the features (only transformed coordinates in this case). If there were more objects to match with, we should also store the object number along with the basis pair.\n5. Repeat the process for a different basis pair (Step 2). It is needed to handle occlusions. Ideally, all the non-colinear pairs should be enumerated. We provide the hash table after two iterations, the pair (P1, P3) is selected for the second one.\n\nHash Table:\n\nMost hash tables cannot have identical keys mapped to different values. So in real life one won’t encode basis keys (1.0, 0.0) and (-1.0, 0.0) in a hash table.\n\n1. Find interesting feature points in the input image.\n2. Choose an arbitrary basis. If there isn't a suitable arbitrary basis, then it is likely that the input image does not contain the target object.\n3. Describe coordinates of the feature points in the new basis. Quantize obtained coordinates as it was done before.\n4. Compare all the transformed point features in the input image with the hash table. If the point features are identical or similar, then increase the count for the corresponding basis (and the type of object, if any).\n5. For each basis such that the count exceeds a certain threshold, verify the hypothesis that it corresponds to an image basis chosen in Step 2. Transfer the image coordinate system to the model one (for the supposed object) and try to match them. If succeed, the object is found. Otherwise, go back to Step 2.\n\nIt seems that this method is only capable of handling scaling, translation, and rotation. However, the input Image may contain the object in mirror transform. Therefore, geometric hashing should be able to find the object, too. In fact, there are two ways to detect mirrored objects.\n\n1. For the vector graph, make the left side as positive, and the right side as negative. Or multiplying the x position by -1 will give the same result.\n2. Use 3 points for the basis. This allows detecting mirror images (or objects). Actually, using 3 points for the basis is another approach for geometric hashing.\n\nSimilar to the example above, hashing applies to higher-dimensional data. For three-dimensional data points, three points are also needed for the basis. The first two points define the x-axis, and the third point defines the y-axis (with the first point). The z-axis is perpendicular to the created axis using the right-hand rule. Notice that the order of the points affects the resulting basis\n\n", "related": "\n- Perceptual hashing\n\n- Wolfson, H.J. & Rigoutsos, I (1997). Geometric Hashing: An Overview. IEEE Computational Science and Engineering, 4(4), 10-21.\n"}
{"id": "9170159", "url": "https://en.wikipedia.org/wiki?curid=9170159", "title": "Binocular disparity", "text": "Binocular disparity\n\nBinocular disparity refers to the difference in image location of an object seen by the left and right eyes, resulting from the eyes’ horizontal separation (parallax). The brain uses binocular disparity to extract depth information from the two-dimensional retinal images in stereopsis. In computer vision, binocular disparity refers to the difference in coordinates of similar features within two stereo images.\n\nA similar disparity can be used in rangefinding by a coincidence rangefinder to determine distance and/or altitude to a target. In astronomy, the disparity between different locations on the Earth can be used to determine various celestial parallax, and Earth's orbit can be used for stellar parallax.\n\nHuman eyes are horizontally separated by about 50–75 mm (interpupillary distance) depending on each individual. Thus, each eye has a slightly different view of the world around. This can be easily seen when alternately closing one eye while looking at a vertical edge. The binocular disparity can be observed from apparent horizontal shift of the vertical edge between both views.\n\nAt any given moment, the line of sight of the two eyes meet at a point in space. This point in space projects to the same location (i.e. the center) on the retinae of the two eyes. Because of the different viewpoints observed by the left and right eye however, many other points in space do not fall on corresponding retinal locations. Visual binocular disparity is defined as the difference between the point of projection in the two eyes and is usually expressed in degrees as the visual angle.\n\nThe term \"binocular disparity\" refers to geometric measurements made external to the eye. The disparity of the images on the actual retina depends on factors internal to the eye, especially the location of the nodal points, even if the cross section of the retina is a perfect circle. Disparity on retina conforms to binocular disparity when measured as degrees, while much different if measured as distance due to the complicated structure inside eye.\n\nFigure 1: The full black circle is the point of fixation. The blue object lies nearer to the observer. Therefore, it has a \"near\" disparity \"d\". Objects lying more far away (green) correspondingly have a \"far\" disparity \"d\". Binocular disparity is the angle between two lines of projection . One of which is the real projection from the object to the actual point of projection. The other one is the imaginary projection running through the nodal point of the fixation point.\n\nIn computer vision, binocular disparity is calculated from stereo images taken from a set of stereo cameras. The variable distance between these cameras, called the baseline, can affect the disparity of a specific point on their respective image plane. As the baseline increases, the disparity increases due to the greater angle needed to align the sight on the point. However, in computer vision, binocular disparity is referenced as coordinate differences of the point between the right and left images instead of a visual angle. The units are usually measured in pixels.\n\nBrain cells (neurons) in a part of the brain responsible for processing visual information coming from the retinae (primary visual cortex) can detect the existence of disparity in their input from the eyes. Specifically, these neurons will be active, if an object with \"their\" special disparity lies within the part of the visual field to which they have access (receptive field).\n\nResearchers investigating precise properties of these neurons with respect to disparity present visual stimuli with different disparities to the cells and look whether they are active or not. One possibility to present stimuli with different disparities is to place objects in varying depth in front of the eyes. However, the drawback to this method may not be precise enough for objects placed further away as they possess smaller disparities while objects closer will have greater disparities. Instead, neuroscientists use an alternate method as schematised in Figure 2.\n\nFigure 2: The disparity of an object with different depth than the fixation point can alternatively be produced by presenting an image of the object to one eye and a laterally shifted version of the same image to the other eye. The full black circle is the point of fixation. Objects in varying depths are placed along the line of fixation of the left eye. The same disparity produced from a shift in depth of an object (filled coloured circles) can also be produced by laterally shifting the object in constant depth in the picture one eye sees (black circles with coloured margin). Note that for near disparities the lateral shift has to be larger to correspond to the same depth compared with far disparities. This is what neuroscientists usually do with random dot stimuli to study disparity selectivity of neurons since the lateral distance required to test disparities is less than the distances required using depth tests. This principle has also been applied in autostereogram illusions.\n\nThe disparity of features between two stereo images are usually computed as a shift to the left of an image feature when viewed in the right image. For example, a single point that appears at the \"x\" coordinate \"t\" (measured in pixels) in the left image may be present at the \"x\" coordinate \"t\" − 3 in the right image. In this case, the disparity at that location in the right image would be 3 pixels.\n\nStereo images may not always be correctly aligned to allow for quick disparity calculation. For example, the set of cameras may be slightly rotated off level. Through a process known as image rectification, both images are rotated to allow for disparities in only the horizontal direction (i.e. there is no disparity in the \"y\" image coordinates). This is a property that can also be achieved by precise alignment of the stereo cameras before image capture.\n\nAfter rectification, the correspondence problem can be solved using an algorithm that scans both the left and right images for matching image features. A common approach to this problem is to form a smaller image patch around every pixel in the left image. These image patches are compared to all possible disparities in the right image by comparing their corresponding image patches. For example, for a disparity of 1, the patch in the left image would be compared to a similar-sized patch in the right, shifted to the left by one pixel. The comparison between these two patches can be made by attaining a computational measure from one of the following equations that compares each of the pixels in the patches. For all of the following equations, \"L\" and \"R\" refer to the left and right columns while \"r\" and \"c\" refer to the current row and column of either images being examined. \"d\" refers to the disparity of the right image.\n\n- Normalized correlation: formula_1\n- Sum of squared differences: formula_2\n- Sum of absolute differences: formula_3\n\nThe disparity with the lowest computed value using one of the above methods is considered the disparity for the image feature. This lowest score indicates that the algorithm has found the best match of corresponding features in both images.\n\nThe method described above is a brute-force search algorithm. With large patch and/or image sizes, this technique can be very time consuming as pixels are constantly being re-examined to find the lowest correlation score. However, this technique also involves unnecessary repetition as many pixels overlap. A more efficient algorithm involves remembering all values from the previous pixel. An even more efficient algorithm involves remembering column sums from the previous row (in addition to remembering all values from the previous pixel). Techniques that save previous information can greatly increase the algorithmic efficiency of this image analyzing process.\n\nKnowledge of disparity can be used in further extraction of information from stereo images. One case that disparity is most useful is for depth/distance calculation. Disparity and distance from the cameras are inversely related. As the distance from the cameras increases, the disparity decreases. This allows for depth perception in stereo images. Using geometry and algebra, the points that appear in the 2D stereo images can be mapped as coordinates in 3D space.\n\nThis concept is particularly useful for navigation. For example, the Mars Exploration Rover uses a similar method for scanning the terrain for obstacles. The rover captures a pair of images with its stereoscopic navigation cameras and disparity calculations are performed in order to detect elevated objects (such as boulders). Additionally, location and speed data can be extracted from subsequent stereo images by measuring the displacement of objects relative to the rover. In some cases, this is the best source of this type of information as the encoder sensors in the wheels may be inaccurate due to tire slippage.\n\nBinocular disparity forms the premise for a sketch from the film \"Wayne's World\" in which Wayne, who is lying in bed as Tia Carrere's character, Cassandra, perches above him, compares the respective images from his left and right eyes while noting which is which by saying \"Camera 1 ... Camera 2 ... Camera 1 ... Camera 2.\"\n\n", "related": "\n- Binocular summation\n- Binocular vision\n- Cyclodisparity\n- Epipolar geometry\n"}
{"id": "33512861", "url": "https://en.wikipedia.org/wiki?curid=33512861", "title": "Texton", "text": "Texton\n\nThe term texton was introduced by Béla Julesz in 1981 to describe \"the putative units of pre-attentive human texture perception.\"\nThe term reemerged in the late 1990s and early 2000s to describe vector quantized responses of a linear filter bank.\n", "related": "NONE"}
{"id": "33512934", "url": "https://en.wikipedia.org/wiki?curid=33512934", "title": "Cuboid (computer vision)", "text": "Cuboid (computer vision)\n\nIn computer vision, the term cuboid is used to describe a small spatiotemporal volume extracted for purposes of behavior recognition. The cuboid is regarded as a basic geometric primitive type and is used to depict three-dimensional objects within a three dimensional representation of a flat, two dimensional image.\n\nCuboids can be produced from both two-dimensional and three-dimensional images.\n\nOne method used to produce cuboids utilizes scene understanding (SUN) primitive databases, which are collections of pictures that already contain cuboids. By sorting through SUN primitive databases with machine learning tools, computers observe the conditions in which cuboids are produced in images from SUN primitive databases and can learn to produce cuboids from other images.\n\nRGB-D images, which are RGB images that also record the depth of each pixel, are occasionally used to produce cuboids because computers no longer need to determine the depth of an object, as they typically do because depth is already recorded.\n\nCuboid production is sensitive to changes in color and illumination, blockage, and background clutter. This means that it is difficult for computers to produce cuboids of objects that are multicolored, irregularly illuminated, or partially covered, or if there are many objects in the background. This is partially due to the fact that algorithms for producing cuboids are still relatively simple.\n\nCuboids are created for point cloud-based three-dimensional maps and can be utilized in various situations such as augmented reality, the automated control of cars, drones, and robots, and object detection.\n\nCuboids allow for software to identify a scene through geometric descriptions in an “object-agnostic” fashion.\n\nInterest points, locations within images that are identified by a computer as essential to identifying the image, created from two-dimensional images can be used with cuboids for image matching, identifying a room or scene, and instance recognition. Interest points created from three dimensional images can be used with cuboids to recognize activities. This is possible because interest points aid software to focus on only the most important aspects of the images.\n\nRGB-D images and SLAM systems are used together in RGB-D SLAM systems, which are employed by Computer-aided design systems to generate point cloud-based three-dimensional maps.\n\nMost industrial multi-axis machining tools use computer-aided manufacturing and subsequently work in cuboid work spaces.\n", "related": "NONE"}
{"id": "33870490", "url": "https://en.wikipedia.org/wiki?curid=33870490", "title": "Image formation", "text": "Image formation\n\nThe study of image formation encompasses the radiometric and geometric processes by which 2D images of 3D objects are formed. In the case of digital images, the image formation process also includes analog to digital conversion and sampling.\n\nImaging\n\nThe imaging process is a mapping of an object to an image plane. Each point on the image corresponds to a point on the object. An illuminated object will scatter light toward a lens and the lens will collect and focus the light to create the image. The ratio of the height of the image to the height of the object is the magnification. The spatial extent of the image surface and the focal length of the lens determine the field of view of the lens.\n\nIllumination\n\nAn object may be illuminated by the light from an emitting source such as the sun, a light bulb or a Light Emitting Diode. The light incident on the object is reflected in a manner dependent on the surface properties of the object. For rough surfaces, the reflected light is scattered in a manner described by the Bi-directional Reflectance Distribution Function (BRDF) of the surface. The BRDF of a surface is the ratio of the exiting power per square meter per steradian (radiance) to the incident power per square meter (irradiance). The BRDF typically varies with angle and may vary with wavelength, but a specific important case is a surface that has constant BRDF. This surface type is referred to as Lambertian and the magnitude of the BRDF is R/π, where R is the reflectivity of the surface. The portion of scattered light that propagates toward the lens is collected by the entrance pupil of the imaging lens over the field of view.\n\nField of View and Imagery\n\nThe Field of view of a lens is limited by the size of the image plane and the focal length of the lens. The relationship between a location on the image and a location on the object is y = f*tan(θ), where y is the max extent of the image plane, f is the focal length of the lens and θ is the field of view. If y is the max radial size of the image then θ is the field of view of the lens. While the image created by a lens is continuous, it can be modeled as a set of discrete field points, each representing a point on the object. The quality of the image is limited by the aberrations in the lens and the diffraction created by the finite aperture stop.\n\nPupils and Stops\n\nThe aperture stop of a lens is a mechanical aperture which limits the light collection for each field point. The entrance pupil is the image of the aperture stop created by the optical elements on the object side of the lens. The light scattered by an object is collected by the entrance pupil and focused onto the image plane via a series of refractive elements. The cone of the focused light at the image plane is set by the size of the entrance pupil and the focal length of the lens. This is often referred to as the f-stop or f-number of the lens. f/# = f/D where D is the diameter of the entrance pupil.\n\nPixelation and Color vs Monochrome\n\nIn typical digital imaging systems, a sensor is placed at the image plane. The light is focused on to the sensor and the continuous image is pixelated. The light incident on each pixel in the sensor will be integrated within the pixel and a proportional electronic signal will be generated. The angular geometric resolution of a pixel is given by atan(p/f), where p is the pitch of the pixel. This is also called the pixel field of view. The sensor may be monochrome or color. In the case of a monochrome sensor, the light incident on each pixel is integrated and the resulting image is a grayscale like picture. For color images, a mosaic color filter is typically placed over the pixels to create a color image. An example is a Bayer filter. The signal incident on each pixel is then digitized to a bit stream.\n\nImage Quality\n\nThe quality of an image is dependent upon both geometric and physical items. Geometrically, higher density of pixels across an image will give less blocky pixelation and thus a better geometric image quality. Lens aberrations also contribute to the quality of the image. Physically, diffraction due to the aperture stop will limit the resolvable spatial frequencies as a function of f-number.\n\nIn the frequency domain, Modulation Transfer Function (MTF) is a measure of the quality of the imaging system. The MTF is a measure of the visibility of a sinusoidal variation in irradiance on the image plane as a function of the frequency of the sinusoid. It includes the effects of diffraction, aberrations and pixelation. For the lens, the MTF is the autocorrelation of the pupil function, so it accounts for the finite pupil extent and the lens aberrations. The sensor MTF is the Fourier Transform of the pixel geometry. For a square pixel, MTF(ξ) = sin(πξp)/πξp where p is the pixel width and ξ is the spatial frequency. The MTF of the combination of the lens and detector is the product of the two component MTFs.\n\nPerception\n\nColor images can be perceived via two means. In the case of computer vision the light incident on the sensor comprises the image. In the case of visual perception, the human eye has a color dependent response to light so this must be accounted for. This is important consideration when converting to grayscale.\n\nImage Formation in Eye\n\nThe principal difference between the lens of the eye and an ordinary optical lens is that the former is flexible. The radius of the curvature of the anterior surface of the lens is greater than the radius of its posterior surface. The shape of the lens is controlled by tension in the fibers of the ciliary body. To focus on distant objects, the controlling muscles cause the lens to be relatively flattened. Similarly, these muscles allow the lens to become thicker in order to focus on objects near the eye.\n\nThe distance between the center of the lens and the retina (focal length) varies from approximately 17 mm to about 14 mm, as the refractive power of the lens increases from its minimum to its maximum. When the eye focuses on an object farther away than about 3 m, the lens exhibits its lowest refractive power. When the eye focuses on a close object, the lens is most strongly refractive.\n", "related": "NONE"}
{"id": "34413019", "url": "https://en.wikipedia.org/wiki?curid=34413019", "title": "Document mosaicing", "text": "Document mosaicing\n\nDocument mosaicing is a process that stitches multiple, overlapping snapshot images of a document together to produce one large, high resolution composite. The document is slid under a stationary, over-the-desk camera by hand until all parts of the document are snapshotted by the camera’s field of view. As the document slid under the camera, all motion of the document is coarsely tracked by the vision system. The document is periodically snapshotted such that the successive snapshots are overlap by about 50%. The system then finds the overlapped pairs and stitches them together repeatedly until all pairs are stitched together as one piece of document.\n\nThe document mosaicing can be divided into four main processes.\n- Tracking\n- Feature detecting\n- Correspondences establishing\n- Images mosaicing.\n\nIn this process, the motion of the document slid under the camera is coarsely tracked by the system. Tracking is performed by a process called simple correlation process. In the first frame of snapshots, a small patch is extracted from the center of the image as a correlation template as shown in Figure 1. The correlation process is performed in the four times size of the patch area of the next frame. The motion of the paper is indicated by the peak in the correlation function. The peak in the correlation function indicates the motion of the paper. The template is resampled from this frame and the tracking continues until the template reaches the edge of the document. After the template reaches the edge of the document, another snapshot is taken and the tracking process performs repeatedly until the whole document is imaged. The snapshots are stored in an ordered list to facilitate pairing the overlapped images in later processes.\n\nFeature detection is the process of finding the transformation that aligns one image with another. There are two main approaches for feature detection.\n- Feature-based approach : Motion parameters are estimated from point correspondences. This approach is suitable for the case that there is plenty supply of stable and detectable features.\n- Featureless approach : When the motion between the two images is small, the motion parameters are estimated using optical flow. On the other hand, when the motion between the two images is large, the motion parameters are estimated using generalised cross-correlation. However, this approach requires a computationally expensive resources.\n\nEach image is segmented into a hierarchy of columns, lines, and words to match the organised sets of features across images. Skew angle estimation and columns, lines and words finding are the examples of feature detection operations.\n\nFirstly, the angle that the rows of text make with the image raster lines (skew angle) is estimated. It is assumed to lie in the range of ±20°. A small patch of text in the image is selected randomly and then rotated in the range of ±20° until the variance of the pixel intensities of the patch summed along the raster lines is maximised. See Figure 2.\n\nTo ensure that the found skew angle is accurate, the document mosaic system performs calculation at many image patches and derive the final estimation by finding the average of the individual angles weighted by the variance of the pixel intensities of each patch.\n\nIn this operation, the de-skewed document is intuitively segmented into a hierarchy of columns, lines and words. The sensitivity to illumination and page coloration of the de-skewed document can be removed by applying a Sobel operator to the de-skewed image and thresholding the output to obtain the binary gradient, de-skewed image. See Figure 3.\n\nThe operation can be roughly separated into 3 steps : column segmentation, line segmentation and word segmentation.\n\n1. Columns are easily segmented from the binary gradient, de-skewed images by summing pixels vertically as shown in Figure 4.\n2. Baselines of each row are segmented in the same way as the column segmentation process but horizontally.\n3. Finally, individual words are segmented by applying the vertical process at each segmented row.\n\nThese segmentations are important because the document mosaic is created by matching the lower right corners of words in overlapping images pair. Moreover, the segmentation operation can organize the list of images in the context of a hierarchy of rows and column reliably.\n\nThe segmentation operation involves a considerable amount of summing in the binary gradient, de-skewed images, which done by construct a matrix of partial sums whose elements are given by\n\nformula_1\nThe matrix of partial sums is calculated in one pass through the binary gradient, de-skewed image.\n\nformula_2\n\nThe two images are now organized in hierarchy of linked lists in following structure :\n- image=list of columns\n- row=list of words\n- column=list of row\n- word=length (in pixels)\n\nAt the bottom of the structure, the length of each word is recorded for establishing correspondence between two images to reduce to search only the corresponding structures for the groups of words with the matching lengths.\n\nA seed match finding is done by comparing each row in image1 with each row in image2. The two rows are then compared to each other by every word. If the length (in pixel) of the two words (one from image1 and one from image2) and their immediate neighbours agree with each other within a predefined tolerance threshold (5 pixels, for example), then they are assumed to match. The row of each image is assumed a match if there are three or more word matches between the two rows. The seed match finding operation is terminated when two pairs of consecutive row match are found.\n\nAfter finishing a seed match finding operation, the next process is to build the match list to generate the correspondences points of the two images. The process is done by searching the matching pairs of rows away from the seed row.\n\nGiven the list of corresponding points of the two images, finding the transformation of the overlapping portion of the images is the next process. Assuming a pinhole camera model, the transformation between pixels (u,v) of image 1 and pixels (u0, v0) of image 2 is demonstrated by a plane-to-plane projectivity.\n\nformula_3\n\nThe parameters of the projectivity is found from four pairs of matching points. RANSAC regression technique is used to reject outlying matches and estimate the projectivity from the remaining good matches.\n\nThe projectivity is fine-tuned using correlation at the corners of the overlapping portion to obtain four correspondences to sub-pixel accuracy. Therefore, image1 is then transformed into image2’s coordinate system using Eq.1. The typical result of the process is shown in Figure 5.\n\nFinally, the whole page composition is built up by mapping all the images into the coordinate system of an “anchor” image, which is normally the one nearest the page center. The transformations to the anchor frame are calculated by concatenating the pair-wise transformations found earlier. The raw document mosaic is shown in Figure 6.\n\nHowever, there might be a problem of non-consecutive images that are overlap. This problem can be solved by performing Hierarchical sub-mosaics. As shown in Figure 7, image1 and image2 are registered, as are image3 and image4, creating two sub-mosaics. These two sub-mosaics are later stitched together in another mosaicing process.\n\nThere are various areas that the technique of document mosaicing can be applied to such as : \n- Text segmentation of images of documents\n- Document Recognition\n- Interaction with paper on the digital desk\n- Video mosaics for virtual environments\n- Image registration techniques\n\n- D.G. Lowe. Perceptual Organization and Visual Recognition. Kluwer Academic Publishers, Boston, 1985.\n- Camera-Based Document Image Mosaicing. (n.d.). Image (Rochester, N.Y.), 1.\n- Sato, T., Ikeda, S., Kanbara, M., Iketani, A., Nakajima, N., Yokoya, N., & Yamada, K. (n.d.). High-resolution Video Mosaicing for Documents and Photos by Estimating Camera Motion. \"Mosaic A Journal For The Interdisciplinary Study Of Literature.\"\n\n- Advanced Vision homepage\n", "related": "NONE"}
{"id": "34641430", "url": "https://en.wikipedia.org/wiki?curid=34641430", "title": "Contextual image classification", "text": "Contextual image classification\n\nContextual image classification, a topic of pattern recognition in computer vision, is an approach of classification based on contextual information in images. \"Contextual\" means this approach is focusing on the relationship of the nearby pixels, which is also called neighbourhood. The goal of this approach is to classify the images by using the contextual information.\n\nSimilar as processing language, a single word may have multiple meanings unless the context is provided, and the patterns within the sentences are the only informative segments we care about. For images, the principle is same. Find out the patterns and associate proper meanings to them.\n\nAs the image illustrated below, if only a small portion of the image is shown, it is very difficult to tell what the image is about.\nEven try another portion of the image, it is still difficult to classify the image.\nHowever, if we increase the contextual of the image, then it makes more sense to recognize.\nAs the full images shows below, almost everyone can classify it easily.\nDuring the procedure of segmentation, the methods which do not use the contextual information are sensitive to noise and variations, thus the result of segmentation will contain a great deal of misclassified regions, and often these regions are small (e.g., one pixel).\n\nCompared to other techniques, this approach is robust to noise and substantial variations for it takes the continuity of the segments into account.\n\nSeveral methods of this approach will be described below.\n\nThis approach is very effective against small regions caused by noise. And these small regions are usually formed by few pixels or one pixel. The most probable label is assigned to these regions.\nHowever, there is a drawback of this method. The small regions also can be formed by correct regions rather than noise, and in this case the method is actually making the classification worse.\nThis approach is widely used in remote sensing applications.\n\nThis is a two-stage classification process:\n1. For each pixel, label the pixel and form a new feature vector for it.\n2. Use the new feature vector and combine the contextual information to assign the final label to the\n\nInstead of using single pixels, the neighbour pixels can be merged into homogeneous regions benefiting from contextual information. And provide these regions to classifier.\n\nThe original spectral data can be enriched by adding the contextual information carried by the neighbour pixels, or even replaced in some occasions. This kind of pre-processing methods are widely used in textured image recognition. The typical approaches include mean values, variances, texture description, etc.\n\nThe classifier uses the grey level and pixel neighbourhood (contextual information) to assign labels to pixels. In such case the information is a combination of spectral and spatial information.\n\nContextual classification of image data is based on the Bayes minimum error classifier (also known as a naive Bayes classifier).\n\nPresent the pixel:\n- A pixel is denoted as formula_1.\n- The neighbourhood of each pixel formula_1 is a vector and denoted as formula_3.\n- The values in the neighbourhood vector is denoted as formula_4.\n- Each pixel is presented by the vector\n\n- The labels (classification) of pixels in the neighbourhood formula_3 are presented as a vector\n\n- A vector presents the labels in the neighbourhood formula_3 without the pixel formula_1\n\nThe neighbourhood:\nSize of the neighbourhood. There is no limitation of the size, but it is considered to be relatively small for each pixel formula_1.\nA reasonable size of neighbourhood would be formula_15 of 4-connectivity or 8-connectivity (formula_1 is marked as red and placed in the centre).\nThe calculation:\n\nApply the minimum error classification on a pixel formula_1, if the probability of a class formula_18 being presenting the pixel formula_1 is the highest among all, then assign formula_18 as its class.\n\nThe contextual classification rule is described as below, it uses the feature vector formula_22 rather than formula_1.\n\nUse the Bayes formula to calculate the posteriori probability formula_25\n\nThe number of vectors is the same as the number of pixels in the image. For the classifier uses a vector corresponding to each pixel formula_27, and the vector is generated from the pixel's neighbourhood.\n\nThe basic steps of contextual image classification:\n1. Calculate the feature vector formula_28 for each pixel.\n2. Calculate the parameters of probability distribution formula_29 and formula_30\n3. Calculate the posterior probabilities formula_31 and all labels formula_32. Get the image classification result.\n\nThe template matching is a \"brute force\" implementation of this approach. The concept is first create a set of templates, and then look for small parts in the image match with a template.\n\nThis method is computationally high and inefficient. It keeps an entire templates list during the whole process and the number of combinations is extremely high. For a formula_33 pixel image, there could be a maximum of formula_34 combinations, which leads to high computation. This method is a top down method and often called table look-up or dictionary look-up.\n\nThe Markov chain also can be applied in pattern recognition. The pixels in an image can be recognised as a set of random variables, then use the lower order Markov chain to find the relationship among the pixels. The image is treated as a virtual line, and the method uses conditional probability.\n\nThe Hilbert curve runs in a unique pattern through the whole image, it traverses every pixel without visiting any of them twice and keeps a continuous curve. It is fast and efficient.\n\nThe lower-order Markov chain and Hilbert space-filling curves mentioned above are treating the image as a line structure. The Markov meshes however will take the two dimensional information into account.\n\nThe dependency tree is a method using tree dependency to approximate probability distributions.\n\n- Advanced Vision homepage\n- The Use of Context in Pattern Recognition\n- Image Analysis and Understanding: contextual image classification\n", "related": "NONE"}
{"id": "34668189", "url": "https://en.wikipedia.org/wiki?curid=34668189", "title": "3D reconstruction from multiple images", "text": "3D reconstruction from multiple images\n\n3D reconstruction from multiple images is the creation of three-dimensional models from a set of images. It is the reverse process of obtaining 2D images from 3D scenes.\n\nThe essence of an image is a projection from a 3D scene onto a 2D plane, during which process the depth is lost. The 3D point corresponding to a specific image point is constrained to be on the line of sight. From a single image, it is impossible to determine which point on this line corresponds to the image point. If two images are available, then the position of a 3D point can be found as the intersection of the two projection rays. This process is referred to as triangulation. The key for this process is the relations between multiple views which convey the information that corresponding sets of points must contain some structure and that this structure is related to the poses and the calibration of the camera.\n\nIn recent decades, there is an important demand for 3D content for computer graphics, virtual reality and communication, triggering a change in emphasis for the requirements. Many existing systems for constructing 3D models are built around specialized hardware (e.g. stereo rigs) resulting in a high cost, which cannot satisfy the requirement of its new applications. This gap stimulates the use of digital imaging facilities (like a camera). An early method was proposed by Tomasi and Kanade. They used an affine factorization approach to extract 3D from images sequences. However, the assumption of orthographic projection is a significant limitation of this system.\n\nThe task of converting multiple 2D images into 3D model consists of a series of processing steps:\n\nCamera calibration consists of intrinsic and extrinsic parameters, without which at some level no arrangement of algorithms can work. The dotted line between Calibration and Depth determination represents that the camera calibration is usually required for determining depth.\n\nDepth determination serves as the most challenging part in the whole process, as it calculates the 3D component missing from any given image – depth. The correspondence problem, finding matches between two images so the position of the matched elements can then be triangulated in 3D space is the key issue here.\n\nOnce you have the multiple depth maps you have to combine them to create a final mesh by calculating depth and projecting out of the camera – registration. Camera calibration will be used to identify where the many meshes created by depth maps can be combined together to develop a larger one, providing more than one view for observation.\n\nBy the stage of Material Application you have a complete 3D mesh, which may be the final goal, but usually you will want to apply the color from the original photographs to the mesh. This can range from projecting the images onto the mesh randomly, through approaches of combining the textures for super resolution and finally to segmenting the mesh by material, such as specular and diffuse properties.\n\nGiven a group of 3D points viewed by N cameras with matrices formula_1, define formula_2 to be the homogeneous coordinates of the projection of the formula_3 point onto the formula_4 camera. The reconstruction problem can be changed to: given the group of pixel coordinates formula_5, find the corresponding set of camera matrices formula_6 and the scene structure formula_7 such that\n\nGenerally, without further restrictions, we will obtain a projective reconstruction. If formula_6 and formula_7 satisfy (1), formula_11 and formula_12 will satisfy (1) with any 4 × 4 nonsingular matrix T.\n\nA projective reconstruction can be calculated by correspondence of points only without any \"a priori\" information.\n\nAutocalibration or self-calibration is the classical approach, in which camera motion and parameters are recovered first, using rigidity, then structure is readily calculated. Two methods implementing this idea are presented as follows:\n\nWith a minimum of three displacements, we can obtain the internal parameters of the camera using a system of polynomial equations due to Kruppa, which are derived from a geometric interpretation of the rigidity constraint.\n\nThe matrix formula_13 is unknown in the Kruppa equations, named Kruppa coefficients matrix. With K and by the method of Cholesky factorization one can obtain the intrinsic parameters easily:\n\nRecently Hartley proposed a simpler form. Let formula_15 be written as formula_16, where\n\nThen the Kruppa equations are rewritten (the derivation can be found in )\n\nThis method is based on the use of rigidity constraint. Design a cost function, which considers the intrinsic parameters as arguments and the fundamental matrices as parameters. formula_17 is defined as the fundamental matrix, formula_18and formula_19 as intrinsic parameters matrices.\n\nRecently, new methods based on the concept of stratification have been proposed. Starting from a projective structure, which can be calculated from correspondences only, upgrade this projective reconstruction to a Euclidean reconstruction, by making use of all the available constraints. With this idea the problem can be stratified into different sections: according to the amount of constraints available, it can be analyzed at a different level, projective, affine or Euclidean.\n\nUsually, the world is perceived as a 3D Euclidean space. In some cases, it is not possible to use the full Euclidean structure of 3D space. The simplest being projective, then the affine geometry which forms the intermediate layers and finally Euclidean geometry. The concept of stratification is closely related to the series of transformations on geometric entities: in the projective stratum is a series of projective transformations (a homography), in the affine stratum is a series of affine transformations, and in Euclidean stratum is a series of Euclidean transformations.\n\nSuppose that a fixed scene is captured by two or more perspective cameras and the correspondences between visible points in different images are already given. However, in practice, the matching is an essential and extremely challenging issue in computer vision. Here, we suppose that formula_20 3D points formula_21 are observed by formula_22 cameras with projection matrices formula_23 Neither the positions of point nor the projection of camera are known. Only the projections formula_24 of the formula_4 point in the formula_3 image are known.\n\nSimple counting indicates we have formula_27 independent measurements and only formula_28 unknowns, so the problem is supposed to be soluble with enough points and images. The equations in homogeneous coordinates can be represented:\n\nSo we can apply a nonsingular 4 × 4 transformation \"H\" to projections formula_30→formula_31 and world points formula_32→formula_33. Hence, without further constraints, reconstruction is only an unknown projective deformation of the 3D world.\n\n\"See affine space for more detailed information about computing the location of the plane at infinity formula_34.\"\nThe simplest way is to exploit prior knowledge, for example the information that lines in the scene are parallel or that a point is the one thirds between two others.\n\nWe can also use prior constraints on the camera motion. By analyzing different images of the same point can obtain a line in the direction of motion. The intersection of several lines is the point at infinity in the motion direction, and one constraint on the affine structure.\n\nBy mapping the projective reconstruction to one that satisfies a group of redundant Euclidean constraints, we can find a projective transformation \"H\" in equation (2).The equations are highly nonlinear and a good initial guess for the structure is required. This can be obtained by assuming a linear projection - parallel projection, which also allows easy reconstruction by SVD decomposition.\n\nInevitably, measured data (i.e., image or world point positions) is noisy and the noise comes from many sources. To reduce the effect of noise, we usually use more equations than necessary and solve with least squares.\n\nFor example, in a typical null-space problem formulation Ax = 0 (like the DLT algorithm), the square of the residual ||Ax|| is being minimized with the least squares method.\n\nIn general, if ||Ax|| can be considered as a distance between the geometrical entities (points, lines, planes, etc.), then what is being minimized is a geometric error, otherwise (when the error lacks a good geometrical interpretation) it is called an algebraic error.\n\nTherefore, compared with algebraic error, we prefer to minimize a geometric error for the reasons listed:\n1. The quantity being minimized has a meaning.\n2. The solution is more stable.\n3. The solution is constant under Euclidean transforms.\n\nAll the linear algorithms (DLT and others) we have seen so far minimize an algebraic error. Actually, there is no justification in minimizing an algebraic error apart from the ease of implementation, as it results in a linear problem. The minimization of a geometric error is often a non-linear problem, that admit only iterative solutions and requires a starting point.\n\nUsually, linear solution based on algebraic residuals serves as a starting point for a non-linear minimization of a geometric cost function, which provides the solution a final “polish”.\n\nThe 2-D imaging has problems of anatomy overlapping with each other and don’t disclose the abnormalities. The 3-D imaging can be used for both diagnostic and therapeutic purposes.\n\n3-D models are used for planning the operation, morphometric studies and has more reliability in orthopedics.\nTo reconstruct 3-D images from 2-D images taken by a camera at multiple angles. Medical imaging techniques like CT scanning and MRI are expensive, and although CT scans are accurate, they can induce high radiation doses which is a risk for patients with certain diseases. Methods based on MRI are not accurate. Since we are exposed to powerful magnetic fields during an MRI scan, this method is not suitable for patients with ferromagnetic metallic implants. Both the methods can be done only when in lying position where the global structure of the bone changes. So, we discuss the following methods which can be performed while standing and require low radiation dose.\n\nThough these techniques are 3-D imaging, the region of interest is restricted to a slice; data are acquired to form a time sequence.\n\nThis method is simple and implemented by identifying the points manually in multi-view radiographs. The first step is to extract the corresponding points in two x-ray images and second step is the 3D reconstruction with algorithms like Discrete Linear Transform. Using DLT, the reconstruction is done only where there are SCPs. By increasing the number of points, the results improve but it is time consuming. This method has low accuracy because of low reproducibility and time consumption. This method is dependent on the skill of the operator. This method is not suitable for bony structures with continuous shape. This method is generally used as an initial solution for other methods.\n\nThis method uses X-ray images for 3D Reconstruction and to develop 3D models with low dose radiations in weight bearing positions.\n\nIn NSCC algorithm, the preliminary step is calculation of an initial solution. Firstly anatomical regions from the generic object are defined. Secondly, manual 2D contours identification on the radiographs is performed. From each radiograph 2D contours are generated using the 3D initial solution object. 3D contours of the initial object surface are projected onto their associated radiograph. The 2D association performed between these 2 set points is based on point-to-point distances and contours derivations developing a correspondence between the 2D contours and the 3D contours. Next step is optimization of the initial solution. Lastly deformation of the optimized solution is done by applying Kriging algorithm to the optimized solution. Finally, by iterating the final step until the distance between two set points is superior to a given precision value the reconstructed object is obtained.\n\nThe advantage of this method is it can be used for bony structures with continuous shape and it also reduced human intervention but they are time consuming.\n\nSurface Rendering technique visualizes a 3D object as a set of surfaces called iso-surfaces. Each surface has points with the same intensity (called iso-value). It is used when we want to see the separated structures e.g. skull from slices of head, blood vessel system from slices of body etc. This technique is used mostly for high contrast data. Two main methods for reconstructing are:\n\n- Contour based reconstruction: Iso-contours are attached to form iso-surfaces\n- Voxel based reconstruction: Voxels having same intensity values are used to form iso-surfaces. One popular algorithm is Marching Cubes. Some similar algorithms as Marching Tetrahedrons, Dividing Cubes can be considered.\n\nOther proposed or developed techniques include Statistical Shape Model Based Methods, Parametric Methods, Hybrid methods.\n\n", "related": "\n- 3D pose estimation\n- 3D reconstruction\n- 3D photography\n- 2D to 3D conversion\n- 3D data acquisition and object reconstruction\n- Epipolar geometry\n- Camera resectioning\n- Computer stereo vision\n- Structure from motion\n- Stereophotogrammetry\n- Comparison of photogrammetry software\n- Visual hull\n- Human image synthesis\n\n- Yasutaka Furukawa and Carlos Hernández (2015) \"Multi-View Stereo: A Tutorial\"\n- Flynn, John, et al. \"Deepstereo: Learning to predict new views from the world's imagery.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016.\n\n- 3D Reconstruction from Multiple Images - discusses methods to extract 3D models from plain images.\n- Visual 3D Modeling from Images and Videos - a tech-report describes the theory, practice and tricks on 3D reconstruction from images and videos.\n- Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes with Deep Generative Networks - Generate and reconstruct 3D shapes via modeling multi-view depth maps or silhouettes.\n"}
{"id": "33270868", "url": "https://en.wikipedia.org/wiki?curid=33270868", "title": "EigenMoments", "text": "EigenMoments\n\nEigenMoments is a set of orthogonal, noise robust, invariant to rotation, scaling and translation and distribution sensitive moments. Their application can be found in signal processing and computer vision as descriptors of the signal or image. The descriptors can later be used for classification purposes.\n\nIt is obtained by performing orthogonalization, via eigen analysis on geometric moments.\n\nEigenMoments are computed by performing eigen analysis on the moment space of an image by maximizing Signal to Noise Ratio in the feature space in form of Rayleigh quotient.\n\nThis approach has several benefits in Image processing applications:\n1. Dependency of moments in the moment space on the distribution of the images being transformed, ensures decorrelation of the final feature space after eigen analysis on the moment space.\n2. The ability of EigenMoments to take into account distribution of the image makes it more versatile and adaptable for different genres.\n3. Generated moment kernels are orthogonal and therefore analysis on the moment space becomes easier. Transformation with orthogonal moment kernels into moment space is analogous to projection of the image onto a number of orthogonal axes.\n4. Nosiy components can be removed. This makes EigenMoments robust for classification applications.\n5. Optimal information compaction can be obtained and therefore a few number of moments are needed to characterize the images.\n\nAssume that a signal vector formula_1 is taken from a certain distribution having coorelation formula_2, i.e. formula_3 where E[.] denotes expected value.\n\nDimension of signal space, n, is often too large to be useful for practical application such as pattern classification, we need to transform the signal space into a space with lower dimensionality.\n\nThis is performed by a two-step linear transformation:\n\nformula_4\n\nwhere formula_5 is the transformed signal, formula_6 a fixed transformation matrix which transforms the signal into the moment space, and formula_7 the transformation matrix which we are going to determine by maximizing the SNR of the feature space resided by formula_8. For the case of Geometric Moments, X would be the monomials. If formula_9, a full rank transformation would result, however usually we have formula_10 and formula_11. This is specially the case when formula_12 is of high dimensions.\n\nFinding formula_13 that maximizes the SNR of the feature space:\n\nformula_14\n\nwhere N is the correlation matrix of the noise signal. The problem can thus be formulated as\n\nformula_15\n\nsubject to constraints:\n\nformula_16 where formula_17 is the Kronecker delta.\n\nIt can be observed that this maximization is Rayleigh quotient by letting formula_18 and formula_19 and therefore can be written as:\n\nformula_20, formula_21\n\nOptimization of Rayleigh quotient has the form:\n\nformula_22\n\nand formula_23 and formula_24, both are symmetric and formula_24 is positive definite and therefore invertible.\nScaling formula_26 does not change the value of the object function and hence and additional scalar constraint formula_27 can be imposed on formula_26 and no solution would be lost when the objective function is optimized.\n\nThis constraint optimization problem can be solved using Lagrangian multiplier:\n\nformula_29 subject to formula_30\n\nformula_31\n\nequating first derivative to zero and we will have:\n\nformula_32\n\nwhich is an instance of Generalized Eigenvalue Problem (GEP).\nThe GEP has the form:\n\nformula_33\n\nfor any pair formula_34 that is a solution to above equation, formula_26 is called a generalized eigenvector and formula_36 is called a generalized eigenvalue.\n\nFinding formula_26 and formula_38 that satisfies this equations would produce the result which optimizes Rayleigh quotient.\n\nOne way of maximizing Rayleigh quotient is through solving the Generalized Eigen Problem. Dimension reduction can be performed by simply choosing the first components formula_39, formula_40, with the highest values for formula_41 out of the formula_42 components, and discard the rest. Interpretation of this transformation is rotating and scaling the moment space, transforming it into a feature space with maximized SNR and therefore, the first formula_43 components are the components with highest formula_43 SNR values.\n\nThe other method to look at this solution is to use the concept of simultaneous diagonalization instead of Generalized Eigen Problem.\n\n1. Let formula_18 and formula_19 as mentioned earlier. We can write formula_13 as two separate transformation matrices:\n\nformula_48\n\n1. formula_49 can be found by first diagonalize B:\n\nformula_50.\n\nWhere formula_51 is a diagonal matrix sorted in increasing order. Since formula_24 is positive definite, thus formula_53. We can discard those eigenvalues that large and retain those close to 0, since this means the energy of the noise is close to 0 in this space, at this stage it is also possible to discard those eigenvectors that have large eigenvalues.\n\nLet formula_54 be the first formula_43 columns of formula_56, now formula_57 where formula_58 is the formula_59 principal submatrix of formula_51.\n\n1. Let\n\nformula_61\n\nand hence:\n\nformula_62.\n\nformula_49 whiten formula_24 and reduces the dimensionality from formula_42 to formula_43. The transformed space resided by formula_67 is called the noise space.\n\n1. Then, we diagonalize formula_68:\n\nformula_69,\n\nwhere formula_70. formula_71 is the matrix with eigenvalues of formula_68 on its diagonal. We may retain all the eigenvalues and their corresponding eigenvectors since the most of the noise are already discarded in previous step.\n\n1. Finally the transformation is given by:\n\nformula_73\n\nwhere formula_13 diagonalizes both the numerator and denominator of the SNR,\n\nformula_75, formula_76 and the transformation of signal formula_77 is defined as formula_78.\n\nTo find the information loss when we discard some of the eigenvalues and eigenvectors we can perform following analysis:\n\nformula_79\n\nEigenmoments are derived by applying the above framework on Geometric Moments. They can be derived for both 1D and 2D signals.\n\nIf we let formula_80, i.e. the monomials, after the transformation formula_81 we obtain Geometric Moments, denoted by vector formula_82, of signal formula_83,i.e. formula_84.\n\nIn practice it is difficult to estimate the correlation signal due to insufficient number of samples, therefore parametric approaches are utilized.\n\nOne such model can be defined as:\n\nformula_85,\n\nwhere formula_86. This model of correlation can be replaced by other models however this model covers general natural images.\n\nSince formula_87 does not affect the maximization it can be dropped.\n\nformula_88\n\nThe correlation of noise can be modelled as formula_89, where formula_90 is the energy of noise. Again formula_90 can be dropped because the constant does not have any effect on the maximization problem.\n\nformula_92\nformula_93\n\nUsing the computed A and B and applying the algorithm discussed in previous section we find formula_13 and set of transformed monomials formula_95 which produces the moment kernels of EM. The moment kernels of EM decorrelate the correlation in the image.\n\nformula_96,\n\nand are orthogonal:\n\nformula_97\n\nTaking formula_98, the dimension of moment space as formula_99 and the dimension of feature space as formula_100, we will have:\n\nformula_101\n\nand\n\nformula_102\n\nThe derivation for 2D signal is the same as 1D signal except that conventional Geometric Moments are directly employed to obtain the set of 2D EigenMoments.\n\nThe definition of Geometric Moments of order formula_103 for 2D image signal is:\n\nformula_104.\n\nwhich can be denoted as formula_105. Then the set of 2D EigenMoments are:\n\nformula_106,\n\nwhere formula_107 is a matrix that contains the set of EigenMoments.\n\nformula_108.\n\nIn order to obtain a set of moment invariants we can use normalized Geometric Moments formula_109 instead of formula_82.\n\nNormalized Geometric Moments are invariant to Rotation, Scaling and Transformation and defined by:\n\nformula_111\n\nwhere:formula_112 is the centroid of the image formula_113 and\n\nformula_114.\n\nformula_115 in this equation is a scaling factor depending on the image. formula_115 is usually set to 1 for binary images.\n\n", "related": "\n- Computer vision\n- Signal processing\n- Image moment\n\n- implementation of EigenMoments in Matlab\n"}
{"id": "1703661", "url": "https://en.wikipedia.org/wiki?curid=1703661", "title": "Scale space", "text": "Scale space\n\nScale-space theory is a framework for multi-scale signal representation developed by the computer vision, image processing and signal processing communities with complementary motivations from physics and biological vision. It is a formal theory for handling image structures at different scales, by representing an image as a one-parameter family of smoothed images, the scale-space representation, parametrized by the size of the smoothing kernel used for suppressing fine-scale structures. The parameter formula_1 in this family is referred to as the \"scale parameter\", with the interpretation that image structures of spatial size smaller than about formula_2 have largely been smoothed away in the scale-space level at scale formula_1.\n\nThe main type of scale space is the \"linear (Gaussian) scale space\", which has wide applicability as well as the attractive property of being possible to derive from a small set of \"scale-space axioms\". The corresponding scale-space framework encompasses a theory for Gaussian derivative operators, which can be used as a basis for expressing a large class of visual operations for computerized systems that process visual information. This framework also allows visual operations to be made \"scale invariant\", which is necessary for dealing with the size variations that may occur in image data, because real-world objects may be of different sizes and in addition the distance between the object and the camera may be unknown and may vary depending on the circumstances.\n\nThe notion of scale space applies to signals of arbitrary numbers of variables. The most common case in the literature applies to two-dimensional images, which is what is presented here. For a given image formula_4, its linear (Gaussian) \"scale-space representation\" is a family of derived signals formula_5 defined by the convolution of formula_4 with the two-dimensional Gaussian kernel\n\nsuch that\n\nwhere the semicolon in the argument of formula_9 implies that the convolution is performed only over the variables formula_10, while the scale parameter formula_1 after the semicolon just indicates which scale level is being defined. This definition of formula_9 works for a continuum of scales formula_13, but typically only a finite discrete set of levels in the scale-space representation would be actually considered.\n\nThe scale parameter formula_14 is the variance of the Gaussian filter and as a limit for formula_15 the filter formula_16 becomes an impulse function such that formula_17 that is, the scale-space representation at scale level formula_18 is the image formula_19 itself. As formula_20 increases, formula_9 is the result of smoothing formula_19 with a larger and larger filter, thereby removing more and more of the details which the image contains. Since the standard deviation of the filter is formula_23, details which are significantly smaller than this value are to a large extent removed from the image at scale parameter formula_20, see the following figure and for graphical illustrations.\n\nWhen faced with the task of generating a multi-scale representation one may ask: could any filter \"g\" of low-pass type and with a parameter \"t\" which determines its width be used to generate a scale space? The answer is no, as it is of crucial importance that the smoothing filter does not introduce new spurious structures at coarse scales that do not correspond to simplifications of corresponding structures at finer scales. In the scale-space literature, a number of different ways have been expressed to formulate this criterion in precise mathematical terms.\n\nThe conclusion from several different axiomatic derivations that have been presented is that the Gaussian scale space constitutes the \"canonical\" way to generate a linear scale space, based on the essential requirement that new structures must not be created when going from a fine scale to any coarser scale.\nConditions, referred to as \"scale-space axioms\", that have been used for deriving the uniqueness of the Gaussian kernel include linearity, shift invariance, semi-group structure, non-enhancement of local extrema, scale invariance and rotational invariance.\nIn the works, the uniqueness claimed in the arguments based on scale invariance originally due to Iijima (1962) has been criticized, and alternative self-similar scale-space kernels have been proposed. The Gaussian kernel is, however, a unique choice according to the scale-space axiomatics based on causality or non-enhancement of local extrema.\n\n\"Equivalently\", the scale-space family can be defined as the solution of the diffusion equation (for example in terms of the heat equation),\n\nwith initial condition formula_26. This formulation of the scale-space representation \"L\" means that it is possible to interpret the intensity values of the image \"f\" as a \"temperature distribution\" in the image plane and that the process which generates the scale-space representation as a function of \"t\" corresponds to heat diffusion in the image plane over time \"t\" (assuming the thermal conductivity of the material equal to the arbitrarily chosen constant ½). Although this connection may appear superficial for a reader not familiar with differential equations, it is indeed the case that the main scale-space formulation in terms of non-enhancement of local extrema is expressed in terms of a sign condition on partial derivatives in the 2+1-D volume generated by the scale space, thus within the framework of partial differential equations. Furthermore, a detailed analysis of the discrete case shows that the diffusion equation provides a unifying link between continuous and discrete scale spaces, which also generalizes to nonlinear scale spaces, for example, using anisotropic diffusion. Hence, one may say that the primary way to generate a scale space is by the diffusion equation, and that the Gaussian kernel arises as the Green's function of this specific partial differential equation.\n\nThe motivation for generating a scale-space representation of a given data set originates from the basic observation that real-world objects are composed of different structures at different scales. This implies that real-world objects, in contrast to idealized mathematical entities such as points or lines, may appear in different ways depending on the scale of observation.\nFor example, the concept of a \"tree\" is appropriate at the scale of meters, while concepts such as leaves and molecules are more appropriate at finer scales.\nFor a computer vision system analysing an unknown scene, there is no way to know a priori what scales are appropriate for describing the interesting structures in the image data.\nHence, the only reasonable approach is to consider descriptions at multiple scales in order to be able to capture the unknown scale variations that may occur.\nTaken to the limit, a scale-space representation considers representations at all scales.\n\nAnother motivation to the scale-space concept originates from the process of performing a physical measurement on real-world data. In order to extract any information from a measurement process, one has to apply \"operators of non-infinitesimal size\" to the data. In many branches of computer science and applied mathematics, the size of the measurement operator is disregarded in the theoretical modelling of a problem. The scale-space theory on the other hand explicitly incorporates the need for a non-infinitesimal size of the image operators as an integral part of any measurement as well as any other operation that depends on a real-world measurement.\n\nThere is a close link between scale-space theory and biological vision. Many scale-space operations show a high degree of similarity with receptive field profiles recorded from the mammalian retina and the first stages in the visual cortex.\nIn these respects, the scale-space framework can be seen as a theoretically well-founded paradigm for early vision, which in addition has been thoroughly tested by algorithms and experiments.\n\nAt any scale in scale space, we can apply local derivative operators to the scale-space representation:\n\nDue to the commutative property between the derivative operator and the Gaussian smoothing operator, such \"scale-space derivatives\" can equivalently be computed by convolving the original image with Gaussian derivative operators. For this reason they are often also referred to as \"Gaussian derivatives\":\n\nThe uniqueness of the Gaussian derivative operators as local operations derived from a scale-space representation can be obtained by similar axiomatic derivations as are used for deriving the uniqueness of the Gaussian kernel for scale-space smoothing.\n\nThese Gaussian derivative operators can in turn be combined by linear or non-linear operators into a larger variety of different types of feature detectors, which in many cases can be well modelled by differential geometry. Specifically, invariance (or more appropriately \"covariance\") to local geometric transformations, such as rotations or local affine transformations, can be obtained by considering differential invariants under the appropriate class of transformations or alternatively by normalizing the Gaussian derivative operators to a locally determined coordinate frame determined from e.g. a preferred orientation in the image domain or by applying a preferred local affine transformation to a local image patch (see the article on affine shape adaptation for further details).\n\nWhen Gaussian derivative operators and differential invariants are used in this way as basic feature detectors at multiple scales, the uncommitted first stages of visual processing are often referred to as a \"visual front-end\". This overall framework has been applied to a large variety of problems in computer vision, including feature detection, feature classification, image segmentation, image matching, motion estimation, computation of shape cues and object recognition. The set of Gaussian derivative operators up to a certain order is often referred to as the \"N-jet\" and constitutes a basic type of feature within the scale-space framework.\n\nFollowing the idea of expressing visual operation in terms of differential invariants computed at multiple scales using Gaussian derivative operators, we can express an edge detector from the set of points that satisfy the requirement that the gradient magnitude\nshould assume a local maximum in the gradient direction\nBy working out the differential geometry, it can be shown that this differential edge detector can equivalently be expressed from the zero-crossings of the second-order differential invariant\n\nthat satisfy the following sign condition on a third-order differential invariant:\n\nSimilarly, multi-scale blob detectors at any given fixed scale can be obtained from local maxima and local minima of either the Laplacian operator (also referred to as the Laplacian of Gaussian)\n\nor the determinant of the Hessian matrix\nIn an analogous fashion, corner detectors and ridge and valley detectors can be expressed as local maxima, minima or zero-crossings of multi-scale differential invariants defined from Gaussian derivatives. The algebraic expressions for the corner and ridge detection operators are, however, somewhat more complex and the reader is referred to the articles on corner detection and ridge detection for further details.\n\nScale-space operations have also been frequently used for expressing coarse-to-fine methods, in particular for tasks such as image matching and for multi-scale image segmentation.\n\nThe theory presented so far describes a well-founded framework for \"representing\" image structures at multiple scales. In many cases it is, however, also necessary to select locally appropriate scales for further analysis. This need for \"scale selection\" originates from two major reasons; (i) real-world objects may have different size, and this size may be unknown to the vision system, and (ii) the distance between the object and the camera can vary, and this distance information may also be unknown \"a priori\".\nA highly useful property of scale-space representation is that image representations can be made invariant to scales, by performing automatic local scale selection based on local maxima (or minima) over scales of normalized derivatives\nwhere formula_36 is a parameter that is related to the dimensionality of the image feature. This algebraic expression for \"scale normalized Gaussian derivative operators\" originates from the introduction of \"formula_37-normalized derivatives\" according to\nIt can be theoretically shown that a scale selection module working according to this principle will satisfy the following \"scale covariance property\": if for a certain type of image feature a local maximum is assumed in a certain image at a certain scale formula_40, then under a rescaling of the image by a scale factor formula_41 the local maximum over scales in the rescaled image will be transformed to the scale level formula_42.\n\nFollowing this approach of gamma-normalized derivatives, it can be shown that different types of \"scale adaptive and scale invariant feature detectors\" can be expressed for tasks such as blob detection, corner detection, ridge detection, edge detection and spatio-temporal interest points (see the specific articles on these topics for in-depth descriptions of how these scale-invariant feature detectors are formulated).\nFurthermore, the scale levels obtained from automatic scale selection can be used for determining regions of interest for subsequent affine shape adaptation to obtain affine invariant interest points or for determining scale levels for computing associated image descriptors, such as locally scale adapted N-jets.\n\nRecent work has shown that also more complex operations, such as scale-invariant object recognition can be performed in this way,\nby computing local image descriptors (N-jets or local histograms of gradient directions) at scale-adapted interest points obtained from scale-space extrema of the normalized Laplacian operator (see also scale-invariant feature transform) or the determinant of the Hessian (see also SURF); see also the Scholarpedia article on the scale-invariant feature transform for a more general outlook of object recognition approaches based on receptive field responses in terms Gaussian derivative operators or approximations thereof.\n\nAn image pyramid is a discrete representation in which a scale space is sampled in both space and scale. For scale invariance, the scale factors should be sampled exponentially, for example as integer powers of 2 or root 2. When properly constructed, the ratio of the sample rates in space and scale are held constant so that the impulse response is identical in all levels of the pyramid. \nFast, O(N), algorithms exist for computing a scale invariant image pyramid in which the image or signal is repeatedly smoothed then subsampled. \nValues for scale space between pyramid samples can easily be estimated using interpolation within and between scales and allowing for scale and position estimates with sub resolution accuracy.\n\nIn a scale-space representation, the existence of a continuous scale parameter makes it possible to track zero crossings over scales leading to so-called \"deep structure\".\nFor features defined as zero-crossings of differential invariants, the implicit function theorem directly defines trajectories across scales, and at those scales where bifurcations occur, the local behaviour can be modelled by singularity theory.\n\nExtensions of linear scale-space theory concern the formulation of non-linear scale-space concepts more committed to specific purposes. These \"non-linear scale-spaces\" often start from the equivalent diffusion formulation of the scale-space concept, which is subsequently extended in a non-linear fashion. A large number of evolution equations have been formulated in this way, motivated by different specific requirements (see the abovementioned book references for further information). It should be noted, however, that not all of these non-linear scale-spaces satisfy similar \"nice\" theoretical requirements as the linear Gaussian scale-space concept. Hence, unexpected artifacts may sometimes occur and one should be very careful of not using the term \"scale-space\" for just any type of one-parameter family of images.\n\nA first-order extension of the isotropic Gaussian scale space is provided by the \"affine (Gaussian) scale space\". One motivation for this extension originates from the common need for computing image descriptors subject for real-world objects that are viewed under a perspective camera model. To handle such non-linear deformations locally, partial invariance (or more correctly covariance) to local affine deformations can be achieved by considering affine Gaussian kernels with their shapes determined by the local image structure, see the article on affine shape adaptation for theory and algorithms. Indeed, this affine scale space can also be expressed from a non-isotropic extension of the linear (isotropic) diffusion equation, while still being within the class of linear partial differential equations.\n\nThere exists a more general extension of the Gaussian scale-space model to affine and spatio-temporal scale-spaces. In addition to variabilities over scale, which original scale-space theory was designed to handle, this \"generalized scale-space theory\" also comprises other types of variabilities caused by geometric transformations in the image formation process, including variations in viewing direction approximated by local affine transformations, and relative motions between objects in the world and the observer, approximated by local Galilean transformations. This generalized scale-space theory leads to predictions about receptive field profiles in good qualitative agreement with receptive field profiles measured by cell recordings in biological vision.\n\nThere are strong relations between scale-space theory and wavelet theory, although these two notions of multi-scale representation have been developed from somewhat different premises.\nThere has also been work on other multi-scale approaches, such as pyramids and a variety of other kernels, that do not exploit or require the same requirements as true scale-space descriptions do.\n\nThere are interesting relations between scale-space representation and biological vision and hearing.\nNeurophysiological studies of biological vision have shown that there are receptive field profiles in the mammalian retina and visual cortex,\nwhich can be well modelled by linear Gaussian derivative operators, in some cases also complemented by a non-isotropic affine scale-space model, a spatio-temporal scale-space model and/or non-linear combinations of such linear operators.\nRegarding biological hearing there are receptive field profiles in the inferior colliculus and the primary auditory cortex that can be well modelled by spectra-temporal receptive fields that can be well modelled by Gaussian derivates over logarithmic frequencies and windowed Fourier transforms over time with the window functions being temporal scale-space kernels.\n\nNormative theories for visual and auditory receptive fields founded on the scale-space framework are described in the article on axiomatic theory of receptive fields.\n\nWhen implementing scale-space smoothing in practice there are a number of different approaches that can be taken in terms of continuous or discrete Gaussian smoothing, implementation in the Fourier domain, in terms of pyramids based on binomial filters that approximate the Gaussian or using recursive filters. More details about this are given in a separate article on scale space implementation.\n\n", "related": "\n- Gaussian function\n- mipmapping\n\n- Lindeberg, Tony, \"Scale-space: A framework for handling image structures at multiple scales\", In: Proc. CERN School of Computing, Egmond aan Zee, The Netherlands, 8-21 September, 1996 (online web tutorial)\n- Lindeberg, Tony: Scale-space theory: A basic tool for analysing structures at different scales, in J. of Applied Statistics, 21(2), pp. 224–270, 1994 (longer pdf tutorial on scale-space)\n- Lindeberg, Tony, \"Principles for automatic scale selection\", In: B. Jähne (et al., eds.), Handbook on Computer Vision and Applications, volume 2, pp 239—274, Academic Press, Boston, USA, 1999. (tutorial on approaches to automatic scale selection)\n- Lindeberg, Tony: \"Scale-space theory\" In: Encyclopedia of Mathematics, (Michiel Hazewinkel, ed) Kluwer, 1997\n- Powers of ten interactive Java tutorial at Molecular Expressions website\n- On-line resource with space-time receptive fields of visual neurons provided by Izumi Ohzawa at Osaka University\n- Web archive backup: Lecture on scale-space at the University of Massachusetts (pdf)\n- Multiscale analysis for optimized vessel segmentation of fundus retina images PhD Thesis\n- Peak detection in 1D data using a scale-space approach BSD-licensed MATLAB code\n"}
{"id": "37412518", "url": "https://en.wikipedia.org/wiki?curid=37412518", "title": "Local ternary patterns", "text": "Local ternary patterns\n\nLocal ternary patterns (LTP) are an extension of Local binary patterns (LBP). Unlike LBP, it does not threshold the pixels into 0 and 1, rather it uses a threshold constant to threshold pixels into three values. Considering k as the threshold constant, c as the value of the center pixel, a neighboring pixel p, the result of threshold is:\nformula_1\n\nIn this way, each thresholded pixel has one of the three values. Neighboring pixels are combined after thresholding into a ternary pattern. Computing a histogram of these ternary values will result in a large range, so the ternary pattern is split into two binary patterns. Histograms are concatenated to generate a descriptor double the size of LBP.\n\n", "related": "\n- Local binary patterns\n"}
{"id": "16373249", "url": "https://en.wikipedia.org/wiki?curid=16373249", "title": "Structured-light 3D scanner", "text": "Structured-light 3D scanner\n\nA structured-light 3D scanner is a 3D scanning device for measuring the three-dimensional shape of an object using projected light patterns and a camera system.\n\nProjecting a narrow band of light onto a three-dimensionally shaped surface produces a line of illumination that appears distorted from other perspectives than that of the projector, and can be used for geometric reconstruction of the surface shape (light section).\n\nA faster and more versatile method is the projection of patterns consisting of many stripes at once, or of arbitrary fringes, as this allows for the acquisition of a multitude of samples simultaneously.\nSeen from different viewpoints, the pattern appears geometrically distorted due to the surface shape of the object.\n\nAlthough many other variants of structured light projection are possible, patterns of parallel stripes are widely used. The picture shows the geometrical deformation of a single stripe projected onto a simple 3D surface. The displacement of the stripes allows for an exact retrieval of the 3D coordinates of any details on the object's surface.\n\nTwo major methods of stripe pattern generation have been established: Laser interference and projection.\n\nThe laser \"interference method\" works with two wide planar laser beam fronts. Their interference results in regular, equidistant line patterns. Different pattern sizes can be obtained by changing the angle between these beams. The method allows for the exact and easy generation of very fine patterns with unlimited depth of field. Disadvantages are high cost of implementation, difficulties providing the ideal beam geometry, and laser typical effects like speckle noise and the possible self interference with beam parts reflected from objects. Typically, there is no means of modulating individual stripes, such as with Gray codes.\n\nThe \"projection method\" uses incoherent light and basically works like a video projector. Patterns are usually generated by passing light through a digital spatial light modulator, typically based on one of the three currently most widespread digital projection technologies, transmissive liquid crystal, reflective liquid crystal on silicon (LCOS) or digital light processing (DLP; moving micro mirror) modulators, which have various comparative advantages and disadvantages for this application. Other methods of projection could be and have been used, however.\n\nPatterns generated by digital display projectors have small discontinuities due to the pixel boundaries in the displays. Sufficiently small boundaries however can practically be neglected as they are evened out by the slightest defocus.\n\nA typical measuring assembly consists of one projector and at least one camera. For many applications, two cameras on opposite sides of the projector have been established as useful.\n\n\"Invisible\" (or \"imperceptible\") structured light uses structured light without interfering with other computer vision tasks for which the projected pattern will be confusing. Example methods include the use of infrared light or of extremely high framerates alternating between two exact opposite patterns.\n\nGeometric distortions by optics and perspective must be compensated by a calibration of the measuring equipment, using special calibration patterns and surfaces. A mathematical model is used for describing the imaging properties of projector and cameras. Essentially based on the simple geometric properties of a pinhole camera, the model also has to take into account the geometric distortions and optical aberration of projector and camera lenses. The parameters of the camera as well as its orientation in space can be determined by a series of calibration measurements, using photogrammetric bundle adjustment.\n\nThere are several depth cues contained in the observed stripe patterns. The displacement of any single stripe can directly be converted into 3D coordinates. For this purpose, the individual stripe has to be identified, which can for example be accomplished by tracing or counting stripes (pattern recognition method). Another common method projects alternating stripe patterns, resulting in binary Gray code sequences identifying the number of each individual stripe hitting the object.\nAn important depth cue also results from the varying stripe widths along the object surface. Stripe width is a function of the steepness of a surface part, i.e. the first derivative of the elevation. Stripe frequency and phase deliver similar cues and can be analyzed by a Fourier transform. Finally, the wavelet transform has recently been discussed for the same purpose.\n\nIn many practical implementations, series of measurements combining pattern recognition, Gray codes and Fourier transform are obtained for a complete and unambiguous reconstruction of shapes.\n\nAnother method also belonging to the area of fringe projection has been demonstrated, utilizing the depth of field of the camera.\n\nIt is also possible to use projected patterns primarily as a means of structure insertion into scenes, for an essentially photogrammetric acquisition.\n\nThe optical resolution of fringe projection methods depends on the width of the stripes used and their optical quality. It is also limited by the wavelength of light.\n\nAn extreme reduction of stripe width proves inefficient due to limitations in depth of field, camera resolution and display resolution. Therefore, the phase shift method has been widely established: A number of at least 3, typically about 10 exposures are taken with slightly shifted stripes. The first theoretical deductions of this method relied on stripes with a sine wave shaped intensity modulation, but the methods work with \"rectangular\" modulated stripes, as delivered from LCD or DLP displays as well. By phase shifting, surface detail of e.g. 1/10 the stripe pitch can be resolved.\n\nCurrent optical stripe pattern profilometry hence allows for detail resolutions down to the wavelength of light, below 1 micrometer in practice or, with larger stripe patterns, to approx. 1/10 of the stripe width. Concerning level accuracy, interpolating over several pixels of the acquired camera image can yield a reliable height resolution and also accuracy, down to 1/50 pixel.\n\nArbitrarily large objects can be measured with accordingly large stripe patterns and setups. Practical applications are documented involving objects several meters in size.\n\nTypical accuracy figures are:\n- Planarity of a wide surface, to .\n- Shape of a motor combustion chamber to (elevation), yielding a volume accuracy 10 times better than with volumetric dosing.\n- Shape of an object large, to about\n- Radius of a blade edge of e.g. , to ±0.4 μm\n\nAs the method can measure shapes from only one perspective at a time, complete 3D shapes have to be combined from different measurements in different angles. This can be accomplished by attaching marker points to the object and combining perspectives afterwards by matching these markers. The process can be automated, by mounting the object on a motorized turntable or CNC positioning device. Markers can as well be applied on a positioning device instead of the object itself.\n\nThe 3D data gathered can be used to retrieve CAD (computer aided design) data and models from existing components (reverse engineering), hand formed samples or sculptures, natural objects or artifacts.\n\nAs with all optical methods, reflective or transparent surfaces raise difficulties. Reflections cause light to be reflected either away from the camera or right into its optics. In both cases, the dynamic range of the camera can be exceeded. Transparent or semi-transparent surfaces also cause major difficulties. In these cases, coating the surfaces with a thin opaque lacquer just for measuring purposes is a common practice. A recent method handles highly reflective and specular objects by inserting a 1-dimensional diffuser between the light source (e.g., projector) and the object to be scanned. Alternative optical techniques have been proposed for handling perfectly transparent and specular objects.\n\nDouble reflections and inter-reflections can cause the stripe pattern to be overlaid with unwanted light, entirely eliminating the chance for proper detection. Reflective cavities and concave objects are therefore difficult to handle. It is also hard to handle translucent materials, such as skin, marble, wax, plants and human tissue because of the phenomenon of sub-surface scattering. Recently, there has been an effort in the computer vision community to handle such optically complex scenes by re-designing the illumination patterns. These methods have shown promising 3D scanning results for traditionally difficult objects, such as highly specular metal concavities and translucent wax candles.\n\nAlthough several patterns have to be taken per picture in most structured light variants, high-speed implementations are available for a number of applications, for example:\n- Inline precision inspection of components during the production process.\n- Health care applications, such as live measuring of human body shapes or the micro structures of human skin.\nMotion picture applications have been proposed, for example the acquisition of spatial scene data for three-dimensional television.\n\n- Industrial Optical Metrology Systems (ATOS) from GOM GmbH utilize Structured Light technology to achieve high accuracy and scalability in measurements. These systems feature self-monitoring for calibration status, transformation accuracy, environmental changes, and part movement to ensure high-quality measuring data.\n- Google Project Tango SLAM (Simultaneous localization and mapping) using depth technologies, including Structured Light, Time of Flight, and Stereo. Time of Flight require the use of an infrared (IR) projector and IR sensor; Stereo does not.\n- A technology by PrimeSense, used in an early version of Microsoft Kinect, used a pattern of projected infrared points to generate a dense 3D image. (Later on, the Microsoft Kinect switched to using a time-of-flight camera instead of structured light.)\n- Occipital\n- Structure Sensor uses a pattern of projected infrared points, calibrated to minimize distortion to generate a dense 3D image.\n- Structure Core uses a stereo camera that matches against a random pattern of projected infrared points to generate a dense 3D image.\n- Intel RealSense camera projects a series of infrared patterns to obtain the 3D structure.\n- Face ID system works by projecting more than 30,000 infrared dots onto a face and producing a 3D facial map.\n- VicoVR sensor uses a pattern of infrared points for skeletal tracking.\n- Chiaro Technologies uses a single engineered pattern of infrared points called Symbolic Light to stream 3D point clouds for industrial applications\n- Made to measure fashion retailing\n- 3D-Automated Optical Inspection\n- Precision shape measurement for production control (e.g. turbine blades)\n- Reverse engineering (obtaining precision CAD data from existing objects)\n- Volume measurement (e.g. combustion chamber volume in motors)\n- Classification of grinding materials and tools\n- Precision structure measurement of ground surfaces\n- Radius determination of cutting tool blades\n- Precision measurement of planarity\n- Documenting objects of cultural heritage\n- Capturing environments for augmented reality gaming\n- Skin surface measurement for cosmetics and medicine\n- Body shape measurement\n- Forensic science inspections\n- Road pavement structure and roughness\n- Wrinkle measurement on cloth and leather\n- Structured Illumination Microscopy\n- Measurement of topography of solar cells\n- GHOST HUNTING\n- 3D vision system enables DHL’s e-fulfillment robot\n\n- 3DUNDERWORLD SLS - OPEN SOURCE\n- DIY 3D scanner based on structured light and stereo vision in Python language\n- SLStudio—Open Source Real Time Structured Light\n\n", "related": "\n- Depth map\n- Laser Dynamic Range Imager\n- Lidar\n- Range imaging\n- Kinect\n- Time-of-flight camera\n- Light stage is an instrumentation setup for primarily for reflectance capture but it is also applied in virtual cinematography to acquire geometries and textures of targets in a similar manner as a structured-light 3D scanner.\n\n- Fechteler, P., Eisert, P., Rurainsky, J.: Fast and High Resolution 3D Face Scanning Proc. of ICIP 2007\n- Fechteler, P., Eisert, P.: Adaptive Color Classification for Structured Light Systems Proc. of CVPR 2008\n- Kai Liu, Yongchang Wang, Daniel L. Lau, Qi Hao, Laurence G. Hassebrook: Gamma Model and its Analysis for Phase Measuring Profilometry. J. Opt. Soc. Am. A, 27: 553-562, 2010\n- Yongchang Wang, Kai Liu, Daniel L. Lau, Qi Hao, Laurence G. Hassebrook: Maximum SNR Pattern Strategy for Phase Shifting Methods in Structured Light Illumination, J. Opt. Soc. Am. A, 27(9), pp. 1962–1971, 2010\n- Hof, C., Hopermann, H.: Comparison of Replica- and In Vivo-Measurement of the Microtopography of Human Skin University of the Federal Armed Forces, Hamburg\n- Frankowski, G., Chen, M., Huth, T.: Real-time 3D Shape Measurement with Digital Stripe Projection by Texas Instruments Micromirror Devices (DMD) Proc. SPIE-Vol. 3958(2000), pp. 90–106\n- Frankowski, G., Chen, M., Huth, T.: Optical Measurement of the 3D-Coordinates and the Combustion Chamber Volume of Engine Cylinder Heads Proc. Of \"Fringe 2001\", pp. 593–598\n- C. Je, S. W. Lee, and R.-H. Park Colour-Stripe Permutation Pattern for Rapid Structured-Light Range Imaging. Optics Communications, Volume 285, Issue 9, pp. 2320-2331, May 1, 2012.\n- C. Je, S. W. Lee, and R.-H. Park. High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range Imaging. Computer Vision – ECCV 2004, LNCS 3021, pp. 95–107, Springer-Verlag Berlin Heidelberg, May 10, 2004.\n- Elena Stoykova, Jana Harizanova, Venteslav Sainov: Pattern Projection Profilometry for 3D Coordinates Measurement of Dynamic Scenes. In: Three Dimensional Television, Springer, 2008,\n- Song Zhang, Peisen Huang: High-resolution, Real-time 3-D Shape Measurement (PhD Dissertation, Stony Brook Univ., 2005)\n- Tao Peng: Algorithms and models for 3-D shape measurement using digital fringe projections (Ph.D. Dissertation, University of Maryland, USA. 2007)\n- W. Wilke: Segmentierung und Approximation großer Punktwolken (Dissertation Univ. Darmstadt, 2000)\n- G. Wiora: Optische 3D-Messtechnik Präzise Gestaltvermessung mit einem erweiterten Streifenprojektionsverfahren (Dissertation Univ. Heidelberg, 2001)\n- Klaus Körner, Ulrich Droste: Tiefenscannende Streifenprojektion (DSFP) University of Stuttgart (further English references on the site)\n- R. Morano, C. Ozturk, R. Conn, S. Dubin, S. Zietz, J. Nissano, \"Structured light using pseudorandom codes\", IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (3)(1998)322–327\n- Fringe 2005, The 5th International Workshop on Automatic Processing of Fringe Patterns Berlin: Springer, 2006.\n"}
{"id": "39799223", "url": "https://en.wikipedia.org/wiki?curid=39799223", "title": "PatchMatch", "text": "PatchMatch\n\nThe core PatchMatch algorithm quickly finds correspondences between small square regions (or patches) of an image. The algorithm can be used in various applications such as object removal from images, reshuffling or moving contents of images, or retargeting or changing aspect ratios of images, optical flow estimation, or stereo correspondence.\n\nThe goal of the algorithm is to find the patch correspondence by defining a nearest-neighbor field (NNF) as a function formula_1 of offsets, which is over all possible matches of patch (location of patch centers) in image A, for some distance function of two patches formula_2. So, for a given patch coordinate formula_3 in image formula_4 and its corresponding nearest neighbor formula_5 in image formula_6, formula_7 is simply formula_8. However, if we search for every point in image formula_6, the work will be too hard to complete. So the following algorithm is done in a randomized approach in order to accelerate the calculation speed. \nThe algorithm has three main components. Initially, the nearest-neighbor field is filled with either random offsets or some prior information. Next, an iterative update process is applied to the NNF, in which good patch offsets are propagated to adjacent pixels, followed by random search in the neighborhood of the best offset found so far. Independent of these three components, the algorithm also use a coarse-to-fine approach by building an image pyramid to obtain the better result.\n\nWhen initializing with random offsets, we use independent uniform samples across the full range of image formula_6. This algorithm avoids using an initial guess from the previous level of the pyramid because in this way the algorithm can avoid being trapped in local minima.\n\nAfter initialization, the algorithm attempted to perform iterative process of improving the formula_11. The iterations examine the offsets in scan order (from left to right, top to bottom), and each undergoes propagation followed by random search.\n\nWe attempt to improve formula_12 using the known offsets of formula_13 and formula_14, assuming that the patch offsets\nare likely to be the same. That is, the algorithm will take new value for formula_12 to be formula_16. \nSo if formula_17 has a correct mapping and is in a coherent region formula_18, then all of formula_18 below and to the right of formula_17 will be filled with the correct mapping. Alternatively, on even iterations, the algorithm search for different direction, fill the new value to be formula_21.\n\nLet formula_22, we attempt to improveformula_17 by testing a sequence of candidate offsets at an exponentially decreasing distance from formula_24\n\nwhere formula_26 is a uniform random in formula_27, formula_28 is a large window search radius which will be set to maximum picture size, and formula_29 is a fixed ratio often assigned as 1/2. This part of the algorithm allows the formula_12 to jump out of local minimum through random process.\n\nThe often used halting criteria is set the iteration times to be about 4~5. Even with low iteration, the algorithm works well.\n\nThis is an efficient algorithm since it only takes a few second on a testing computer with Intel Core i5 CPU and Photoshop CS5.\n\n", "related": "\n- Nearest neighbor search\n\n- Connelly Barnes, Eli Shechtman, Adam Finkelstein, Dan B Goldman(2009), PatchMatch: A Randomized Correspondence Algorithm for Structural Image Editing\n"}
{"id": "39922027", "url": "https://en.wikipedia.org/wiki?curid=39922027", "title": "Cyclodisparity", "text": "Cyclodisparity\n\nCyclodisparity refers to the difference in the rotation angle of an object or scene viewed by the left and right eyes. Cyclodisparity can result from the eyes' torsional rotation (\"cyclorotation\") or can be created artificially by presenting to the eyes two images that need to be rotated relative to each other for binocular fusion to take place.\n\nThe eyes and visual system can compensate for cyclodisparity up to a certain point; if the cyclodisparity is larger than a threshold, the images cannot be fused, resulting stereoblindness, and in double vision in subjects who otherwise have full stereo vision.\n\nWhen a human subject is presented with images that have artificial cyclodisparity, cyclovergence is evoked, that is, a motor response of the eye muscles that rotates the two eyes in opposite directions, thereby reducing cyclodisparity. Visually-induced cyclovergence of up to 8 degrees has been observed in normal subjects. Furthermore, up to about 8 degrees that can usually be compensated by purely sensory means, that is, without physical eye rotation. This means that the normal human observer can achieve binocular image fusion in presence of cyclodisparity of up to approximately 16 degrees.\n\nCyclodisparity due to images having been rotated inward can be compensated better when the gaze is directed downwards, and cyclodisparity due to an outward rotation can be compensated better when the gaze is directed upwards. A proposed explanation for this phenomenon is that the motor system is coordinated in such a way that the eyes perform a torsional movement to reduce the size of the search zones and thus the computational load required for solving the correspondence problem. The resulting cyclovergence at near gaze is smaller than the cyclovergence predicted by Listing's law.\n\nActive camera torsion can be used in machine and computer vision for several purposes. For instance, camera torsion can be used to make improved use of the search range over which matching detectors or stereo matching algorithms operate, or to make a 3D slanted surface appear frontoparallel for further stereo processing.\n\nFor image compression purposes, images with cyclodisparity are advantageously encoded using global motion compensation using a rotational motion model.\n", "related": "NONE"}
{"id": "5481404", "url": "https://en.wikipedia.org/wiki?curid=5481404", "title": "Scale-space axioms", "text": "Scale-space axioms\n\nIn image processing and computer vision, a scale space framework can be used to represent an image as a family of gradually smoothed images. This framework is very general and a variety of scale space representations exist. A typical approach for choosing a particular type of scale space representation is to establish a set of scale-space axioms, describing basic properties of the desired scale-space representation and often chosen so as to make the representation useful in practical applications. Once established, the axioms narrow the possible scale-space representations to a smaller class, typically with only a few free parameters.\n\nA set of standard scale space axioms, discussed below, leads to the linear Gaussian scale-space, which is the most common type of scale space used in image processing and computer vision.\n\nThe linear scale space representation formula_1 of signal formula_2 obtained by smoothing with the Gaussian kernel formula_3 satisfies a number of properties 'scale-space axioms' that make it a special form of multi-scale representation:\n\n- \"linearity\"\nwhere formula_5 and formula_6 are signals while formula_7 and formula_8 are constants,\n- \"shift invariance\"\nwhere formula_10 denotes the shift (translation) operator formula_11\n- the \"semi-group structure\nwith the associated \"cascade smoothing property\"\n- existence of an \"infinitesimal generator\" formula_14\n- \"non-creation of local extrema\" (zero-crossings) in one dimension,\n- \"non-enhancement of local extrema\" in any number of dimensions\n- \"rotational symmetry\"\n- \"scale invariance\nfor some functions formula_21 and formula_22 where formula_23 denotes the Fourier transform of formula_24,\n- \"positivity\":\n- \"normalization\":\n\nIn fact, it can be shown that the Gaussian kernel is a \"unique choice\" given several different combinations of subsets of these scale-space axioms:\nmost of the axioms (linearity, shift-invariance, semigroup) correspond to scaling being a semigroup of shift-invariant linear operator, which is satisfied by a number of families integral transforms, while \"non-creation of local extrema\" for one-dimensional signals or \"non-enhancement of local extrema\" for higher-dimensional signals are the crucial axioms which relate scale-spaces to smoothing (formally, parabolic partial differential equations), and hence select for the Gaussian.\n\nThe Gaussian kernel is also separable in Cartesian coordinates, i.e. formula_27. Separability is, however, not counted as a scale-space axiom, since it is a coordinate dependent property related to issues of implementation. In addition, the requirement of separability in combination with rotational symmetry per se fixates the smoothing kernel to be a Gaussian.\n\nThere exists a generalization of the Gaussian scale-space theory to more general affine and spatio-temporal scale-spaces. In addition to variabilities over scale, which original scale-space theory was designed to handle, this \"generalized scale-space theory\" also comprises other types of variabilities, including image deformations caused by viewing variations, approximated by local affine transformations, and relative motions between objects in the world and the observer, approximated by local Galilean transformations. In this theory, rotational symmetry is not imposed as a necessary scale-space axiom and is instead replaced by requirements of affine and/or Galilean covariance. The generalized scale-space theory leads to predictions about receptive field profiles in good qualitative agreement with receptive field profiles measured by cell recordings in biological vision.\n\nIn the computer vision, image processing and signal processing literature there are many other multi-scale approaches, using wavelets and a variety of other kernels, that do not exploit or require the same requirements as scale space descriptions do; please see the article on related multi-scale approaches. There has also been work on discrete scale-space concepts that carry the scale-space properties over to the discrete domain; see the article on scale space implementation for examples and references.\n\n", "related": "\n- Scale space\n- Scale space implementation\n"}
{"id": "40374554", "url": "https://en.wikipedia.org/wiki?curid=40374554", "title": "Point set registration", "text": "Point set registration\n\nIn computer vision, pattern recognition, and robotics, point set registration, also known as point cloud registration or scan matching, is the process of finding a spatial transformation (\"e.g.,\" scaling, rotation and translation) that aligns two point clouds. The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model (or coordinate frame), and mapping a new measurement to a known data set to identify features or to estimate its pose. Raw 3D point cloud data are typically obtained from Lidars and RGB-D cameras. 3D point clouds can also be generated from computer vision algorithms such as triangulation, bundle adjustment, and more recently, monocular image depth estimation using deep learning. For 2D point set registration used in image processing and feature-based image registration, a point set may be 2D pixel coordinates obtained by feature extraction from an image, for example corner detection. Point cloud registration has extensive applications in autonomous driving, motion estimation and 3D reconstruction, object detection and pose estimation, robotic manipulation, simultaneous localization and mapping (SLAM), panorama stitching, virtual and augmented reality, and medical imaging.\n\nAs a special case, registration of two point sets that only differ by a 3D rotation (\"i.e.,\" there is no scaling and translation), is called the Wahba Problem and also related to the orthogonal procrustes problem.\n\nThe problem may be summarized as follows:\nLet formula_1 be two finite size point sets in a finite-dimensional real vector space formula_2, which contain formula_3 and formula_4 points respectively (\"e.g.,\" formula_5 recovers the typical case of when formula_6 and formula_7 are 3D point sets). The problem is to find a transformation to be applied to the moving \"model\" point set formula_6 such that the difference (typically defined in the sense of point-wise Euclidean distance) between formula_6 and the static \"scene\" set formula_7 is minimized. In other words, a mapping from formula_2 to formula_2 is desired which yields the best alignment between the transformed \"model\" set and the \"scene\" set. The mapping may consist of a rigid or non-rigid transformation. The transformation model may be written as formula_13, using which the transformed, registered model point set is:\n\nThe output of a point set registration algorithm is therefore the optimal transformation formula_14 such that formula_6 is best aligned to formula_7, according to some defined notion of distance function formula_17:\n\nwhere formula_18 is used to denote the set of all possible transformations that the optimization tries to search for. The most popular choice of the distance function is to take the square of the Euclidean distance for every pair of points:\n\nwhere formula_19 denotes the vector 2-norm, formula_20 is the corresponding point in set formula_7 that attains the shortest distance to a given point formula_22 in set formula_6. Minimizing such a function in rigid registration is equivalent to solving a least squares problem. When the correspondences (\"i.e.,\" formula_24) are given before the optimization, for example, using feature matching techniques, then the optimization only needs to estimate the transformation. This type of registration is called correspondence-based registration. On the other hand, if the correspondences are unknown, then the optimization is required to jointly find out the correspondences and transformation together. This type of registration is called Simultaneous Pose and Correspondence registration.\n\nGiven two point sets, rigid registration yields a rigid transformation which maps one point set to the other. A rigid transformation is defined as a transformation that does not change the distance between any two points. Typically such a transformation consists of translation and rotation. In rare cases, the point set may also be mirrored. In robotics and computer vision, rigid registration has the most applications.\n\nGiven two point sets, non-rigid registration yields a non-rigid transformation which maps one point set to the other. Non-rigid transformations include affine transformations such as scaling and shear mapping. However, in the context of point set registration, non-rigid registration typically involves nonlinear transformation. If the eigenmodes of variation of the point set are known, the nonlinear transformation may be parametrized by the eigenvalues. A nonlinear transformation may also be parametrized as a thin plate spline.\n\nSome approaches to point set registration use algorithms that solve the more general graph matching problem. However, the computational complexity of such methods tend to be high and they are limited to rigid registrations. Algorithms specific to the point set registration problem are described in the following sections.\nThe PCL (Point Cloud Library) is an open-source framework for n-dimensional point cloud and 3D geometry processing. It includes several point registration algorithms.\n\nIn this section, we will only consider algorithms for rigid registration, where the transformation is assumed to contain 3D rotations and translations (possibly also including a uniform scaling).\n\nCorrespondence-based methods assume the putative correspondences formula_25 are given for every point formula_26. Therefore, we arrive at a setting where both point sets formula_6 and formula_7 have formula_4 points and the correspondences formula_30are given.\n\nIn the simplest case, one can assume that all the correspondences are correct, meaning that the points formula_31 are generated as follows:where formula_32 is a uniform scaling factor (in many cases formula_33 is assumed), formula_34 is a proper 3D rotation matrix (formula_35 is the special orthogonal group of degree formula_36), formula_37 is a 3D translation vector and formula_38 models the unknown additive noise (\"e.g.,\" Gaussian noise). Specifically, if the noise formula_39 is assumed to follow a zero-mean isotropic Gaussian distribution with standard deviation formula_40, \"i.e.,\" formula_41, then the following optimization can be shown to yield the maximum likelihood estimate for the unknown scale, rotation and translation:Note that when the scaling factor is 1 and the translation vector is zero, then the optimization recovers the formulation of the Wahba problem. Despite the non-convexity of the optimization () due to non-convexity of the set formula_42, seminal work by Berthold K.P. Horn showed that () actually admits a closed-form solution, by decoupling the estimation of scale, rotation and translation. Similar results were discovered by Arun \"et al\". In addition, in order to find a unique transformation formula_43, at least formula_44 non-collinear points in each point set are required.\n\nMore recently, Briales and Gonzalez-Jimenez have developed a semidefinite relaxation using Lagrangian duality, for the case where the model set formula_6 contains different 3D primitives such as points, lines and planes (which is the case when the model formula_6 is a 3D mesh). Interestingly, the semidefinite relaxation is empirically tight, \"i.e.,\" a certifiably globally optimal solution can be extracted from the solution of the semidefinite relaxation.\n\nThe least squares formulation () is known to perform arbitrarily bad in the presence of outliers. An outlier correspondence is a pair of measurements formula_47 that departs from the generative model (). In this case, one can consider a different generative model as follows:where if the formula_48th pair formula_47 is an inlier, then it obeys the outlier-free model (), \"i.e.,\" formula_50 is obtained from formula_51 by a spatial transformation plus some small noise; however, if the formula_48th pair formula_47 is an outlier, then formula_50 can be any arbitrary vector formula_55. Since one does not know which correspondences are outliers beforehand, robust registration under the generative model () is of paramount importance for computer vision and robotics deployed in the real world, because current feature matching techniques tend to output highly corrupted correspondences where over formula_56 of the correspondences can be outliers.\n\nNext, we describe several common paradigms for robust registration.\n\nMaximum consensus seeks to find the largest set of correspondences that are consistent with the generative model () for some choice of spatial transformation formula_43. Formally speaking, maximum consensus solves the following optimization:where formula_58 denotes the cardinality of the set formula_59. The constraint in () enforces that every pair of measurements in the inlier set formula_59 must have residuals smaller than a pre-defined threshold formula_61. Unfortunately, recent analyses have shown that globally solving problem (cb.4) is NP-Hard, and global algorithms typically have to resort to branch-and-bound (BnB) techniques that take exponential-time complexity in the worst case.\n\nAlthough solving consensus maximization exactly is hard, there exist efficient heuristics that perform quite well in practice. One of the most popular heuristics is the Random Sample Consensus (RANSAC) scheme. RANSAC is an iterative hypothesize-and-verity method. At each iteration, the method first randomly samples 3 out of the total number of formula_4 correspondences and computes a hypothesis formula_43 using Horn's method, then the method evaluates the constraints in () to count how many correspondences actually agree with such a hypothesis (i.e., it computes the residual formula_64 and compares it with the threshold formula_61 for each pair of measurements). The algorithm terminates either after it has found a consensus set that has enough correspondences, or after it has reached the total number of allowed iterations. RANSAC is highly efficient because the main computation of each iteration is carrying out the closed-form solution in Horn's method. However, RANSAC is non-deterministic and only works well in the low-outlier-ratio regime (\"e.g.,\" below formula_66), because its runtime grows exponentially with respect to the outlier ratio.\n\nTo fill the gap between the fast but inexact RANSAC scheme and the exact but exhaustive BnB optimization, recent researches have developed deterministic approximate methods to solve consensus maximization.\n\nOutlier removal methods seek to pre-process the set of highly corrupted correspondences before estimating the spatial transformation. The motivation of outlier removal is to significantly reduce the number of outlier correspondences, while maintaining inlier correspondences, so that optimization over the transformation becomes easier and more efficient (\"e.g.,\" RANSAC works poorly when the outlier ratio is above formula_56 but performs quite well when outlier ratio is below formula_66).\n\nParra \"et al.\" have proposed a method called Guaranteed Outlier Removal (GORE) that uses geometric constraints to prune outlier correspondences while guaranteeing to preserve inlier correspondences. GORE has been shown to be able to drastically reduce the outlier ratio, which can significantly boost the performance of consensus maximization using RANSAC or BnB. Yang and Carlone have proposed to build pairwise translation-and-rotation-invariant measurements (TRIMs) from the original set of measurements and embed TRIMs as the edges of a graph whose nodes are the 3D points. Since inliers are pairwise consistent in terms of the scale, they must form a clique within the graph. Therefore, using efficient algorithms for computing the maximum clique of a graph can find the inliers and effectively prune the outliers. The maximum clique based outlier removal method is also shown to be quite useful in real-world point set registration problems. Similar outlier removal ideas were also proposed by Parra \"et al.\".\n\nM-estimation replaces the least squares objective function in () with a robust cost function that is less sensitive to outliers. Formally, M-estimation seeks to solve the following problem:where formula_69 represents the choice of the robust cost function. Note that choosing formula_70 recovers the least squares estimation in (). Popular robust cost functions include formula_71-norm loss, Huber loss, Geman-McClure loss and truncated least squares loss. M-estimation has been one of the most popular paradigms for robust estimation in robotics and computer vision. Because robust objective functions are typically non-convex (\"e.g.,\" the truncated least squares loss v.s. the least squares loss), algorithms for solving the non-convex M-estimation are typically based on local optimization, where first an initial guess is provided, following by iterative refinements of the transformation to keep decreasing the objective function. Local optimization tends to work well when the initial guess is close to the global minimum, but it is also prone to get stuck in local minima if provided with poor initialization.\n\nGraduated non-convexity (GNC) is a general-purpose framework for solving non-convex optimization problems without initialization. It has achieved success in early vision and machine learning applications. The key idea behind GNC is to solve the hard non-convex problem by starting from an easy convex problem. Specifically, for a given robust cost function formula_69, one can construct a surrogate function formula_73 with a hyper-parameter formula_74, tuning which can gradually increase the non-convexity of the surrogate function formula_73 until it converges to the target function formula_69. Therefore, at each level of the hyper-parameter formula_74, the following optimization is solved:Black and Rangarajan proved that the objective function of each optimization () can be dualized into a sum of weighted least squares and a so-called outlier process function on the weights that determine the confidence of the optimization in each pair of measurements. Using Black-Rangarajan duality and GNC tailored for the Geman-McClure function, Zhou \"et al.\" developed the fast global registration algorithm that is robust against about formula_78 outliers in the correspondences. More recently, Yang \"et al.\" showed that the joint use of GNC (tailored to the Geman-McClure function and the truncated least squares function) and Black-Rangarajan duality can lead to a general-purpose solver for robust registration problems, including point clouds and mesh registration.\n\nAlmost none of the robust registration algorithms mentioned above (except the BnB algorithm that runs in exponential-time in the worst case) comes with performance guarantees, which means that these algorithms can return completely incorrect estimates without notice. Therefore, these algorithms are undesirable for safety-critical applications like autonomous driving.\n\nVery recently, Yang \"et al.\" has developed the first certifiably robust registration algorithm, named \"Truncated least squares Estimation And SEmidefinite Relaxation\" (TEASER). For point cloud registration, TEASER not only outputs an estimate of the transformation, but also quantifies the optimality of the given estimate. TEASER adopts the following truncated least squares (TLS) estimator:which is obtained by choosing the TLS robust cost function formula_79, where formula_80is a pre-defined constant that determines the maximum allowed residuals to be considered inliers. The TLS objective function has the property that for inlier correspondences (formula_81), the usual least square penalty is applied; while for outlier correspondences (formula_82), no penalty is applied and the outliers are discarded. If the TLS optimization () is solved to global optimality, then it is equivalent to running Horn's method on only the inlier correspondences.\n\nHowever, solving () is quite challenging due to its combinatorial nature. TEASER solves () as follows : (i) It builds invariant measurements such that the estimation of scale, rotation and translation can be decoupled and solved separately, a strategy that is inspired by the original Horn's method; (ii) The same TLS estimation is applied for each of the three sub-problems, where the scale TLS problem can be solved exactly using an algorithm called adaptive voting, the rotation TLS problem can relaxed to a semidefinite program (SDP) where the relaxation is exact in practice, even with large amount of outliers; the translation TLS problem can solved using component-wise adaptive voting. A fast implementation leveraging GNC is open-sourced here. In practice, TEASER can tolerate more than formula_83 outlier correspondences and runs in milliseconds.\n\nIn addition to developing TEASER, Yang \"et al.\" also prove that, under some mild conditions on the point cloud data, TEASER's estimated transformation has bounded errors from the ground-truth transformation.\n\nThe iterative closest point (ICP) algorithm was introduced by Besl and McKay.\nThe algorithm performs rigid registration in an iterative fashion by alternating in (i) given the transformation, finding the closest point in formula_7 for every point in formula_6; and (ii) given the correspondences, finding the best rigid transformation by solving the least squares problem (). As such, it works best if the initial pose of formula_6 is sufficiently close to formula_7. In pseudocode, the basic algorithm is implemented as follows:\n\nHere, the function codice_1 performs least squares optimization to minimize the distance in each of the formula_88 pairs, using the closed-form solutions by Horn and Arun.\n\nBecause the cost function of registration depends on finding the closest point in formula_7 to every point in formula_6, it can change as the algorithm is running. As such, it is difficult to prove that ICP will in fact converge exactly to the local optimum. In fact, empirically, ICP and EM-ICP do not converge to the local minimum of the cost function. Nonetheless, because ICP is intuitive to understand and straightforward to implement, it remains the most commonly used point set registration algorithm. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy.\nFor example, the expectation maximization algorithm is applied to the ICP algorithm to form the EM-ICP method, and the Levenberg-Marquardt algorithm is applied to the ICP algorithm to form the LM-ICP method.\n\nRobust point matching (RPM) was introduced by Gold et al. The method performs registration using deterministic annealing and soft assignment of correspondences between point sets. Whereas in ICP the correspondence generated by the nearest-neighbour heuristic is binary, RPM uses a \"soft\" correspondence where the correspondence between any two points can be anywhere from 0 to 1, although it ultimately converges to either 0 or 1. The correspondences found in RPM is always one-to-one, which is not always the case in ICP. Let formula_51 be the formula_92th point in formula_6 and formula_94 be the formula_95th point in formula_7. The \"match matrix\" formula_97 is defined as such:\n\nThe problem is then defined as: Given two point sets formula_6 and formula_7 find the Affine transformation formula_13 and the match matrix formula_97 that best relates them. Knowing the optimal transformation makes it easy to determine the match matrix, and vice versa. However, the RPM algorithm determines both simultaneously. The transformation may be decomposed into a translation vector and a transformation matrix:\n\nThe matrix formula_103 in 2D is composed of four separate parameters formula_104, which are scale, rotation, and the vertical and horizontal shear components respectively. The cost function is then:\n\nsubject to formula_105, formula_106, formula_107. The formula_108 term biases the objective towards stronger correlation by decreasing the cost if the match matrix has more ones in it. The function formula_109 serves to regularize the Affine transformation by penalizing large values of the scale and shear components:\n\nfor some regularization parameter formula_111.\n\nThe RPM method optimizes the cost function using the Softassign algorithm. The 1D case will be derived here. Given a set of variables formula_112 where formula_113. A variable formula_114 is associated with each formula_115 such that formula_116. The goal is to find formula_97 that maximizes formula_118. This can be formulated as a continuous problem by introducing a control parameter formula_119. In the deterministic annealing method, the control parameter formula_120 is slowly increased as the algorithm runs. Let formula_97 be:\n\nthis is known as the softmax function. As formula_120 increases, it approaches a binary value as desired in Equation (). The problem may now be generalized to the 2D case, where instead of maximizing formula_118, the following is maximized:\n\nwhere\n\nThis is straightforward, except that now the constraints on formula_74 are doubly stochastic matrix constraints: formula_126 and formula_127. As such the denominator from Equation () cannot be expressed for the 2D case simply. To satisfy the constraints, it is possible to use a result due to Sinkhorn, which states that a doubly stochastic matrix is obtained from any square matrix with all positive entries by the iterative process of alternating row and column normalizations. Thus the algorithm is written as such:\n\nwhere the deterministic annealing control parameter formula_120 is initially set to formula_129 and increases by factor formula_130 until it reaches the maximum value formula_131. The summations in the normalization steps sum to formula_132 and formula_133 instead of just formula_3 and formula_4 because the constraints on formula_74 are inequalities. As such the formula_132th and formula_133th elements are slack variables.\n\nThe algorithm can also be extended for point sets in 3D or higher dimensions. The constraints on the correspondence matrix formula_97 are the same in the 3D case as in the 2D case. Hence the structure of the algorithm remains unchanged, with the main difference being how the rotation and translation matrices are solved.\n\nThe thin plate spline robust point matching (TPS-RPM) algorithm by Chui and Rangarajan augments the RPM method to perform non-rigid registration by parametrizing the transformation as a thin plate spline.\nHowever, because the thin plate spline parametrization only exists in three dimensions, the method cannot be extended to problems involving four or more dimensions.\n\nThe kernel correlation (KC) approach of point set registration was introduced by Tsin and Kanade.\nCompared with ICP, the KC algorithm is more robust against noisy data. Unlike ICP, where, for every model point, only the closest scene point is considered, here every scene point affects every model point. As such this is a \"multiply-linked\" registration algorithm. For some kernel function formula_140, the kernel correlation formula_141 of two points formula_142 is defined thus:\n\nThe kernel function formula_140 chosen for point set registration is typically symmetric and non-negative kernel, similar to the ones used in the Parzen window density estimation. The Gaussian kernel typically used for its simplicity, although other ones like the Epanechnikov kernel and the tricube kernel may be substituted. The kernel correlation of an entire point set formula_144 is defined as the sum of the kernel correlations of every point in the set to every other point in the set:\n\nThe logarithm of KC of a point set is proportional, within a constant factor, to the information entropy. Observe that the KC is a measure of a \"compactness\" of the point set—trivially, if all points in the point set were at the same location, the KC would evaluate to a large value. The cost function of the point set registration algorithm for some transformation parameter formula_145 is defined thus:\n\nSome algebraic manipulation yields:\n\nThe expression is simplified by observing that formula_146 is independent of formula_145. Furthermore, assuming rigid registration, formula_148 is invariant when formula_145 is changed because the Euclidean distance between every pair of points stays the same under rigid transformation. So the above equation may be rewritten as:\n\nThe kernel density estimates are defined as:\n\nThe cost function can then be shown to be the correlation of the two kernel density estimates:\n\nHaving established the cost function, the algorithm simply uses gradient descent to find the optimal transformation. It is computationally expensive to compute the cost function from scratch on every iteration, so a discrete version of the cost function Equation () is used. The kernel density estimates formula_152 can be evaluated at grid points and stored in a lookup table. Unlike the ICP and related methods, it is not necessary to find the nearest neighbour, which allows the KC algorithm to be comparatively simple in implementation.\n\nCompared to ICP and EM-ICP for noisy 2D and 3D point sets, the KC algorithm is less sensitive to noise and results in correct registration more often.\n\nThe kernel density estimates are sums of Gaussians and may therefore be represented as Gaussian mixture models (GMM). Jian and Vemuri use the GMM version of the KC registration algorithm to perform non-rigid registration parametrized by thin plate splines.\n\nCoherent point drift (CPD) was introduced by Myronenko and Song.\nThe algorithm takes a probabilistic approach to aligning point sets, similar to the GMM KC method. Unlike earlier approaches to non-rigid registration which assume a thin plate spline transformation model, CPD is agnostic with regard to the transformation model used. The point set formula_6 represents the Gaussian mixture model (GMM) centroids. When the two point sets are optimally aligned, the correspondence is the maximum of the GMM posterior probability for a given data point. To preserve the topological structure of the point sets, the GMM centroids are forced to move coherently as a group. The expectation maximization algorithm is used to optimize the cost function.\n\nLet there be points in formula_6 and points in formula_7. The GMM probability density function for a point is:\n\nwhere, in dimensions, formula_156 is the Gaussian distribution centered on point formula_157.\n\nThe membership probabilities formula_159 is equal for all GMM components. The weight of the uniform distribution is denoted as formula_160. The mixture model is then:\n\nThe GMM centroids are re-parametrized by a set of parameters formula_145 estimated by maximizing the likelihood. This is equivalent to minimizing the negative log-likelihood function:\n\nwhere it is assumed that the data is independent and identically distributed. The correspondence probability between two points formula_51 and formula_94 is defined as the posterior probability of the GMM centroid given the data point:\n\nThe expectation maximization (EM) algorithm is used to find formula_145 and formula_166. The EM algorithm consists of two steps. First, in the E-step or \"estimation\" step, it guesses the values of parameters (\"old\" parameter values) and then uses Bayes' theorem to compute the posterior probability distributions formula_167 of mixture components. Second, in the M-step or \"maximization\" step, the \"new\" parameter values are then found by minimizing the expectation of the complete negative log-likelihood function, i.e. the cost function:\n\nIgnoring constants independent of formula_145 and formula_169, Equation () can be expressed thus:\n\n(i|s_j) \\lVert s_j - T(m_i,\\theta) \\rVert^2 \n\nwhere\n\nwith formula_171 only if formula_172. The posterior probabilities of GMM components computed using previous parameter values formula_173 is:\n\n(i|s_j) = \n\nMinimizing the cost function in Equation () necessarily decreases the negative log-likelihood function in Equation () unless it is already at a local minimum. Thus, the algorithm can be expressed using the following pseudocode, where the point sets formula_6 and formula_7 are represented as formula_176 and formula_177 matrices formula_178 and formula_179 respectively:\n\nwhere the vector formula_180 is a column vector of ones. The codice_2 function differs by the type of registration performed. For example, in rigid registration, the output is a scale , a rotation matrix formula_181, and a translation vector formula_182. The parameter formula_145 can be written as a tuple of these:\n\nwhich is initialized to one, the identity matrix, and a column vector of zeroes:\n\nThe aligned point set is:\n\nThe codice_3 function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper.\n\nFor affine registration, where the goal is to find an affine transformation instead of a rigid one, the output is an affine transformation matrix formula_187 and a translation formula_182 such that the aligned point set is:\n\nThe codice_4 function for rigid registration can then be written as follows, with derivation of the algebra explained in Myronenko's 2010 paper.\n\nIt is also possible to use CPD with non-rigid registration using a parametrization derived using calculus of variations.\n\nSums of Gaussian distributions can be computed in linear time using the fast Gauss transform (FGT). Consequently, the time complexity of CPD is formula_190, which is asymptotically much faster than formula_191 methods.\n\nThis algorithm was introduced in 2013 by H. Assalih to accommodate sonar image registration. These kinds of images tend to have high amounts of noise, so it is expected to have lots of outliers in the point sets to match. SCS delivers high robustness against outliers and can surpass ICP and CPD performance in the presence of outliers. SCS doesn't use iterative optimization in high dimensional space and is neither probabilistic nor spectral. SCS can match rigid and non-rigid transformations, and performs best when the target transformation is between three and six degrees of freedom.\n\n- Reference implementation of thin plate spline robust point matching\n- Reference implementation of kernel correlation point set registration\n- Reference implementation of coherent point drift\n- Reference implementation of ICP variants\n", "related": "NONE"}
{"id": "289860", "url": "https://en.wikipedia.org/wiki?curid=289860", "title": "Alignments of random points", "text": "Alignments of random points\n\nAlignments of random points in a plane can be demonstrated by statistics to be counter-intuitively easy to find when a large number of random points are marked on a bounded flat surface. This has been put forward as a demonstration that ley lines and other similar mysterious alignments believed by some to be phenomena of deep significance might exist solely due to chance alone, as opposed to the supernatural or anthropological explanations put forward by their proponents. The topic has also been studied in the fields of computer vision and astronomy.\n\nA number of studies have examined the mathematics of alignment of random points on the plane. In all of these, the width of the line - the allowed displacement of the positions of the points from a perfect straight line - is important. It allows the fact that real-world features are not mathematical points, and that their positions need not line up exactly for them to be considered in alignment. Alfred Watkins, in his classic work on ley lines \"The Old Straight Track\", used the width of a pencil line on a map as the threshold for the tolerance of what might be regarded as an alignment. For example, using a 1 mm pencil line to draw alignments on a 1:50,000 Ordnance Survey map, the corresponding width on the ground would be 50 m.\n\nContrary to intuition, finding alignments between randomly placed points on a landscape gets progressively easier as the geographic area to be considered increases. One way of understanding this phenomenon is to see that the increase in the number of possible combinations of sets of points in that area overwhelms the decrease in the probability that any given set of points in that area line up.\n\nOne definition which expresses the generally accepted meaning of \"alignment\" is:\n\nMore precisely, a path of width \"w\" may be defined as the set of all points within a distance of \"w/2\" of a straight line on a plane, or a great circle on a sphere, or in general any geodesic on any other kind of manifold. Note that, in general, any given set of points that are aligned in this way will contain a large number of infinitesimally different straight paths. Therefore, only the existence of at least one straight path is necessary to determine whether a set of points is an alignment. For this reason, it is easier to count the sets of points, rather than the paths themselves.\nThe number of alignments found is very sensitive to the allowed width \"w\", increasing approximately proportionately to \"w\", where \"k\" is the number of points in an alignment.\n\nThe following is a very approximate order-of-magnitude estimate of the likelihood of alignments, assuming a plane covered with uniformly distributed \"significant\" points.\n\nConsider a set of \"n\" points in a compact area with approximate diameter \"L\" and area approximately \"L\". Consider a valid line to be one where every point is within distance \"w\"/2 of the line (that is, lies on a track of width \"w\", where \"w\" ≪ \"L\").\n\nConsider all the unordered sets of \"k\" points from the \"n\" points, of which there are:\n\n(see factorial and binomial coefficient for notation).\n\nTo make a rough estimate of the probability that any given subset of \"k\" points is approximately collinear in the way defined above, consider the line between the \"leftmost\" and \"rightmost\" two points in that set (for some arbitrary left/right axis: we can choose top and bottom for the exceptional vertical case). These two points are by definition on this line. For each of the remaining \"k\"-2 points, the probability that the point is \"near enough\" to the line is roughly \"w\"/\"L\", which can be seen by considering the ratio of the area of the line tolerance zone (roughly \"wL\") and the overall area (roughly \"L\").\n\nSo, the expected number of k-point alignments, by this definition, is very roughly:\n\nAmong other things this can be used to show that, contrary to intuition, the number of k-point lines expected from random chance in a plane covered with points at a given density, for a given line width, increases much more than linearly with the size of the area considered, since the combinatorial explosion of growth in the number of possible combinations of points more than makes up for the increase in difficulty of any given combination lining up.\n\nA more precise expression for the number of 3-point alignments of maximum width \"w\" and maximum length \"d\" expected by chance among \"n\" points placed randomly on a square of side \"L\" is \n\nIf edge effects (alignments lost over the boundaries of the square) are included, then the expression becomes\n\nA generalisation to \"k\"-point alignments (ignoring edge effects) is\nwhich has roughly similar asymptotic scaling properties as the crude approximation in the previous section, with combinatorial explosion for large \"n\" overwhelming the effects of other variables.\n\nComputer simulations show that points on a plane tend to form alignments similar to those found by ley hunters in numbers consistent with the order-of-magnitude estimates above, suggesting that ley lines may also be generated by chance. This phenomenon occurs regardless of whether the points are generated pseudo-randomly by computer, or from data sets of mundane features such as pizza restaurants or telephone booths.\n\nIt is easy to find alignments of 4 to 8 points in reasonably small data sets with \"w\" = 50 m.\nChoosing large areas or larger values of \"w\" makes it easy to find alignments of 20 or more points.\n\n", "related": "\n- Apophenia\n- Clustering illusion\n- Coincidence\n- Combinatorial explosion\n- Complete spatial randomness\n- General position\n- Ley lines\n- Pattern recognition\n- Procrustes analysis\n- Ramsey theory, for an interesting and important notion of \"unavoidable coincidences\"\n- Statistical shape analysis\n- \"The Old Straight Track\"\n"}
{"id": "6596", "url": "https://en.wikipedia.org/wiki?curid=6596", "title": "Computer vision", "text": "Computer vision\n\nComputer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nThe scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\n\nSub-domains of computer vision include scene reconstruction, event detection, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.\n\nComputer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. \"Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.\n\nIn the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it \"describe what it saw\".\n\nWhat distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.\n\nThe next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.\nBy the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.\n\nRecent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks.. \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods. \n\nAreas of artificial intelligence deal with autonomous planning or deliberation for robotic systems to navigate through an environment. A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot.\n\nArtificial intelligence and computer vision share other topics such as pattern recognition and learning techniques. Consequently, computer vision is sometimes seen as a part of the artificial intelligence field or the computer science field in general.\n\nComputer vision is often considered to be part of information engineering.\n\nSolid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible or infra-red light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Also, various measurement problems in physics can be addressed using computer vision, for example motion in fluids.\n\nA third field which plays an important role is neurobiology, specifically the study of the biological vision system. Over the last century, there has been an extensive study of eyes, neurons, and the brain structures devoted to processing of visual stimuli in both humans and various animals. This has led to a coarse, yet complicated, description of how \"real\" vision systems operate in order to solve certain vision-related tasks. These results have led to a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems, at different levels of complexity. Also, some of the learning-based methods developed within computer vision (\"e.g.\" neural net and deep learning based image and feature analysis and classification) have their background in biology.\n\nSome strands of computer vision research are closely related to the study of biological vision – indeed, just as many strands of AI research are closely tied with research into human consciousness, and the use of stored knowledge to interpret, integrate and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, studies and describes the processes implemented in software and hardware behind artificial vision systems. Interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.\n\nYet another field related to computer vision is signal processing. Many methods for processing of one-variable signals, typically temporal signals, can be extended in a natural way to processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n\nBeside the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion ecommerce, inventory management, patent search, furniture, and the beauty industry.\n\nThe fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented.\n\nComputer graphics produces image data from 3D models, computer vision often produces 3D models from image data. There is also a trend towards a combination of the two disciplines, \"e.g.\", as explored in augmented reality.\n\nThe following characterizations appear relevant but should not be taken as universally accepted::\n\n- Image processing and image analysis tend to focus on 2D images, how to transform one image to another, \"e.g.\", by pixel-wise operations such as contrast enhancement, local operations such as edge extraction or noise removal, or geometrical transformations such as rotating the image. This characterization implies that image processing/analysis neither require assumptions nor produce interpretations about the image content.\n- Computer vision includes 3D analysis from 2D images. This analyzes the 3D scene projected onto one or several images, \"e.g.\", how to reconstruct structure or other information about the 3D scene from one or several images. Computer vision often relies on more or less complex assumptions about the scene depicted in an image.\n- Machine vision is the process of applying a range of technologies & methods to provide imaging-based automatic inspection, process control and robot guidance in industrial applications. Machine vision tends to focus on applications, mainly in manufacturing, \"e.g.\", vision-based robots and systems for vision-based inspection, measurement, or picking (such as bin picking). This implies that image sensor technologies and control theory often are integrated with the processing of image data to control a robot and that real-time processing is emphasised by means of efficient implementations in hardware and software. It also implies that the external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision, which can enable the use of different algorithms.\n- There is also a field called imaging which primarily focuses on the process of producing images, but sometimes also deals with processing and analysis of images. For example, medical imaging includes substantial work on the analysis of image data in medical applications.\n- Finally, pattern recognition is a field which uses various methods to extract information from signals in general, mainly based on statistical approaches and artificial neural networks. A significant part of this field is devoted to applying these methods to image data.\n\nPhotogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n\nApplications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n\n- Automatic inspection, \"e.g.\", in manufacturing applications;\n- Assisting humans in identification tasks, e.g., a species identification system;\n- Controlling processes, \"e.g.\", an industrial robot;\n- Detecting events, \"e.g.\", for visual surveillance or people counting, e.g., in the restaurant industry;\n- Interaction, \"e.g.\", as the input to a device for computer-human interaction;\n- Modeling objects or environments, \"e.g.\", medical image analysis or topographical modeling;\n- Navigation, \"e.g.\", by an autonomous vehicle or mobile robot; and\n- Organizing information, \"e.g.\", for indexing databases of images and image sequences.\nOne of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is detection of tumours, arteriosclerosis or other malign changes; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: \"e.g.\", about the structure of the brain, or about the quality of medical treatments. Applications of computer vision in the medical area also includes enhancement of images interpreted by humans—ultrasonic images or X-ray images for example—to reduce the influence of noise.\n\nA second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a manufacturing process. One example is quality control where details or final products are being automatically inspected in order to find defects. Another example is measurement of position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material, a process called optical sorting.\n\nMilitary applications are probably one of the largest areas for computer vision. The obvious examples are detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene which can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\nOne of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, \"e.g.\" for knowing where it is, or for producing a map of its environment (SLAM) and for detecting obstacles. It can also be used for detecting certain task specific events, \"e.g.\", a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars, but this technology has still not reached a level where it can be put on the market. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, \"e.g.\", NASA's Mars Exploration Rover and ESA's ExoMars Rover.\n\nOther application areas include:\n\n- Support of visual effects creation for cinema and broadcast, \"e.g.\", camera tracking (matchmoving).\n- Surveillance.\n- Tracking and counting organisms in the biological sciences\n\nEach of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n\nComputer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, \"e.g.\", in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n\nThe classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of the recognition problem are described in the literature:\n\n- Object recognition (also called object classification)one or several pre-specified or learned objects or object classes can be recognized, usually together with their 2D positions in the image or 3D poses in the scene. Blippar, Google Goggles and LikeThat provide stand-alone programs that illustrate this functionality.\n- Identificationan individual instance of an object is recognized. Examples include identification of a specific person's face or fingerprint, identification of handwritten digits, or identification of a specific vehicle.\n- Detectionthe image data are scanned for a specific condition. Examples include detection of possible abnormal cells or tissues in medical images or detection of a vehicle in an automatic road toll system. Detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce a correct interpretation.\n\nCurrently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and hundreds of object classes. Performance of convolutional neural networks, on the ImageNet tests, is now close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.\n\nSeveral specialized tasks based on recognition exist, such as:\n\n- Content-based image retrievalfinding all images in a larger set of images which have a specific content. The content can be specified in different ways, for example in terms of similarity relative a target image (give me all images similar to image X), or in terms of high-level search criteria given as text input (give me all images which contain many houses, are taken during winter, and have no cars in them).\n\n- Pose estimationestimating the position or orientation of a specific object relative to the camera. An example application for this technique would be assisting a robot arm in retrieving objects from a conveyor belt in an assembly line situation or picking parts from a bin.\n- Optical character recognition (OCR)identifying characters in images of printed or handwritten text, usually with a view to encoding the text in a format more amenable to editing or indexing (\"e.g.\" ASCII).\n- 2D code readingreading of 2D codes such as data matrix and QR codes.\n- Facial recognition\n- Shape Recognition Technology (SRT) in people counter systems differentiating human beings (head and shoulder patterns) from objects\n\nSeveral tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene, or even of the camera that produces the images . Examples of such tasks are:\n\n- Egomotiondetermining the 3D rigid motion (rotation and translation) of the camera from an image sequence produced by the camera.\n- Trackingfollowing the movements of a (usually) smaller set of interest points or objects (\"e.g.\", vehicles, humans or other organisms) in the image sequence.\n- Optical flowto determine, for each point in the image, how that point is moving relative to the image plane, \"i.e.\", its apparent motion. This motion is a result both of how the corresponding 3D point is moving in the scene and how the camera is moving relative to the scene.\n\nGiven one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.\n\nThe aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look, to distinguish them from noise. By first analysing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n\nAn example in this field is inpainting.\n\nThe organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n\n- Image acquisition – A digital image is produced by one or several image sensors, which, besides various types of light-sensitive cameras, include range sensors, tomography devices, radar, ultra-sonic cameras, etc. Depending on the type of sensor, the resulting image data is an ordinary 2D image, a 3D volume, or an image sequence. The pixel values typically correspond to light intensity in one or several spectral bands (gray images or colour images), but can also be related to various physical measures, such as depth, absorption or reflectance of sonic or electromagnetic waves, or nuclear magnetic resonance.\n- Pre-processing – Before a computer vision method can be applied to image data in order to extract some specific piece of information, it is usually necessary to process the data in order to assure that it satisfies certain assumptions implied by the method. Examples are:\n- Re-sampling to assure that the image coordinate system is correct.\n- Noise reduction to assure that sensor noise does not introduce false information.\n- Contrast enhancement to assure that relevant information can be detected.\n- Scale space representation to enhance image structures at locally appropriate scales.\n- Feature extraction – Image features at various levels of complexity are extracted from the image data. Typical examples of such features are:\n- Lines, edges and ridges.\n- Localized interest points such as corners, blobs or points.\n- Detection/segmentation – At some point in the processing a decision is made about which image points or regions of the image are relevant for further processing. Examples are:\n- Selection of a specific set of interest points.\n- Segmentation of one or multiple image regions that contain a specific object of interest.\n- Segmentation of image into nested scene architecture comprising foreground, object groups, single objects or salient object parts (also referred to as spatial-taxon scene hierarchy), while the visual salience is often implemented as spatial and temporal attention.\n- Segmentation or co-segmentation of one or multiple videos into a series of per-frame foreground masks, while maintaining its temporal semantic continuity.\n- High-level processing – At this step the input is typically a small set of data, for example a set of points or an image region which is assumed to contain a specific object. The remaining processing deals with, for example:\n- Verification that the data satisfy model-based and application-specific assumptions.\n- Estimation of application-specific parameters, such as object pose or object size.\n- Image recognition – classifying a detected object into different categories.\n- Image registration – comparing and combining two different views of the same object.\n- Decision making Making the final decision required for the application, for example:\n- Pass/fail on automatic inspection applications.\n- Match/no-match in recognition applications.\n- Flag for further human review in medical, military, security and recognition applications.\n\nImage-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\n\nThe representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n\nWhile inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.\n\nThere are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories such as camera supports, cables and connectors.\n\nMost computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n\nA few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n\nWhile traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.\n\nEgocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\n\nAs of 2016, vision processing units are emerging as a new class of processor, to complement CPUs and graphics processing units (GPUs) in this role.\n\n", "related": "\n- Computational imaging\n- Computational photography\n- Machine vision glossary\n- Space mapping\n- Teknomo–Fernandez algorithm\n- Visual system\n- Visual perception\n- Vision science\n- Egocentric vision\n- Visual agnosia\n\n- List of computer vision topics\n- List of emerging technologies\n- Outline of artificial intelligence\n- Outline of computer vision\n\n\n- USC Iris computer vision conference list\n- Computer vision papers on the web A complete list of papers of the most relevant computer vision conferences.\n- Computer Vision Online News, source code, datasets and job offers related to computer vision.\n- Keith Price's Annotated Computer Vision Bibliography\n- CVonline Bob Fisher's Compendium of Computer Vision.\n- British Machine Vision Association Supporting computer vision research within the UK via the BMVC and MIUA conferences, \"Annals of the BMVA\" (open-source journal), BMVA Summer School and one-day meetings\n"}
{"id": "44534652", "url": "https://en.wikipedia.org/wiki?curid=44534652", "title": "Spatial verification", "text": "Spatial verification\n\nThe spatial verification consists in verify a spatial correlation between certain points of a pair of images.\n\nThe main problem is that outliers (that does not fit or does not match the selected model) affect adjustment called least squares (numerical analysis technique framed in mathematical optimization, which, given an set of ordered pairs: independent variable, dependent variable, and a family of functions, try to find the continuous function).\n\n- Effective when one is able to find safe features without clutter.\n- Good results for correspondence in specific instances.\n\n- The scaling models.\n- The spatial verification can not be used as post-processing.\n\nThe most widely used for spatial verification and avoid errors caused by these outliers methods are:\n\nSeeks to avoid the impact of outliers, that not fit with the model, so only considers inline which match the model in question. If an outlier is chosen to calculate the current setting, then the resulting line will have little support from the rest of the points.\nThe algorithm that is performed is a loop that performs the following steps:\n1. Of the entire input data set, takes a subset randomly to estimate the model.\n2. Compute model subset. The model is estimated with standard linear algorithms.\n3. Find the matching values of transformation.\n4. If the error is minimal model, this is accepted, and if the number of correspondences is long enough, the subset of points involved consensus assembly is referred. And it becomes to compute the estimated model in all correspondences.\nThe goal is to keep the model with the highest number of matches and the main problem is the number of times you have to repeat the process to obtain the best estimate of the model.\nRANSAC set in advance the number of iterations of the algorithm.\n\nTo specify scenes or objects, is commonly used affine transformations to perform the spatial verification.\n\nThis is a technique for detecting shapes in digital images that solves the veracity of space by clusters of points belonging to the model through a voting procedure on a set of parametric figures.\n\nNot all possible combinations comprovar characteristics by adjusting a model for every possible subset, so that the voting technique, in which a vote is stored for each possible line in which each point is used. Then observe what were the lines with the most votes and those are selected.\n\nIf we use the local characteristics of scale, rotation and translation invariant, each feature coincidence gives a hypothesis alignment for scaling, translation and orientation of the model in the picture.\n\nOne hypothesis generated by a single match can be unreliable, so for each match (match), a vote is done to get a stronger hypothesis in the Hough space.\nSo we have two major phases:\n- \"Training\": For each characteristic model, 2D is saved the location, scale and orientation of the model.\n- \"Test\": each match is allowed to be performed by the algorithm SIFT and model characteristics vote in the Hough space.\n\nThe main disadvantages are:\n- The noise or clutter can display more feedback from those aiming to provide objective.\n- The size of the storage array should be chosen carefully.\n\n- \" 'Recovery System Google' \". The goal is to recover objects or scenes with ease, speed and pitch in the Google search engine is a website that contains certain words.\n\n1. Garuman, Kristen. \"Recognizing object instances\", 9 August 2012. Retrieved on 24 November 2014.\n2. Sivic, Josef. \"Video Google Demo\", 13 August 2004. Retrieved on 24 November 2014.\n3. M. A. Fischler, R. C. Bolles. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography. Comm. of the ACM, Vol 24, pp 381–395, 1981.\n4. Sivic, Josef. \"Distinctive Image Features\", 5 January 2004. Retrieved on 24 November 2014.\n", "related": "NONE"}
{"id": "12555662", "url": "https://en.wikipedia.org/wiki?curid=12555662", "title": "Active contour model", "text": "Active contour model\n\nActive contour model, also called snakes, is a framework in computer vision introduced by Michael Kass, Andrew Witkin and Demetri Terzopoulos for delineating an object outline from a possibly noisy 2D image. The snakes model is popular in computer vision, and snakes are widely used in applications like object tracking, shape recognition, segmentation, edge detection and stereo matching.\n\nA snake is an energy minimizing, deformable spline influenced by constraint and image forces that pull it towards object contours and internal forces that resist deformation. Snakes may be understood as a special case of the general technique of matching a deformable model to an image by means of energy minimization. In two dimensions, the active shape model represents a discrete version of this approach, taking advantage of the point distribution model to restrict the shape range to an explicit domain learnt from a training set.\nSnakes do not solve the entire problem of finding contours in images, since the method requires knowledge of the desired contour shape beforehand. Rather, they depend on other mechanisms such as interaction with a user, interaction with some higher level image understanding process, or information from image data adjacent in time or space.\n\nIn computer vision, contour models describe the boundaries of shapes in an image. Snakes in particular are designed to solve problems where the approximate shape of the boundary is known. By being a deformable model, snakes can adapt to differences and noise in stereo matching and motion tracking. Additionally, the method can find Illusory contours in the image by ignoring missing boundary information.\n\nCompared to classical feature extraction techniques, snakes have multiple advantages:\n- They autonomously and adaptively search for the minimum state.\n- External image forces act upon the snake in an intuitive manner.\n- Incorporating Gaussian smoothing in the image energy function introduces scale sensitivity.\n- They can be used to track dynamic objects.\n\nThe key drawbacks of the traditional snakes are\n\n- They are sensitive to local minima states, which can be counteracted by simulated annealing techniques.\n- Minute features are often ignored during energy minimization over the entire contour.\n- Their accuracy depends on the convergence policy.\n\nA simple elastic snake is defined by a set of \"n\" points formula_1 for formula_2, the internal elastic energy term formula_3, and the external edge-based energy term formula_4. The purpose of the internal energy term is to control the deformations made to the snake, and the purpose of the external energy term is to control the fitting of the contour onto the image. The external energy is usually a combination of the forces due to the image itself formula_5 and the constraint forces introduced by the user formula_6\n\nThe energy function of the snake is the sum of its external energy and internal energy, or\n\nThe internal energy of the snake is composed of the continuity of the contour formula_8 and the smoothness of the contour formula_9.\n\nThis can be expanded as\n\nwhere formula_12 and formula_13 are user-defined weights; these control the internal energy function's sensitivity to the amount of stretch in the snake and the amount of curvature in the snake, respectively, and thereby control the number of constraints on the shape of the snake.\n\nIn practice, a large weight formula_12 for the continuity term penalizes changes in distances between points in the contour. A large weight formula_13 for the smoothness term penalizes oscillations in the contour and will cause the contour to act as a thin plate.\n\nEnergy in the image is some function of the features of the image. This is one of the most common points of modification in derivative methods. Features in images and images themselves can be processed in many and various ways.\n\nFor an image formula_16, lines, edges, and terminations present in the image, the general formulation of energy due to the image is\n\nwhere formula_18, formula_19, formula_20 are weights of these salient features. Higher weights indicate that the salient feature will have a larger contribution to the image force.\n\nThe line functional is the intensity of the image, which can be represented as\n\nThe sign of formula_18 will determine whether the line will be attracted to either dark lines or light lines.\n\nSome smoothing or noise reduction may be used on the image, which then the line functional appears as\n\nThe edge functional is based on the image gradient. One implementation of this is\n\nA snake originating far from the desired object contour may erroneously converge to some local minimum. Scale space continuation can be used in order to avoid these local minima. This is achieved by using a blur filter on the image and reducing the amount of blur as the calculation progresses to refine the fit of the snake. The energy functional using scale space continuation is\n\nwhere formula_26 is a Gaussian with standard deviation formula_27. Minima of this function fall on the zero-crossings of formula_28 which define edges as per Marr–Hildreth theory.\n\nCurvature of level lines in a slightly smoothed image can be used to detect corners and terminations in an image. Using this method, let formula_29 be the image smoothed by\n\nwith gradient angle\n\nunit vectors along the gradient direction\n\nand unit vectors perpendicular to the gradient direction\n\nThe termination functional of energy can be represented as\n\nSome systems, including the original snakes implementation, allowed for user interaction to guide the snakes, not only in initial placement but also in their energy terms. Such constraint energy formula_35 can be used to interactively guide the snakes towards or away from particular features.\n\nGiven an initial guess for a snake, the energy function of the snake is iteratively minimized. Gradient descent minimization is one of the simplest optimizations which can be used to minimize snake energy. Each iteration takes one step in the negative gradient of the point with controlled step size formula_36 to find local minima. This gradient-descent minimization can be implemented as\n\nWhere formula_38 is the force on the snake, which is defined by the negative of the gradient of the energy field.\n\nAssuming the weights formula_40 and formula_41 are constant with respect to formula_42, this iterative method can be simplified to\n\nIn practice, images have finite resolution and can only be integrated over finite time steps formula_44. As such, discrete approximations must be made for practical implementation of snakes.\n\nThe energy function of the snake can be approximated by using the discrete points on the snake.\n\nConsequentially, the forces of the snake can be approximated as\n\nGradient approximation can be done through any finite approximation method with respect to \"s\", such as Finite difference.\n\nThe introduction of discrete time into the algorithm can introduce updates which the snake is moved past the minima it is attracted to; this further can cause oscillations around the minima or lead to a different minima being found.\n\nThis can be avoided through tuning the time step such that the step size is never greater than a pixel due to the image forces. However, in regions of low energy, the internal energies will dominate the update.\n\nAlternatively, the image forces can be normalized for each step such that the image forces only update the snake by one pixel. This can be formulated as\n\nwhere formula_48 is near the value of the pixel size. This avoids the problem of dominating internal energies that arise from tuning the time step.\n\nThe energies in a continuous image may have zero-crossing that do not exist as a pixel in the image. In this case, a point in the snake would oscillate between the two pixels that neighbor this zero-crossing. This oscillation can be avoided by using interpolation between pixels instead of nearest neighbor.\n\nThe following pseudocode implements the snakes method in a general form\n\nfunction v = snakes (I, v)\n\nend\n\nWhere \"generateImageEnergy (I)\" can be written as\nfunction E_image = generateImageEnergy (I)\n\nend\n\nThe default method of snakes has various limitation and corner cases where the convergence performs poorly. Several alternatives exist which addresses issues of the default method, though with their own trade-offs. A few are listed here.\n\nThe gradient vector flow (GVF) snake model addresses two issues with snakes:\n\n- poor convergence performance for concave boundaries\n- poor convergence performance when snake is initialized far from minimum\n\nIn 2D, the GVF vector field formula_49 minimizes the energy functional\n\nwhere formula_51 is a controllable smoothing term. This can be solved by solving the Euler equations\n\nThis can be solved through iteration towards a steady-state value.\n\nThis result replaces the default external force.\n\nThe primary issue with using GVF is the smoothing term formula_51 causes rounding of the edges of the contour. Reducing the value of formula_51 reduces the rounding but weakens the amount of smoothing.\n\nThe balloon model addresses these problems with the default active contour model:\n\n- The snake is not attracted to distant edges.\n- The snake will shrink inwards if no substantial images forces are acting upon it.\n- a snake larger than the minima contour will eventually shrink into it, but a snake smaller than the minima contour will not find the minima and instead continue to shrink.\n\nThe balloon model introduces an inflation term into the forces acting on the snake\n\nwhere formula_60 is the normal unitary vector of the curve at formula_61 and formula_62 is the magnitude of the force. formula_62 should have the same magnitude as the image normalization factor formula_64 and be smaller in value than formula_64 to allow forces at image edges to overcome the inflation force.\n\nThree issues arise from using the balloon model:\n- Instead of shrinking, the snake expands into the minima and will not find minima contours smaller than it.\n- The outward force causes the contour to be slightly larger than the actual minima. This can be solved by decreasing the balloon force after a stable solution has been found.\n- The inflation force can overpower forces from weak edges, amplifying the issue with snakes ignoring weaker features in an image.\n\nThe diffusion snake model addresses the sensitivity of snakes to noise, clutter, and occlusion. It implements a modification of the Mumford–Shah functional and its cartoon limit and incorporates statistical shape knowledge. The default image energy functional formula_5 is replaced with\n\nwhere formula_68 is based on a modified Mumford–Shah functional\n\nwhere formula_70 is the piecewise smooth model of the image formula_71 of domain formula_72. Boundaries formula_73 are defined as\n\nwhere formula_75 are quadratic B-spline basis functions and formula_76 are the control points of the splines. The modified cartoon limit is obtained as formula_77 and is a valid configuration of formula_68.\n\nThe functional formula_79 is based on training from binary images of various contours and is controlled in strength by the parameter formula_80. For a Gaussian distribution of control point vectors formula_81 with mean control point vector formula_82 and covariance matrix formula_83 , the quadratic energy that corresponds to the Gaussian probability is\n\nThe strength of this method relies on the strength of the training data as well as the tuning of the modified Mumford–Shah functional. Different snakes will require different training data sets and tunings.\n\nGeometric active contour, or geodesic active contour (GAC) or conformal active contours employs ideas from Euclidean curve shortening evolution. Contours split and merge depending on the detection of objects in the image. These models are largely inspired by level sets, and have been extensively employed in medical image computing.\n\nFor example, the gradient descent curve evolution equation of GAC is \n\nwhere formula_86 is a halting function, \"c\" is a Lagrange multiplier, formula_87 is the curvature, and formula_88 is the unit inward normal. This particular form of curve evolution equation is only dependent on the velocity in the normal direction. It therefore can be rewritten equivalently in an Eulerian form by inserting the level set function formula_89 into it as follows\n\nThis simple yet powerful level-set reformation enables active contours to handle topology changes during the gradient descent curve evolution. It has inspired tremendous progress in the related fields, and using numerical methods to solve the level-set reformulation is now commonly known as the level-set method. Although the level set method has become quite a popular tool for implementing active contours, Wang and Chan argued that not all curve evolution equations should be \"directly\" solved by it.\n\nMore recent developments in active contours address modeling of regional properties, incorporation of flexible shape priors and fully automatic segmentation, etc.\n\nStatistical models combining local and global features have been formulated by Lankton and Allen Tannenbaum.\n\nGraph cuts, or max-flow/min-cut, is a generic method for minimizing a particular form of energy called Markov random field (MRF) energy. The Graph cuts method has been applied to image segmentation as well, and it sometimes outperforms the level set method when the model is MRF or can be approximated by MRF.\n\n- David Young, March 1995\n- Snakes: Active Contours, CVOnline\n- ICBE, University of Manchester\n- Active contours implementation & test platform GUI\n- A simple implementation of snakes by Associate Professor Cris Luengo\n- MATLAB documentation for activecontour, which segments an image using active contours\n\n- Practical examples of different snakes developed by Prince and Xu\n- Basic tool to play with snakes (active contour models) from Tim Cootes, University of Manchester\n- Matlab implementation of 2D and 3D snake including GVF and balloon force\n- Matlab Snake Demo by Chris Bregler and Malcolm Slaney, Interval Research Corporation.\n- A Demonstration Using Java\n- Active Contours implementation & test platform GUI by Nikolay S. and Alex Blekhman implementing \"Active Contours without Edges\"\n- Active Contour Segmentation by Shawn Lankton implementing \"Active Contours without Edges\"\n- Geometric Active Contour Code by Jarno Ralli\n- Morphological Snakes\n", "related": "NONE"}
{"id": "44632031", "url": "https://en.wikipedia.org/wiki?curid=44632031", "title": "M-Theory (learning framework)", "text": "M-Theory (learning framework)\n\nIn Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.\n\nThe core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex.\n\nA great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions. It can be seen from different distances, different viewpoints, under different lighting, partially occluded, etc. In addition, for particular classes objects, such as faces, highly complex specific transformations may be relevant, such as changing facial expressions. For learning to recognize images, it is greatly beneficial to factor out these variations. It results in much simpler classification problem and, consequently, in great reduction of sample complexity of the model.\n\nA simple computational experiment illustrates this idea. Two instances of a classifier were trained to distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One can see that the second classifier performed quite well even after receiving a single example from each category, while performance of the first classifier was close to random guess even after seeing 20 examples.\n\nInvariant representations has been incorporated into several learning architectures, such as neocognitrons. Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps to take into account some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-Theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities.\n\nAnother core idea of M-Theory is close in spirit to ideas from the field of compressed sensing. An implication from Johnson–Lindenstrauss lemma says that a particular number of images can be embedded into a low-dimensional feature space with the same distances between images by using random projections. This result suggests that dot product between the observed image and some other image stored in memory, called template, can be used as a feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly.\n\nThe two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations. The key observation is how dot product between image formula_1 and a template formula_2 behaves when image is transformed (by such transformations as translations, rotations, scales, etc.). If transformation formula_3 is a member of a unitary group of transformations, then the following holds:\n\nformula_4\n\nIn other words, the dot product of transformed image and a template is equal to the dot product of original image and inversely transformed template. For instance, for image rotated by 90 degrees, the inversely transformed template would be rotated by -90 degrees.\n\nConsider the set of dot products of an image formula_1 to all possible transformations of template: formula_6. If one applies a transformation formula_3 to formula_1, the set would become formula_9. But because of the property (1), this is equal to formula_10. The set formula_11 is equal to just the set of all elements in formula_12. To see this, note that every formula_13 is in formula_12 due to the closure property of groups, and for every formula_15 in G there exist its prototype formula_16 such as formula_17 (namely, formula_18). Thus, formula_19. One can see that the set of dot products remains the same despite that a transformation was applied to the image! This set by itself may serve as a (very cumbersome) invariant representation of an image. More practical representations can be derived from it.\n\nIn the introductory section, it was claimed that M-Theory allows to learn invariant representations. This is because templates and their transformed versions can be learned from visual experience - by exposing the system to sequences of transformations of objects. It is plausible that similar visual experiences occur in early period of human life, for instance when infants twiddle toys in their hands. Because templates may be totally unrelated to images that the system later will try to classify, memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life. However, as it is shown later, for some kinds of transformations, specific templates are needed.\n\nTo implement the ideas described in previous sections, one need to know how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.\n\nOrbit formula_20 is a set of images formula_21 generated from a single image formula_1 under the action of the group formula_23.\n\nIn other words, images of an object and of its transformations correspond to an orbit formula_20. If two orbits have a point in common they are identical everywhere, i.e. an orbit is an invariant and unique representation of an image. So, two images are called equivalent when they belong to the same orbit: formula_25 if formula_26 such that formula_27. Conversely, two orbits are different if none of the images in one orbit coincide with any image in the other.\n\nA natural question arises: how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution formula_28 induced by the group's action on images formula_1 (formula_21 can be seen as a realization of a random variable).\nThis probability distribution formula_28 can be almost uniquely characterized by formula_32 one-dimensional probability distributions formula_33 induced by the (one-dimensional) results of projections formula_34, where formula_35 are a set of templates (randomly chosen images) (based on the Cramer-Wold theorem and concentration of measures).\nConsider formula_36 images formula_37. Let formula_38 , where formula_39 is a universal constant. Then\n\nformula_40\n\nwith probability formula_41, for all formula_42 formula_43 formula_44.\nThis result (informally) says that an approximately invariant and unique representation of an image formula_1 can be obtained from the estimates of formula_32 1-D probability distributions formula_47 for formula_48. The number formula_32 of projections needed to discriminate formula_36 orbits, induced by formula_36 images, up to precision formula_52 (and with confidence formula_41) is formula_38, where formula_39 is a universal constant.\nTo classify an image, the following \"recipe\" can be used:\n1. Memorize a set of images/objects called templates;\n2. Memorize observed transformations for each template;\n3. Compute dot products of its transformations with image;\n4. Compute histogram of the resulting values, called \"signature\" of the image;\n5. Compare the obtained histogram with signatures stored in memory.\n\nEstimates of such one-dimensional probability density functions (PDFs) formula_33 can be written in terms of histograms as formula_57, where formula_58 is a set of nonlinear functions. These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation.\n\nIn the \"recipe\" for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is compact.\n\nSuch groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are locally compact. For locally compact groups, invariance is achievable within certain range of transformations.\n\nAssume that formula_59 is a subset of transformations from formula_12 for which the transformed patterns exist in memory. For an image formula_1 and template formula_62, assume that formula_63 is equal to zero everywhere except some subset of formula_59. This subset is called support of formula_63 and denoted as formula_66. It can be proven that if for a transformation formula_16, support set will also lie within formula_68, then signature of formula_1 is invariant with respect to formula_16. This theorem determines the range of transformations for which invariance is guaranteed to hold.\n\nOne can see that the smaller is formula_66, the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small formula_72 for a generic image. This property is called localization: templates are sensitive only to images within a small range of transformations. Note that although minimizing formula_72 is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates: Gabor functions.\n\nThe desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex. The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon.\n\nMany interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized.\n\nAs it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects. More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition. Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture.\n\nThe previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well.\n\nFirstly, hierarchical architectures best accomplish the goal of ‘parsing’ a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchical architectures, representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy.\n\nSecondly, hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts. This facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts. As a result, sample complexity of learning compositional concepts may be greatly reduced.\n\nFinally, hierarchical architectures have better tolerance to clutter. Clutter problem arises when the target object is in front of a non-uniform background, which functions as a distractor for the visual task. Hierarchical architecture provides signatures for parts of target objects, which do not include parts of background and are not affected by background variations.\n\nIn hierarchical architectures, one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, as in the case of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide \"covariant\" rather than invariant, signatures. The property of covariance can be written as formula_74, where formula_75 is a layer, formula_76 is the signature of image on that layer, and formula_77 stands for \"distribution of values of the expression for all formula_78\".\n\nM-theory is based on a quantitative theory of the ventral stream of visual cortex. Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art machine vision systems that usually require a lot of data in order to recognize objects. Prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters. No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in V1 and V2. Although some of the systems inspired - to various degrees - by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases.\n\nM-theory learning framework employs a novel hypothesis about the main computational function of the ventral stream: the representation of new objects/images in terms of a signature, which is invariant to transformations learned during visual experience. This allows recognition from very few labeled examples - in the limit, just one.\n\nNeuroscience suggests that natural functionals for a neuron to compute is a high-dimensional dot product between an \"image patch\" and another image patch (called template) \nwhich is stored in terms of synaptic weights (synapses per neuron). The standard computational model of a neuron is based on a dot product and a threshold. Another important feature of the visual cortex is that it consists of simple and complex cells. This idea was originally proposed by Hubel and Wiesel. M-theory employs this idea. Simple cells compute dot products of an image and transformations of templates formula_79 for formula_80 (formula_81 is a number of simple cells). Complex cells are responsible for pooling and computing empirical histograms or statistical moments of it. The following formula for constructing histogram can be computed by neurons:\n\nformula_82\n\nwhere formula_83 is a smooth version of step function, formula_84 is the width of a histogram bin, and formula_36 is the number of the bin.\n\nIn authors applied M-theory to unconstrained face recognition in natural photographs. Unlike the DAR (detection, alignment, and recognition) method, which handles clutter by detecting objects and cropping closely around them so that very little background remains, this approach accomplishes detection and alignment implicitly by storing transformations of training images (templates) rather than explicitly detecting and aligning or cropping faces at test time. This system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems. \nThe resulting end-to-end system achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult: significantly jittered (misaligned) version of LFW and SUFR-W (for example, the model's accuracy in the LFW \"unaligned & no outside data used\" category is 87.55±1.41% compared to state-of-the-art APEM (adaptive probabilistic elastic matching): 81.70±1.78%).\n\nThe theory was also applied to a range of recognition tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) scene understanding tasks that requires the recognition of both shape-based as well as texture-based objects (on StreetScenes data set). The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems constellation models, the hierarchical SVM-based face-detection system. A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.\n\nThis theory can also be extended for the speech recognition domain.\nAs an example, in an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.\n", "related": "NONE"}
{"id": "44390264", "url": "https://en.wikipedia.org/wiki?curid=44390264", "title": "OrCam device", "text": "OrCam device\n\nOrCam devices such as \"OrCam MyEye\" are portable, artificial vision devices that allow visually impaired people to understand text and identify objects through audio feedback, describing what they are unable to see.\n\nReuters described an important part of how it works as \"a wireless smartcamera\" which, when attached outside eyeglass frames, can read and verbalize text, and also supermarket barcodes. This information is converted to spoken words and entered \"into the user’s ear.\" Face-recognition is also part of OrCam's feature set.\n\nOrCam Technologies Ltd has created three devices; OrCam MyEye 2.0, OrCam MyEye 1, and OrCam MyReader.\n\nOrCam My Eye 2.0: \n- OrCam debuted the second-generation model, the OrCam MyEye 2.0 in December 2017.\n- About the size of a finger, the MyEye 2.0 is battery-powered, and has been compressed into a self-contained device.\n- The device snaps onto any eyeglass frame magnetically.\n- Orcam 2.0 is smaller and lighter (only 22.5 grams/0.8 ounces) with more functionality to restore independence to the visually impaired.\n\nJAMA Ophthalmology:\nIn 2016 JAMA Ophthalmology conducted a study involving 12 legally blind participants to evaluate the usefulness of a portable artificial vision device (OrCam) for patients with low vision. The results showed that the OrCam device improved the patient's ability to perform tasks simulating those of daily living, such as reading a message on an electronic device, a newspaper article or a menu.\n\nWills Eye:\nWills Eye was a clinical study designed to measure the impact of the OrCam device on the quality of life of patients with End-stage Glaucoma. The conclusion was that OrCam, a novel artificial vision device using a mini-camera mounted on eyeglasses, allowed legally blind patients with end-stage glaucoma to read independently, subsequently improving their quality of life.\n\nThe New York Times described how a pre-release OrCam device was used by a Coloboma-impaired employee of the device's developer in 2013 for grocery shopping. It was the small size of the prototype rather than the functionality that gave her added mobility in an Israeli store's aisles.\n\nAdded life-enhancement was described: \"to both recognize and speak .. bus numbers ..\ntraffic lights.\"\n\nIn contrast to an early version of Google Glass, which \"failed ... because .. Glass wearers were ..mocked,\", early OrCam devices used designs that \"clip unobtrusively on your shirt or perhaps your belt.\"\n\nIn addition, it does not record sounds or images, what was called \"the privacy puzzle that stumped Google.\n\nOne 2018 technology reviewer wrote that he wished it had a headphone jack \"so it would be less disruptive in places where others are working.\" An attempt was made to use bone conduction.\n\nIn 2018 a team headed by New York (State) Assemblyman Dov Hikind introduced use of OrCam devices to ten individuals screened for what he termed \"new Israeli technology that really makes a difference to the blind.\"\n\nAlthough not the first USA success, it was more focused than a publicly funded project that was authorized in 2016 by a California government agency. Also in 2016 the Chicago Lighthouse for the Blind demonstrated its use.\n\nIn the area of hardware, miniaturization has been quite important, but one major area, software, was mentioned by Assemblyman Hikind, and reported by The Times of Israel\n\nIn addition to reading printed text, it can also aid in \"seeing\" what is on a television or computer screen. Although OrCam can't help with handwritten information, it can reuse information, the basis of recognizing \"US currency, and even faces.\"\n\nWhile early language support was for English, French, German, Hebrew and Spanish, others now available include Danish, Dutch, Finnish, Italian, Norwegian, Portuguese and Swedish.\n\nOrCam Technologies Ltd was founded in 2010 by Professor Amnon Shashua and Ziv Aviram. Before co-founding OrCam, the two in 1999 co-founded Mobileye, an Israeli company that develops vision-based advanced driver-assistance systems (ADAS) providing warnings for collision prevention and mitigation, which was acquired by Intel for $15.3 billion in 2017.\nOrCam launched OrCam MyEye in 2013 after years of development and testing, and began selling it commercially in \n2015.\n\nIn its early years, the company raised $22 million, $6 million of which came from Intel Capital. By 2014, Intel, which was also investing in \"Google Glass\", had invested $15 million in Orcam. In March 2017, OrCam had raised $41 million in capital, making it worth $600 million.\n\nOne outcome of initial marketing in the USA, Canada and Great Britain is that they've \"reached a deal with the California Department of Rehabilitation, ...qualifying blind and visually impaired state residents.\"\nOrCam Technologies Ltd. is the Israeli-based company producing these OrCam devices, which are wearable artificial intelligence space. The company develops and manufactures assistive technology devices for individuals who are visually impaired, partially sighted, blind, print disabilities, or have other disabilities. OrCam headquarters is located in Jerusalem operating under the company name OrCam Technologies Ltd.\n\nOrCam has over 150 employees, is headquartered in Jerusalem, and has offices in New York, Toronto, and London.\n\n- 2018 Last Gadget Standing Winner\n- 2018 CES Innovation Awards Honoree in Accessible Tech\n- 2017 NAIDEX Innovation Award\n- 2016 Louise Braille Corporate Recognition Award\n- 2016 Silmo-d-Or Award\n\n- OrCam Facebook page\n- Foundation Fighting Blindness website\n- National Center for Health Statistics website\n", "related": "NONE"}
{"id": "35026656", "url": "https://en.wikipedia.org/wiki?curid=35026656", "title": "Color normalization", "text": "Color normalization\n\nColor normalization is a topic in computer vision concerned with artificial color vision and object recognition. In general, the distribution of color values in an image depends on the illumination, which may vary depending on lighting conditions, cameras, and other factors. Color normalisation allows for object recognition techniques based on colour to compensate for these variations.\n\nColor constancy is a feature of the human internal model of perception, which provides humans with the ability to assign a relatively constant color to objects even under different illumination conditions. This is helpful for object recognition as well as identification of light sources in an environment. For example, humans see an object approximately as the same color when the sun is bright or when the sun is dim.\n\nColor normalization has been used for object recognition on color images in the field of robotics, bioinformatics and general artificial intelligence, when it is important to remove all intensity values from the image while preserving color values. One example is in case of a scene shot by a surveillance camera over the day, where it is important to remove shadows or lighting changes on same color pixels and recognize the people that passed. Another example is automated screening tools used for the detection of diabetic retinopathy as well as molecular diagnosis of cancer states, where it is important to include color information during classification.\n\nThe main issue about certain applications of color normalization is that the end result looks unnatural or too distant from the original colors. In cases where there is a subtle variation between important aspects, this can be problematic. More specifically, the side effect can be that pixels become divergent and not reflect the actual color value of the image.\nA way of combating this issue is to use color normalization in combination with thresholding to correctly and consistently segment a colored image.\n\nThere is a vast array of different transformations and algorithms for achieving color normalization and a limited list is presented here. The performance of an algorithm is dependent on the task and one algorithm which performs better than another in one task might perform worse in another (no free lunch theorem). Additionally, the choice of the algorithm depends on the preferences of the user for the end-result, e.g. they may want a more natural-looking color image.\n\nThe grey world normalization makes the assumption that changes in the lighting spectrum can be modelled by three constant factors applied to the red, green and blue channels of color. More specifically, a change in illuminated color can be modelled as a scaling α, β and γ in the R, G and B color channels and as such the grey world algorithm is invariant to illumination color variations. Therefore, a constancy solution can be achieved by dividing each color channel by its average value as shown in the following formula:\n\nformula_1\n\nAs mentioned above, grey world color normalization is invariant to illuminated color variations α, β and γ, however it has one important problem: it does not account for all variations of illumination intensity and it is not dynamic; when new objects appear in the scene it fails. To solve this problem there are several variants of the grey world algorithm.\nAdditionally there is an iterative variation of the grey world normalization, however it was not found to perform significantly better.\n\nHistogram equalization is a non-linear transform which maintains pixel rank and is capable of normalizing for any monotonically increasing color transform function. It is considered to be a more powerful normalization transformation than the grey world method. The results of histogram equalization tend to have an exaggerated blue channel and look unnatural, due to the fact that in most images the distribution of the pixel values is usually more similar to a Gaussian distribution, rather than uniform.\n\nHistogram specification transforms the red, green and blue histograms to match the shapes of three specific histograms, rather than simply equalizing them. It refers to a class of image transforms which aims to obtain images of which the histograms have a desired shape.\nAs specified, firstly it is necessary to convert the image so that it has a particular histogram.\nAssume an image x. The following formula is the equalization transform of this image:\n\nformula_2\n\nThen assume wanted image z. The equalization transform of this image is:\n\nformula_3\n\nOf course formula_4 is the histogram of the output image.\nThe formula to find the inverse of the above transform is:\n\nformula_5\n\nTherefore, since images y and y' have the same equalized histogram they are actually the same image, meaning y = y' and the transform from the given image x to the wanted image z is:\n\nformula_6\n\nHistogram specification has the advantage of producing more realistic looking images, as it does not exaggerate the blue channel like histogram equalization.\n\nThe comprehensive color normalization is shown to increase localization and object classification results in combination with color indexing. It is an iterative algorithm which works in two stages. The first stage is to use the red, green and blue color space with the intensity normalized, to normalize each pixel. The second stage is to normalize each color channel separately, so that the sum of the color components is equal to one third of the number of pixels. The iterations continue until convergence, meaning no additional changes. Formally:\n\nNormalize the color image\n\nformula_7\n\nwhich consists of color vectors\n\nformula_8\n\nFor the first step explained above, compute:\n\nformula_9\n\nwhich leads to\n\nformula_10\n\nand\n\nformula_11\n\nFor the second step explained above, compute:\n\nformula_12\n\nand normalize\n\nformula_13\n\nOf course the same process is done for b' and g'. Then these two steps are repeated until the changes between iteration t and t+2 are less than some set threshold.\n\nComprehensive color normalization, just like the histogram equalization method previously mentioned, produces results that may look less natural due to the reduction in the number of color values.\n\n\n- Image Enhancement by Contrast Transform\n- Color Constancy MIT\n", "related": "NONE"}
{"id": "48661480", "url": "https://en.wikipedia.org/wiki?curid=48661480", "title": "Extensible Device Metadata", "text": "Extensible Device Metadata\n\nThe Extensible Device Metadata (XDM) specification is an open file format for embedding device-related metadata in JPEG and other common image files without breaking compatibility with ordinary image viewers. The metadata types include: depth map, camera pose, point cloud, lens model, image reliability data, and identifying info about the hardware components. This metadata can be used, for instance, to create depth effects such as a bokeh filter, recreate the exact location and position of the camera when the picture was taken, or create 3D data models of environments or objects.\n\nThe format uses XML and is based on the XMP standard. It can support multiple \"cameras\" (image sources and types) in a single image file, and each can include data about its position and orientation relative to the primary camera. A camera data structure may include an image, depth map, etc. The XDM 1.0 documentation uses JPEG as the basic model, but states that the concepts generally apply to other image-file types supported by XMP, including PNG, TIFF, and GIF.\n\nThe XDM specification is developed and maintained by a working group that includes engineers from Intel and Google. The version 1.01 specification is posted at the website xdm.org; an earlier 1.0 version was posted at the Intel website in late 2015.\n\nXDM builds upon the Depthmap Metadata specification, introduced in 2014 and used in commercial applications including Google Lens Blur and Intel RealSense Depth Enabled Photography (DEP). That original specification was designed only for depth-photography use cases. Due to changes and expansions of the data structure, and the use of different namespaces, the two standards are not compatible. Existing applications that used that older standard will not work with XDM without modifications.\n\n", "related": "\n- Extensible Metadata Platform\n\n- XDM 1.01 beta documentation\n- XDM 1.0 beta documentation\n- Adobe XMP Main Page\n- XMP Specification\n- Depthmap Metadata specification\n"}
{"id": "42003835", "url": "https://en.wikipedia.org/wiki?curid=42003835", "title": "Tango (platform)", "text": "Tango (platform)\n\nTango (formerly named Project Tango, while in testing) was an augmented reality computing platform, developed and authored by the Advanced Technology and Projects (ATAP), a skunkworks division of Google. It used computer vision to enable mobile devices, such as smartphones and tablets, to detect their position relative to the world around them without using GPS or other external signals. This allowed application developers to create user experiences that include indoor navigation, 3D mapping, physical space measurement, environmental recognition, augmented reality, and windows into a virtual world.\n\nThe first product to emerge from ATAP, Tango was developed by a team led by computer scientist Johnny Lee, a core contributor to Microsoft's Kinect. In an interview in June 2015, Lee said, \"We're developing the hardware and software technologies to help everything and everyone understand precisely where they are, anywhere.\"\n\nGoogle produced two devices to demonstrate the Tango technology: the Peanut phone and the Yellowstone 7-inch tablet. More than 3,000 of these devices had been sold as of June 2015, chiefly to researchers and software developers interested in building applications for the platform. In the summer of 2015, Qualcomm and Intel both announced that they were developing Tango reference devices as models for device manufacturers who use their mobile chipsets.\n\nAt CES, in January 2016, Google announced a partnership with Lenovo to release a consumer smartphone during the summer of 2016 to feature Tango technology marketed at consumers, noting a less than $500 price-point and a small form factor below 6.5 inches. At the same time, both companies also announced an application incubator to get applications developed to be on the device on launch.\n\nOn 15 December 2017, Google announced that they would be ending support for Tango on March 1, 2018, in favor of ARCore.\n\nTango was different from other contemporary 3D-sensing computer vision products, in that it was designed to run on a standalone mobile phone or tablet and was chiefly concerned with determining the device's position and orientation within the environment.\n\nThe software worked by integrating three types of functionality:\n- Motion-tracking: using visual features of the environment, in combination with accelerometer and gyroscope data, to closely track the device's movements in space\n- Area learning: storing environment data in a map that can be re-used later, shared with other Tango devices, and enhanced with metadata such as notes, instructions, or points of interest\n- Depth perception: detecting distances, sizes, and surfaces in the environment\n\nTogether, these generate data about the device in \"six degrees of freedom\" (3 axes of orientation plus 3 axes of position) and detailed three-dimensional information about the environment.\n\nProject Tango was also the first project to graduate from Google X in 2012 \n\nApplications on mobile devices use Tango's C and Java APIs to access this data in real time. In addition, an API was also provided for integrating Tango with the Unity game engine; this enabled the conversion or creation of games that allow the user to interact and navigate in the game space by moving and rotating a Tango device in real space. These APIs were documented on the Google developer website.\n\nTango enabled apps to track a device's position and orientation within a detailed 3D environment, and to recognize known environments. This allowed the creations of applications such as in-store navigation, visual measurement and mapping utilities, presentation and design tools, and a variety of immersive games. At Augmented World Expo 2015, Johnny Lee demonstrated a construction game that builds a virtual structure in real space, an AR showroom app that allows users to view a full-size virtual automobile and customize its features, a hybrid Nerf gun with mounted Tango screen for dodging and shooting AR monsters superimposed on reality, and a multiplayer VR app that lets multiple players converse in a virtual space where their avatar movements match their real-life movements.\n\nTango apps are distributed through Play. Google has encouraged the development of more apps with hackathons, an app contest, and promotional discounts on the development tablet.\n\nAs a platform for software developers and a model for device manufacturers, Google created two Tango devices.\n\n\"Peanut\" was the first production Tango device, released in the first quarter of 2014. It was a small Android phone with a Qualcomm MSM8974 quad-core processor and additional special hardware including a fisheye motion camera, \"RGB-IR\" camera for color image and infrared depth detection, and Movidius Vision processing units. A high-performance accelerometer and gyroscope were added after testing several competing models in the MARS lab at the University of Minnesota.\n\nSeveral hundred Peanut devices were distributed to early-access partners including university researchers in computer vision and robotics, as well as application developers and technology startups. Google stopped supporting the Peanut device in September 2015, as by then the Tango software stack had evolved beyond the versions of Android that run on the device.\n\n\"Yellowstone\" was a 7-inch tablet with full Tango functionality, released in June 2014, and sold as the Project Tango Tablet Development Kit. It featured a 2.3 GHz quad-core Nvidia Tegra K1 processor, 128GB flash memory, 1920x1200-pixel touchscreen, 4MP color camera, fisheye-lens (motion-tracking) camera, an IR projector with RGB-IR camera for integrated depth sensing, and 4G LTE connectivity. As of May 27, 2017, the Tango tablet is considered officially unsupported by Google.\n\nIn May 2014, two Peanut phones were delivered to the International Space Station to be part of a NASA project to develop autonomous robots that navigate in a variety of environments, including outer space. The soccer-ball-sized, 18-sided polyhedral SPHERES robots were developed at the NASA Ames Research Center, adjacent to the Google campus in Mountain View, California. Andres Martinez, SPHERES manager at NASA, said \"We are researching how effective [Tango's] vision-based navigation abilities are for performing localization and navigation of a mobile free flyer on ISS.\n\nAnnounced at Intel's Developer Forum in August 2015, and offered to public through a Developer Kit since January 2016. It incorporated a RealSense ZR300 camera which had optical features required for Tango, such as the fisheye camera.\n\nLenovo Phab 2 Pro was the first commercial smartphone with the Tango Technology, the device was announced at the beginning of 2016, launched in August, and available for purchase in the US in November. The Phab 2 Pro had a 6.4 inch screen, a Snapdragon 652 processor, and 64 GB of internal storage, with a rear facing 16 Megapixels camera and 8 MP front camera.\n\nAsus Zenfone AR, announced at CES 2017, was the second commercial smartphone with the Tango Technology. It ran Tango AR & Daydream VR on Snapdragon 821, with 6GB or 8GB of RAM and 128 or 256GB of internal memory depending on the configuration.\n\nIn creating Project Tango, Google collaborated with developers in nine countries and several organizations including Bosch, Bsquare, CompalComm, ETH Zurich, Flyby Media, George Washington University, MMSolutions, Movidius, University of Minnesota MARS Lab, JPL Computer Vision Group, OLogic, OmniVision, Open Source Robotics Foundation, Paracosm, Sunny Optical Technology, PMD Technologies, Mantis Vision, Prizmiq, Intermodalics and SagivTech.\n\nPartnerships were announced with companies that include apelab, Autodesk, Aisle411, Bosch, Defective Studios, Durovis (Dive), Infineon, JPL, Left Field Labs, Legacy Games, Limbic, moBack, NVYVE, OmniVision, Otherworld Interactive, PMD Technologies, Sagivtech, SideKick, Speck Design, Stinkdigital, and Inuitive.\n\n", "related": "\n- Computer vision\n- Vision processing unit\n- RGB\n- Simultaneous localization and mapping\n\n- Project Tango developer site (API and documentation).\n- Project Tango developer community.\n- Project Tango Smartphone Development Platform page at Qualcomm.\n- \"Intel Expands Developer Opportunities As Computing Expands Across All Areas of Peoples' Lives\". Intel Developer Forum. 20 August 2015.\n- \"Google and Intel bring RealSense to phones with Project Tango dev kit\". Engadget. 18 August 2015.\n- \"Google I/O 2015 - Project Tango - Mobile 3D tracking and perception\". Johnny Lee, Google I/O 2015. YouTube. 29 May 2015.\n- \"Project Tango Concepts\". Johnny Lee, YouTube. 21 April 2015.\n- \"Project Tango Tablet Teardown\". iFixit. 15 August 2014.\n"}
{"id": "48715673", "url": "https://en.wikipedia.org/wiki?curid=48715673", "title": "Saliency map", "text": "Saliency map\n\nIn computer vision, a saliency map is an image that shows each pixel's unique quality. The goal of a saliency map is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. For example, if a pixel has a high grey level or other unique color quality in a color image, that pixel's quality will show in the saliency map and in an obvious way. Saliency is a kind of image segmentation.\n\nSaliency estimation may be viewed as an instance of image segmentation. In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels, also known as superpixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n\nFirst, we should calculate the distance of each pixel to the rest of pixels in the same frame:\n\nformula_2 is the value of pixel formula_3, in the range of [0,255]. The following equation is the expanded form of this equation. \nWhere N is the total number of pixels in the current frame. Then we can further restructure our formula. We put the value that has same I together.\nWhere is the frequency of . And the value of n belongs to [0,255]. The frequencies is expressed in the form of histogram, and the computational time of histogram is time complexity.\n\nThis saliency map algorithm has time complexity. Since the computational time of histogram is time complexity which N is the number of pixel's number of a frame. Besides, the minus part and multiply part of this equation need 256 times operation. Consequently, the time complexity of this algorithm is which equals to .\n\nAll of the following code is pseudo matlab code. First, read Data from video sequences.\n\nAfter we read data, we do superpixel process to each frame.\nSpnum1 and Spnum2 represent the pixel number of current frame and previous pixel.\n\nThen we calculate the color distance of each pixel, this process we call it contract function.\n\nAfter this two process, we will get a saliency map, and then store all of these maps into a new FileFolder.\n\nThe major difference between function one and two is the difference of contract function. If spnum1 and spnum2 both represent the current frame's pixel number, then this contract function is for the first saliency function. If spnum1 is the current frame's pixel number and spnum2 represent the previous frame's pixel number, then this contract function is for second saliency function. If we use the second contract function which using the pixel of the same frame to get center distance to get a saliency map, then we apply this saliency function to each frame and use current frame's saliency map minus previous frame's saliency map to get a new image which is the new saliency result of the third saliency function.\n\n- VLfeat: http://www.vlfeat.org/index.html\n- Saliency map at Scholarpedia\n\n", "related": "\n- Image segmentation\n- Salience (neuroscience)\n"}
{"id": "44722153", "url": "https://en.wikipedia.org/wiki?curid=44722153", "title": "Underwater computer vision", "text": "Underwater computer vision\n\nUnderwater computer vision is a subfield of computer vision. In recent years, with the development of underwater vehicles ( ROV, AUV, gliders), the need to be able to record and process huge amounts of information has become increasingly important. Applications range from inspection of underwater structures for the offshore industry to the identification and counting of fishes for biological research. However, no matter how big the impact of this technology can be to industry and research, it still is in a very early stage of development compared to traditional computer vision. One reason for this is that, the moment the camera goes into the water, a whole new set of challenges appear. On one hand, cameras have to be made waterproof, marine corrosion deteriorates materials quickly and access and modifications to experimental setups are costly, both in time and resources. On the other hand, the physical properties of the water make light behave differently, changing the appearance of a same object with variations of depth, organic material, currents, temperature etc.\n\n- Seafloor Survey\n- Vehicle Navigation and Positioning\n- Biological monitoring\n- Video Mosaics as Visual Navigation Maps\n- Pipeline Inspection\n- Wreckage visualization\n- Maintenance of Underwater Structures\n- Drowning detection, e.g. pool safety\n\nIn air, light comes from the whole hemisphere on cloudy days, and is dominated by the sun. In water lightning comes from a finite cone above the scene. This phenomenon is called Snell's window.\n\nAs opposite to air, water attenuates light exponentially. This results in hazy images with very low contrast. The main reasons for light attenuation are light absorption (where energy is removed from the light) and light scattering, by which the direction of light is changed. Light scattering can further be divided into forward scattering, which results in an increased blurriness and backward scattering that limits the contrast and is responsible for the characteristic veil of underwater images. Both scattering and attenuation are heavily influenced by the amount of organic matter dissolved in the water.\n\nAnother problem with water is that light attenuation is a function of the wavelength. This means that different color get attenuated faster than others, leading to color degradation. Red and orange light is the first to get attenuated, followed by yellows and greens. Blue is the less attenuated wavelength.\n\nIn high level computer vision, human structures are frequently used as image features for image matching in different applications. However, the sea bottom lacks such features, making it hard to find correspondences in two images.\n\nIn order to be able to use a camera in the water, a watertight housing is required. However, refraction will happen at the water-glass and glass-air interface due to differences in density of the materials. This has the effect of introducing a non-linear image deformation.\n\nThe motion of the vehicle presents another special challenge. Underwater vehicles are constantly moving due to currents and other phenomena. This introduces another uncertainty to algorithms, where small motions may appear in all directions. This can be specially important for video tracking. In order to reduce this problem image stabilization algorithms may be applied.\n\nImage restoration aims to model the degradation process and then invert it, obtaining the new image after solving. It is generally a complex approach that requires plenty of parameters that vary a lot between different water conditions.\n\nImage enhancement on the opposite only tries to provide a visually more appealing image without taking the physical image formation process into account. These methods are usually simpler and less computational intensive.\n\nDifferent algorithms exist that perform automatic color correction. The UCM (Unsupervised Color Correction Method), for example, does this in the following steps: It firstly reduces the color cast by equalizing the color values. Then it enhances contrast by stretching the red histogram towards the maximum and finally Saturation and Intensity components are optimized.\n\nIt is usually assumed that stereo cameras have been calibrated previously, geometrically and radiometrically. This leads to the assumption that corresponding pixels should have the same color. However this can not be guaranteed in an underwater scene, because of dispersion and backscatter as mentioned earlier. However, it is possible to model this phenomenon and create a virtual image with those effects removed\n\nIn recent years imaging sonars have become more and more accessible and gained resolution, delivering better images. Sidescan sonars are used to produce complete maps of regions of the sea floor stitching together sequences of sonar images. However, imaging sonar images often lack proper contrast and are degraded by artefacts and distortions due to noise, attitude changes of the AUV/ROV carrying the sonar or non uniform beam patterns. Another common problem with sonar computer vision is the comparatively low frame rate of sonar images.\n", "related": "NONE"}
{"id": "10531718", "url": "https://en.wikipedia.org/wiki?curid=10531718", "title": "Graph cuts in computer vision", "text": "Graph cuts in computer vision\n\nAs applied in the field of computer vision, graph cut optimization can be employed to efficiently solve a wide variety of low-level computer vision problems (\"early vision\"), such as image smoothing, the stereo correspondence problem, image segmentation, and many other computer vision problems that can be formulated in terms of energy minimization. Many of these energy minimization problems can be approximated by solving a maximum flow problem in a graph (and thus, by the max-flow min-cut theorem, define a minimal cut of the graph). Under most formulations of such problems in computer vision, the minimum energy solution corresponds to the maximum a posteriori estimate of a solution. Although many computer vision algorithms involve cutting a graph (e.g., normalized cuts), the term \"graph cuts\" is applied specifically to those models which employ a max-flow/min-cut optimization (other graph cutting algorithms may be considered as graph partitioning algorithms).\n\n\"Binary\" problems (such as denoising a binary image) can be solved exactly using this approach; problems where pixels can be labeled with more than two different labels (such as stereo correspondence, or denoising of a grayscale image) cannot be solved exactly, but solutions produced are usually near the global optimum.\n\nThe theory of graph cuts used as an optimization method was first applied in computer vision in the seminal paper by Greig, Porteous and Seheult of Durham University. Seheult and Porteous were members of Durham's much lauded statistics group of the time, lead by Julian Besag and Peter Green (statistician), with the optimisation expert Margaret Greig also notable as the first ever female member of staff of the Durham Mathematical Sciences Department. \n\nIn the Bayesian statistical context of smoothing noisy (or corrupted) images, they showed how the maximum a posteriori estimate of a binary image can be obtained \"exactly\" by maximizing the flow through an associated image network, involving the introduction of a \"source\" and \"sink\". The problem was therefore shown to be efficiently solvable. Prior to this result, \"approximate\" techniques such as simulated annealing (as proposed by the Geman brothers), or iterated conditional modes (a type of greedy algorithm as suggested by Julian Besag) were used to solve such image smoothing problems.\n\nAlthough the general formula_1-colour problem remains unsolved for formula_2 the approach of Greig, Porteous and Seheult has turned out to have wide applicability in general computer vision problems. Greig, Porteous and Seheult approaches are often applied iteratively to a sequence of binary problems, usually yielding near optimal solutions.\n\nIn 2011, C. Couprie et al. proposed a general image segmentation framework, called the \"Power Watershed\", that minimized a real-valued indicator function from [0,1] over a graph, constrained by user seeds (or unary terms) set to 0 or 1, in which the minimization of the indicator function over the graph is optimized with respect to an exponent formula_3. When formula_4, the Power Watershed is optimized by graph cuts, when formula_5 the Power Watershed is optimized by shortest paths, formula_6 is optimized by the Random walker algorithm and formula_7 is optimized by the Watershed (image processing) algorithm. In this way, the Power Watershed may be viewed as a generalization of graph cuts that provides a straightforward connection with other energy optimization segmentation/clustering algorithms.\n\n- Image: formula_8\n- Output: Segmentation (also called opacity) formula_9 (soft segmentation). For hard segmentation formula_10\n- Energy function: formula_11 where C is the color parameter and λ is the coherence parameter.\n- formula_12\n- Optimization: The segmentation can be estimated as a global minimum over S: formula_13\n\n- Standard Graph cuts: optimize energy function over the segmentation (unknown S value).\n- Iterated Graph cuts:\n1. First step optimizes over the color parameters using K-means.\n2. Second step performs the usual graph cuts algorithm.\n\n- Dynamic graph cuts:Allows to re-run the algorithm much faster after modifying the problem (e.g. after new seeds have been added by a user).\n\nwhere the energy formula_15 is composed of two different models (formula_16 and formula_17):\n\nformula_16 — unary term describing the likelihood of each color.\n- This term can be modeled using different local (e.g. texons) or global (e.g. histograms, GMMs, Adaboost likelihood) approaches that are described below.\n\n- We use intensities of pixels marked as seeds to get histograms for object (foreground) and background intensity distributions: P(I|O) and P(I|B).\n- Then, we use these histograms to set the regional penalties as negative log-likelihoods.\n\n- We usually use two distributions: one for background modelling and another for foreground pixels.\n- Use a Gaussian mixture model (with 5–8 components) to model those 2 distributions.\n- Goal: Try to pull apart those two distributions.\n\n- A texon (or texton) is a set of pixels that has certain characteristics and is repeated in an image.\n- Steps:\n1. Determine a good natural scale for the texture elements.\n2. Compute non-parametric statistics of the model-interior texons, either on intensity or on Gabor filter responses.\n\n- Examples:\n- Deformable-model based Textured Object Segmentation\n- Contour and Texture Analysis for Image Segmentation\n\nformula_17 — binary term describing the coherence between neighborhood pixels.\n- In practice, pixels are defined as neighbors if they are adjacent either horizontally, vertically or diagonally (4 way connectivity or 8 way connectivity for 2D images).\n- Costs can be based on local intensity gradient, Laplacian zero-crossing, gradient direction, color mixture model...\n- Different energy functions have been defined:\n- Standard Markov random field: Associate a penalty to disagreeing pixels by evaluating the difference between their segmentation label (crude measure of the length of the boundaries). See Boykov and Kolmogorov ICCV 2003\n- Conditional random field: If the color is very different, it might be a good place to put a boundary. See Lafferty et al. 2001; Kumar and Hebert 2003\n\nGraph cuts methods have become popular alternatives to the level set-based approaches for optimizing the location of a contour (see for an extensive comparison). However, graph cut approaches have been criticized in the literature for several issues:\n- Metrication artifacts: When an image is represented by a 4-connected lattice, graph cuts methods can exhibit unwanted \"blockiness\" artifacts. Various methods have been proposed for addressing this issue, such as using additional edges or by formulating the max-flow problem in continuous space.\n- Shrinking bias: Since graph cuts finds a minimum cut, the algorithm can be biased toward producing a small contour. For example, the algorithm is not well-suited for segmentation of thin objects like blood vessels (see for a proposed fix).\n- Multiple labels: Graph cuts is only able to find a global optimum for binary labeling (i.e., two labels) problems, such as foreground/background image segmentation. Extensions have been proposed that can find approximate solutions for multilabel graph cuts problems.\n- Memory: the memory usage of graph cuts increase quickly as the image size increase. As an illustration, the Boykov-Kolmogorov max-flow algorithm v2.2 allocates formula_20 bytes (formula_21 and formula_22 are respectively the number of nodes and edges in the graph). Nevertheless, some amount of work has been recently done in this direction for reducing the graphs before the maximum-flow computation.\n\n- Minimization is done using a standard minimum cut algorithm.\n- Due to the Max-flow min-cut theorem we can solve energy minimization by maximizing the flow over the network. The Max Flow problem consists of a directed graph with edges labeled with capacities, and there are two distinct nodes: the source and the sink. Intuitively, it's easy to see that the maximum flow is determined by the bottleneck.\n\nThe Boykov-Kolmogorov algorithm is an efficient way to compute the max-flow for computer vision related graph.\n\nThe Sim Cut algorithm approximates the graph cut. The algorithm implements a solution by simulation of an electrical network. This is the approach suggested by Cederbaum's maximum flow theorem. Acceleration of the algorithm is possible through parallel computing.\n\n- http://pub.ist.ac.at/~vnk/software.html — An implementation of the maxflow algorithm described in \"An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Computer Vision\" by Vladimir Kolmogorov\n- http://vision.csd.uwo.ca/code/ — some graph cut libraries and MATLAB wrappers\n- http://gridcut.com/ — fast multi-core max-flow/min-cut solver optimized for grid-like graphs\n- http://virtualscalpel.com/ — An implementation of the Sim Cut; an algorithm for computing an approximate solution of the minimum s-t cut in a massively parallel manner.\n", "related": "NONE"}
{"id": "155555", "url": "https://en.wikipedia.org/wiki?curid=155555", "title": "Image registration", "text": "Image registration\n\nImage registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements.\n\nImage registration or image alignment algorithms can be classified into intensity-based and feature-based. One of the images is referred to as the \"moving\" or \"source\" and the others are referred to as the \"target\", \"fixed\" or \"sensed\" images. Image registration involves spatially transforming the source/moving image(s) to align with the target image. The reference frame in the target image is stationary, while the other datasets are transformed to match to the target. Intensity-based methods compare intensity patterns in images via correlation metrics, while feature-based methods find correspondence between image features such as points, lines, and contours. Intensity-based methods register entire images or sub-images. If sub-images are registered, centers of corresponding sub images are treated as corresponding feature points. Feature-based methods establish a correspondence between a number of especially distinct points in images. Knowing the correspondence between a number of points in images, a geometrical transformation is then determined to map the target image to the reference images, thereby establishing point-by-point correspondence between the reference and target images. Methods combining intensity-based and feature-based information have also been developed.\n\nImage registration algorithms can also be classified according to the transformation models they use to relate the target image space to the reference image space. The first broad category of transformation models includes linear transformations, which include rotation, scaling, translation, and other affine transforms. Linear transformations are global in nature, thus, they cannot model local geometric differences between images.\n\nThe second category of transformations allow 'elastic' or 'nonrigid' transformations. These transformations are capable of locally warping the target image to align with the reference image. Nonrigid transformations include radial basis functions (thin-plate or surface splines, multiquadrics, and compactly-supported transformations), physical continuum models (viscous fluids), and large deformation models (diffeomorphisms).\n\nTransformations are commonly described by a parametrization, where the model dictates the number of parameters. For instance, the translation of a full image can be described by a single parameter, a translation vector. These models are called parametric models. Non-parametric models on the other hand, do not follow any parameterization, allowing each image element to be displaced arbitrarily.\n\nThere are a number of programs that implement both estimation and application of a warp-field. It is a part of the SPM and AIR programs.\n\nAlternatively, many advanced methods for spatial normalization are building on structure preserving transformations homeomorphisms and diffeomorphisms since they carry smooth submanifolds smoothly during transformation. Diffeomorphisms are generated in the modern field of Computational Anatomy based on flows since diffeomorphisms are not additive although they form a group, but a group under the law of function composition. For this reason, flows which generalize the ideas of additive groups allow for generating large deformations that preserve topology, providing 1-1 and onto transformations. Computational methods for generating such transformation are often called LDDMM which provide flows of diffeomorphisms as the main computational tool for connecting coordinate systems corresponding to the geodesic flows of Computational Anatomy.\n\nThere are a number of programs which generate diffeomorphic transformations of coordinates via diffeomorphic mapping including MRI Studio and MRI Cloud.org\n\nSpatial methods operate in the image domain, matching intensity patterns or features in images. Some of the feature matching algorithms are outgrowths of traditional techniques for performing manual image registration, in which an operator chooses corresponding control points (CP) in images. When the number of control points exceeds the minimum required to define the appropriate transformation model, iterative algorithms like RANSAC can be used to robustly estimate the parameters of a particular transformation type (e.g. affine) for registration of the images.\n\nFrequency-domain methods find the transformation parameters for registration of the images while working in the transform domain. Such methods work for simple transformations, such as translation, rotation, and scaling. Applying the phase correlation method to a pair of images produces a third image which contains a single peak. The location of this peak corresponds to the relative translation between the images. Unlike many spatial-domain algorithms, the phase correlation method is resilient to noise, occlusions, and other defects typical of medical or satellite images. Additionally, the phase correlation uses the fast Fourier transform to compute the cross-correlation between the two images, generally resulting in large performance gains. The method can be extended to determine rotation and scaling differences between two images by first converting the images to log-polar coordinates. Due to properties of the Fourier transform, the rotation and scaling parameters can be determined in a manner invariant to translation.\n\nAnother classification can be made between single-modality and multi-modality methods. Single-modality methods tend to register images in the same modality acquired by the same scanner/sensor type, while multi-modality registration methods tended to register images acquired by different scanner/sensor types.\n\nMulti-modality registration methods are often used in medical imaging as images of a subject are frequently obtained from different scanners. Examples include registration of brain CT/MRI images or whole body PET/CT images for tumor localization, registration of contrast-enhanced CT images against non-contrast-enhanced CT images for segmentation of specific parts of the anatomy, and registration of ultrasound and CT images for prostate localization in radiotherapy.\n\nRegistration methods may be classified based on the level of automation they provide. Manual, interactive, semi-automatic, and automatic methods have been developed. Manual methods provide tools to align the images manually. Interactive methods reduce user bias by performing certain key operations automatically while still relying on the user to guide the registration. Semi-automatic methods perform more of the registration steps automatically but depend on the user to verify the correctness of a registration. Automatic methods do not allow any user interaction and perform all registration steps automatically.\n\nImage similarities are broadly used in medical imaging. An image similarity measure quantifies the degree of similarity between intensity patterns in two images. The choice of an image similarity measure depends on the modality of the images to be registered. Common examples of image similarity measures include cross-correlation, mutual information, sum of squared intensity differences, and ratio image uniformity. Mutual information and normalized mutual information are the most popular image similarity measures for registration of multimodality images. Cross-correlation, sum of squared intensity differences and ratio image uniformity are commonly used for registration of images in the same modality.\n\nMany new features have been derived for cost functions based on matching methods via large deformations have emerged in the field Computational Anatomy including \n\nThere is a level of uncertainty associated with registering images that have any spatio-temporal differences. A confident registration with a measure of uncertainty is critical for many change detection applications such as medical diagnostics.\n\nIn remote sensing applications where a digital image pixel may represent several kilometers of spatial distance (such as NASA's LANDSAT imagery), an uncertain image registration can mean that a solution could be several kilometers from ground truth. Several notable papers have attempted to quantify uncertainty in image registration in order to compare results. However, many approaches to quantifying uncertainty or estimating deformations are computationally intensive or are only applicable to limited sets of spatial transformations.\n\nImage registration has applications in remote sensing (cartography updating), and computer vision. Due to the vast range of applications to which image registration can be applied, it is impossible to develop a general method that is optimized for all uses.\n\nMedical image registration (for data of the same patient taken at different points in time such as change detection or tumor monitoring) often additionally involves \"elastic\" (also known as \"nonrigid\") registration to cope with deformation of the subject (due to breathing, anatomical changes, and so forth). Nonrigid registration of medical images can also be used to register a patient's data to an anatomical atlas, such as the Talairach atlas for neuroimaging.\n\nIn astrophotography image alignment and stacking are often used to increase the signal to noise ratio for faint objects. Without stacking it may be used to produce a timelapse of events such as a planets rotation of a transit across the Sun. Using control points (automatically or manually entered), the computer performs transformations on one image to make major features align with a second or multiple images. This technique may also be used for images of different sizes, to allow images taken through different telescopes or lenses to be combined.\n\nIn cryo-TEM instability causes specimen drift and many fast acquisitions with accurate image registration is required to preserve high resolution and obtain high signal to noise images. For low SNR data, the best image registration is achieved by cross-correlating all permutations of images in an image stack. \n\nImage registration is an essential part of panoramic image creation. There are many different techniques that can be implemented in real time and run on embedded devices like cameras and camera-phones.\n\n", "related": "\n- Computational Anatomy\n- Correspondence problem\n- Georeferencing\n- Image rectification\n- Inverse consistency\n- Point set registration\n- Rubbersheeting\n- Spatial normalization\n\n- Richard Szeliski, Image Alignment and Stitching: A Tutorial. Foundations and Trends in Computer Graphics and Computer Vision, 2:1-104, 2006.\n- B. Fischer, J. Modersitzki: Ill-posed medicine – an introduction to image registration. Inverse Problems, 24:1–19, 2008\n- Barbara Zitová, Jan Flusser: Image registration methods: a survey. Image Vision Comput. 21(11): 977-1000 (2003).\n- C. Je and H.-M. Park. Optimized Hierarchical Block Matching for Fast and Accurate Image Registration. Signal Processing: Image Communication, Volume 28, Issue 7, pp. 779–791, August, 2013.\n- Registering Multimodal MRI Images using Matlab.\n- elastix: a toolbox for rigid and nonrigid registration of images.\n- niftyreg: a toolbox for doing near real-time robust rigid, affine (using block matching) and non-rigid image registration (using a refactored version of the free form deformation algorithm).\n- Image Registration techniques using MATLAB\n"}
{"id": "5104401", "url": "https://en.wikipedia.org/wiki?curid=5104401", "title": "Outline of computer vision", "text": "Outline of computer vision\n\nThe following outline is provided as an overview of and topical guide to computer vision:\n\nComputer vision – interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. Computer vision tasks include methods for acquiring digital images (through image sensors), image processing, and image analysis, to reach an understanding of digital images. In general, it deals with the extraction of high-dimensional data from the real world in order to produce numerical or symbolic information that the computer can interpret. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images.\n\n- Computer stereo vision\n- Underwater computer vision\n\nHistory of computer vision\n\n- Image denoising\n- Image histogram\n- Inpainting\n- Histogram equalization\n- Tone mapping\n- Retinex\n- Gamma correction\n- Anisotropic diffusion (Perona–Malik equation)\n\n- Affine transform\n- Homography (computer vision)\n- Hough transform\n- Radon transform\n- Walsh–Hadamard transform\n\n- Image compression\n- Filter bank\n- Gabor filter\n- JPEG 2000\n- Adaptive filtering\n\n- Visual perception\n- Human visual system model\n- Color matching function\n- Color space\n- Color appearance model\n- Color management system\n- Color mapping\n- Color model\n- Color profile\n\n- Active contour\n- Blob detection\n- Canny edge detector\n- Contour detection\n- Edge detection\n- Edge linking\n- Harris Corner Detector\n- Histogram of oriented gradients (HOG)\n- Random sample consensus (RANSAC)\n- Scale-invariant feature transform (SIFT)\n\n- Bundle adjustment\n- Articulated body pose estimation (BoPoE)\n- Direct linear transformation (DLT)\n- Epipolar geometry\n- Fundamental matrix\n- Pinhole camera model\n- Projective geometry\n- Trifocal tensor\n\n- Active appearance model (AAM)\n- Cross-correlation\n- Geometric hashing\n- Graph cut segmentation\n- Least squares estimation\n- Image pyramid\n- Image segmentation\n- Level-set method\n- Markov random fields\n- Medial axis\n- Motion field\n- Motion vector\n- Multispectral imaging\n- Normalized cut segmentation\n- Optical flow\n- Particle filtering\n- Scale space\n\n- Object recognition\n- Scale-invariant feature transform (SIFT)\n- Gesture recognition\n- Bag-of-words model in computer vision\n- Kadir–Brady saliency detector\n- Eigenface\n\n- 5DX\n- Aphelion (software)\n- Microsoft PixelSense\n- Poseidon drowning detection system\n- Visage SDK\n\n- 3D reconstruction from multiple images\n- Audio-visual speech recognition\n- Augmented reality\n- Augmented reality-assisted surgery\n- Automated optical inspection\n- Automatic image annotation\n- Automatic number plate recognition\n- Automatic target recognition\n- Check weigher\n- Closed-circuit television\n- Computer stereo vision\n- Contextual image classification\n- DARPA LAGR Program\n- Digital video fingerprinting\n- Document mosaicing\n- Facial recognition systems\n- GazoPa\n- Geometric feature learning\n- Gesture recognition\n- Image collection exploration\n- Image retrieval\n- Content-based image retrieval\n- Reverse image search\n- Image-based modeling and rendering\n- Integrated mail processing\n- Iris recognition\n- Machine vision\n- Mobile mapping\n- Navigation system components for:\n- Autonomous cars\n- Mobile robots\n- Object detection\n- Optical braille recognition\n- Optical character recognition\n- Intelligent character recognition\n- Pedestrian detection\n- People counter\n- Physical computing\n- Red light camera\n- Remote sensing\n- Smart camera\n- Traffic enforcement camera\n- Traffic sign recognition\n- Vehicle infrastructure integration\n- Velocity Moments\n- Video content analysis\n- View synthesis\n- Visual sensor network\n- Visual Word\n- Water remote sensing\n\n- 3DFLOW\n- Automatix\n- Clarifai\n- Cognex Corporation\n- Diffbot\n- IBM\n- InspecVision\n- Isra Vision\n- Kinesense\n- Mobileye\n- Scantron Corporation\n- Teledyne DALSA\n- VIEW Engineering\n\n- Electronic Letters on Computer Vision and Image Analysis\n- International Journal of Computer Vision\n\n- Conference on Computer Vision and Pattern Recognition\n- European Conference on Computer Vision\n- International Conference on Computer Vision\n- International Conferences in Central Europe on Computer Graphics, Visualization and Computer Vision\n\n", "related": "\n- Outline of artificial intelligence\n- Outline of robotics\n- List of computer graphics and descriptive geometry topics\n- Virtual Design and Construction\n\n- USC Iris computer vision conference list\n- Computer vision papers on the web A complete list of papers of the most relevant computer vision conferences.\n- Computer Vision Online News, source code, datasets and job offers related to computer vision.\n- Keith Price's Annotated Computer Vision Bibliography\n- CVonline Bob Fisher's Compendium of Computer Vision.\n- British Machine Vision Association Supporting computer vision research within the UK via the BMVC and MIUA conferences, \"Annals of the BMVA\" (open-source journal), BMVA Summer School and one-day meetings\n"}
{"id": "51396023", "url": "https://en.wikipedia.org/wiki?curid=51396023", "title": "Dynamic Graphics Project", "text": "Dynamic Graphics Project\n\nThe Dynamic Graphics Project (commonly referred to as dgp) is an interdisciplinary research laboratory at the University of Toronto devoted to projects involving Computer Graphics, Computer Vision, and Human Computer Interaction. The lab began as the computer graphics research group of Computer Science Professor Leslie Mezei in 1967. Mezei invited Bill Buxton, a pioneer of human–computer interaction to join. In 1972, Ronald Baecker, another HCI pioneer joined dgp, establishing dgp as the first Canadian university group focused on computer graphics and human-computer interaction. According to csrankings.org, for the combined subfields of computer graphics, HCI, and visualization the dgp is the Number One research institution in the world. \n\nSince then, dgp has hosted many well known faculty and students in computer graphics, computer vision and HCI (e.g., Alain Fournier, Bill Reeves, Jos Stam, Demetri Terzopoulos, Marilyn Tremaine). dgp also occasionally hosts artists in residence (e.g., Oscar-winner Chris Landreth). Many past and current researchers at Autodesk (and before that Alias Wavefront) graduated after working at dgp. dgp is located in the St. George Campus of University of Toronto in the Bahen Centre for Information Technology. dgp researchers regularly publish at ACM SIGGRAPH, ACM SIGCHI and ICCV.\n\ndgp hosts the Toronto User Experience (TUX) Speaker Series and the Sanders Series Lectures .\n\n- Bill Buxton (MS 1978)\n- James McCrae (PhD 2013)\n- Dimitris Metaxas (PhD 1992)\n- Bill Reeves (MS 1976, Ph.D. 1980)\n- Jos Stam (MS 1991, Ph.D. 1995)\n", "related": "NONE"}
{"id": "40914533", "url": "https://en.wikipedia.org/wiki?curid=40914533", "title": "Vicarious (company)", "text": "Vicarious (company)\n\nVicarious is an artificial intelligence company based in the San Francisco Bay Area, California. They are using the theorized computational principles of the brain to build software that can think and learn like a human.\n\nThe company was founded in 2010 by D. Scott Phoenix and Dr. Dileep George. Before co-founding Vicarious, Phoenix was Entrepreneur in Residence at Founders Fund and CEO of Frogmetrics, a touchscreen analytics company he co-founded through the Y Combinator incubator program. Previously, Dr. George was Chief Technology Officer at Numenta, a company he co-founded with Jeff Hawkins and Donna Dubinsky (PALM, Handspring) while completing his PhD at Stanford University.\n\nThe company launched in February 2011 with funding from Founders Fund, Dustin Moskovitz, Adam D’Angelo (former Facebook CTO and co-founder of Quora), Felicis Ventures, and Palantir co-founder Joe Lonsdale. In August 2012, in its Series A round of funding, it raised an additional $15 million. The round was led by Good Ventures; Founders Fund, Open Field Capital and Zarco Investment Group also participated.\n\nThe company received $40 million in its Series B round of funding. The round was led by such notables as Mark Zuckerberg, Elon Musk, Peter Thiel, Vinod Khosla, and Ashton Kutcher. An additional undisclosed amount was later contributed by Amazon.com CEO Jeff Bezos, Yahoo! co-founder Jerry Yang, Skype co-founder Janus Friis and Salesforce.com CEO Marc Benioff.\n\nVicarious is developing machine learning software based on the computational principles of the human brain. One such software is a vision system known as the Recursive Cortical Network (RCN), it is a generative graphical visual perception system that interprets the contents of photographs and videos in a manner similar to humans. The system is powered by a balanced approach that takes sensory data, mathematics, and biological plausibility into consideration.\nOn October 22, 2013, beating CAPTCHA, Vicarious announced its model was reliably able to solve modern CAPTCHAs, with character recognition rates of 90% or better when trained on one style. However, Luis von Ahn, a pioneer of early CAPTCHA and founder of reCAPTCHA, expressed skepticism, stating: \"It's hard for me to be impressed since I see these every few months.\" He pointed out that 50 similar claims to that of Vicarious had been made since 2003. Vicarious later published their findings in peer-reviewed journal Science showing they had definitively cracked the popular Turing test.\n\nVicarious has indicated that its AI was not specifically designed to complete CAPTCHAs and its success at the task is a product of its advanced vision system. Because Vicarious's algorithms are based on insights from the human brain, it is also able to recognize photographs, videos, and other visual data.\n\nThe Recursive Cortical Network uses a Markov Random Field to segment images (similar to the visual cortex) and process it through a hierarchy consisting of lateral connections. Thanks to the architecture, the model has extreme data efficiency (almost 900,000x more data efficient than traditional deep neural networks). The RCN is part of a model that incorporates a causal model and concept induction system recently created by Vicarious.\n\n", "related": "\n- Artificial intelligence\n- Glossary of artificial intelligence\n"}
{"id": "40671192", "url": "https://en.wikipedia.org/wiki?curid=40671192", "title": "Umoove", "text": "Umoove\n\nUmoove is a high tech startup company that has developed and patented a software-only face and eye tracking technology. The idea was first conceived as an attempt to aid people with disabilities but has since evolved. The only compatibility qualification for tablet computers and smartphones to run Umoove software is a front-facing camera. \nUmoove headquarters are in Israel on Jerusalem’s Har Hotzvim.\n\nUmoove has 15 employees and received two million dollars in financing in 2012. The company's original founders invested around $800,000 to start the business in 2010.\nIn 2013 Umoove was named one of the top three most promising Israeli start ups by Newsgeeks magazine.\nThe company also participated in the 2013 LeWeb conference in Paris, France, where innovative technology startups are showcased.\n\nThe technology uses information extracted from previous frames, such as the angle of the user's head to predict where to look for facial targets in the next frame. This anticipation minimizes the amount of computation needed to scan each image.\nUmoove accounts for variances in environment, lighting conditions and user hand shake/movement. The technology is designed to provide a consistent experience, whether you're in a brightly lit area or a darkened basement, and to work fluidly between them by adapting its processing when it detects color and brightness shifts. It uses an active stabilization technique to filter out natural body movements from an unstable camera in order to minimize false-positive motion detection.\n\nRunning the Umoove software on a Samsung Galaxy S3 is said to take up only 2% CPU.\nUmoove works exclusively with software and there is no hardware add-on necessary. It can be run on any smartphone or tablet computer that has a front-facing camera. Umoove claims that even a low-quality camera on an old device will run their software flawlessly.\n\nIn January 2014 Umoove released its first game onto the app store. The Umoove Experience game lets users control where they are 'flying' in the game through simple gestures and motions with their head. The avatar will basically go toward wherever the user looks. The game was created to showcase the technology for game developers but that did not stop some from criticizing its simplicity. Umoove also announced that they raised another one million dollars and that they are opening offices in Silicon Valley, California.\n\nIn February 2014, Umoove announced that their face-tracking software development kit is available for Android developers as well as iOS.\n\nThe Umoove Experience garnered mostly positive reviews from bloggers and mainstream media with some predicting that it could be the future of mobile gaming. Mashable wrote that Umoove's technology could be the emergence of gesture recognition technology in the mobile space, similar to Kinect with console gaming and what Leap Motion has done with desktop computers.\n\nSome, however, remain skeptical. CNET, for example, did not give the game a positive review and called the eye tracking technology ‘freaky but cool’. They also noted that pioneering technologies have been known to fall short of expectations, citing Apple Inc’s Siri as an example. The technology blog GigaOM said that the Umoove Experience is ’awesome’ and technology evangelist Robert Scoble has called Umoove \"brilliant\".\n\nIn January 2015, Umoove released uHealth, a mobile application that uses eye tracking game-like exercise to challenge the user's ability to be attentive, continuously focus, follow commands and avoid distractions. The app is designed in the form of two games, one to improve attention and another that hones focus.\nuHealth is a training tool, not a diagnostic. Umoove has stated that they want to use their technology for diagnosing neurological disorders but this will depend on clinical tests and FDA approval. The company cites the direct relationship between eye movements and brain activity as well as various vision based therapies have been backed by many scientific studies conducted over the past decades. uHealth is the first time this type of therapy is delivered right to the end user through a simple download.\n\nIn March 2013 there were rumors on the internet that Umoove would be the functioning software embedded into the Samsung Galaxy S4, which was due to launch that month. This rumor was perpetrated by, among others, New York Times, Techcrunch and Yahoo. \nOnce Samsung launched without the Umoove technology rumors about a potential collaboration with Apple Inc hit the web. It has been said that due to the fact that Apple Inc is losing market share and stock value to Samsung they will be more aggressive and eye tracking is a logical place to make that move.\n", "related": "NONE"}
{"id": "10019306", "url": "https://en.wikipedia.org/wiki?curid=10019306", "title": "Digital graffiti", "text": "Digital graffiti\n\nDigital graffiti is the act of creating graffiti art using a computer vision system. Various groups and companies have pioneered digital graffiti since technology advances made it possible. Most notably is the Graffiti Research Lab based in the US with their L.A.S.E.R. Tag system.\n\nInspired by the New York laser graffiti movement, in 2008 the first commercially available digital graffiti wall was produced by Luma, named the YrWall. A specially adapted spray can emits IR light instead of paint, which is then tracked by a computer vision system to recreate the \"sprayed\" image onto the wall using a projector.\n\nAny system that allows art to be created on a large scale in a similar manner to more traditional graffiti falls under the heading digital graffiti.\n\nCisco Systems has released a mobile application called [Digital Graffiti] patented by Cisco Systems, Inc. to allow people to place messages of varying size, color, length of time visible, and viewing distance (say visible from 20 feet away) on a physical location, say a building, an office, a cubicle, or a specific location using their augmented reality mobile application. This message alerts other visitors approaching the message coordinates by playing the Cisco chime and the mobile user's country origin filter when the app was installed. It is like a virtual yellow stickie note, that can be delivered to an individual when they arrive at a message location. Digital Graffiti leverages the Cisco MSE location server (which tracks users mobile devices and provides x, y coordinates of the mobile devices over WiFi).\n\n", "related": "\n- Virtual graffiti\n\n- GeekGaps.com\n"}
{"id": "53065847", "url": "https://en.wikipedia.org/wiki?curid=53065847", "title": "3D body scanning", "text": "3D body scanning\n\n3D body scanning is an application of various technologies such as Structured-light 3D scanner, 3D depth sensing, stereoscopic vision and others for ergonomic and anthropometric investigation of the human form as a point-cloud. The technology and practice within research has found 3D body scanning measurement extraction methodologies to be comparable to traditional anthropometric measurement techniques.\n\nWhile the technology is still developing in its application, the technology has regularly been applied in the areas of:\n- Adapted performance sportswear\n- Garment design (e.g. underwear)\n- 3D printed figurines (3D selfies)\n- 3D morphometric evaluation (i.e. for weight-loss purposes)\n- Ergonomic body measurement\n- Apparel design (e.g. helmets)\n- Body shape classification\n- Comparison of changes in body positions\n\nHowever, despite the potential for the technology to have an impact in made-to-measure and mass customisation of items with ergonomic properties, 3D body scanning has yet to reach an early adopter or early majority stage of innovation diffusion. This in part due to the lack of ergonomic theory relating to how to identify key landmarks on the body morphology. The suitability of 3D Body scanning is also context dependent as the measurements taken and the precision of the machine are highly relative to the task in hand rather than being an absolute. Additionally, a key limitation of 3D body scanning has been the upfront cost of the equipment and the required skills by which to collect data and apply it to scientific and technical fields.\n\nAlthough the process has been established for a considerable amount of time with international conferences held annually for industry and academics (e.g. the International Conference and Exhibition on 3D Body Scanning Technologies), the protocol and process of how to scan individuals is yet to be universally formalised. However, earlier research \nhas proposed a standardised protocol of body scanning based on research and practice that demonstrates how non-standardised protocol and posture significantly influences body measurements; including the hip.\n\n", "related": "\n- 3D scanner\n- Finger tracking\n- Gesture recognition\n"}
{"id": "28803763", "url": "https://en.wikipedia.org/wiki?curid=28803763", "title": "INDECT", "text": "INDECT\n\nINDECT is a research project in the area of intelligent security systems performed by several European universities since 2009 and funded by the European Union. The purpose of the project is to involve European scientists and researchers in the development of solutions to and tools for automatic threat detection through e.g. processing of CCTV camera data streams, standardization of video sequence quality for user applications, threat detection in computer networks as well as data and privacy protection.\n\nThe area of research, applied methods and techniques are described in the public deliverables which are available to the public on the project's website. Practically, all information related to the research is public. Only documents that comprise information related to financial data or information that could negatively influence the competitiveness and law enforcement capabilities of parties involved in the project are not published. This follows regulations and practices applied in EU research projects.\n\nThe main end-user of INDECT solutions are police forces and security services.\n\nThe principle of operation of the project is detecting threats and identifying source of threats, without monitoring and searching for particular citizens or groups of citizens. Then, the system operator (i.e. police officer) decides whether an intervention of services responsible for public security are required or not. Further investigation eventually leading to persons related to threats are performed, preserving the presumption of innocence, on the basis of existing procedures already used by police services and prosecutors. As it can be found in the project deliverables, INDECT does not involve storage of personal data (such as names, addresses, identity document numbers, etc.).\n\nA similar, behaviour based surveillance program was SAMURAI \"(Suspicious and Abnormal behaviour Monitoring Using a netwoRk of cAmeras & sensors for sItuation awareness enhancement)\".\n\nThe main expected results of the INDECT project are:\n- Trial of intelligent analysis of video and audio data for threat detection in urban environments,\n- Creation of tools and technology for privacy and data protection during storage and transmission of information using quantum cryptography and new methods of digital watermarking,\n- Performing computer-aided detection of threats and targeted crimes in Internet resources with privacy-protecting solutions,\n- Construction of a search engine for rapid semantic search based on watermarking of content related to child pornography and human organ trafficking,\n- Implementation of a distributed computer system that is capable of effective intelligent processing.\n\nSome media and other sources accuse INDECT of privacy abuse, collecting personal data, and keeping information from the public. Consequently, these issues have been commented and discussed by some Members of the European Parliament.\n\nAs can be seen in the project's documentation, INDECT does not involve mobile phone tracking or call interception.\n\nThe rumours about testing INDECT during 2012 UEFA European Football Championship also turned out to be false.\n\nThe mid-term review of the Seventh Framework Programme to the European Parliament strongly urges the European Commission to immediately make all documents available and to define a clear and strict mandate for the research goal, the application, and the end users of INDECT, and stresses a thorough investigation of the possible impact on fundamental rights. Nevertheless, according to Mr. Paweł Kowal, MEP, the project had the ethical review on 15 March 2011 in Brussels with the participation of ethics experts from Austria, France, Netherlands, Germany and Great Britain.\n\n", "related": "\n- Facial recognition system\n- Mass surveillance#European Union\n- Behavioral Recognition Systems, Inc.\n- Video content analysis\n- Artificial intelligence for video surveillance\n\n- Official website\n- INDECT Evaluation of Components, D9.4 WP9\n- Official INDECT YouTube channel\n- European FP7-SEC: On detecting Internet-based criminal threats with XplicoAlerts\n- Answer given by European Parliament concerning INDECT\n- List of EU Security Research projects\n"}
{"id": "230834", "url": "https://en.wikipedia.org/wiki?curid=230834", "title": "CAPTCHA", "text": "CAPTCHA\n\nA CAPTCHA (, a contrived acronym for \"completely automated public Turing test to tell computers and humans apart\") is a type of challenge–response test used in computing to determine whether or not the user is human.\n\nThe term was coined in 2003 by Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford. The most common type of CAPTCHA (displayed as Version 1.0) was first invented in 1997 by two groups working in parallel. This form of CAPTCHA requires someone to correctly evaluate and enter a sequence of letters or numbers perceptible in a distorted image displayed on their screen. Because the test is administered by a computer, in contrast to the standard Turing test that is administered by a human, a CAPTCHA is sometimes described as a reverse Turing test.\n\nThis user identification procedure has received many criticisms, especially from people with disabilities, but also from other people who feel that their everyday work is slowed down by distorted words that are difficult to read. It takes the average person approximately 10 seconds to solve a typical CAPTCHA.\n\nSince the early days of the Internet, users have wanted to make text illegible to computers. The first such people were hackers, posting about sensitive topics to Internet forums they thought were being automatically monitored on keywords. To circumvent such filters, they replaced a word with look-alike characters. \"HELLO\" could become or , as well as numerous other variants, such that a filter could not possibly detect \"all\" of them. This later became known as leetspeak.\n\nOne of the earliest commercial uses of CAPTCHAs was in the Gausebeck–Levchin test. In 2000, idrive.com began to protect its signup page with a CAPTCHA and prepared to file a patent on this seemingly novel technique. In 2001, PayPal used such tests as part of a fraud prevention strategy in which they asked humans to \"retype distorted text that programs have difficulty recognizing.\" PayPal cofounder and CTO Max Levchin helped commercialize this early use.\n\nA popular deployment of CAPTCHA technology, reCAPTCHA, was acquired by Google in 2009. In addition to preventing bot fraud for its users, Google used reCAPTCHA and CAPTCHA technology to digitize the archives of \"The New York Times\" and books from Google Books in 2011.\n\nTwo teams have claimed to be the first to invent the CAPTCHAs used widely on the web today. The first team with Mark D. Lillibridge, Martín Abadi, Krishna Bharat, and Andrei Broder, used CAPTCHAs in 1997 at AltaVista to prevent bots from adding Uniform Resource Locator (URLs) to their web search engine. Looking for a way to make their images resistant to optical character recognition (OCR) attack, the team looked at the manual of their Brother scanner, which had recommendations for improving OCR's results (similar typefaces, plain backgrounds, etc.). The team created puzzles by attempting to simulate what the manual claimed would cause bad OCR.\n\nThe second team to claim to be the first to invent CAPTCHAs with Luis von Ahn, Manuel Blum, Nicholas J. Hopper, and John Langford, first described CAPTCHAs in a 2003 publication and subsequently received much coverage in the popular press. Their notion of CAPTCHA covers any program that can distinguish humans from computers.\n\nThe controversy of inventorship has been resolved by the existence of a 1997 priority date patent application by Eran Reshef, Gili Raanan and Eilon Solan (second group) who worked at Sanctum on Application Security Firewall. Their patent application details that \"The invention is based on applying human advantage in applying sensory and cognitive skills to solving simple problems that prove to be extremely hard for computer software. Such skills include, but are not limited to processing of sensory information such as identification of objects and letters within a noisy graphical environment\". Lillibridge, Abadi, Bharat, and Broder (first group) published their patent in 1998. Both patents predate other publications by several years, though they do not use the term CAPTCHA, they describe the ideas in detail and precisely depict the graphical CAPTCHAs used in the Web today.\n\nCAPTCHAs are, by definition, fully automated, requiring little human maintenance or intervention to administer, producing benefits in cost and reliability.\n\nThe algorithm used to create the CAPTCHA must be made public, though it may be covered by a patent. This is done to demonstrate that breaking it requires the solution to a difficult problem in the field of artificial intelligence (AI) rather than just the discovery of the (secret) algorithm, which could be obtained through reverse engineering or other means.\n\nModern text-based CAPTCHAs are designed such that they require the simultaneous use of three separate abilities—invariant recognition, segmentation, and parsing—to correctly complete the task with any consistency.\n\n- Invariant recognition refers to the ability to recognize the large amount of variation in the shapes of letters. There are nearly an infinite number of versions for each character that a human brain can successfully identify. The same is not true for a computer, and teaching it to recognize all those differing formations is an extremely challenging task.\n- Segmentation, or the ability to separate one letter from another, is also made difficult in CAPTCHAs, as characters are crowded together with no white space in between.\n- Context is also critical. The CAPTCHA must be understood holistically to correctly identify each character. For example, in one segment of a CAPTCHA, a letter might look like an \"m\". Only when the whole word is taken into context does it become clear that it is a \"u\" and an \"n\".\n\nEach of these problems poses a significant challenge for a computer, even in isolation. The presence of all three at the same time is what makes CAPTCHAs difficult to solve.\n\nUnlike computers, humans excel at this type of task. While segmentation and recognition are two separate processes necessary for understanding an image for a computer, they are part of the same process for a person. For example, when an individual understands that the first letter of a CAPTCHA is an \"a\", that individual also understands where the contours of that \"a\" are, and also where it melds with the contours of the next letter. Additionally, the human brain is capable of dynamic thinking based upon context. It is able to keep multiple explanations alive and then pick the one that is the best explanation for the whole input based upon contextual clues. This also means it will not be fooled by variations in letters.\n\nWhile used mostly for security reasons, CAPTCHAs also serve as a benchmark task for artificial intelligence technologies. According to an article by Ahn, Blum and Langford, \"any program that passes the tests generated by a CAPTCHA can be used to solve a hard unsolved AI problem.\"\n\nThey argue that the advantages of using hard AI problems as a means for security are twofold. Either the problem goes unsolved and there remains a reliable method for distinguishing humans from computers, or the problem is solved and a difficult AI problem is resolved along with it. In the case of image and text based CAPTCHAs, if an AI were capable of accurately completing the task without exploiting flaws in a particular CAPTCHA design, then it would have solved the problem of developing an AI that is capable of complex object recognition in scenes.\n\nCAPTCHAs based on reading text — or other visual-perception tasks — prevent blind or visually impaired users from accessing the protected resource. However, CAPTCHAs do not have to be visual. Any hard artificial intelligence problem, such as speech recognition, can be used as the basis of a CAPTCHA. Some implementations of CAPTCHAs permit users to opt for an audio CAPTCHA, though a 2011 paper demonstrated a technique for defeating the popular schemes at the time.\n\nFor non-sighted users (for example blind users, or color blind people on a color-using test), visual CAPTCHAs present serious problems. Because CAPTCHAs are designed to be unreadable by machines, common assistive technology tools such as screen readers cannot interpret them. Since sites may use CAPTCHAs as part of the initial registration process, or even every login, this challenge can completely block access. In certain jurisdictions, site owners could become targets of litigation if they are using CAPTCHAs that discriminate against certain people with disabilities. For example, a CAPTCHA may make a site incompatible with Section 508 in the United States. In other cases, those with sight difficulties can choose to identify a word being read to them.\n\nWhile providing an audio CAPTCHA allows blind users to read the text, it still hinders those who are both blind and deaf. According to sense.org.uk, about 4% of people over 60 in the UK have both vision and hearing impairments. There are about 23,000 people in the UK who have serious vision and hearing impairments. According to The National Technical Assistance Consortium for Children and Young Adults Who Are Deaf-Blind (NTAC), the number of deafblind children in the USA increased from 9,516 to 10,471 during the period 2004 to 2012. Gallaudet University quotes 1980 to 2007 estimates which suggest upwards of 35,000 fully deafblind adults in the USA. Deafblind population estimates depend heavily on the degree of impairment used in the definition.\n\nThe use of CAPTCHA thus excludes a small number of individuals from using significant subsets of such common Web-based services as PayPal, Gmail, Orkut, Yahoo!, many forum and weblog systems, etc.\n\nEven for perfectly sighted individuals, new generations of graphical CAPTCHAs, designed to overcome sophisticated recognition software, can be very hard or impossible to read.\n\nA method of improving the CAPTCHA to ease the work with it was proposed by ProtectWebForm and was called \"Smart CAPTCHA\". Developers advise to combine the CAPTCHA with JavaScript support. Since it is too hard for most of spam robots to parse and execute JavaScript, using a simple script which fills the CAPTCHA fields and hides the image and the field from human eyes was proposed.\n\nOne alternative method involves displaying to the user a simple mathematical equation and requiring the user to enter the solution as verification. Although these are much easier to defeat using software, they are suitable for scenarios where graphical imagery is not appropriate, and they provide a much higher level of accessibility for blind users than the image-based CAPTCHAs. These are sometimes referred to as MAPTCHAs (M = \"mathematical\"). However, these may be difficult for users with a cognitive disorder.\n\nOther kinds of challenges, such as those that require understanding the meaning of some text (e.g., a logic puzzle, trivia question, or instructions on how to create a password) can also be used as a CAPTCHA. Again, there is little research into their resistance against countermeasures.\n\nThere are a few approaches to defeating CAPTCHAs: using cheap human labor to recognize them, exploiting bugs in the implementation that allow the attacker to completely bypass the CAPTCHA, and finally using machine learning to build an automated solver. According to former Google \"click fraud czar\" Shuman Ghosemajumder, there are numerous services which solve CAPTCHAs automatically.\n\nIn its earliest iterations there was not a systematic methodology for designing or evaluating CAPTCHAs. As a result, there were many instances in which CAPTCHAs were of a fixed length and therefore automated tasks could be constructed to successfully make educated guesses about where segmentation should take place. Other early CAPTCHAs contained limited sets of words, which made the test much easier to game. Still others made the mistake of relying too heavily on background confusion in the image. In each case, algorithms were created that were successfully able to complete the task by exploiting these design flaws. These methods proved brittle however, and slight changes to the CAPTCHA were easily able to thwart them. Modern CAPTCHAs like reCAPTCHA no longer rely just on fixed patterns but instead present variations of characters that are often collapsed together, making segmentation almost impossible. These newest iterations have been much more successful at warding off automated tasks.\n\nIn October 2013, artificial intelligence company Vicarious claimed that it had developed a generic CAPTCHA-solving algorithm that was able to solve modern CAPTCHAs with character recognition rates of up to 90%. However, Luis von Ahn, a pioneer of early CAPTCHA and founder of reCAPTCHA, expressed skepticism, stating: \"It's hard for me to be impressed since I see these every few months.\" He pointed out that 50 similar claims to that of Vicarious had been made since 2003.\n\nIn August 2014 at Usenix WoOT conference, Bursztein et al. presented the first generic CAPTCHA-solving algorithm based on reinforcement learning and demonstrated its efficiency against many popular CAPTCHA schemas. They concluded that text distortion based CAPTCHAs schemes should be considered insecure moving forward.\n\nIn October 2018 at ACM CCS'18 conference, Ye et al. presented a deep learning-based attack that could successfully solve all 11 text captcha schemes used by the top-50 popular website in 2018 with a high success rate. Their work shows that an effective CAPTCHA solver can be trained using as few as 500 real CAPTCHAs, showing that it is possible to quickly launch an attack of a new text CAPTCHA scheme.\n\nIt is possible to subvert CAPTCHAs by relaying them to a sweatshop of human operators who are employed to decode CAPTCHAs. A 2005 paper from a W3C working group stated that such an operator could verify hundreds per hour. In 2010 the University of California at San Diego conducted a large scale study of those CAPTCHA's farms and found out that the retail price for solving one million CAPTCHAs is as low as $1,000.\n\nAnother technique used consists of using a script to re-post the target site's CAPTCHA as a CAPTCHA to a site owned by the attacker, which unsuspecting humans visit and correctly solve within a short while for the script to use.\nHowever, there is controversy around the economic viability of such an attack.\n\nThere are multiple Internet companies like 2Captcha and DeathByCaptcha that offer human and machine backed CAPTCHA solving services for as low as US$0.50 per 1000 solved CAPTCHAs. These services offer APIs and libraries that enable users to integrate CAPTCHA circumvention into the tools that CAPTCHAs were designed to block in the first place.\n\nHoward Yeend has identified two implementation issues with poorly designed CAPTCHA systems:\n- Some CAPTCHA protection systems can be bypassed without using OCR simply by reusing the session ID of a known CAPTCHA image\n- CAPTCHAs residing on shared servers also present a problem; a security issue on another virtual host may leave the CAPTCHA issuer's site vulnerable\n\nSometimes, if part of the software generating the CAPTCHA is client-side (the validation is done on a server but the text that the user is required to identify is rendered on the client side), then users can modify the client to display the un-rendered text. Some CAPTCHA systems use MD5 hashes stored client-side, which may leave the CAPTCHA vulnerable to a brute-force attack.\n\nSome notable attacks against various CAPTCHAs schemas include:\n- Mori et al. published a paper in IEEE CVPR'03 detailing a method for defeating one of the most popular CAPTCHAs, EZ-Gimpy, which was tested as being 92% accurate in defeating it. The same method was also shown to defeat the more complex and less-widely deployed Gimpy program 33% of the time. However, the existence of implementations of their algorithm in actual use is indeterminate at this time.\n- PWNtcha has made significant progress in defeating commonly used CAPTCHAs, which has contributed to a general migration towards more sophisticated CAPTCHAs.\n- Podec, a trojan discovered by the security company Kaspersky, forwards CAPTCHA requests to an online human translation service that converts the image to text, fooling the system. Podec targets Android mobile devices.\n\nWith the demonstration that text distortion based \nCAPTCHAs are vulnerable to machine learning based attacks, some researchers have proposed alternatives including image recognition CAPTCHAs which require users to identify simple objects in the images presented. The argument in favor of these schemes is that tasks like object recognition are typically more complex to perform than text recognition and therefore should be more resilient to machine learning based attacks. Here are some of notable alternative CAPTCHA schemas:\n- Chew et al. published their work in the 7th International Information Security Conference, ISC'04, proposing three different versions of image recognition CAPTCHAs, and validating the proposal with user studies. It is suggested that one of the versions, the anomaly CAPTCHA, is best with 100% of human users being able to pass an anomaly CAPTCHA with at least 90% probability in 42 seconds.\n- Datta et al. published their paper in the ACM Multimedia '05 Conference, named IMAGINATION (IMAge Generation for INternet AuthenticaTION), proposing a systematic way to image recognition CAPTCHAs. Images are distorted in such a way that state-of-the-art image recognition approaches (which are potential attack technologies) fail to recognize them.\n- Microsoft (Jeremy Elson, John R. Douceur, Jon Howell, and Jared Saul) claim to have developed Animal Species Image Recognition for Restricting Access (ASIRRA) which ask users to distinguish cats from dogs. Microsoft had a beta version of this for websites to use. They claim \"Asirra is easy for users; it can be solved by humans 99.6% of the time in under 30 seconds. Anecdotally, users seemed to find the experience of using Asirra much more enjoyable than a text-based CAPTCHA.\" This solution was described in a 2007 paper to Proceedings of 14th ACM Conference on Computer and Communications Security (CCS). However, this project was closed in October 2014 and is no longer available.\n\n", "related": "\n- Defense strategy (computing)\n- NuCaptcha\n- Proof-of-work system\n\n- von Ahn, L; M. Blum and J. Langford. (2004) \"Telling humans and computers apart (automatically)\". \"Communications of the ACM\", 47(2):57–60.\n\n- Verification of a human in the loop, or Identification via the Turing Test, Moni Naor, 1996.\n- Inaccessibility of CAPTCHA: Alternatives to Visual Turing Tests on the Web, a W3C Working Group Note.\n- CAPTCHA History from PARC.\n- Reverse Engineering CAPTCHAs Abram Hindle, Michael W. Godfrey, Richard C. Holt, 2009-08-24\n"}
{"id": "54397496", "url": "https://en.wikipedia.org/wiki?curid=54397496", "title": "Eyes of Things", "text": "Eyes of Things\n\nEyes of Things (EoT) is the name of a project funded by the European Union’s Horizon 2020 Research and Innovation Programme under grant agreement number 643924. The purpose of the project, which is funded under the Smart Cyber-physical systems topic, is to develop a generic hardware-software platform for embedded, efficient (i.e. battery-operated, wearable, mobile), computer vision, including deep learning inference.\n\nOn November 29, 2018, the European Space Agency announced that it was testing the suitability of the device for space applications in advance of a flight in a Cubesat. \nEoT is based on the following tenets:\n- Future embedded systems will have more intelligence and cognitive functionality. Vision is paramount to such intelligent capacity\n- Unlike other sensors, vision requires intensive processing. Power consumption must be optimized if vision is to be used in mobile and wearable applications\n- Cloud processing of edge-captured images is not sustainable. The sheer amount of visual data generated cannot be transferred to the cloud. Bandwidth is not sufficient and cloud servers cannot cope with it.\n\n- VISILAB group at University of Castilla–La Mancha (Coordinator)\n- Movidius\n- Awaiba\n- Thales Security Solutions & Systems\n- DFKI\n- Fluxguide\n- Evercam\n- nVISO\n\n- 2019 Electronic Component and Systems Innovation Award by the European Commission\n- 2018 HiPEAC Tech Transfer Award\n- 2018 EC Innovation Radar - highlighting excellent innovations Award\n- 2018 Internet of Things (IoT) Technology Research Award Pilot by Google\n- 2016 Semifinalist \"THE VISION SHOW STARTUP COMPETITION\", Global Association for Vision Information, Boston US\n\n", "related": "\n- Wearable camera\n- Computer vision\n- Internet of Things\n- Embedded systems\n- Edge computing\n"}
{"id": "54637700", "url": "https://en.wikipedia.org/wiki?curid=54637700", "title": "Teknomo–Fernandez algorithm", "text": "Teknomo–Fernandez algorithm\n\nThe Teknomo–Fernandez algorithm (TF algorithm), is an efficient algorithm for generating the background image of a given video sequence.\n\nBy assuming that the background image is shown in the majority of the video, the algorithm is able to generate a good background image of a video in formula_1-time using only a small number of binary operations and Boolean Bit operations, which require a small amount of memory and has built-in operators found in many programming languages such as C, C++, and Java.\n\nPeople tracking from videos usually involves some form of background subtraction to segment foreground from background. Once foreground images are extracted, then desired algorithms (such as those for motion tracking, object tracking, and facial recognition) may be executed using these images.\n\nHowever, background subtraction requires that the background image is already available and unfortunately, this is not always the case. Traditionally, the background image is searched for manually or automatically from the video images when there are no objects. More recently, automatic background generation through object detection, medial filtering, medoid filtering, approximated median filtering, linear predictive filter, non-parametric model, Kalman filter, and adaptive smoothening have been suggested; however, most of these methods have high computational complexity and are resource-intensive.\n\nThe Teknomo–Fernandez algorithm is also an automatic background generation algorithm. Its advantage, however, is its computational speed of only formula_1-time, depending on the resolution formula_3 of an image and its accuracy gained within a manageable number of frames. Only at least three frames from a video is needed to produce the background image assuming that for every pixel position, the background occurs in the majority of the videos. Furthermore, it can be performed for both grayscale and colored videos.\n\n- The camera is stationary.\n- The light of the environment changes only slowly relative to the motions of the people in the scene.\n- The number of people does not occupy the scene for the most of the time at the same place.\n\nGenerally, however, the algorithm will certainly work whenever the following single important assumption holds: For each pixel position, the majority of the pixel values in the entire video contain the pixel value of the actual background image (at that position).As long as each part of the background is shown in the majority of the video, the entire background image needs not to appear in any of its frames. The algorithm is expected to work accurately.\n\n1. For three frames of image sequence formula_4, formula_5, and formula_6, the background image formula_7 is obtained using <br>     formula_8\n2. The Boolean mode function formula_9 of the table occurs when the number of 1 entries is larger than half of the number of images such that<br>     formula_10\n3. For three images, the background image formula_7 can be taken as the value\n\nAt the first level, three frames are selected at random from the image sequence to produce a background image by combining them using the first equation. This yields a better background image at the second level. The procedure is repeated until desired level formula_13.\n\nAt level formula_14, the probability formula_15 that the modal bit predicted is the actual modal bit is represented by the equation formula_16.\nThe table below gives the computed probability values across several levels using some specific initial probabilities. It can be observed that even if the modal bit at the considered position is at a low 60% of the frames, the probability of accurate modal bit determination is already more than 99% at 6 levels.\nThe space requirement of the Teknomo–Fernandez algorithm is given by the function formula_17, depending on the resolution formula_3 of the image, the number formula_19 of frames in the video, and the desired number formula_13 of levels. However, the fact that formula_13 will probably not exceed 6 reduces the space complexity to formula_22.\n\nThe entire algorithm runs in formula_1-time, only depending on the resolution of the image. Computing the modal bit for each bit can be done in formula_24-time while the computation of the resulting image from the three given images can be done in formula_1-time. The number of the images to be processed in formula_13 levels is formula_27. However, since formula_28, then this is actually formula_24, thus the algorithm runs in formula_1.\n\nA variant of the Teknomo–Fernandez algorithm that incorporates the Monte-Carlo method named CRF has been developed. Two different configurations of CRF were implemented: CRF9,2 and CRF81,1. Experiments on some colored video sequences showed that the CRF configurations outperform the TF algorithm in terms of accuracy. However, the TF algorithm remains more efficient in terms of processing time.\n\n- Object detection\n- Face detection\n- Face recognition\n- Pedestrian detection\n- Video surveillance\n- Motion capture\n- Human-computer interaction\n- Content-based video coding\n- Traffic monitoring\n- Real-time gesture recognition\n\n\n- Background Image Generation Using Boolean Operations – describes the TF algorithm, its assumptions, processes, accuracy, time and space complexity, and sample results.\n- A Monte-Carlo-based Algorithm for Background Generation – a variant of the Teknomo–Fernandez algorithm that incorporates the Monte-Carlo method was developed in this study.\n", "related": "NONE"}
{"id": "48411649", "url": "https://en.wikipedia.org/wiki?curid=48411649", "title": "Harris Corner Detector", "text": "Harris Corner Detector\n\nHarris Corner Detector is a corner detection operator that is commonly used in computer vision algorithms to extract corners and infer features of an image. It was first introduced by Chris Harris and Mike Stephens in 1988 upon the improvement of Moravec's corner detector. Compared to the previous one, Harris' corner detector takes the differential of the corner score into account with reference to direction directly, instead of using shifting patches for every 45 degree angles, and has been proved to be more accurate in distinguishing between edges and corners. Since then, it has been improved and adopted in many algorithms to preprocess images for subsequent applications.\n\nA corner is a point whose local neighborhood stands in two dominant and different edge directions. In other words, a corner can be interpreted as the junction of two edges, where an edge is a sudden change in image brightness. Corners are the important features in the image, and they are generally termed as interest points which are invariant to translation, rotation and illumination. Although corners are only a small percentage of the image, they contain the most important features in restoring image information, and they can be used to minimize the amount of processed data for motion tracking, image stitching, building 2D mosaics, stereo vision, image representation and other related computer vision areas.\n\nIn order to capture the corners from the image, researchers have proposed many different corner detectors including the Kanade-Lucas-Tomasi (KLT) operator and the Harris operator which are most simple, efficient and reliable for use in corner detection. These two popular methodologies are both closely associated with and based on the local structure matrix. Compared to the Kanade-Lucas-Tomasi corner detector, the Harris corner detector provides good repeatability under changing illumination and rotation, and therefore, it is more often used in stereo matching and image database retrieval. Although there still exists drawbacks and limitations, the Harris corner detector is still an important and fundamental technique for many computer vision applications.\n\nWithout loss of generality, we will assume a grayscale 2-dimensional image is used. Let this image be given by formula_1. Consider taking an image patch formula_2(window) and shifting it by formula_3. The \"sum of squared differences\" (SSD) between these two patches, denoted formula_4, is given by:\n\nformula_6 can be approximated by a Taylor expansion. Let formula_7 and formula_8 be the partial derivatives of formula_9, such that\n\nThis produces the approximation\n\nwhich can be written in matrix form:\n\nwhere \"M\" is the structure tensor,\n\nCommonly, Harris corner detector algorithm can be divided into five steps.\n1. Color to grayscale\n2. Spatial derivative calculation\n3. Structure tensor setup\n4. Harris response calculation\n5. Non-maximum suppression\n\nIf we use Harris corner detector in a color image, the first step is to convert it into a grayscale image, which will enhance the processing speed.\n\nThe value of the gray scale pixel can be computed as a weighted sums of the values R, B and G of the color image, \n\nwhere, e.g., \n\nNext, we are going to compute formula_16 and formula_17.\n\nWith formula_18 , formula_19, we can construct the structure tensor formula_20.\n\nFor formula_21, one has \nformula_22\nIn this step, we compute the smallest eigenvalue of the structure tensor using that approximation:\n\nformula_23\n\nwith the trace formula_24. \n\nAnother commonly used Harris response calculation is shown as below,\n\nformula_25\n\nwhere formula_26 is an empirically determined constant; formula_27 .\n\nIn order to pick up the optimal values to indicate corners, we find the local maxima as corners within the window which is a 3 by 3 filter.\n\n1. Harris-Laplace Corner Detector\n\n2. Differential Morphological Decomposition Based Corner Detector\n\n3. Multi-scale Bilateral Structure Tensor Based Corner Detector\n\n1. Image Alignment, Stitching and Registration\n\n2. 2D Mosaics Creation\n\n3. 3D Scene Modeling and Reconstruction\n\n4. Motion Detection\n\n5. Object Recognition\n\n6. Image Indexing and Content-based Retrieval\n\n7. Video Tracking\n\n", "related": "\n- Structure tensor\n- Harris affine region detector\n- Corner detection\n- Feature detection (computer vision)\n- Computer vision\n- List of computer vision topics\n\n- \"Learn OpenCV by Examples : Harris Corner Detection\"\n- \"Harris Corner Detection - OpenCV Documentation\"\n- \"Harris Corner Detection - OpenCV-Python Tutorials\"\n- Online Implementation of the Harris Corner Detector - IPOL\n"}
{"id": "56084743", "url": "https://en.wikipedia.org/wiki?curid=56084743", "title": "3D selfie", "text": "3D selfie\n\nA 3D selfie is a 3D-printed scale replica of a person or their face. These three-dimensional selfies are also known as 3D portraits, 3D figurines, 3D-printed figurines, mini-me figurines and miniature statues. In 2014 a first 3D printed bust of a President, Barack Obama, was made. 3D-digital-imaging specialists used handheld 3D scanners to create an accurate representation of the President. \n\nThe capture of a subject as a 3D model can be accomplished in many ways. One of the methods, is called photogrammetry. Many systems use one or more digital cameras to take 2D pictures of the subject, under normal lighting, under projected light patterns, or a combination of these. Inexpensive systems use a single camera which is moved around the subject in 360° at various heights, over minutes, while the subject stays immobile. More elaborate systems have a vertical bar of cameras rotate around the subject, usually achieving a full scan in 10 seconds. Most expensive systems have an enclosed 3D photo booth with 50 to 100 cameras statically embedded in walls and the ceiling, firing all at once, eliminating differences in image capture caused by movements of the subject. A piece of software then reconstructs a 3D model of the subject from these pictures. \nOne of the 3D photo booth, which creates life-like portraits, is called Veronica Chorographic Scanner. The scanner participated in the project of Royal Academy of Arts, where people could have themselves scanned. The scanner utilized 8 cameras taking 96 photographs of a person from each angle. Photogrammetry scanning is generally considered more life-like, than scanning with 3D scanners. \n\nAnother method for capturing a 3D selfie uses dedicated 3D scanning equipment which may more accurately capture geometry and texture, but take longer to perform. Scanners may be handheld, tripod mounted or fitted to another system that will allow the full geometry of a person to be captured. One of the well-known full body 3D scanners are Shapify booth, based on Artec Eva 3D scanners and Twindom Twinstant Mobile . \n\nProduction of 3D selfies is enabled by 3D printing technologies. This includes the ability to 3D print in full color using gypsum-based binder jetting techniques, giving the figurine a sandstone-like texture and look. Other 3D printing process may be used depending on the desired result.\n\n", "related": "\n- 3D reconstruction\n- Digitization\n- Depth map\n- Full body scanner\n- Photogrammetry\n- Range imaging\n"}
{"id": "56249073", "url": "https://en.wikipedia.org/wiki?curid=56249073", "title": "Egocentric vision", "text": "Egocentric vision\n\nEgocentric vision or first-person vision is a sub-field of computer vision that entails analyzing images and videos captured by a wearable camera, which is typically worn on the head or on the chest and naturally approximates the visual field of the camera wearer. Consequently, visual data capture the part of the scene on which the user focuses to carry out the task at hand and offer a valuable perspective to understand the user's activities and their context in a naturalistic setting.\n\nThe wearable camera looking forwards is often supplemented with a camera looking inward at the user’s eye and able to measure a user’s eye gaze, which is useful to reveal attention and to better understand the\nuser’s activity and intentions.\n\nThe idea of using a wearable camera to gather visual data from a first-person perspective dates back to the 70s, when Steve Mann invented \"Eye Glass\", a device that, when worn, causes the human eye itself to effectively become both an electronic camera and a television display. But it was only after the introduction to the market of the Microsoft SenseCam in 2006 that wearable cameras were used for the first time in large scale experimental health research works. The interest of the computer vision community into the egocentric paradigm has been arising slowly entering the 2010s and it is rapidly growing in recent years, boosted by both the impressive advanced in the field of wearable technology and by the increasingly number of potential applications.\n\nThe prototypical first-person vision system described by Kanade and Hebert, in 2012 is composed by three basic components: a localization component able to estimate the surrounding, a recognition component able to identify object and people, and an activity recognition component, able to provide information about the current activity of the user. Together, these three components provide a complete situational awareness of the user, which in turn can be used to provide assistance to the itself or to the caregiver. Following this idea, the first computational techniques for egocentric analysis focused on hand-related activity recognition and social interaction analysis. Also, given the unconstrained nature of the video and the huge amount of data generated, temporal segmentation and summarization where among the first problem addressed. After almost ten years of egocentric vision (2007 - 2017), the field is still undergoing diversification. Emerging research topics include:\n\n- Social saliency estimation\n- Multi-agent egocentric vision systems\n- Privacy preserving techniques and applications\n- Attention-based activity analysis\n- Social interaction analysis\n- Hand pose analysis\n- Ego graphical User Interfaces (EUI)\n- Understanding social dynamics and attention\n- Revisiting robotic vision and machine vision as egocentric sensing\n- Activity forecasting\n\nToday's wearable cameras are small and lightweight digital recording devices that can acquire images and videos automatically, without the user intervention, with different resolutions and frame rates, and from a first-person point of view. Therefore, wearable cameras are naturally primed to gather visual information from our everyday interactions since they offer an intimate perspective of the visual field of the camera wearer.\n\nDepending on the frame rate, it is common to distinguish between photo-cameras (also called lifelogging cameras) and video-cameras. \n- The former (e.g., Narrative Clip and Microsoft SenseCam), are commonly worn on the chest, and are characterized by a very low frame rate (up to 2fpm) that allows to capture images over a long period of time without the need of recharging the battery. Consequently, they offer considerable potential for inferring knowledge about e.g. behaviour patterns, habits or lifestyle of the user. However, due the low frame-rate and the free motion of the camera, temporally adjacent images typically present abrupt appearance changes so that motion features cannot be reliably estimated.\n- The latter (e.g., Google Glass, GoPro), are commonly mounted on the head, and capture conventional video (around 35fps) that allows to capture fine temporal details of interactions. Consequently, they offer potential for in-depth analysis of daily or special activities. However, since the camera is moving with the wearer head, it becomes more difficult to estimate the global motion of the wearer and in the case of abrupt movements, the images can result blurred.\n\nIn both cases, since the camera is worn in a naturalistic setting, visual data present a huge variability in terms of illumination conditions and object appearance.\nMoreover, the camera wearer is not visible in the image and what he/she is doing has to be inferred from the information in the visual field of the camera, implying that important information about the wearer, such for instance as pose or facial expression estimation, is not available.\n\nA collection of studies published in a special theme issue of the American Journal of Preventive Medicine has demonstrated the potential of lifelogs captured through wearable cameras from a number of viewpoints. In particular, it has been shown that used as a tool for understanding and tracking lifestyle behaviour, lifelogs would enable the prevention of noncommunicable diseases associated to unhealthy trends and risky profiles (such as obesity, depression, etc.). In addition, used as a tool of re-memory cognitive training, lifelogs would enable the prevention of cognitive and functional decline in elderly people.\n\nMore recently, egocentric cameras have been used to study human and animal cognition, human-human social interaction, human-robot interaction, human expertise in complex tasks.\nOther applications include navigation/assistive technologies for the blind, monitoring and assistance of industrial workflows.\n\n", "related": "\n- Eye tracking\n- Lifelog\n- Quantified self\n- Smartglasses\n- Sousveillance\n"}
{"id": "47319644", "url": "https://en.wikipedia.org/wiki?curid=47319644", "title": "Error level analysis", "text": "Error level analysis\n\nError level analysis (ELA) is the analysis of compression artifacts in digital data with lossy compression such as JPEG.\n\nWhen used, lossy compression is normally applied uniformly to a set of data, such as an image, resulting in a uniform level of compression artifacts.\n\nAlternatively, the data may consist of parts with different levels of compression artifacts. This difference may arise from the different parts having been repeatedly subjected to the same lossy compression a different number of times, or the different parts having been subjected to different kinds of lossy compression. A difference in the level of compression artifacts in different parts of the data may therefore indicate that the data has been edited.\n\nIn the case of JPEG, even a composite with parts subjected to matching compressions will have a difference in the compression artifacts.\n\nIn order to make the typically faint compression artifacts more readily visible, the data to be analyzed is subjected to an additional round of lossy compression, this time at a known, uniform level, and the result is subtracted from the original data under investigation. The resulting difference image is then inspected manually for any variation in the level of compression artifacts. In 2007, N. Krawetz denoted this method \"error level analysis\".\n\nAdditionally, digital data formats such as JPEG sometimes include metadata describing the specific lossy compression used. If in such data the observed compression artifacts differ from those expected from the given metadata description, then the metadata may not describe the actual compressed data, and thus indicate that the data have been edited.\n\nBy its nature, data without lossy compression, such as a PNG image, cannot be subjected to error level analysis. Consequently, since editing could have been performed on data without lossy compression with lossy compression applied uniformly to the edited, composite data, the presence of a uniform level of compression artifacts does not rule out editing of the data.\n\nAdditionally, any non-uniform compression artifacts in a composite may be removed by subjecting the composite to repeated, uniform lossy compression. Also, if the image color space is reduced to 256 colors or less, for example, by conversion to GIF, then error level analysis will generate useless results.\n\nMore significant, the actual interpretation of the level of compression artifacts in a given segment of the data is subjective, and the determination of whether editing has occurred is therefore not robust.\n\nIn May 2013, Dr Neal Krawetz used error level analysis on the 2012 World Press Photo of the Year and concluded on his \"Hacker Factor\" blog that it was \"a composite\" with modifications that \"fail to adhere to the acceptable journalism standards used by Reuters, Associated Press, Getty Images, National Press Photographer's Association, and other media outlets\". The World Press Photo organizers responded by letting two independent experts analyze the image files of the winning photographer and subsequently confirmed the integrity of the files. One of the experts, \nHany Farid, said about error level analysis that \"It incorrectly labels altered images as original and incorrectly labels original images as altered with the same likelihood\". Krawetz responded by clarifying that \"It is up to the user to interpret the results. Any errors in identification rest solely on the viewer\".\n\nIn May 2015, the citizen journalism team Bellingcat wrote that error level analysis revealed that the Russian Ministry of Defense had edited satellite images related to the Malaysia Airlines Flight 17 disaster. In a reaction to this, image forensics expert Jens Kriese said about error level analysis: \"The method is subjective and not based entirely on science\", and that it is \"a method used by hobbyists\". On his Hacker Factor Blog, the inventor of error level analysis Neal Krawetz criticized both Bellingcat's use of error level analysis as \"misinterpreting the results\" but also on several points Jens Kriese's \"ignorance\" regarding error level analysis.\n\n", "related": "\n- Image analysis\n\n- Image Forensics : Error Level Analysis\n- FotoForensics\n"}
{"id": "10999922", "url": "https://en.wikipedia.org/wiki?curid=10999922", "title": "Mean shift", "text": "Mean shift\n\nMean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.\n\nThe mean shift procedure was originally presented in 1975 by Fukunaga and Hostetler.\n\nMean shift is a procedure for locating the maxima—the modes—of a density function given discrete data sampled from that function. This is an iterative method, and we start with an initial estimate formula_1. Let a kernel function formula_2 be given. This function determines the weight of nearby points for re-estimation of the mean. Typically a Gaussian kernel on the distance to the current estimate is used, formula_3. The weighted mean of the density in the window determined by formula_4 is\n\nwhere formula_6 is the neighborhood of formula_1, a set of points for which formula_8.\n\nThe difference formula_9 is called \"mean shift\" in Fukunaga and Hostetler. \nThe \"mean-shift algorithm\" now sets formula_10, and repeats the estimation until formula_11 converges.\n\nAlthough the mean shift algorithm has been widely used in many applications, a rigid proof for the convergence of the algorithm using a general kernel in a high dimensional space is still not known. Aliyari Ghassabeh showed the convergence of the mean shift algorithm in one-dimension with a differentiable, convex, and strictly decreasing profile function. However, the one-dimensional case has limited real world applications. Also, the convergence of the algorithm in higher dimensions with a finite number of the (or isolated) stationary points has been proved. However, sufficient conditions for a general kernel function to have finite (or isolated) stationary points have not been provided.\n\nLet data be a finite set formula_12 embedded in the formula_13-dimensional Euclidean space, formula_14. Let formula_15 be a flat kernel that is the characteristic function of the formula_16-ball in formula_14,\nIn each iteration of the algorithm, formula_18 is performed for all formula_19 simultaneously. The first question, then, is how to estimate the density function given a sparse set of samples. One of the simplest approaches is to just smooth the data, e.g., by convolving it with a fixed kernel of width formula_20,\nwhere formula_21 are the input samples and formula_22 is the kernel function (or \"Parzen window\"). formula_20 is the only parameter in the algorithm and is called the bandwidth. This approach is known as \"kernel density estimation\" or the Parzen window technique. Once we have computed formula_24 from equation above, we can find its local maxima using gradient ascent or some other optimization technique. The problem with this \"brute force\" approach is that, for higher dimensions, it becomes computationally prohibitive to evaluate formula_24 over the complete search space. Instead, mean shift uses a variant of what is known in the optimization literature as \"multiple restart gradient descent\". Starting at some guess for a local maximum, formula_26, which can be a random input data point formula_27, mean shift computes the gradient of the density estimate formula_24 at formula_26 and takes an uphill step in that direction.\n\nKernel definition: Let formula_14 be the formula_13-dimensional Euclidean space, formula_32. The norm of formula_33 is a non-negative number, formula_34. A function formula_35 is said to be a kernel if there exists a \"profile\", formula_36 , such that\n\nformula_37\nand \n- k is non-negative.\n- k is non-increasing: formula_38 if formula_39.\n- k is piecewise continuous and formula_40\n\nThe two most frequently used kernel profiles for mean shift are:\n\n- Flat kernel\n\n- Gaussian kernel\nwhere the standard deviation parameter formula_41 works as the bandwidth parameter, formula_42.\n\nConsider a set of points in two-dimensional space. Assume a circular window centered at C and having radius r as the kernel. Mean shift is a hill climbing algorithm which involves shifting this kernel iteratively to a higher density region until convergence. Every shift is defined by a mean shift vector. The mean shift vector always points toward the direction of the maximum increase in the density. At every iteration the kernel is shifted to the centroid or the mean of the points within it. The method of calculating this mean depends on the choice of the kernel. In this case if a Gaussian kernel is chosen instead of a flat kernel, then every point will first be assigned a weight which will decay exponentially as the distance from the kernel's center increases. At convergence, there will be no direction at which a shift can accommodate more points inside the kernel.\n\nThe mean shift algorithm can be used for visual tracking. The simplest such algorithm would create a confidence map in the new image based on the color histogram of the object in the previous image, and use mean shift to find the peak of a confidence map near the object's old position. The confidence map is a probability density function on the new image, assigning each pixel of the new image a probability, which is the probability of the pixel color occurring in the object in the previous image. A few algorithms, such as kernel-based object tracking, \nensemble tracking,\nCAMshift \nexpand on this idea.\n\nLet formula_21 and formula_44 be the formula_45-dimensional input and filtered image pixels in the joint spatial-range domain. For each pixel,\n\n- Initialize formula_46 and formula_47\n- Compute formula_48 according to formula_49 until convergence, formula_50.\n- Assign formula_51. The superscripts s and r denote the spatial and range components of a vector, respectively. The assignment specifies that the filtered data at the spatial location axis will have the range component of the point of convergence formula_52.\n\n1. Mean shift is an application-independent tool suitable for real data analysis.\n2. Does not assume any predefined shape on data clusters.\n3. It is capable of handling arbitrary feature spaces.\n4. The procedure relies on choice of a single parameter: bandwidth.\n5. The bandwidth/window size 'h' has a physical meaning, unlike \"k\"-means.\n\n1. The selection of a window size is not trivial.\n2. Inappropriate window size can cause modes to be merged, or generate additional “shallow” modes.\n3. Often requires using adaptive window size.\n\nVariants of the algorithm can be found in machine learning and image processing packages:\n\n- ELKI. Java data mining tool with many clustering algorithms.\n- ImageJ. Image filtering using the mean shift filter.\n- OpenCV contains mean-shift implementation via cvMeanShift Method\n- Orfeo toolbox. A C++ implementation.\n- Scikit-learn Numpy/Python implementation uses ball tree for efficient neighboring points lookup\n\n", "related": "\n- DBSCAN\n- OPTICS algorithm\n- Kernel density estimation (KDE)\n- Kernel (statistics)\n"}
{"id": "54655906", "url": "https://en.wikipedia.org/wiki?curid=54655906", "title": "Scene text", "text": "Scene text\n\nScene text is text that appears in an image captured by a camera in an outdoor environment. The detection and recognition of scene text from camera captured images are computer vision tasks which became important after smart phones with good cameras became ubiquitous. The text in scene images varies in shape, font, colour and position. The recognition of scene text is further complicated sometimes by non-uniform illumination and focus. \nTo improve scene text recognition, the International Conference on Document Analysis and Recognition (ICDAR) conducts a robust reading competition once in two years. The competition was held in 2003, 2005 and during every ICDAR conference. International association for pattern recognition (IAPR) has created a list of datasets as Reading systems.\n\nText detection is the process of detecting the text present in the image, followed by surrounding it with a rectangular bounding box. Text detection can be carried out using image based techniques or frequency based techniques.\n\nIn image based techniques, an image is segmented into multiple segments. Each segment is a connected component of pixels with similar characteristics. The statistical features of connected components are utilised to group them and form the text. Machine learning approaches such as support vector machine and convolutional neural networks are used to classify the components into text and non-text.\n\nIn frequency based techniques, discrete Fourier transform (DFT) or discrete wavelet transform (DWT) are used to extract the high frequency coefficients. It is assumed that the text present in an image has high frequency components and selecting only the high frequency coefficients filters the text from the non-text regions in an image.\n\nIn word recognition, the text is assumed to be already detected and located and the rectangular bounding box containing the text is available. The word present in the bounding box needs to be recognized. The methods available to perform word recognition can be broadly classified into top-down and bottom-up approaches.\n\nIn the top-down approaches, a set of words from a dictionary is used to identify which word suits the given image. Images are not segmented in most of these methods. Hence, the top-down approach is sometimes referred as segmentation free recognition.\n\nIn the bottom-up approaches, the image is segmented into multiple components and the segmented image is passed through a recognition engine. Either an off the shelf Optical character recognition (OCR) engine or a custom-trained one is used to recognise the text.\n", "related": "NONE"}
{"id": "348692", "url": "https://en.wikipedia.org/wiki?curid=348692", "title": "Eigenface", "text": "Eigenface\n\nAn eigenface () is the name given to a set of eigenvectors when used in the computer vision problem of human face recognition. The approach of using eigenfaces for recognition was developed by Sirovich and Kirby (1987) and used by Matthew Turk and Alex Pentland in face classification. The eigenvectors are derived from the covariance matrix of the probability distribution over the high-dimensional vector space of face images. The eigenfaces themselves form a basis set of all images used to construct the covariance matrix. This produces dimension reduction by allowing the smaller set of basis images to represent the original training images. Classification can be achieved by comparing how faces are represented by the basis set.\n\nThe eigenface approach began with a search for a low-dimensional representation of face images. Sirovich and Kirby (1987) showed that principal component analysis could be used on a collection of face images to form a set of basis features. These basis images, known as eigenpictures, could be linearly combined to reconstruct images in the original training set. If the training set consists of \"M\" images, principal component analysis could form a basis set of \"N\" images, where \"N < M\". The reconstruction error is reduced by increasing the number of eigenpictures; however, the number needed is always chosen less than \"M\". For example, if you need to generate a number of \"N\" eigenfaces for a training set of \"M\" face images, you can say that each face image can be made up of \"proportions\" of all the \"K\" \"features\" or eigenfaces: Face image = (23% of E) + (2% of E) + (51% of E) + ... + (1% E).\n\nIn 1991 M. Turk and A. Pentland expanded these results and presented the eigenface method of face recognition. In addition to designing a system for automated face recognition using eigenfaces, they showed a way of calculating the eigenvectors of a covariance matrix such that computers of the time could perform eigen-decomposition on a large number of face images. Face images usually occupy a high-dimensional space and conventional principal component analysis was intractable on such data sets. Turk and Pentland's paper demonstrated ways to extract the eigenvectors based on matrices sized by the number of images rather than the number of pixels.\n\nOnce established, the eigenface method was expanded to include methods of preprocessing to improve accuracy. Multiple manifold approaches were also used to build sets of eigenfaces for different subjects and different features, such as the eyes.\n\nA set of eigenfaces can be generated by performing a mathematical process called principal component analysis (PCA) on a large set of images depicting different human faces. Informally, eigenfaces can be considered a set of \"standardized face ingredients\", derived from statistical analysis of many pictures of faces. Any human face can be considered to be a combination of these standard faces. For example, one's face might be composed of the average face plus 10% from eigenface 1, 55% from eigenface 2, and even −3% from eigenface 3. Remarkably, it does not take many eigenfaces combined together to achieve a fair approximation of most faces. Also, because a person's face is not recorded by a digital photograph, but instead as just a list of values (one value for each eigenface in the database used), much less space is taken for each person's face.\n\nThe eigenfaces that are created will appear as light and dark areas that are arranged in a specific pattern. This pattern is how different features of a face are singled out to be evaluated and scored. There will be a pattern to evaluate symmetry, whether there is any style of facial hair, where the hairline is, or an evaluation of the size of the nose or mouth. Other eigenfaces have patterns that are less simple to identify, and the image of the eigenface may look very little like a face.\n\nThe technique used in creating eigenfaces and using them for recognition is also used outside of face recognition: handwriting recognition, lip reading, voice recognition, sign language/hand gestures interpretation and medical imaging analysis. Therefore, some do not use the term eigenface, but prefer to use 'eigenimage'.\n\nTo create a set of eigenfaces, one must:\n1. Prepare a training set of face images. The pictures constituting the training set should have been taken under the same lighting conditions, and must be normalized to have the eyes and mouths aligned across all images. They must also be all resampled to a common pixel resolution (\"r\" × \"c\"). Each image is treated as one vector, simply by concatenating the rows of pixels in the original image, resulting in a single column with \"r\" × \"c\" elements. For this implementation, it is assumed that all images of the training set are stored in a single matrix T, where each column of the matrix is an image.\n2. Subtract the mean. The average image a has to be calculated and then subtracted from each original image in T.\n3. Calculate the eigenvectors and eigenvalues of the covariance matrix S. Each eigenvector has the same dimensionality (number of components) as the original images, and thus can itself be seen as an image. The eigenvectors of this covariance matrix are therefore called eigenfaces. They are the directions in which the images differ from the mean image. Usually this will be a computationally expensive step (if at all possible), but the practical applicability of eigenfaces stems from the possibility to compute the eigenvectors of S efficiently, without ever computing S explicitly, as detailed below.\n4. Choose the principal components. Sort the eigenvalues in descending order and arrange eigenvectors accordingly. The number of principal components \"k\" is determined arbitrarily by setting a threshold ε on the total variance. Total variance , = number of components.\n5. k is the smallest number that satisfies formula_1\n\nThese eigenfaces can now be used to represent both existing and new faces: we can project a new (mean-subtracted) image on the eigenfaces and thereby record how that new face differs from the mean face. The eigenvalues associated with each eigenface represent how much the images in the training set vary from the mean image in that direction. Information is lost by projecting the image on a subset of the eigenvectors, but losses are minimized by keeping those eigenfaces with the largest eigenvalues. For instance, working with a 100 × 100 image will produce 10,000 eigenvectors. In practical applications, most faces can typically be identified using a projection on between 100 and 150 eigenfaces, so that most of the 10,000 eigenvectors can be discarded.\n\nHere is an example of calculating eigenfaces with Extended Yale Face Database B. To evade computational and storage bottleneck, the face images are sampled down by a factor 4×4=16.\n\nNote that although the covariance matrix S generates many eigenfaces, only a fraction of those are needed to represent the majority of the faces. For example, to represent 95% of the total variation of all face images, only the first 43 eigenfaces are needed. To calculate this result, implement the following code:\n\nPerforming PCA directly on the covariance matrix of the images is often computationally infeasible. If small images are used, say 100 × 100 pixels, each image is a point in a 10,000-dimensional space and the covariance matrix S is a matrix of 10,000 × 10,000 = 10 elements. However the rank of the covariance matrix is limited by the number of training examples: if there are \"N\" training examples, there will be at most \"N\" − 1 eigenvectors with non-zero eigenvalues. If the number of training examples is smaller than the dimensionality of the images, the principal components can be computed more easily as follows.\n\nLet T be the matrix of preprocessed training examples, where each column contains one mean-subtracted image. The covariance matrix can then be computed as S = TT and the eigenvector decomposition of S is given by\nHowever TT is a large matrix, and if instead we take the eigenvalue decomposition of\nthen we notice that by pre-multiplying both sides of the equation with T, we obtain\nMeaning that, if u is an eigenvector of TT, then v = Tu is an eigenvector of S. If we have a training set of 300 images of 100 × 100 pixels, the matrix TT is a 300 × 300 matrix, which is much more manageable than the 10,000 × 10,000 covariance matrix. Notice however that the resulting vectors v are not normalised; if normalisation is required it should be applied as an extra step.\n\nLet denote the data matrix with column as the image vector with mean subtracted. Then,\n\nLet the singular value decomposition (SVD) of be:\n\nThen the eigenvalue decomposition for formula_7 is:\n\n- Delac, K., Grgic, M., Liatsis, P. (2005). \"Appearance-based Statistical Methods for Face Recognition\". \"Proceedings of the 47th International Symposium ELMAR-2005 focused on Multimedia Systems and Applications\", Zadar, Croatia, 08-10 June 2005, pp. 151–158\n\n- Face Recognition Homepage\n- PCA on the FERET Dataset\n- Developing Intelligence Eigenfaces and the Fusiform Face Area\n- A Tutorial on Face Recognition Using Eigenfaces and Distance Classifiers\n- Matlab example code for eigenfaces\n- OpenCV + C++Builder6 implementation of PCA\n- Java applet demonstration of eigenfaces\n- Introduction to eigenfaces\n- Face Recognition Function in OpenCV\n- Eigenface-based Facial Expression Recognition in Matlab\n", "related": "NONE"}
{"id": "14669989", "url": "https://en.wikipedia.org/wiki?curid=14669989", "title": "Viola–Jones object detection framework", "text": "Viola–Jones object detection framework\n\nThe Viola–Jones object detection framework is the first object detection framework to provide competitive object detection rates in real-time proposed in 2001 by Paul Viola and Michael Jones. Although it can be trained to detect a variety of object classes, it was motivated primarily by the problem of face detection.\n\nThe problem to be solved is detection of faces in an image. A human can do this easily, but a computer needs precise instructions and constraints. To make the task more manageable, Viola–Jones requires full view frontal upright faces. Thus in order to be detected, the entire face must point towards the camera and should not be tilted to either side. While it seems these constraints could diminish the algorithm's utility somewhat, because the detection step is most often followed by a recognition step, in practice these limits on pose are quite acceptable.\n\nThe characteristics of Viola–Jones algorithm which make it a good detection algorithm are:\n- Robust – very high detection rate (true-positive rate) & very low false-positive rate always.\n- Real time – For practical applications at least 2 frames per second must be processed.\n- Face detection only (not recognition) - The goal is to distinguish faces from non-faces (detection is the first step in the recognition process).\n\nThe algorithm has four stages:\n1. Haar Feature Selection\n2. Creating an Integral Image\n3. Adaboost Training\n4. Cascading Classifiers\n\nThe features sought by the detection framework universally involve the sums of image pixels within rectangular areas. As such, they bear some resemblance to Haar basis functions, which have been used previously in the realm of image-based object detection. However, since the features used by Viola and Jones all rely on more than one rectangular area, they are generally more complex. The figure on the right illustrates the four different types of features used in the framework. The value of any given feature is the sum of the pixels within clear rectangles subtracted from the sum of the pixels within shaded rectangles. Rectangular features of this sort are primitive when compared to alternatives such as steerable filters. Although they are sensitive to vertical and horizontal features, their feedback is considerably coarser.\n\nAll human faces share some similar properties. These regularities may be matched using Haar Features.\n\nA few properties common to human faces:\n- The eye region is darker than the upper-cheeks.\n- The nose bridge region is brighter than the eyes.\n\nComposition of properties forming matchable facial features:\n- Location and size: eyes, mouth, bridge of nose\n- Value: oriented gradients of pixel intensities\n\nThe four features matched by this algorithm are then sought in the image of a face (shown at right).y\n\nRectangle features:\n- Value = Σ (pixels in black area) - Σ (pixels in white area)\n- Three types: two-, three-, four-rectangles, Viola & Jones used two-rectangle features\n- For example: the difference in brightness between the white & black rectangles over a specific area\n- Each feature is related to a special location in the sub-window\n\nAn image representation called the integral image evaluates rectangular features in \"constant\" time, which gives them a considerable speed advantage over more sophisticated alternative features. Because each feature's rectangular area is always adjacent to at least one other rectangle, it follows that any two-rectangle feature can be computed in six array references, any three-rectangle feature in eight, and any four-rectangle feature in nine.\n\nThe speed with which features may be evaluated does not adequately compensate for their number, however. For example, in a standard 24x24 pixel sub-window, there are a total of possible features, and it would be prohibitively expensive to evaluate them all when testing an image. Thus, the object detection framework employs a variant of the learning algorithm AdaBoost to both select the best features and to train classifiers that use them. This algorithm constructs a “strong” classifier as a linear combination of weighted simple “weak” classifiers.\n\nEach weak classifier is a threshold function based on the feature formula_2.\n\nThe threshold value formula_4 and the polarity formula_5 are determined in the training, as well as the coefficients formula_6.\n\nHere a simplified version of the learning algorithm is reported:\n\nInput: Set of positive and negative training images with their labels formula_7. If image is a face formula_8, if not formula_9.\n1. Initialization: assign a weight formula_10 to each image .\n2. For each feature formula_2 with formula_12\n1. Renormalize the weights such that they sum to one.\n2. Apply the feature to each image in the training set, then find the optimal threshold and polarity formula_13 that minimizes the weighted classification error. That is formula_14 where formula_15\n3. Assign a weight formula_6 to formula_17 that is inversely proportional to the error rate. In this way best classifiers are considered more.\n4. The weights for the next iteration, i.e. formula_18, are reduced for the images that were correctly classified.\n3. Set the final classifier to formula_19\n\n- On average only 0.01% of all sub-windows are positive (faces)\n- Equal computation time is spent on all sub-windows\n- Must spend most time only on potentially positive sub-windows.\n- A simple 2-feature classifier can achieve almost 100% detection rate with 50% FP rate.\n- That classifier can act as a 1st layer of a series to filter out most negative windows\n- 2nd layer with 10 features can tackle “harder” negative-windows which survived the 1st layer, and so on...\n- A cascade of gradually more complex classifiers achieves even better detection rates. The evaluation of the strong classifiers generated by the learning process can be done quickly, but it isn't fast enough to run in real-time. For this reason, the strong classifiers are arranged in a cascade in order of complexity, where each successive classifier is trained only on those selected samples which pass through the preceding classifiers. If at any stage in the cascade a classifier rejects the sub-window under inspection, no further processing is performed and continue on searching the next sub-window. The cascade therefore has the form of a degenerate tree. In the case of faces, the first classifier in the cascade – called the attentional operator – uses only two features to achieve a false negative rate of approximately 0% and a false positive rate of 40%. The effect of this single classifier is to reduce by roughly half the number of times the entire cascade is evaluated.\n\nIn cascading, each stage consists of a strong classifier. So all the features are grouped into several stages where each stage has certain number of features.\n\nThe job of each stage is to determine whether a given sub-window is definitely not a face or may be a face. A given sub-window is immediately discarded as not a face if it fails in any of the stages.\n\nA simple framework for cascade training is given below:\n\n- f = the maximum acceptable false positive rate per layer.\n- d = the minimum acceptable detection rate per layer.\n- Ftarget = target overall false positive rate.\n- P = set of positive examples.\n- N = set of negative examples.\n\nThe cascade architecture has interesting implications for the performance of the individual classifiers. Because the activation of each classifier depends entirely on the behavior of its predecessor, the false positive rate for an entire cascade is:\n\nSimilarly, the detection rate is:\n\nThus, to match the false positive rates typically achieved by other detectors, each classifier can get away with having surprisingly poor performance. For example, for a 32-stage cascade to achieve a false positive rate of , each classifier need only achieve a false positive rate of about 65%. At the same time, however, each classifier needs to be exceptionally capable if it is to achieve adequate detection rates. For example, to achieve a detection rate of about 90%, each classifier in the aforementioned cascade needs to achieve a detection rate of approximately 99.7%.\n\nIn videos of moving objects, one need not apply object detection to each frame. Instead, one can use tracking algorithms like the KLT algorithm to detect salient features within the detection bounding boxes and track their movement between frames. Not only does this improve tracking speed by removing the need to re-detect objects in each frame, but it improves the robustness as well, as the salient features are more resilient than the Viola-Jones detection framework to rotation and photometric changes.\n\n- Slides Presenting the Framework\n- Information Regarding Haar Basis Functions\n- Extension of Viola–Jones framework using SURF feature\n- IMMI - Rapidminer Image Mining Extension - open-source tool for image mining\n- Robust Real-Time Face Detection\n- An improved algorithm on Viola-Jones object detector\n- Citations of the Viola–Jones algorithm in Google Scholar\n- - Adaboost Explanation from ppt by Qing Chen, Discovery Labs, University of Ottawa and a video lecture by Ramsri Goutham.\n\n- Implementing the Viola–Jones Face Detection Algorithm by Ole Helvig Jensen\n- MATLAB: ,\n- OpenCV: implemented as codice_1.\n- Haar Cascade Detection in OpenCV\n- Cascade Classifier Training in OpenCV\n", "related": "NONE"}
{"id": "40409788", "url": "https://en.wikipedia.org/wiki?curid=40409788", "title": "Convolutional neural network", "text": "Convolutional neural network\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.\n\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\n\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n\nThe name “convolutional neural\nnetwork” indicates that the network employs a mathematical operation called\nconvolution. Convolution is a specialized kind of linear operation. Convolutional\nnetworks are simply neural networks that use convolution in place of general matrix\nmultiplication in at least one of their layers.\n\nA convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that \"convolve\" with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.\n\nThough the layers are colloquially referred to as convolutions, this is only by convention. Mathematically, it is technically a \"sliding dot product\" or cross-correlation. This has significance for the indices in the matrix, in that it affects how weight is determined at a specific index point.\n\nWhen programming a CNN, the input is a tensor with shape (number of images) x (image width) x (image height) x (image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map width) x (feature map height) x (feature map channels). A convolutional layer within a neural network should have the following attributes:\n\n- Convolutional kernels defined by a width and height (hyper-parameters).\n- The number of input channels and output channels (hyper-parameter).\n- The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map.\n\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for \"each\" neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.\n\nConvolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer. In addition, pooling may compute a max or an average. \"Max pooling\" uses the maximum value from each of a cluster of neurons at the prior layer. \"Average pooling\" uses the average value from each of a cluster of neurons at the prior layer.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from \"every\" element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its \"receptive field\". So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.\n\nEach neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.\n\nThe vector of weights and the bias are called \"filters\" and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces memory footprint because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.\n\nCNN design follows vision processing in living organisms.\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n\nTheir 1968 paper identified two basic visual cell types in the brain:\n\n- simple cells, whose output is maximized by straight edges having particular orientations within their receptive field\n- complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\n\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe \"neocognitron\" was introduced by Kunihiko Fukushima in 1980.\nIt was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.\n\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\n\nThe neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.\n\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance. It did so by utilizing weight sharing in combination with Backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights, instead of a local one.\n\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution. Since these TDNNs operated on spectrograms the resulting phoneme recognition system was invariant to both, shifts in time and in frequency. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.\n\nTDNNs now achieve the best performance in far distance speech recognition.\n\nIn 1990 Yamaguchi et al. introduced the concept of max pooling. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n\nA system to recognize hand-written ZIP Code numbers involved convolutions in which the kernel coefficients had been laboriously hand designed.\n\nYann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\n\nThis approach became a foundation of modern computer vision.\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks () digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n\nSimilarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.\n\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.\n\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.\n\nIn 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results. In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions. In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).\n\nSubsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.\n\nCompared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.\nA notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).\nCHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n\nIn the past, traditional multilayer perceptron (MLP) models have been used for image recognition. However, due to the full connectivity between nodes, they suffered from the curse of dimensionality, and did not scale well with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.\n\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n\nConvolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n- 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\n- Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\n- Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting feature map to be equivariant under changes in the locations of input features in the visual field, i.e. they grant translational equivariance.\n- Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\n\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\n\n- The \"depth\" of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\n- \"Stride\" controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. When the stride is 2 then the filters jump 2 pixels at a time as they slide around. Similarly, for any integer formula_1 a stride of \"S\" causes the filter to be translated by \"S\" units at a time per output. In practice, stride lengths of formula_2 are rare. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions when stride length is increased.\n- Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.\n\nThe spatial size of the output volume can be computed as a function of the input volume size formula_3, the kernel field size of the convolutional layer neurons formula_4, the stride with which they are applied formula_5, and the amount of zero padding formula_6 used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by\n\nformula_7\n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be formula_8 when the stride is formula_9 ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a \"depth slice\", the neurons in each depth slice are constrained to use the same weights and bias.\n\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which \"max pooling\" is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.\n\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n\nThe pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\nformula_10\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.\n\nDue to the aggressive reduction in the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.\n\nPooling is an important component of convolutional neural networks for object detection based on Fast R-CNN architecture.\n\nReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function formula_11. It effectively removes negative values from an activation map by setting them to zero. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n\nOther functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent formula_12, formula_13, and the sigmoid function formula_14. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\nThe \"loss layer\" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used.\n\nSoftmax loss is used for predicting a single class of \"K\" mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting \"K\" independent probability values in formula_15. Euclidean loss is used for regressing to real-valued labels formula_16.\n\nCNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n\nSince feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values \"v\" with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\nCommon filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n\nThe challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.\n\nTypical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either \"dropped out\" of the net with probability formula_17 or kept with probability formula_18, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n\nIn the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n\nAt testing time after training has finished, we would ideally like to find a sample average of all possible formula_19 dropped-out networks; unfortunately this is unfeasible for large values of formula_20. However, we can find an approximation by using the full network with each node's output weighted by a factor of formula_18, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates formula_19 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability formula_17. Each unit thus receives input from a random subset of units in the previous layer.\n\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n\nIn stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\n\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\nSince the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n\nL1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector formula_24 of every neuron to satisfy formula_25. Typical values of formula_26 are order of 3–4. Some papers report improvements when using this form of regularization.\n\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\n\nCurrently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\n\nThus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\nCNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called \nAlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\n\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\n\nIn 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.\n\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\n\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\nCNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.\n\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\n\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\n\nFor many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.\n\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs.\n\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n- Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\n- Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\n- Dlib: A toolkit for making real world machine learning and data analysis applications in C++.\n- Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\n- TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\n- Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\n- Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.\n\n- Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.\n\n", "related": "\n- Convolution\n- Deep learning\n- Natural-language processing\n- Neocognitron\n- Scale-invariant feature transform\n- Time delay neural network\n- Vision processing unit\n\n- CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision\n- An Intuitive Explanation of Convolutional Neural Networks — A beginner level introduction to what Convolutional Neural Networks are and how they work\n- Convolutional Neural Networks for Image Classification — Literature Survey\n"}
{"id": "97922", "url": "https://en.wikipedia.org/wiki?curid=97922", "title": "Digital image processing", "text": "Digital image processing\n\nIn computer science, digital image processing is the use of a digital computer to \"process\" digital images through an \"algorithm\". As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.\n\nMany of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The purpose of early image processing was to improve the quality of the image. It was aimed for human beings to improve the visual effect of people. In image processing, the input is a low-quality image, and the output is an image with improved quality. Common image processing include image enhancement, restoration, encoding, and compression. The first successful application was the American Jet Propulsion Laboratory (JPL). They used image processing techniques such as geometric correction, gradation transformation, noise removal, etc. on the thousands of lunar photos sent back by the Space Detector Ranger 7 in 1964, taking into account the position of the sun and the environment of the moon. The impact of the successful mapping of the moon's surface map by the computer has been a huge success. Later, more complex image processing was performed on the nearly 100,000 photos sent back by the spacecraft, so that the topographic map, color map and panoramic mosaic of the moon were obtained, which achieved extraordinary results and laid a solid foundation for human landing on the moon.\n\nThe cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. This led to images being processed in real-time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing, and is generally used because it is not only the most versatile method, but also the cheapest.\n\nThe basis for modern image sensors is metal-oxide-semiconductor (MOS) technology, which originates from the invention of the MOSFET (MOS field-effect transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. This led to the development of digital semiconductor image sensors, including the charge-coupled device (CCD) and later the CMOS sensor.\n\nThe charge-coupled device was invented by Willard S. Boyle and George E. Smith at Bell Labs in 1969. While researching MOS technology, they realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straighforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next. The CCD is a semiconductor circuit that was later used in the first digital video cameras for television broadcasting.\n\nThe NMOS active-pixel sensor (APS) was invented by Olympus in Japan during the mid-1980s. This was enabled by advances in MOS semiconductor device fabrication, with MOSFET scaling reaching smaller micron and then sub-micron levels. The NMOS APS was fabricated by Tsutomu Nakamura's team at Olympus in 1985. The CMOS active-pixel sensor (CMOS sensor) was later developed by Eric Fossum's team at the NASA Jet Propulsion Laboratory in 1993. By 2007, sales of CMOS sensors had surpassed CCD sensors.\n\nAn important development in digital image compression technology was the discrete cosine transform (DCT), a lossy compression technique first proposed by Nasir Ahmed in 1972. DCT compression became the basis for JPEG, which was introduced by the Joint Photographic Experts Group in 1992. JPEG compresses images down to much smaller file sizes, and has become the most widely used image file format on the Internet. Its highly efficient DCT compression algorithm was largely responsible for the wide proliferation of digital images and digital photos, with several billion JPEG images produced every day as of 2015.\n\nElectronic signal processing was revolutionized by the wide adoption of MOS technology in the 1970s. MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s, and then the first single-chip digital signal processor (DSP) chips in the late 1970s. DSP chips have since been widely used in digital image processing.\n\nThe discrete cosine transform (DCT) image compression algorithm has been widely implemented in DSP chips, with many companies developing DSP chips based on DCT technology. DCTs are widely used for encoding, decoding, video coding, audio coding, multiplexing, control signals, signaling, analog-to-digital conversion, formatting luminance and color differences, and color formats such as YUV444 and YUV411. DCTs are also used for encoding operations such as motion estimation, motion compensation, inter-frame prediction, quantization, perceptual weighting, entropy encoding, variable encoding, and motion vectors, and decoding operations such as the inverse operation between different color formats (YIQ, YUV and RGB) for display purposes. DCTs are also commonly used for high-definition television (HDTV) encoder/decoder chips.\n\nIn 1972, the engineer from British company EMI Housfield invented the X-ray computed tomography device for head diagnosis, which is what we usually called CT(Computer Tomography). The CT nucleus method is based on the projection of the human head section and is processed by computer to reconstruct the cross-sectional image, which is called image reconstruction. In 1975, EMI successfully developed a CT device for the whole body, which obtained a clear tomographic image of various parts of the human body. In 1979, this diagnostic technique won the Nobel Prize. Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.\n\nDigital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analogue means.\n\nIn particular, digital image processing is a concrete application of, and a practical technology based on:\n- Classification\n- Feature extraction\n- Multi-scale signal analysis\n- Pattern recognition\n- Projection\n\nSome techniques which are used in digital image processing include:\n- Anisotropic diffusion\n- Hidden Markov models\n- Image editing\n- Image restoration\n- Independent component analysis\n- Linear filtering\n- Neural networks\n- Partial differential equations\n- Pixelation\n- Point feature matching\n- Principal components analysis\n- Self-organizing maps\n- Wavelets\n\nDigital filters are used to blur and sharpen digital images. Filtering can be performed by:\n- convolution with specifically designed kernels (filter array) in the spatial domain\n- masking specific frequency regions in the frequency (Fourier) domain\n\nThe following examples show both methods:\n\nImages are typically padded before being transformed to the Fourier space, the highpass filtered images below illustrate the consequences of different padding techniques:\n\nNotice that the highpass filter shows extra edges when zero padded compared to the repeated edge padding.\n\nMATLAB example for spatial domain highpass filtering.\n\nAffine transformations enable basic image transformations including scale, rotate, translate, mirror and shear as is shown in the following examples:\nTo apply the affine matrix to an image, the image is converted to matrix in which each entry corresponds to the pixel intensity at that location. Then each pixel's location can be represented as a vector indicating the coordinates of that pixel in the image, [x, y], where x and y are the row and column of a pixel in the image matrix. This allows the coordinate to be multiplied by an affine-transformation matrix, which gives the position that the pixel value will be copied to in the output image.\n\nHowever, to allow transformations that require translation transformations, 3 dimensional homogeneous coordinates are needed. The third dimension is usually set to a non-zero constant, usually 1, so that the new coordinate is [x, y, 1]. This allows the coordinate vector to be multiplied by a 3 by 3 matrix, enabling translation shifts. So the third dimension, which is the constant 1, allows translation.\n\nBecause matrix multiplication is associative, multiple affine transformations can be combined into a single affine transformation by multiplying the matrix of each individual transformation in the order that the transformations are done. This results in a single matrix that, when applied to a point vector, gives the same result as all the individual transformations performed on the vector [x, y, 1] in sequence. Thus a sequence of affine transformation matrices can be reduced to a single affine transformation matrix.\n\nFor example, 2 dimensional coordinates only allow rotation about the origin (0, 0). But 3 dimensional homogeneous coordinates can be used to first translate any point to (0, 0), then perform the rotation, and lastly translate the origin (0, 0) back to the original point (the opposite of the first translation). These 3 affine transformations can be combined into a single matrix, thus allowing rotation around any point in the image.\n\nDigital cameras generally include specialized digital image processing hardware – either dedicated chips or added circuitry on other chips – to convert the raw data from their image sensor into a color-corrected image in a standard image file format.\n\n\"Westworld\" (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.\n\n", "related": "\n- Computer graphics\n- Computer vision\n- CVIPtools\n- Digitizing\n- Free boundary condition\n- GPGPU\n- Homomorphic filtering\n- Image analysis\n- IEEE Intelligent Transportation Systems Society\n- Multidimensional systems\n- Remote sensing software\n- Standard test image\n- Superresolution\n- Total variation denoising\n- Machine Vision\n- Bounded variation\n- Radiomics\n\n\n- Rafael C. Gonzalez (2008). \"Digital Image Processing. Prentice Hall. \"\n\n- Lectures on Image Processing, by Alan Peters. Vanderbilt University. Updated 7 January 2016.\n- IPRG Open group related to image processing research resources\n- Processing digital images with computer algorithms\n- IPOL Open research journal on image processing with software and web demos.\n"}
{"id": "58162731", "url": "https://en.wikipedia.org/wiki?curid=58162731", "title": "Bin picking", "text": "Bin picking\n\nBin picking (also referred to as random bin picking, even sometimes referred to as \"The Holy Grail in Sight\") is a core problem in computer vision and robotics. The goal is to have a robot with sensors and cameras attached to it pick-up known objects with random poses out of a bin using a suction gripper, parallel gripper, or other kind of robot end effector. Amazon previously held a competition focused on bin picking referred to as the \"Amazon Picking Challenge\", which was held from 2015 to 2017. The challenge tasked entrants with building their own robot hardware and software that could attempt simplified versions of the general task of picking and stowing items on shelves. The robots were scored by how many items were picked and stowed in a fixed amount of time. The first Amazon Robotics challenge was won by a team from TU Berlin in 2015, followed by a team from TU Delft in 2016. The last Amazon Robotics Challenge was won by the Australian Centre for Robotic Vision at Queensland University of Technology with their robot named Cartman. The Amazon Robotics/Picking Challenge was discontinued following the 2017 competition.\n\nAlthough there can be some overlap, bin picking is not to be confused with each picking or the bin packing problem.\n\n", "related": "\n- Bowl feeder\n"}
{"id": "58420390", "url": "https://en.wikipedia.org/wiki?curid=58420390", "title": "Object co-segmentation", "text": "Object co-segmentation\n\nIn computer vision, object co-segmentation is a special case of image segmentation, which is defined as jointly segmenting semantically similar objects in multiple images or video frames.\n\nIt is often challenging to extract segmentation masks of a target/object from a noisy collection of images or video frames, which involves object discovery coupled with segmentation. A noisy collection implies that the object/target is present sporadically in a set of images or the object/target disappears intermittently throughout the video of interest.\n\nA joint object discover and co-segmentation method based on coupled dynamic Markov networks has been proposed recently, which claims significant improvements in robustness against irrelevant/noisy video frames.\n\nIn action localization applications, \"object co-segmentation\" is also implemented as the segment-tube spatio-temporal detector. Inspired by the recent spatio-temporal action localization efforts with tubelets (sequences of bounding boxes), Le \"et al.\" present a new spatio-temporal action localization detector Segment-tube, which consists of sequences of per-frame segmentation masks. This Segment-tube detector can temporally pinpoint the starting/ending frame of each action category in the presence of preceding/subsequent interference actions in untrimmed videos. Simultaneously, the Segment-tube detector produces per-frame segmentation masks instead of bounding boxes, offering superior spatial accuracy to tubelets. This is achieved by alternating iterative optimization between temporal action localization and spatial action segmentation.\n\nThe proposed segment-tube detector is illustrated in the flowchart on the right. The sample input is an untrimmed video containing all frames in a pair figure skating video, with only a portion of these frames belonging to a relevant category (e.g., the DeathSpirals). Initialized with saliency based image segmentation on individual frames, this method first performs temporal action localization step with a cascaded 3D CNN and LSTM, and pinpoints the starting frame and the ending frame of a target action with a coarse-to-fine strategy. Subsequently, the segment-tube detector refines per-frame spatial segmentation with graph cut by focusing on relevant frames identified by the temporal action localization step. The optimization alternates between the temporal action localization and spatial action segmentation in an iterative manner. Upon practical convergence, the final spatio-temporal action localization results are obtained in the format of a sequence of per-frame segmentation masks (bottom row in the flowchart) with precise starting/ending frames. \n\n", "related": "\n- Image segmentation\n- Object detection\n- Video content analysis\n- Image analysis\n- Digital image processing\n- Activity recognition\n- Computer vision\n- Convolutional neural network\n- Long short-term memory\n"}
{"id": "58469853", "url": "https://en.wikipedia.org/wiki?curid=58469853", "title": "Visual temporal attention", "text": "Visual temporal attention\n\nVisual temporal attention is a special case of visual attention that involves directing attention to specific instant of time. Similar to its spatial counterpart visual spatial attention, these attention modules have been widely implemented in video analytics in computer vision to provide enhanced performance and human interpretable explanation of deep learning models. \n\nAs visual spatial attention mechanism allows human and/or computer vision systems to focus more on semantically more substantial regions in space, visual temporal attention modules enable machine learning algorithms to emphasize more on critical video frames in video analytics tasks, such as human action recognition. In convolutional neural network-based systems, the prioritization introduced by the attention mechanism is regularly implemented as a linear weighting layer with parameters determined by labeled training data. \n\nRecent video segmentation algorithms often exploits both spatial and temporal attention mechanisms. Research in human action recognition has accelerated significantly since the introduction of powerful tools such as Convolutional Neural Networks (CNNs). However, effective methods for incorporation of temporal information into CNNs are still being actively explored. Motivated by the popular recurrent attention models in natural language processing, the Attention-aware Temporal Weighted CNN (ATW CNN) is proposed in videos, which embeds a visual attention model into a temporal weighted multi-stream CNN. This attention model is implemented as temporal weighting and it effectively boosts the recognition performance of video representations. Besides, each stream in the proposed ATW CNN framework is capable of end-to-end training, with both network parameters and temporal weights optimized by stochastic gradient descent (SGD) with back-propagation. Experimental results show that the ATW CNN attention mechanism contributes substantially to the performance gains with the more discriminative snippets by focusing on more relevant video segments.\n\n", "related": "\n- Attention\n- Visual spatial attention\n- Action Recognition\n- Video content analysis\n- Convolutional neural network\n- Computer vision\n"}
{"id": "4217714", "url": "https://en.wikipedia.org/wiki?curid=4217714", "title": "Salience (neuroscience)", "text": "Salience (neuroscience)\n\nThe salience (also called saliency) of an item is the state or quality by which it stands out from its neighbors. Saliency detection is considered to be a key attentional mechanism that facilitates learning and survival by enabling organisms to focus their limited perceptual and cognitive resources on the most pertinent subset of the available sensory data.\n\nSaliency typically arises from contrasts between items and their neighborhood, such as a red dot surrounded by white dots, a flickering message indicator of an answering machine, or a loud noise in an otherwise quiet environment. Saliency detection is often studied in the context of the visual system, but similar mechanisms operate in other sensory systems. What is salient can be influenced by training: for example, for human subjects particular letters can become salient by training.\n\nWhen attention deployment is driven by salient stimuli, it is considered to be bottom-up, memory-free, and reactive. Conversely, attention can also be guided by top-down, memory-dependent, or anticipatory mechanisms, such as when looking ahead of moving objects or sideways before crossing streets. Humans and other animals have difficulty paying attention to more than one item simultaneously, so they are faced with the challenge of continuously integrating and prioritizing different bottom-up and top-down influences.\n\nThe brain component named the hippocampus helps with the assessment of salience and context by using past memories to filter new incoming stimuli, and placing those that are most important into long term memory. The entorhinal cortex is the pathway into and out of the hippocampus, and is an important part of the brain's memory network; research shows that it is a brain region that suffers damage early on in Alzheimer's disease, one of the effects of which is altered (diminished) salience.\n\nThe pulvinar nuclei (in the thalamus) modulates physical/perceptual salience in attentional selection.\n\nOne group of neurons (i.e., D1-type medium spiny neurons) within the nucleus accumbens shell (NAcc shell) assigns appetitive motivational salience (\"want\" and \"desire\", which includes a motivational component), aka incentive salience, to rewarding stimuli, while another group of neurons (i.e., D2-type medium spiny neurons) within the NAcc shell assigns aversive motivational salience to aversive stimuli.\n\nThe term is widely used in the study of perception and cognition to refer to any aspect of a stimulus that, for any of many reasons, stands out from the rest. Salience may be the result of emotional, motivational or cognitive factors and is not necessarily associated with physical factors such as intensity, clarity or size. Although salience is thought to determine attentional selection, salience associated with physical factors does not necessarily influence selection of a stimulus.\n\nSalience bias (also known as perceptual salience) is the cognitive bias that predisposes individuals to focus on items that are more prominent or emotionally striking and ignore those that are unremarkable, even though this difference is often irrelevant by objective standards. Salience bias is closely related to the concept of availability in behavioral economics:\n\nKapur (2003) proposed that a hyperdopaminergic state, at a \"brain\" level of description, leads to an aberrant assignment of salience to the elements of one's experience, at a \"mind\" level. These aberrant salience attributions have been associated with altered activities in the mesolimbic system, including the striatum, the amygdala, the hippocampus, and the parahippocampal gyrus. Dopamine mediates the conversion of the neural representation of an external stimulus from a neutral bit of information into an attractive or aversive entity, i.e. a salient event. Symptoms of schizophrenia may arise out of 'the aberrant assignment of salience to external objects and internal representations', and antipsychotic medications reduce positive symptoms by attenuating aberrant motivational salience via blockade of the dopamine D2 receptors (Kapur, 2003).\n\nAlternative areas of investigation include supplementary motor areas, frontal eye fields and parietal eye fields. These areas of the brain are involved with calculating predictions and visual salience. Changing expectations on where to look restructures these areas of the brain. This cognitive repatterning can result in some of the symptoms found in such disorders.\n\nIn the domain of psychology, efforts have been made in modeling the mechanism of human attention, including the learning of prioritizing the different bottom-up and top-down influences.\n\nIn the domain of computer vision, efforts have been made in modeling the mechanism of human attention, especially the bottom-up attentional mechanism, including both spatial and temporal attention. Such a process is also called visual saliency detection.\n\nGenerally speaking, there are two kinds of models to mimic the bottom-up saliency mechanism. One way is based on the spatial contrast analysis: for example, a center-surround mechanism is used to define saliency across scales, which is inspired by the putative neural mechanism. The other way is based on the frequency domain analysis. While they used the amplitude spectrum to assign saliency to rarely occurring magnitudes, Guo et al. use the phase spectrum instead.\nRecently, Li et al. introduced a system that uses both the amplitude and the phase information.\n\nA key limitation in many such approaches is their computational complexity leading to less than real-time performance, even on modern computer hardware. Some recent work attempts to overcome these issues at the expense of saliency detection quality under some conditions. Other work suggests that saliency and associated speed-accuracy phenomena may be a fundamental mechanisms determined during recognition through gradient descent, needing not be spatial in nature.\n\n", "related": "\n- Availability heuristic\n- Dopamine hypothesis of schizophrenia\n- Latent inhibition\n- Schizophrenia\n- Schizotypy\n- Spatial attention\n- Temporal attention\n\n- iLab at the University of Southern California\n- Scholarpedia article on visual saliency by Prof. Laurent Itti\n- Saliency map at Scholarpedia\n"}
{"id": "1059791", "url": "https://en.wikipedia.org/wiki?curid=1059791", "title": "Computational photography", "text": "Computational photography\n\nComputational photography refers to digital image capture and processing techniques that use digital computation instead of optical processes. Computational photography can improve the capabilities of a camera, or introduce features that were not possible at all with film based photography, or reduce the cost or size of camera elements. Examples of computational photography include in-camera computation of digital panoramas, high-dynamic-range images, and light field cameras. Light field cameras use novel optical elements to capture three dimensional scene information which can then be used to produce 3D images, enhanced depth-of-field, and selective de-focusing (or \"post focus\"). Enhanced depth-of-field reduces the need for mechanical focusing systems. All of these features use computational imaging techniques.\n\nThe definition of computational photography has evolved to cover a number of\nsubject areas in computer graphics, computer vision, and applied\noptics. These areas are given below, organized according to a taxonomy\nproposed by Shree K. Nayar. Within each area is a list of techniques, and for\neach technique one or two representative papers or books are cited.\nDeliberately omitted from the\ntaxonomy are image processing (see also digital image processing)\ntechniques applied to traditionally captured\nimages in order to produce better images. Examples of such techniques are\nimage scaling, dynamic range compression (i.e. tone mapping),\ncolor management, image completion (a.k.a. inpainting or hole filling),\nimage compression, digital watermarking, and artistic image effects.\nAlso omitted are techniques that produce range data,\nvolume data, 3D models, 4D light fields,\n4D, 6D, or 8D BRDFs, or other high-dimensional image-based representations. Epsilon Photography is a sub-field of computational photography.\n\nPhotos taken using computational photography can sometimes look better than those taken by professionals using significantly more expensive equipment.\n\nThis is controlling photographic illumination in a structured fashion, then processing the captured images,\nto create new images. The applications include image-based relighting, image enhancement, image deblurring, geometry/material recovery and so forth.\n\nHigh-dynamic-range imaging uses differently exposed pictures of the same scene to extend dynamic range. Other examples include processing and merging differently illuminated images of the same subject matter (\"lightspace\").\n\nThis is capture of optically coded images, followed by computational decoding to produce new images.\nCoded aperture imaging was mainly applied in astronomy or X-ray imaging to boost the image quality. Instead of a single pin-hole, a pinhole pattern is applied in imaging, and deconvolution is performed to recover the image. In coded exposure imaging, the on/off state of the shutter is coded to modify the kernel of motion blur. In this way motion deblurring becomes a well-conditioned problem. Similarly, in a lens based coded aperture, the aperture can be modified by inserting a broadband mask. Thus, out of focus deblurring becomes a well-conditioned problem. The coded aperture can also improve the quality in light field acquisition using Hadamard transform optics.\n\nCoded aperture patterns can also be designed using color filters, in order to apply different codes at different wavelengths. This allows to increase the amount of light that reaches the camera sensor, compared to binary masks.\n\nComputational imaging is a set of imaging techniques that combine data acquisition and data processing to create the image of an object through indirect means to yield enhanced resolution, additional information such as optical phase or 3D reconstruction. The information is often recorded without using a conventional optical microscope configuration or with limited datasets.\n\nComputational imaging allows to go beyond physical limitations of optical systems, such as numerical aperture\n, or even obliterates the need for optical elements\n\nFor parts of the optical spectrum where imaging elements such as objectives are difficult to manufacture or image sensors cannot be miniaturized, computational imaging provides useful alternatives, in fields such as X-Ray and THz radiations.\n\nAmong common computational imaging techniques are lensless imaging, computational speckle imaging, ptychography and Fourier ptychography.\n\nComputational imaging technique often draws on compressive sensing or phase retrieval techniques, where the angular spectrum of the object is being reconstructed. Other techniques are related to the field of computational imaging, such as digital holography, computer vision and inverse problems such as tomography.\n\nThis is processing of non-optically-coded images to produce new images.\n\nThese are detectors that combine sensing and processing, typically in hardware, like the oversampled binary image sensor. \n\nAlthough computational photography is a currently popular buzzword in computer graphics, many of its\ntechniques first appeared in the computer vision literature,\neither under other names or within papers aimed at 3D shape analysis.\n\nComputational photography, as an art form, has been practiced by capture of differently exposed pictures of the same subject matter, and combining them together. This was the inspiration for the development of the wearable computer in the 1970s and early 1980s. Computational photography was inspired by the work of Charles Wyckoff, and thus computational photography datasets (e.g. differently exposed pictures of the same subject matter that are taken in order to make a single composite image) are sometimes referred to as Wyckoff Sets, in his honor.\n\nEarly work in this area (joint estimation of image projection and exposure value) was undertaken by Mann and Candoccia.\n\nCharles Wyckoff devoted much of his life to creating special kinds of 3-layer photographic films that captured different exposures of the same subject matter. A picture of a nuclear explosion, taken on Wyckoff's film, appeared on the cover of Life Magazine and showed the dynamic range from dark outer areas to inner core.\n\n", "related": "\n- Adaptive Optics\n- Multispectral imaging\n- Simultaneous localization and mapping\n- Super-resolution microscopy\n- Time-of-flight camera\n\n- Nayar, Shree K. (2007). \"Computational Cameras\", \"Conference on Machine Vision Applications\".\n- \"Computational Photography\" (Raskar, R., Tumblin, J.,), A.K. Peters. In press.\n- Special issue on Computational Photography, IEEE Computer, August 2006.\n- Camera Culture and Computational Journalism: Capturing and Sharing Visual Experiences, IEEE CG&A Special Issue, Feb 2011.\n- Rick Szeliski (2010), \"Computer Vision: Algorithms and Applications\", Springer.\n- Computational Photography: Methods and Applications (Ed. Rastislav Lukac), CRC Press, 2010.\n- Intelligent Image Processing (John Wiley and Sons book information).\n- Comparametric Equations.\n- GJB-1: Increasing the dynamic range of a digital camera by using the Wyckoff principle\n- Examples of wearable computational photography as an art form\n- Siggraph Course in Computational Photography\n"}
{"id": "59392032", "url": "https://en.wikipedia.org/wiki?curid=59392032", "title": "Dynamic texture", "text": "Dynamic texture\n\nDynamic texture ( sometimes referred to as temporal texture) is the texture with motion which can be found in videos of sea-waves, fire, smoke, wavy trees, etc. Dynamic texture has a spatially repetitive pattern with time-varying visual pattern. Modeling and analyzing dynamic texture is a topic of images processing and pattern recognition in computer vision.\n\nExtracting features that describe the dynamic texture can be utilized for tasks of images sequences classification, segmentation, recognition and retrieval.Comparing with texture found within static images, analyzing dynamic texture is a challenging problem. It is important that the extracted features from dynamic texture combine motion and appearance description, and also be invariance to some transformation such as rotation,translation and illumination.\n\nThe methods of dynamic texture recognition can categorized as follows:\n\n1. Methods based on optical flow: by applying optical flow to the dynamic texture, velocity with direction and magnitude can be detected and used to recognize the dynamic texture. Due to simplicity of its computation, it is currently the most popular method.\n2. Methods computing geometric properties: this methods track the surfaces of motion trajectories in spatiotemporal domain.\n3. Methods based on local spatiotemporal filtering : this methods analyze the local spatiotemporal patterns and its orientation and energy and employ them as feature used for classification.\n4. Methods based on global spatiotemporal transform: this method characterize the motion at different scale using wavelets that can decompose the motion into local and global.\n5. Model-based methods : These methods aims at generating a model to describe the motion by a set of parameters.\n\n- Segmenting the sequence images of natural scenes. This helps on differentiate between streets and grass alongside these streets which could be used in the application of navigations.\n\n- Motion detection : Dynamic texture features extracted from footage videos can be exploited to detect abnormal crowd activities.\n\n- Video classification: video of natural scenes or other scenes that exhibit dynamic textures.\n\n- Video retrieval : Dynamic textures can be employed as a feature retrieve videos that contain, for example, sea-waves, smoke, clouds, wavy trees.\n", "related": "NONE"}
{"id": "59654517", "url": "https://en.wikipedia.org/wiki?curid=59654517", "title": "Graph cut optimization", "text": "Graph cut optimization\n\nGraph cut optimization is a combinatorial optimization method applicable to a family of functions of discrete variables, named after the concept of cut in the theory of flow networks. Thanks to the max-flow min-cut theorem, determining the minimum cut over a graph representing a flow network is equivalent to computing the maximum flow over the network. Given a pseudo-Boolean function formula_1, if it is possible to construct a flow network with positive weights such that\n- each cut formula_2 of the network can be mapped to an assignment of variables formula_3 to formula_1 (and vice versa), and\n- the flow through formula_2 equals formula_6 (up to an additive constant)\nthen it is possible to find the global optimum of formula_1 in polynomial time by computing a minimum cut of the graph. The mapping between cuts and variable assignments is done by representing each variable with one node in the graph and, given a cut, each variable will have a value of 0 if the corresponding node belongs to the component connected to the source, or 1 if it belong to the component connected to the sink.\n\nNot all pseudo-Boolean functions can be represented by a flow network, and in the general case the global optimization problem is NP-hard. There exist sufficient conditions to characterise families of functions that can be optimised through graph cuts, such as submodular quadratic functions. Graph cut optimization can be extended to functions of discrete variables with a finite number of values, that can be approached with iterative algorithms with strong optimality properties, computing one graph cut at each iteration.\n\nGraph cut optimization is an important tool for inference over graphical models such as Markov random fields or conditional random fields, and it has applications in computer vision problems such as image segmentation, denoising, registration and stereo matching..\n\nA pseudo-Boolean function formula_8 is said to be \"representable\" if there exists a graph formula_9 with non-negative weights and with source and sink nodes formula_10 and formula_11 respectively, and there exists a set of nodes formula_12 such that, for each tuple of values formula_13 assigned to the variables, formula_14 equals (up to a constant) the value of the flow determined by a minimum cut formula_15 of the graph formula_16 such that formula_17 if formula_18 and formula_19 if formula_20.\n\nIt is possible to classify pseudo-Boolean functions according to their order, determined by the maximum number of variables contributing to each single term. All first order functions, where each term depends upon at most one variable, are always representable. Quadratic functions\n\nare representable if and only if they are submodular, i.e. for each quadratic term formula_22 the following condition is satisfied\n\nCubic functions\n\nare representable if and only if they are \"regular\", i.e. all possible binary projections to two variables, obtained by fixing the value of the remaining variable, are submodular. For higher-order functions, regularity is a necessary condition for representability.\n\nGraph construction for a representable function is simplified by the fact that the sum of two representable functions formula_25 and formula_26 is representable, and its graph formula_27 is the union of the graphs formula_28 and formula_29 representing the two functions. Such theorem allows to build separate graphs representing each term and combine them to obtain a graph representing the entire function.\n\nThe graph representing a quadratic function of formula_30 variables contains formula_31 vertices, two of them representing the source and sink and the others representing the variables. When representing higher-order functions, the graph contains auxiliary nodes that allow to model higher-order interactions.\n\nA unary term formula_32 depends only on one variable formula_33 and can be represented by a graph with one non-terminal node formula_34 and one edge formula_35 with weight formula_36 if formula_37, or formula_38 with weight formula_39 if formula_40.\n\nA quadratic (or binary) term formula_22 can be represented by a graph containing two non-terminal nodes formula_34 and formula_43. The term can be rewritten as\n\nwith\n\nIn this expression, the first term is constant and it is not represented by any edge, the two following terms depend on one variable and are represented by one edge, as shown in the previous section for unary terms, while the third term is represented by an edge formula_46 with weight formula_47 (submodularity guarantees that the weight is non-negative).\n\nA cubic (or ternary) term formula_48 can be represented by a graph with four non-terminal nodes, three of them (formula_34, formula_43 and formula_51) associated to the three variables plus one fourth auxiliary node formula_52. A generic ternary term can be rewritten as the sum of a constant, three unary terms, three binary terms, and a ternary term in simplified form. There may be two different cases, according to the sign of formula_53. If formula_54 then\n\nwith\n\nIf formula_57 the construction is similarly, but the variables will have opposite value. If the function is regular, then all its projections of two variables will be submodular, implying that formula_58, formula_59 and formula_60 are positive and then all terms in the new representation are submodular.\n\nIn this decomposition, the constant, unary and binary terms can be represented as shown in the previous sections. If formula_54 the ternary term can be represented with a graph with four edges formula_62, formula_63, formula_64, formula_65, all with weight formula_66, while if formula_57 the term can be represented by four edges formula_68, formula_69, formula_70, formula_71 with weight formula_72.\n\nAfter building a graph representing a pseudo-Boolean function, it is possible to compute a minimum cut using one among the various algorithms developed for flow networks, such as Ford–Fulkerson, Edmonds–Karp, and Boykov–Kolmogorov algorithm. The result is a partition of the graph in two connected components formula_73 and formula_74 such that formula_75 and formula_76, and the function attains its global minimum when formula_18 for each formula_78 such that the corresponding node formula_79, and formula_20 for each formula_78 such that the corresponding node formula_19.\n\nMax-flow algorithms such as Boykov–Kolmogorov's are very efficient in practice for sequential computation, but they are difficult to parallelise, making them not suitable for distributed computing applications and preventing them from exploiting the potential of modern CPUs. Parallel max-flow algorithms were developed, such as push-relabel and jump-flood, that can also take advantage of hardware acceleration in GPGPU implementations.\n\nThe previous construction allows global optimization of pseudo-Boolean functions only, but it can be extended to quadratic functions of discrete variables with a finite number of values, in the form\n\nwhere formula_84 and formula_85. The function formula_86 represents the unary contribution of each variable (often referred as \"data term\"), while the function formula_87 represents binary interactions between variables (\"smoothness term\"). In the general case, optimization of such functions is a NP-hard problem, and stochastic optimization methods such as simulated annealing are sensitive to local minima and in practice they can generate arbitrarily sub-optimal results. With graph cuts it is possible to construct move-making algorithms that allow to reach in polynomial time a local minima with strong optimality properties for a wide family of quadratic functions of practical interest (when the binary interaction formula_87 is a metric or a semimetric), such that the value of the function in the solution lies within a constant and known factor from the global optimum.\n\nGiven a function formula_89 with formula_90, and a certain assignment of values formula_91 to the variables, it is possible to associate each assignment formula_3 to a partition formula_93 of the set of variables, such that, formula_94. Give two distinct assignments formula_95 and formula_96 and a value formula_97, a move that transforms formula_95 into formula_96 is said to be an formula_100-expansion if formula_101 and formula_102. Given a couple of values formula_100 and formula_104, a move is said to be an formula_105-swap if formula_106. Intuitively, an formula_100-expansion move from formula_3 assigns the value of formula_100 to some variables that have a different value in formula_3, while an formula_105-swap move assigns formula_100 to some variables that have value formula_104 in formula_3 and vice versa.\n\nFor each iteration, the formula_100-expansion algorithm computes, for each possible value formula_100, the minimum of the function among all assignments formula_117 that can be reached with a single formula_100-expansion move from the current temporary solution formula_3, and takes it as the new temporary solution.\n\nThe formula_105-swap algorithm is similar, but it searches for the minimum among all assignments formula_130 reachable with a single formula_105-swap move from formula_3.\n\nIn both cases, the optimization problem in the innermost loop can be solved exactly and efficiently with a graph cut. Both algorithms terminate certainly in a finite number of iterations of the outer loop, and in practice such number is small, with most of the improvement happening at the first iteration. The algorithms can generate different solutions depending on the initial guess, but in practice they are robust with respect to initialisation, and starting with a point where all variables are assigned to the same random value is usually sufficient to produce good quality results.\n\nThe solution generated by such algorithms is not necessarily a global optimum, but it has strong guarantees of optimality. If formula_87 is a metric and formula_3 is a solution generated by the formula_100-expansion algorithm, or if formula_87 is a semimetric and formula_3 is a solution generated by the formula_105-swap algorithm, then formula_6 lies within a known and constant factor from the global minimum formula_149:\n\nGenerally speaking, the problem of optimizing a non-submodular pseudo-Boolean function is NP-hard and cannot be solved in polynomial time with a simple graph cut. The simplest approach is to approximate the function with a similar but submodular one, for instance truncating all non-submodular terms or replacing them with similar submodular expressions. Such approach is generally sub-optimal, and it produces acceptable results only if the number of non-submodular terms is relatively small.\n\nIn case of quadratic non-submodular functions, it is possible to compute in polynomial time a partial solution using algorithms such as QPBO. Higher-order functions can be reduced in polynomial time to a quadratic form that can be optimised with QPBO.\n\nQuadratic functions are extensively studied and were characterised in detail, but more general results were derived also for higher-order functions. While quadratic functions can indeed model many problems of practical interest, they are limited by the fact they can represent only binary interactions between variables. The possibility to capture higher-order interactions allows to better capture the nature of the problem and it can provide higher quality results that could be difficult to achieve with quadratic models. For instance in computer vision applications, where each variable represents a pixel or voxel of the image, higher-order interactions can be used to model texture information, that would be difficult to capture using only quadratic functions.\n\nSufficient conditions analogous to submodularity were developed to characterise higher-order pseudo-Boolean functions that can be optimised in polynomial time, and there exists algorithms analogous to formula_100-expansion and formula_105-swap for some families of higher-order functions. The problem is NP-hard in the general case, and approximate methods were developed for fast optimization of functions that do not satisfy such conditions.\n\n\n- Implementation (C++) of several graph cut algorithms by Vladimir Kolmogorov.\n- GCO, graph cut optimization library by Olga Veksler and Andrew Delong.\n", "related": "NONE"}
{"id": "57193995", "url": "https://en.wikipedia.org/wiki?curid=57193995", "title": "Legendre moment", "text": "Legendre moment\n\nIn mathematics, Legendre moments are a type of image moment and are achieved by using the Legendre polynomial. Legendre moments are used in areas of image processing including: pattern and object recognition, image indexing, line fitting, feature extraction, edge detection, and texture analysis. Legendre moments have been studied as a means to reduce image moment calculation complexity by limiting the amount of information redundancy through approximation.\n\nWith order of \"m\" + \"n\", and object intensity function \"f\"(\"x\",\"y\"):\n\nwhere \"m\",\"n\" = 1, 2, 3, ... with the \"n\"th-order Legendre polynomials being:\n\nwhich can also be written:\n\nwhere \"D\"(\"n\") = floor(\"n\"/2). The set of Legendre polynomials {\"P\"(\"x\")} form an orthogonal set on the interval [−1,1]:\n\nA recurrence relation can be used to compute the Legendre polynomial:\n\n\"f\"(\"x\",\"y\") can be written as an infinite series expansion in terms of Legendre polynomials [−1 ≤ \"x\",\"y\" ≤ 1.]:\n\n", "related": "\n- Image moment\n- Legendre polynomial\n- Zernike polynomials\n"}
{"id": "31486641", "url": "https://en.wikipedia.org/wiki?curid=31486641", "title": "Computer vision dazzle", "text": "Computer vision dazzle\n\nComputer vision dazzle also known as CV dazzle, dazzle makeup, or anti-surveillance makeup, is a type of camouflage used to hamper facial recognition software, inspired by dazzle camouflage used by warships. CV dazzle combines stylized makeup, asymmetric hair, and sometimes infrared lights built in to glasses or clothing to break up detectable facial patterns recognized by computer vision algorithms in much the same way that warships contrasted color and used sloping lines and curves to distort the structure of a vessel. It has been shown to be somewhat successful at defeating face detection software in common use, including that employed by Facebook. CV dazzle attempts to block detection by facial recognition technologies such as DeepFace \"by creating an 'anti-face'\". It uses occlusion, covering certain facial features; transformation, altering the shape or colour of parts of the face; and a combination of the two. Prominent artists employing this technique include Adam Harvey and Jillian Mayer.\n\n", "related": "\n- Adversarial machine learning\n- Dazzle camouflage\n"}
{"id": "61980052", "url": "https://en.wikipedia.org/wiki?curid=61980052", "title": "Semi-global matching", "text": "Semi-global matching\n\nSemi-global matching (SGM) is a computer vision algorithm for the estimation of a dense disparity map from a rectified stereo image pair, introduced in 2005 by Heiko Hirschmüller. Given its predictable run time, its favourable trade-off between quality of the results and computing time, and its suitability for fast parallel implementation in FPGA, it has encountered wide adoption in real-time stereo vision applications such as robotics and advanced driver assistance systems.\n\nPixelwise stereo matching allows to perform real-time calculation of disparity maps by measuring the similarity of each pixel in one stereo image to each pixel within a subset in the other stereo image. Given a rectified stereo image pair, for a pixel with coordinates formula_1 the set of pixels in the other image is usually selected as formula_2, where formula_3 is a maximum allowed disparity shift.\n\nA simple search for the best matching pixel produces many spurious matches, and this problem can be mitigated with the addition of a regularisation term that penalises jumps in disparity between adjacent pixels, with a cost function in the form\nwhere formula_5 is the pixel-wise dissimilarity cost at pixel formula_6 with disparity formula_7, and formula_8 is the regularisation cost between pixels formula_6 and formula_10 with disparities formula_7 and formula_12 respectively, for all pairs of neighbouring pixels formula_13. Such constraint can be efficiently enforced on a per-scanline basis by using dynamic programming (e.g. the Viterbi algorithm), but such limitation can still introduce streaking artefacts in the depth map, because little or no regularisation is performed across scanlines.\n\nA possible solution is to perform global optimisation in 2D, which is however an NP-complete problem in the general case. For some families of cost functions (e.g. submodular functions) a solution with strong optimality properties can be found in polymonial time using graph cut optimization, however such global methods are generally too expensive for real-time processing.\n\nThe idea behind SGM is to perform line optimisation along multiple directions and computing an aggregated cost formula_14 by summing the costs to reach pixel formula_6 with disparity formula_16 from each direction. The number of directions affects the run time of the algorithm, and while 16 directions usually ensure good quality, a lower number can be used to achieve faster execution. A typical 8-direction implementation of the algorithm can compute the cost in two passes, a forward pass accumulating the cost from the left, top-left, top, and top-right, and a backward pass accumulating the cost from right, bottom-right, bottom, and bottom-left. A single-pass algorithm can be implemented with only five directions.\n\nThe cost is composed by a matching term formula_17 and a binary regularisation term formula_18. The former can be in principle any local image dissimilarity measure, and commonly used functions are absolute or squared intensity difference (usually summed over a window around the pixel, and after applying a high-pass filter to the images to gain some illumination invariance), Birchfield–Tomasi dissimilarity, Hamming distance of the census transform, Pearson correlation (normalized cross-correlation). Even mutual information can be approximated as a sum over the pixels, and thus used as a local similarity metric. The regularisation term has the form\nwhere formula_20 and formula_21 are two constant parameters, with formula_22. The three-way comparison allows to assign a smaller penalty for unitary changes in disparity, thus allowing smooth transitions corresponding e.g. to slanted surfaces, and penalising larger jumps while preserving discontinuities due to the constant penalty term. To further preserve discontinuities, the gradient of the intensity can be used to adapt the penalty term, because discontinuities in depth usually correspond to a discontinuity in image intensity formula_23, by setting\nfor each pair of pixels formula_6 and formula_10.\n\nThe accumulated cost formula_27 is the sum of all costs formula_28 to reach pixel formula_6 with disparity formula_16 along direction formula_31. Each term can be expressed recursively as\nwhere the minimum cost at the previous pixel formula_33 is subtracted for numerical stability, since it is constant for all values of disparity at the current pixel and therefore it does not affect the optimisation.\n\nThe value of disparity at each pixel is given by formula_34, and sub-pixel accuracy can be achieved by fitting a curve in formula_35 and its neighbouring costs and taking the minimum along the curve. Since the two images in the stereo pair are not treated symmetrically in the calculations, a consistency check can be performed by computing the disparity a second time in the opposite direction, swapping the role of the left and right image, and invalidating the result for the pixels where the result differs between the two calculations. Further post-processing techniques for the refinement of the disparity image include morphological filtering to remove outliers, intensity consistency checks to refine textureless regions, and interpolation to fill in pixels invalidated by consistency checks.\n\nThe cost volume formula_36 for all values of formula_37 and formula_16 can be precomputed and in an implementation of the full algorithm, using formula_3 possible disparity shifts and formula_40 directions, each pixel is subsequently visited formula_40 times, therefore the computational complexity of the algorithm for an image of size formula_42 is formula_43.\n\nThe main drawback of SGM is its memory consumption. An implementation of the two-pass 8-directions version of the algorithm requires to store formula_44 elements, since the accumulated cost volume has a size of formula_45 and to compute the cost for a pixel during each pass it is necessary to keep track of the formula_3 path costs of its left or right neighbour along one direction and of the formula_47 path costs of the pixels in the row above or below along 3 directions. One solution to reduce memory consumption is to compute SGM on partially overlapping image tiles, interpolating the values over the overlapping regions. This method also allows to apply SGM to very large images, that would not fit within memory in the first place.\n\nA memory-efficient approximation of SGM stores for each pixel only the costs for the disparity values that represent a minimum along some direction, instead of all possible disparity values. The true minimum is highly likely to be predicted by the minima along the eight directions, thus yielding similar quality of the results. The algorithm uses eight directions and three passes, and during the first pass it stores for each pixel the cost for the optimal disparity along the four top-down directions, plus the two closest lower and higher values (for sub-pixel interpolation). Since the cost volume is stored in a sparse fashion, the four values of optimal disparity need also to be stored. In the second pass, the other four bottom-up directions are computed, completing the calculations for the four disparity values selected in the first pass, that now have been evaluated along all eight directions. An intermediate value of cost and disparity is computed from the output of the first pass and stored, and the memory of the four outputs from the first pass is replaced with the four optimal disparity values and their costs from the directions in the second pass. A third pass goes again along the same directions used in the first pass, completing the calculations for the disparity values from the second pass. The final result is then selected among the four minima from the third pass and the intermediate result computed during the second pass.\n\nIn each pass four disparity values are stored, together with three cost values each (the minimum and its two closest neighbouring costs), plus the disparity and cost values of the intermediate result, for a total of eighteen values for each pixel, making the total memory consumption equal to formula_48, at the cost in time of an additional pass over the image.\n\n", "related": "NONE"}
{"id": "62008819", "url": "https://en.wikipedia.org/wiki?curid=62008819", "title": "Birchfield–Tomasi dissimilarity", "text": "Birchfield–Tomasi dissimilarity\n\nIn computer vision, the Birchfield–Tomasi dissimilarity is a pixelwise image dissimilarity measure that is robust with respect to sampling effects. In the comparison of two image elements, it fits the intensity of one pixel to the linearly interpolated intensity around a corresponding pixel on the other image. It is used as a dissimilarity measure in stereo matching, where one-dimensional search for correspondences is performed to recover a dense disparity map from a stereo image pair.\n\nWhen performing pixelwise image matching, the measure of dissimilarity between pairs of pixels from different images is affected by differences in image acquisition such as illumination bias and noise. Even when assuming no difference in these aspects between an image pair, additional inconsistencies are introduced by the pixel sampling process, because each pixel is a sample obtained integrating the continuous light signal over a finite region of space, and two pixels matching the same feature of the image content may correspond to slightly different regions of the real object that can reflect light differently and can be subject to partial occlusion, depth discontinuity, or different lens defocus, thus generating different intensity signals.\n\nThe Birchfield–Tomasi measure compensates for the sampling effect by considering the linear interpolation of the samples. Pixel similarity is then determined by finding the best match between the intensity of a pixel sample in one image and the interpolated function in an interval around a location in the other image.\n\nConsidering the stereo matching problem for a rectified stereo pair, where the search for correspondences is performed in one dimension, given two columns formula_1 and formula_2 along the same scanline for the left and right image respectively, it is possible to define two symmetric functions\nwhere formula_4 and formula_5 are the linear interpolation functions of the left and right image intensity formula_6 and formula_6 along the scanline. The Birchfield–Tomasi dissimilarity can then be defined as\n\nIn practice the measure can be computed with only a small and constant overhead with respect to the calculation of the simple intensity difference, because it is not necessary to reconstruct the interpolant function. Given that the interpolant is linear within each unit interval centred around a pixel, its minimum is located in one of its extremities. Therefore, formula_9 can be written as\nwhere\ndenoting with formula_12 and formula_13 the values of the interpolated intensities at the rightmost and leftmost extremities of a one-pixel interval centred around formula_2\nThe other function formula_16 can be similarly rewritten, completing the expression for formula_17.\n\n", "related": "NONE"}
{"id": "62293001", "url": "https://en.wikipedia.org/wiki?curid=62293001", "title": "Inverse depth parametrization", "text": "Inverse depth parametrization\n\nIn computer vision, the inverse depth parametrization is a parametrization used in methods for 3D reconstruction from multiple images such as simultaneous localization and mapping (SLAM). Given a point formula_1 in 3D space observed by a monocular pinhole camera from multiple views, the inverse depth parametrization of the point's position is a 6D vector that encodes the optical centre of the camera formula_2 when in first observed the point, and the position of the point along the ray passing through formula_1 and formula_2.\n\nInverse depth parametrization generally improves numerical stability and allows to represent points with zero parallax. Moreover, the error associated to the observation of the point's position can be modelled with a Gaussian distribution when expressed in inverse depth. This is an important property required to apply methods, such as Kalman filters, that assume normality of the measurement error distribution. The major drawback is the larger memory consumption, since the dimensionality of the point's representation is doubled.\n\nGiven 3D point formula_5 with world coordinates in a reference frame formula_6, observed from different views, the inverse depth parametrization formula_7 of formula_1 is given by:\nwhere the first five components encode the camera pose in the first observation of the point, being formula_10 the optical centre, formula_11 the azimuth, formula_12 the elevation angle, and formula_13 the inverse depth of formula_14 at the first observation.\n\n", "related": "NONE"}
{"id": "62305152", "url": "https://en.wikipedia.org/wiki?curid=62305152", "title": "Inverse consistency", "text": "Inverse consistency\n\nIn image registration, inverse consistency measures the consistency of mappings between images produced by a registration algorithm. The inverse consistency error, introduced by Christiansen and Johnson in 2001, quantifies the distance between the composition of the mappings from each image to the other, produced by the registration procedure, and the identity function, and is used as a regularisation constraint in the loss function of many registration algorithms to enforce consistent mappings. Inverse consistency is necessary for good image registration but it is not sufficient, since a mapping can be perfectly consistent but not register the images at all.\n\nImage registration is the process of establishing a common coordinate system between two images, and given two images\nregistering a source image formula_2 to a target image formula_3 consists of determining a transformation formula_4 that maps points from the target space to the source space. An ideal registration algorithm should not be sensitive to which image in the pair is used as source or target, and the registration operator should be antisymmetric such that the mappings\nproduced when registering formula_2 to formula_3 and formula_3 to formula_2 respectively should be the inverse of each other, i.e. formula_10 and formula_11 or, equivalently, formula_12 and formula_13, where formula_14 denotes the function composition operator.\n\nReal algorithms are not perfect, and when swapping the role of source and target image in a registration problem the so obtained transformations are not the inverse of each other. Inverse consistency can be enforced by adding to the loss function of the registration a symmetric regularisation term that penalises inconsistent transformations\n\nInverse consistency can be used as a quality metric to evaluate image registration results. The inverse consistency error (formula_16) measures the distance between the composition of the two transforms and the identity function, and it can be formulated in terms of both average (formula_17) or maximum (formula_18) over a region of interest formula_19 of the image:\nWhile inverse consistency is a necessary property of good registration algorithms, inverse consistency error alone is not a sufficient metric to evaluate the quality of image registration results, since a perfectly consistent mapping, with no other constraint, may be not even close to correctly register a pair of images.\n\n\n- Inverse consistency error\n", "related": "NONE"}
{"id": "25904910", "url": "https://en.wikipedia.org/wiki?curid=25904910", "title": "Cognition Network Technology", "text": "Cognition Network Technology\n\nCognition Network Technology (CNT), also known as Definiens Cognition Network Technology, is an object-based image analysis method developed by Nobel laureate Gerd Binnig together with a team of researchers at Definiens AG in Munich, Germany. \nIt serves for extracting information from images using a hierarchy of image objects (groups of pixels), as opposed to traditional pixel processing methods. \n\nTo emulate the human mind's cognitive powers, Definiens used patented image segmentation and classification processes, and developed a method to render knowledge in a semantic network. CNT examines pixels not in isolation, but in context. It builds up a picture iteratively, recognizing groups of pixels as objects. It uses the color, shape, texture and size of objects as well as their context and relationships to draw conclusions and inferences, similar to a human analyst.\n\nIn 1994 Professor Gerd Binnig founded \"Definiens\".\nCNT was first available with the launch of the \"eCognition\" software in May 2000.\nIn June 2010, Trimble Navigation Ltd (NASDAQ: TRMB) acquired Definiens business asset in earth sciences markets, including eCognition software, and also licensed Definiens' patented CNT.\nIn 2014, Definiens was acquired by MedImmune, the global biologics research and development arm of AstraZeneca, for an initial consideration of $150 million.\n\n\"Definiens Tissue Studio\" is a digital pathology image analysis software application based on CNT.\nThe intended use of Definiens Tissue Studio is for biomarker translational research in formalin-fixed, paraffin-embedded tissue samples which have been treated with immunohistochemical staining assays, or hematoxylin and eosin (H&E). \n\nThe central concept behind Definiens Tissue Studio is a user interface that facilitates machine learning from example digital histopathology images in order to derive an image analysis solution suitable for the measurement of biomarkers and/or histological features within pre-defined regions of interest on a cell-by-cell basis, and within sub-cellular compartments. The derived image analysis solution is then automatically applied to subsequent digital images in order to objectively measure defined sets of multiparametric image features. These data sets are used for further understanding the underlying biological processes that drive cancer and other diseases. Image processing and data analysis are performed either on a local desktop computer workstation, or on a server grid. \n\nThe \"eCognition\" suite offers three components which can be used stand-alone or in combination to solve image analysis tasks. eCognition Developer is a development environment for object-based image analysis. It is used in earth sciences to develop rule sets (or applications) for the analysis of remote sensing data. eCognition Architect enables non-technical users to configure, calibrate and execute image analysis workflows created in eCognition Developer. eCognition Server software provides a processing environment for batch execution of image analysis jobs.\n\neCognition software is utilized in numerous remote sensing and geospatial application scenarios and environments, using a variety of data types:\n- Generic: Rapid Mapping, Change Detection, Object Recognition\n- By environment: Diverse Landcover Mapping, Urban Analysis (i.e. impervious surface area analysis for taxation, property assessment for insurance, inventory of green infrastructure), Forestry (i.e. biomass measurement, species identification, firescar measurement), Agriculture (i.e. regional planning, precision farming, crisis response), Marine and Riparian (i.e. ecosystem evaluation, disaster management, harbor monitoring).\n- Other: Defense, security, atmosphere and climate\n\nThe online eCognition community was launched in July 2009 and had 2813 members as of July 9, 2010. Membership is distributed globally and user conferences are held regularly, the last having taken place in November 2009 in Munich, Germany. The bi-annual GEOBIA (Geographic Object-Based Image Analysis) conference is heavily attended by eCognition users, with the majority of presentations based on eCognition software.\n\n", "related": "NONE"}
{"id": "59750846", "url": "https://en.wikipedia.org/wiki?curid=59750846", "title": "Image destriping", "text": "Image destriping\n\nImage destriping is the process of removing stripes or streaks from images and videos. These artifacts plague a range of fields in scientific imaging including atomic force microscopy, light sheet fluorescence microscopy, and planetary satellite imaging.\n\nThe most common image processing techniques to reduce stripe artifacts is with Fourier filtering. Unfortunately, filtering methods risk altering or suppressing useful image data. Methods developed for multiple-sensor imaging systems in planetary satellites use statistical-based methods to match signal distribution across multiple sensors. More recently, a new class of approaches leverage compressed sensing, to regularize an optimization problem, and recover stripe free images. In many cases, these destriped images have little to no artifacts, even at low signal to noise ratios.\n", "related": "NONE"}
{"id": "53769524", "url": "https://en.wikipedia.org/wiki?curid=53769524", "title": "Matroid (company)", "text": "Matroid (company)\n\nMatroid, Inc. is a Computer Vision company that offers a platform for creating computer vision models, called detectors, to search visual media for objects, persons, events, emotions, and actions. Matroid provides real-time notifications once the object of interest has been detected, as well as the ability to search past events.\n\nMatroid was founded in 2016 by Reza Zadeh, a Stanford professor. The company raised $3.5 million in Series A funding from New Enterprise Associates in 2016, and an additional $10 million from Intel in 2017.\n\nOnce a detector has been trained using the Matroid GUI, it automatically finds the objects of interest in real-time video and archived footage. Users can explore detection information via reports, notifications, or a calendar interface to view events and identify trends. Matroid’s functionality is also exposed via a developer API.\n\nSupported hardware platforms:\n\n- On-cloud: www.matroid.com, allows for scaling based on workload\n\n- On-prem: contains the same functionality of www.matroid.com in a secure, offline environment for applications where data privacy and security are key concerns\n- On-device: runs on embedded devices such as cameras, sensors, etc.\n\nThe company has a range of customers in Media, Security, Healthcare, Industrial IoT, AI chip, and other industries.\nMatroid annually holds a conference, Scaled Machine Learning, where technical speakers lead discussions about running and scaling machine learning algorithms, artificial intelligence, and computing platforms, such as GPUs, CPUs, TPUs, & the nascent AI chip industry.\n\nPast speakers include Turing Award Winners, creators of Keras, TensorFlow, PyTorch, Caffe, OpenAI, Kubernetes, Horovod, Allen Institute for AI, Apache Spark, Apache Arrow, MLPerf, Matroid, and others.\n\n2019 - Matroid was selected by Gartner, Inc. as a “Cool Vendor” for Cool Vendors in AI Core Technologies.\n\n2018 - Matroid announced a partnership with HP for their on-prem platform. Matroid certified a selection of HP Z computers as Computer-Vision-Ready (CV-Ready) for monitoring video streams. \n\n2018 - Oracle announced their software integration with Matroid to provide real-time and analytics based on people monitoring.\n\n2016 - Matroid was awarded a Best Paper Award at KDD 2016. \n\nTogether with Stanford Hospital and hospitals in Hong Kong, India, and Nepal, Matroid used computer vision in the field of Ophthalmology. The company created a model that learns to predict glaucoma from areas of the eye previously ignored during diagnosis, specifically the Lamina Cribrosa, as no established automated metrics existed for this region yet. Matroid is able to detect glaucoma on OCT scans of the eye, with an F1 score of 96% and similar AUC and accuracy.\n\nFusionNet was released as a leading neural networks architecture at the Princeton ModelNet competition.  It is a fusion of three convolutional neural networks, one trained on pixel representation and two networks trained on voxelized objects. It exploits the strength of each component network in order to improve the classification performance. Each component network of FusionNet considers multiple views or orientations of each object before classifying it. While it is intuitive that one can get more information from multiple views of the object than a single view, it is not trivial to put the information together in order to enhance the accuracy. Matroid used information from 20 views for pixel representation and 60 CAD object orientations for voxel representation before predicting the object class. FusionNet outperformed the current leading submission on the Princeton ModelNet leaderboard in both the 10 class and the 40 class datasets.\n\nMatroid released a book with co-author Bharath Ramsundar, TensorFlow for Deep Learning. It introduces the fundamentals of machine learning through TensorFlow and explains how to use TensorFlow to build systems capable of detecting objects in images, understanding human text, and predicting the properties of potential medicines.\n", "related": "NONE"}
{"id": "62617596", "url": "https://en.wikipedia.org/wiki?curid=62617596", "title": "Zero-shot learning", "text": "Zero-shot learning\n\nZero-shot learning, ZSL, is a problem setup in machine learning, where at test time, a learner observes samples from classes that were not observed during training, and needs to predict the category they belong to.  This problem is widely studied in computer vision, natural language processing and machine perception (review..). Unlike standard generalization in machine learning, where classifiers are expected to correctly classify new samples to classes they have already observed during training, in ZSL, no samples from the classes have been given during training the classifier. It can therefore be viewed as an extreme case of domain adaptation.\n\nNaturally, some form of side information has to be given about these zero-shot classes, and this type of information can be of several types. \n\n- Learning with attributes: classes are accompanied by pre-defined structured description. For example, for bird descriptions, this could include \"red head\", \"long beak\" . These attributes are often organized in a structured compositional way, and taking that structure into account improves learning\n\n- Learning from textual description. Here classes are accompanied by free-text natural-language description. This could include for example a wikipedia description of the class\n\n- class-class similarity. Here, classes are embedded in a continuous space. a zero-shot classifier can predict that a samples correspond to some position in that space, and the nearest embedded class is used as a predicted class, even if no such samples were observed during training.\n\nThe above ZSL learning setup assumes that at test time, only zero-shot samples are given, namely, samples from new unseen classes. In generalized zero-shot learning, samples from both new and known classes, may appear at test time. This poses new challenges for classifiers at test time, because it is very challenging to estimate if a given sample is new or known. Few approaches to handle this include: \n\n- A gating approach. Here an additional module is first trained to decide if a given sample comes from a new class or from an old one. The gater could output a hard decision  , but emmiting a soft probabilistic decision further improves the accuracy of this line of approaches\n\n- Generative approaches. Here, a generative model is trained to generate feature representation of the unseen classes. Then a standard classifier is trained given samples from all classes, seen and unseen.\n", "related": "NONE"}
