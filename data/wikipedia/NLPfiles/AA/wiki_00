{"id": "1191936", "url": "https://en.wikipedia.org/wiki?curid=1191936", "title": "Bongard problem", "text": "Bongard problem\n\nA Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his 1967 book on pattern recognition. The objective is to spot the differences between the two sides. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov.\n\nThe idea of a Bongard problem is to present two sets of relatively simple diagrams, say \"A\" and \"B\". All the diagrams from set \"A\" have a common factor or attribute, which is lacking in all the diagrams of set \"B\". The problem is to find, or to formulate, convincingly, the common factor. The problems were popularised by their occurrence in the 1979 book \"Gödel, Escher, Bach\" by Douglas Hofstadter, himself a composer of Bongard problems. Bongard problems are also at the heart of the game Zendo.\n\nMany computational architectures have been devised to solve Bongard problems, the most extensive of which being Phaeaco, by Harry Foundalis, who left the field in 2008 due to ethical concerns regarding machines that can pass as human. He has since returned, because suicide bombers pose the same threat without AI.\n\n- Bongard, M. M. (1970). Pattern Recognition. Rochelle Park, N.J.: Hayden Book Co., Spartan Books. (Original publication: Проблема Узнавания, Nauka Press, Moscow, 1967)\n- Maksimov, V. V. (1975). Система, обучающаяся классификации геометрических изображений (A system capable of learning to classify geometric images; as translated from the Russian by Marina Eskina), in Моделирование Обучения и Поведения (Modeling of Learning and Behavior, in Russian), M.S. Smirnov, V.V. Maksimov (eds.), Nauka, Moskva.\n- Hofstadter, D. R. (1979). Gödel, Escher, Bach: an Eternal Golden Braid. New York: Basic Books.\n- Montalvo, F. S. (1985). Diagram Understanding: the Intersection of Computer Vision and Graphics. M.I.T. Artificial Intelligence Laboratory, A. I. Memo 873, November 1985.\n- Saito, K., and Nakano, R. (1993) A Concept Learning Algorithm with Adaptive Search. Proceedings of Machine Intelligence 14 Workshop. Oxford University Press. See pp. 347–363.\n- Hofstadter, D. R. and the Fluid Analogies Research Group (1995). Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought. New York: Basic Books.\n- Hofstadter, D. R. (1995). On Seeing A’s and Seeing As. Stanford Humanities Review 4/2 pp. 109–121.\n- Hofstadter, D. R. (1997). Le Ton beau de Marot. New York: Basic Books.\n- Linhares, A. (2000). A glimpse at the metaphysics of Bongard problems. Artificial Intelligence, Volume 121, Issue 1-2, pp. 251–270.\n- Foundalis, H. (2006). Phaeaco: A Cognitive Architecture Inspired by Bongard’s Problems. Doctoral dissertation, Indiana University, Center for Research on Concepts and Cognition (CRCC), Bloomington, Indiana.\n- Anastasiade, J., and Szalwinski, C. (2010). Building Computer-Based Tutors to Help Learners Solve Ill-Structured Problems. In Proceedings of the World Conference on Educational Multimedia, Hypermedia and Telecommunications 2010. Toronto, Ontario, Canada: Association for the Advancement of Computing in Education. pp. 3726–3732.\n\n- Index of Bongard problems\n", "related": "NONE"}
{"id": "1222578", "url": "https://en.wikipedia.org/wiki?curid=1222578", "title": "Generative model", "text": "Generative model\n\nIn statistical classification, including machine learning, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following :\n- Given an observable variable \"X\" and a target variable \"Y\", a generative model is a statistical model of the joint probability distribution on \"X\" × \"Y\", formula_1;\n- A discriminative model is a model of the conditional probability of the target \"Y\", given an observation \"x\", symbolically, formula_2; and\n- Classifiers computed without using a probability model are also referred to loosely as \"discriminative\".\nThe distinction between these last two classes is not consistently made; refers to these three classes as \"generative learning\", \"conditional learning\", and \"discriminative learning\", but only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\n\nStandard examples of each, all of which are linear classifiers, are:\n\n- generative classifiers:\n- naive Bayes classifier and\n- linear discriminant analysis\n- discriminative model:\n- logistic regression\n- non-model classifier:\n-  perceptron and\n-  support vector machine.\n\nIn application to classification, one wishes to go from an observation \"x\" to a label \"y\" (or probability distribution on labels). One can compute this directly, without using a probability distribution (\"distribution-free classifier\"); one can estimate the probability of a label given an observation, formula_3 (\"discriminative model\"), and base classification on that; or one can estimate the joint distribution formula_1 (\"generative model\"), from that compute the conditional probability formula_3, and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches.\n\nAn alternative division defines these symmetrically as:\n\n- a generative model is a model of the conditional probability of the observable \"X\", given a target \"y\", symbolically, formula_6\n- a discriminative model is a model of the conditional probability of the target \"Y\", given an observation \"x\", symbolically, formula_2\n\nRegardless of precise definition, the terminology is constitutional because a generative model can be used to \"generate\" random instances (outcomes), either of an observation and target formula_8, or of an observation \"x\" given a target value \"y\", while a discriminative model or discriminative classifier (without a model) can be used to \"discriminate\" the value of the target variable \"Y\", given an observation \"x\". The difference between \"discriminate\" (distinguish) and \"classify\" is subtle, and these are not consistently distinguished. (The term \"discriminative classifier\" becomes a pleonasm when \"discrimination\" is equivalent to \"classification\".)\n\nThe term \"generative model\" is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables. Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers.\n\nIn application to classification, the observable \"X\" is frequently a continuous variable, the target \"Y\" is generally a discrete variable consisting of a finite set of labels, and the conditional probability formula_9 can also be interpreted as a (non-deterministic) target function formula_10, considering \"X\" as inputs and \"Y\" as outputs.\n\nGiven a finite set of labels, the two definitions of \"generative model\" are closely related. A model of the conditional distribution formula_6 is a model of the distribution of each label, and a model of the joint distribution is equivalent to a model of the distribution of label values formula_12, together with the distribution of observations given a label, formula_13; symbolically, formula_14 Thus, while a model of the joint probability distribution is more informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished.\n\nGiven a model of the joint distribution, formula_1, the distribution of the individual variables can be computed as the marginal distributions formula_16 and formula_17 (considering \"X\" as continuous, hence integrating over it, and \"Y\" as discrete, hence summing over it), and either conditional distribution can be computed from the definition of conditional probability: formula_18 and formula_19.\n\nGiven a model of one conditional probability, and estimated probability distributions for the variables \"X\" and \"Y\", denoted formula_20 and formula_12, one can estimate the opposite conditional probability using Bayes' rule:\nFor example, given a generative model for formula_13, one can estimate:\nand given a discriminative model for formula_9, one can estimate:\nNote that Bayes' rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well.\n\nA generative algorithm models how the data was generated in order to categorize a signal. It asks the question: based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn formula_27 directly from the data and then try to classify data. On the other hand, generative algorithms try to learn formula_28 which can be transformed into formula_27 later to classify the data. One of the advantages of generative algorithms is that you can use formula_28 to generate new data similar to existing data. On the other hand, discriminative algorithms generally give better performance in classification tasks.\n\nDespite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. They don't necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure.\n\nTypes of generative models are:\n\n- Gaussian mixture model (and other types of mixture model)\n- Hidden Markov model\n- Probabilistic context-free grammar\n- Bayesian network (e.g. Naive bayes, Autoregressive model)\n- Averaged one-dependence estimators\n- Latent Dirichlet allocation\n- Boltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network)\n- Variational autoencoder\n- Generative adversarial network\n- Flow-based generative model\n- Energy based model\n\nIf the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the \"true\" distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case.\n\n- k-nearest neighbors algorithm\n- Logistic regression\n- Support Vector Machines\n- Maximum-entropy Markov models\n- Conditional random fields\n- Neural networks\n\nSuppose the input data is formula_31, the set of labels for formula_32 is formula_33, and there are the following 4 data points:\nformula_34\n\nFor the above data, estimating the joint probability distribution formula_28 from the empirical measure will be the following:\nwhile formula_27 will be following:\n gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with \"representing and speedily is an good\"; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.\n\n", "related": "\n- Discriminative model\n- Graphical model\n\n- , (mirror, mirror), published as book (above)\n"}
{"id": "173926", "url": "https://en.wikipedia.org/wiki?curid=173926", "title": "Inductive bias", "text": "Inductive bias\n\nThe inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered.\n\nIn machine learning, one aims to construct algorithms that are able to \"learn\" to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training. Without any additional assumptions, this problem cannot be solved exactly since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase \"inductive bias\".\n\nA classical example of an inductive bias is Occam's razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here \"consistent\" means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm.\n\nApproaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. However, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of artificial neural networks), or not at all.\n\nThe following is a list of common inductive biases in machine learning algorithms.\n\n- Maximum conditional independence: if the hypothesis can be cast in a Bayesian framework, try to maximize conditional independence. This is the bias used in the Naive Bayes classifier.\n- Minimum cross-validation error: when trying to choose among hypotheses, select the hypothesis with the lowest cross-validation error. Although cross-validation may seem to be free of bias, the \"no free lunch\" theorems show that cross-validation must be biased.\n- Maximum margin: when drawing a boundary between two classes, attempt to maximize the width of the boundary. This is the bias used in support vector machines. The assumption is that distinct classes tend to be separated by wide boundaries.\n- Minimum description length: when forming a hypothesis, attempt to minimize the length of the description of the hypothesis. The assumption is that simpler hypotheses are more likely to be true. See Occam's razor.\n- Minimum features: unless there is good evidence that a feature is useful, it should be deleted. This is the assumption behind feature selection algorithms.\n- Nearest neighbors: assume that most of the cases in a small neighborhood in feature space belong to the same class. Given a case for which the class is unknown, guess that it belongs to the same class as the majority in its immediate neighborhood. This is the bias used in the k-nearest neighbors algorithm. The assumption is that cases that are near each other tend to belong to the same class.\n\nAlthough most learning algorithms have a static bias, some algorithms are designed to shift their bias as they acquire more data. This does not avoid bias, since the bias shifting process itself must have a bias.\n\n", "related": "\n- Algorithmic bias\n- Cognitive bias\n- No free lunch in search and optimization\n"}
{"id": "2829632", "url": "https://en.wikipedia.org/wiki?curid=2829632", "title": "Semi-supervised learning", "text": "Semi-supervised learning\n\nSemi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data).\n\nUnlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.\n\nA set of formula_1 independently identically distributed examples formula_2 with corresponding labels formula_3 and formula_4 unlabeled examples formula_5 are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.\n\nSemi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data formula_6 only. The goal of inductive learning is to infer the correct mapping from formula_7 to formula_8.\n\nIntuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.\n\nIt is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.\n\nIn order to make any use of unlabeled data, some relationship to the underlying distribution of data must exist. Semi-supervised learning algorithms make use of at least one of the following assumptions:\n\n\"Points that are close to each other are more likely to share a label.\" This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so few points are close to each other but in different classes.\n\n\"The data tend to form discrete clusters, and points in the same cluster are more likely to share a label\" (although data that shares a label may spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.\n\n\"The data lie approximately on a manifold of much lower dimension than the input space.\" In this case learning the manifold using both the labeled and unlabeled data can avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.\n\nThe manifold assumption is practical when high-dimensional data are generated by some process that may be hard to model directly, but which has only a few degrees of freedom. For instance, human voice is controlled by a few vocal folds, and images of various facial expressions are controlled by a few muscles. In these cases distances and smoothness in the natural space of the generating problem, is superior to considering the space of all possible acoustic waves or images, respectively.\n\nThe heuristic approach of \"self-training\" (also known as \"self-learning\" or \"self-labeling\") is historically the oldest approach to semi-supervised learning, with examples of applications starting in the 1960s).\n\nThe transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s. Interest in inductive learning using generative models also began in the 1970s. A \"probably approximately correct\" learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995.\n\nSemi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images.\n\nGenerative approaches to statistical learning first seek to estimate formula_9, the distribution of data points belonging to each class. The probability formula_10 that a given point formula_11 has label formula_12 is then proportional to formula_13 by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about formula_14) or as an extension of unsupervised learning (clustering plus some labels).\n\nGenerative models assume that the distributions take some particular form formula_15 parameterized by the vector formula_16. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone.\nHowever, if the assumptions are correct, then the unlabeled data necessarily improves performance.\n\nThe unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.\n\nThe parameterized joint distribution can be written as formula_17 by using the chain rule. Each parameter vector formula_16 is associated with a decision function formula_19. \nThe parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by formula_20:\n\nAnother major class of methods attempts to place boundaries in regions with few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss formula_22 for labeled data, a loss function formula_23 is introduced over the unlabeled data by letting formula_24. TSVM then selects formula_25 from a reproducing kernel Hilbert space formula_26 by minimizing the regularized empirical risk:\n\nAn exact solution is intractable due to the non-convex term formula_23, so research focuses on useful approximations.\n\nOther approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).\n\nGraph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its formula_29 nearest neighbors or to examples within some distance formula_30. The weight formula_31 of an edge between formula_32 and formula_33 is then set to formula_34.\n\nWithin the framework of manifold regularization, the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes\n\nwhere formula_26 is a reproducing kernel Hilbert space and formula_37 is the manifold on which the data lie. The regularization parameters formula_38 and formula_39 control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian formula_40 where formula_41 and formula_42 the vector formula_43, we have\n\nThe Laplacian can also be used to extend the supervised learning algorithms: regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.\n\nSome methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples formula_45 may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples.\n\n\"Self-training\" is a wrapper method for semi-supervised learning. First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident in are added at each step.\n\nCo-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.\n\nHuman responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data). More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).\n\nHuman infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces. Infants and children take into account not only unlabeled examples, but the sampling process from which labeled examples arise.\n\n", "related": "\n- PU learning\n- Weak supervision\n\n- Manifold Regularization A freely available MATLAB implementation of the graph-based semi-supervised algorithms Laplacian support vector machines and Laplacian regularized least squares.\n- KEEL: A software tool to assess evolutionary algorithms for Data Mining problems (regression, classification, clustering, pattern mining and so on) KEEL module for semi-supervised learning.\n- Semi-Supervised Learning Software Semi-Supervised Learning Software\n- 1.14. Semi-Supervised — scikit-learn 0.22.1 documentation Semi-Supervised algorithms in scikit-learn .\n"}
{"id": "3274742", "url": "https://en.wikipedia.org/wiki?curid=3274742", "title": "Learning automaton", "text": "Learning automaton\n\nA learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used.\n\nResearch in learning automata can be traced back to the work of Michael Lvovitch Tsetlin in the early 1960s in the Soviet Union. Together with some colleagues, he published a collection of papers on how to use matrices to describe automata functions. Additionally, Tsetlin worked on \"reasonable\" and \"collective automata behaviour\", and on \"automata games\". Learning automata were also investigated by researches in the United States in the 1960s. However, the term \"learning automaton\" was not used until Narendra and Thathachar introduced it in a survey paper in 1974.\n\nA learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. The actions are chosen according to a specific probability distribution which is updated based on the environment response the automaton obtains by performing a particular action.\n\nWith respect to the field of reinforcement learning, learning automata are characterized as policy iterators. In contrast to other reinforcement learners, policy iterators directly manipulate the policy π. Another example for policy iterators are evolutionary algorithms.\n\nFormally, Narendra and Thathachar define a stochastic automaton to consist of:\n- a set \"X\" of possible inputs,\n- a set Φ = { Φ, ..., Φ } of possible internal states,\n- a set α = { α, ..., α } of possible outputs, or actions, with \"r\" ≤ \"s\",\n- an initial state probability vector \"p(0)\" = ≪ \"p\"(0), ..., \"p\"(0) ≫,\n- a computable function \"A\" which after each time step \"t\" generates \"p\"(\"t\"+1) from \"p\"(\"t\"), the current input, and the current state, and\n- a function \"G\": Φ → α which generates the output at each time step.\nIn their paper, they investigate only stochastic automata with \"r\" = \"s\" and \"G\" being bijective, allowing them to confuse actions and states.\nThe states of such an automaton correspond to the states of a \"discrete-state discrete-parameter Markov process\".\n\nAt each time step \"t\"=0,1,2,3..., the automaton reads an input from its environment, updates \"p\"(\"t\") to \"p\"(\"t\"+1) by \"A\", randomly chooses a successor state according to the probabilities \"p\"(\"t\"+1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton.\n\nFrequently, the input set \"X\" = { 0,1 } is used, with 0 and 1 corresponding to a \"nonpenalty\" and a \"penalty\" response of the environment, respectively; in this case, the automaton should learn to minimize the number of \"penalty\" responses, and the feedback loop of automaton and environment is called a \"P-model\". More generally, a \"Q-model\" allows an arbitrary finite input set \"X\", and an \"S-model\" uses the interval [0,1] of real numbers as \"X\".\n\nFinite action-set learning automata (FALA) are a class of learning automata for which the number of possible actions is finite or, in more mathematical terms, for which the size of the action-set is finite.\n\n", "related": "\n- Reinforcement learning\n- Game theory\n- Automata Theory\n\n- Philip Aranzulla and John Mellor (Home page):\n- Mellor J and Aranzulla P (2000): \"Using an S-Model Response Environment with Learnng Automata Based Routing Schemes for IP Networks \", Proc. Eighth IFIP Workshop on Performance Modelling and Evaluation of ATM and IP Networks, pp 56/1-56/12, Ilkley, UK.\n- Aranzulla P and Mellor J (1997): \"Comparing two routing algorithms requiring reduced signalling when applied to ATM networks\", Proc. Fourteenth UK Teletraffic Symposium on Performance Engineering in Information Systems, pp 20/1-20/4, UMIST, Manchester, UK.\n- Tsetlin M.L. Automation theory and modeling of biological systems. Academic Press; 1973.\n"}
{"id": "4118276", "url": "https://en.wikipedia.org/wiki?curid=4118276", "title": "Conditional random field", "text": "Conditional random field\n\nConditional random fields (CRFs) are a class of statistical modeling method often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering \"neighboring\" samples, a CRF can take context into account. To do so, the prediction is modeled as a graphical model, which implements dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, linear chain CRFs are popular, which implement sequential dependencies in the predictions. In image processing the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.\n\nOther examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.\n\nCRFs are a type of discriminative undirected probabilistic graphical model. \n\nLafferty, McCallum and Pereira define a CRF on observations formula_1 and random variables formula_2 as follows:\n\nLet formula_3 be a graph such that\nformula_4,\nThen formula_7 is a conditional random field when the random variables formula_8, conditioned on formula_1, obey the Markov property with\nrespect to the graph: formula_10, where formula_11 means\nthat formula_12 and formula_13 are neighbors in formula_6.\nWhat this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets formula_1 and formula_2, the observed and output variables, respectively; the conditional distribution formula_17 is then modeled.\n\nFor general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an MRF and the same arguments hold.\nHowever, there exist special cases for which exact inference is feasible:\n\n- If the graph is a chain or a tree, message passing algorithms yield exact solutions. The algorithms used in these cases are analogous to the forward-backward and Viterbi algorithm for the case of HMMs.\n- If the CRF only contains pair-wise potentials and the energy is submodular, combinatorial min cut/max flow algorithms yield exact solutions.\n\nIf exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include:\n- Loopy belief propagation\n- Alpha expansion\n- Mean field inference\n- Linear programming relaxations\n\nLearning the parameters formula_18 is usually done by maximum likelihood learning for formula_19. If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex. It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm. On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used.\n\nIn sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables formula_20 represents a sequence of observations and formula_21 represents a hidden (or unknown) state variable that needs to be inferred given the observations. The formula_22 are structured to form a chain, with an edge between each formula_23 and formula_22. As well as having a simple interpretation of the formula_22 as \"labels\" for each element in the input sequence, this layout admits efficient algorithms for:\n- model \"training\", learning the conditional distributions between the formula_22 and feature functions from some corpus of training data.\n- \"decoding\", determining the probability of a given label sequence formula_21 given formula_20.\n- \"inference\", determining the \"most likely\" label sequence formula_21 given formula_20.\n\nThe conditional dependency of each formula_22 on formula_20 is defined through a fixed set of \"feature functions\" of the form formula_33, which can be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for formula_22. The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for formula_22.\n\nLinear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence.\n\nNotably, in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence formula_20 at any point during inference, and the range of the feature functions need not have a probabilistic interpretation.\n\nCRFs can be extended into higher order models by making each formula_22 dependent on a fixed number formula_38 of previous variables formula_39. In conventional formulations of higher order CRFs, training and inference are only practical for small values of formula_38 (such as \"k\" ≤ 5), since their computational cost increases exponentially with formula_38.\n\nHowever, another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of Bayesian nonparametrics. Specifically, the CRF-infinity approach constitutes a CRF-type model that is capable of learning infinitely-long temporal dynamics in a scalable fashion. This is effected by introducing a novel potential function for CRFs that is based on the Sequence Memoizer (SM), a nonparametric Bayesian model for learning infinitely-long dynamics in sequential observations. To render such a model computationally tractable, CRF-infinity employs a mean-field approximation of the postulated novel potential functions (which are driven by an SM). This allows for devising efficient approximate training and inference algorithms for the model, without undermining its capability to capture and model temporal dependencies of arbitrary length.\n\nThere exists another generalization of CRFs, the semi-Markov conditional random field (semi-CRF), which models variable-length \"segmentations\" of the label sequence formula_21. This provides much of the power of higher-order CRFs to model long-range dependencies of the formula_22, at a reasonable computational cost.\n\nFinally, large-margin models for structured prediction, such as the structured Support Vector Machine can be seen as an alternative training procedure to CRFs.\n\nLatent-dynamic conditional random fields (LDCRF) or discriminative probabilistic latent variable models (DPLVM) are a type of CRFs for sequence tagging tasks. They are latent variable models that are trained discriminatively.\n\nIn an LDCRF, like in any sequence tagging task, given a sequence of observations x = formula_44, the main problem the model must solve is how to assign a sequence of labels y = formula_45 from one finite set of labels . Instead of directly modeling (y|x) as an ordinary linear-chain CRF would do, a set of latent variables h is \"inserted\" between x and y using the chain rule of probability:\n\nThis allows capturing latent structure between the observations and labels. While LDCRFs can be trained using quasi-Newton methods, a specialized version of the perceptron algorithm called the latent-variable perceptron has been developed for them as well, based on Collins' structured perceptron algorithm. These models find applications in computer vision, specifically gesture recognition from video streams and shallow parsing.\n\nThis is a partial list of software that implement generic CRF tools.\n- RNNSharp CRFs based on recurrent neural networks (C#, .NET)\n- CRF-ADF Linear-chain CRFs with fast online ADF training (C#, .NET)\n- CRFSharp Linear-chain CRFs (C#, .NET)\n- GCO CRFs with submodular energy functions (C++, Matlab)\n- DGM General CRFs (C++)\n- GRMM General CRFs (Java)\n- factorie General CRFs (Scala)\n- CRFall General CRFs (Matlab)\n- Sarawagi's CRF Linear-chain CRFs (Java)\n- HCRF library Hidden-state CRFs (C++, Matlab)\n- Accord.NET Linear-chain CRF, HCRF and HMMs (C#, .NET)\n- Wapiti Fast linear-chain CRFs (C)\n- CRFSuite Fast restricted linear-chain CRFs (C)\n- CRF++ Linear-chain CRFs (C++)\n- FlexCRFs First-order and second-order Markov CRFs (C++)\n- crf-chain1 First-order, linear-chain CRFs (Haskell)\n- imageCRF CRF for segmenting images and image volumes (C++)\n- MALLET Linear-chain for sequence tagging (Java)\n- PyStruct Structured Learning in Python (Python)\n- Pycrfsuite A python binding for crfsuite (Python)\n- Figaro Probabilistic programming language capable of defining CRFs and other graphical models (Scala)\n- CRF Modeling and computational tools for CRFs and other undirected graphical models (R)\n- OpenGM Library for discrete factor graph models and distributive operations on these models (C++)\n- UPGMpp Library for building, training, and performing inference with Undirected Graphical Models (C++)\n- KEG_CRF Fast Linear CRFs (C++)\n\nThis is a partial list of software that implement CRF related tools.\n- MedaCy Medical Named Entity Recognizer (Python)\n- Conrad CRF based gene predictor (Java)\n- Stanford NER Named Entity Recognizer (Java)\n- BANNER Named Entity Recognizer (Java)\n\n", "related": "\n- Hammersley–Clifford theorem\n- Graphical model\n- Markov random field\n- Maximum entropy Markov model (MEMM)\n\n- McCallum, A.: Efficiently inducing features of conditional random fields. In: \"Proc. 19th Conference on Uncertainty in Artificial Intelligence\". (2003)\n- Wallach, H.M.: Conditional random fields: An introduction. Technical report MS-CIS-04-21, University of Pennsylvania (2004)\n- Sutton, C., McCallum, A.: An Introduction to Conditional Random Fields for Relational Learning. In \"Introduction to Statistical Relational Learning\". Edited by Lise Getoor and Ben Taskar. MIT Press. (2006) Online PDF\n- Klinger, R., Tomanek, K.: Classical Probabilistic Models and Conditional Random Fields. Algorithm Engineering Report TR07-2-013, Department of Computer Science, Dortmund University of Technology, December 2007. ISSN 1864-4503. Online PDF\n"}
{"id": "5767980", "url": "https://en.wikipedia.org/wiki?curid=5767980", "title": "Cross-entropy method", "text": "Cross-entropy method\n\nThe cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. It is applicable to both combinatorial and continuous problems, with either a static or noisy objective.\n\nThe method approximates the optimal importance sampling estimator by repeating two phases:\n\n1. Draw a sample from a probability distribution.\n2. Minimize the \"cross-entropy\" between this distribution and a target distribution to produce a better sample in the next iteration.\n\nReuven Rubinstein developed the method in the context of \"rare event simulation\", where tiny probabilities must be estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The method has also been applied to the traveling salesman, quadratic assignment, DNA sequence alignment, max-cut and buffer allocation problems.\n\nConsider the general problem of estimating the quantity\n\nformula_1,\n\nwhere formula_2 is some \"performance function\" and formula_3 is a member of some parametric family of distributions. Using importance sampling this quantity can be estimated as\n\nformula_4,\n\nwhere formula_5 is a random sample from formula_6. For positive formula_2, the theoretically \"optimal\" importance sampling density (PDF) is given by\n\nformula_8.\n\nThis, however, depends on the unknown formula_9. The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the Kullback–Leibler sense) to the optimal PDF formula_10.\n\n1. Choose initial parameter vector formula_11; set t = 1.\n2. Generate a random sample formula_5 from formula_13\n3. Solve for formula_14, where<br>formula_15\n4. If convergence is reached then stop; otherwise, increase t by 1 and reiterate from step 2.\n\nIn several cases, the solution to step 3 can be found \"analytically\". Situations in which this occurs are\n- When formula_16 belongs to the natural exponential family\n- When formula_16 is discrete with finite support\n- When formula_18 and formula_19, then formula_14 corresponds to the maximum likelihood estimator based on those formula_21.\n\nThe same CE algorithm can be used for optimization, rather than estimation. \nSuppose the problem is to maximize some function formula_22, for example, \nformula_23. \nTo apply CE, one considers first the \"associated stochastic problem\" of estimating\nformula_24\nfor a given \"level\" formula_25, and parametric family formula_26, for example the 1-dimensional \nGaussian distribution,\nparameterized by its mean formula_27 and variance formula_28 (so formula_29 here).\nHence, for a given formula_25, the goal is to find formula_31 so that\nformula_32\nis minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above.\nIt turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and\nparametric family are the sample mean and sample variance corresponding to the \"elite samples\", which are those samples that have objective function value formula_33.\nThe worst of the elite samples is then used as the level parameter for the next iteration.\nThis yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an estimation of distribution algorithm.\n\n // Initialize parameters\n\n- Simulated annealing\n- Genetic algorithms\n- Harmony search\n- Estimation of distribution algorithm\n- Tabu search\n- Natural Evolution Strategy\n\n", "related": "\n- Cross entropy\n- Kullback–Leibler divergence\n- Randomized algorithm\n- Importance sampling\n\n- De Boer, P-T., Kroese, D.P, Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. \"Annals of Operations Research\", 134 (1), 19–67.\n- Rubinstein, R.Y. (1997). Optimization of Computer simulation Models with Rare Events, \"European Journal of Operational Research\", 99, 89–112.\n\n- CEoptim R package\n"}
{"id": "3118600", "url": "https://en.wikipedia.org/wiki?curid=3118600", "title": "Concept drift", "text": "Concept drift\n\nIn predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.\n\nThe term \"concept\" refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.\n\nIn a fraud detection application the target concept may be a binary attribute FRAUDULENT with values \"yes\" or \"no\" that indicates whether a given transaction is fraudulent. Or, in a weather prediction application, there may be several target concepts such as TEMPERATURE, PRESSURE, and HUMIDITY.\n\nThe behavior of the customers in an online shop may change over time. For example, if weekly merchandise sales are to be predicted, and a predictive model has been developed that works satisfactorily. The model may use inputs such as the amount of money spent on advertising, promotions being run, and other metrics that may affect sales. The model is likely to become less and less accurate over time – this is concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. Perhaps there will be higher sales in the winter holiday season than during the summer, for example.\n\nTo prevent deterioration in prediction accuracy because of concept drift, both active and passive solutions can be adopted. Active solutions rely on triggering mechanisms, e.g., change-detection tests (Basseville and Nikiforov 1993; Alippi and Roveri, 2007) to explicitly detect concept drift as a change in the statistics of the data-generating process. In stationary conditions, any fresh information made available can be integrated to improve the model. Differently, when concept drift is detected, the current model is no more up-to-date and must be substituted with a new one to maintain the prediction accuracy (Gama et al., 2004; Alippi et al., 2011). On the contrary, in passive solutions the model is continuously updated, e.g., by retraining the model on the most recently observed samples (Widmer and Kubat, 1996), or enforcing an ensemble of classifiers (Elwell and Polikar 2011).\n\nContextual information, when available, can be used to better explain the causes of the concept drift: for instance, in the sales prediction application, concept drift might be compensated by adding information about the season to the model. By providing information about the time of the year, the rate of deterioration of your model is likely to decrease, concept drift is unlikely to be eliminated altogether. This is because actual shopping behavior does not follow any static, finite model. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change.\n\nConcept drift cannot be avoided for complex phenomena that are not governed by fixed laws of nature. All processes that arise from human activity, such as socioeconomic processes, and biological processes are likely to experience concept drift. Therefore, periodic retraining, also known as refreshing, of any model is necessary.\n\n- RapidMiner (formerly YALE (Yet Another Learning Environment)): free open-source software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept (if used in combination with its data stream mining plugin (formerly: concept drift plugin))\n- EDDM (Early Drift Detection Method): free open-source implementation of drift detection methods in Weka (machine learning).\n- MOA (Massive Online Analysis): free open-source software specific for mining data streams with concept drift. It contains a prequential evaluation method, the EDDM concept drift methods, a reader of ARFF real datasets, and artificial stream generators as SEA concepts, STAGGER, rotating hyperplane, random tree, and random radius based functions. MOA supports bi-directional interaction with Weka (machine learning).\n\n- Airline, approximately 116 million flight arrival and departure records (cleaned and sorted) compiled by E. Ikonomovska. Reference: Data Expo 2009 Competition . Access\n- Chess.com (online games) and Luxembourg (social survey) datasets compiled by I. Zliobaite. Access\n- ECUE spam 2 datasets each consisting of more than 10,000 emails collected over a period of approximately 2 years by an individual. Access from S.J.Delany webpage\n- Elec2, electricity demand, 2 classes, 45,312 instances. Reference: M. Harries, Splice-2 comparative evaluation: Electricity pricing, Technical report, The University of South Wales, 1999. Access from J.Gama webpage. Comment on applicability.\n- PAKDD'09 competition data represents the credit evaluation task. It is collected over a five-year period. Unfortunately, the true labels are released only for the first part of the data. Access\n- Sensor stream and Power supply stream datasets are available from X. Zhu's Stream Data Mining Repository. Access\n- SMEAR is a benchmark data stream with a lot of missing values. Environment observation data over 7 years. Predict cloudiness. Access\n- Text mining, a collection of text mining datasets with concept drift, maintained by I. Katakis. Access\n- Gas Sensor Array Drift Dataset, a collection of 13,910 measurements from 16 chemical sensors utilized for drift compensation in a discrimination task of 6 gases at various levels of concentrations. Access\n\n- KDD'99 competition data contains \"simulated\" intrusions in a military network environment. It is often used as a benchmark to evaluate handling concept drift. Access\n\n- Extreme verification latency benchmark, Souza, V.M.A.; Silva, D.F.; Gama, J.; Batista, G.E.A.P.A. : Data Stream Classification Guided by Clustering on Nonstationary Environments and Extreme Verification Latency. SIAM International Conference on Data Mining (SDM), pp. 873–881, 2015. Access from Nonstationary Environments – Archive.\n- Sine, Line, Plane, Circle and Boolean Data Sets, L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730–742, 2010. Access from L.Minku webpage.\n- SEA concepts, N.W.Street, Y.Kim, A streaming ensemble algorithm (SEA) for large-scale classification, KDD'01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, 2001. Access from J.Gama webpage.\n- STAGGER, J.C.Schlimmer, R.H.Granger, Incremental Learning from Noisy Data, Mach. Learn., vol.1, no.3, 1986.\n- Mixed, J.Gama, P.Medas, G.Castillo, P.Rodrigues, Learning with drift detection, 2004.\n\n- L.L.Minku, A.P.White, X.Yao, The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift, IEEE Transactions on Knowledge and Data Engineering, vol.22, no.5, pp. 730–742, 2010. Download from L.Minku webpage.\n- Lindstrom P, SJ Delany & B MacNamee (2008) Autopilot: Simulating Changing Concepts in Real Data In: Proceedings of the 19th Irish Conference on Artificial Intelligence & Cognitive Science, D Bridge, K Brown, B O'Sullivan & H Sorensen (eds.) p272-263 PDF\n- Narasimhamurthy A., L.I. Kuncheva, A framework for generating data to simulate changing environments, Proc. IASTED, Artificial Intelligence and Applications, Innsbruck, Austria, 2007, 384–389 PDF Code\n\n- INFER: Computational Intelligence Platform for Evolving and Robust Predictive Systems (2010–2014), Bournemouth University (UK), Evonik Industries (Germany), Research and Engineering Centre (Poland)\n- HaCDAIS: Handling Concept Drift in Adaptive Information Systems (2008–2012), Eindhoven University of Technology (the Netherlands)\n- KDUS: Knowledge Discovery from Ubiquitous Streams, INESC Porto and Laboratory of Artificial Intelligence and Decision Support (Portugal)\n- ADEPT: Adaptive Dynamic Ensemble Prediction Techniques, University of Manchester (UK), University of Bristol (UK)\n- ALADDIN: autonomous learning agents for decentralised data and information networks (2005–2010)\n\n- NAB: The Numenta Anomaly Benchmark, benchmark for evaluating algorithms for anomaly detection in streaming, real-time applications. (2014–2018)\n\n- 2014\n- Special Session on \"Concept Drift, Domain Adaptation & Learning in Dynamic Environments\" @IEEE IJCNN 2014\n- 2013\n- RealStream Real-World Challenges for Data Stream Mining Workshop-Discussion at the ECML PKDD 2013, Prague, Czech Republic.\n- LEAPS 2013 The 1st International Workshop on Learning stratEgies and dAta Processing in nonStationary environments\n- 2011\n- LEE 2011 Special Session on Learning in evolving environments and its application on real-world problems at ICMLA'11\n- HaCDAIS 2011 The 2nd International Workshop on Handling Concept Drift in Adaptive Information Systems\n- ICAIS 2011 Track on Incremental Learning\n- IJCNN 2011 Special Session on Concept Drift and Learning Dynamic Environments\n- CIDUE 2011 Symposium on Computational Intelligence in Dynamic and Uncertain Environments\n- 2010\n- HaCDAIS 2010 International Workshop on Handling Concept Drift in Adaptive Information Systems: Importance, Challenges and Solutions\n- ICMLA10 Special Session on Dynamic learning in non-stationary environments\n- SAC 2010 Data Streams Track at ACM Symposium on Applied Computing\n- SensorKDD 2010 International Workshop on Knowledge Discovery from Sensor Data\n- StreamKDD 2010 Novel Data Stream Pattern Mining Techniques\n- Concept Drift and Learning in Nonstationary Environments at IEEE World Congress on Computational Intelligence\n- MLMDS’2010 Special Session on Machine Learning Methods for Data Streams at the 10th International Conference on Intelligent Design and Applications, ISDA’10\n\nMany papers have been published describing algorithms for concept drift detection. Only reviews, surveys and overviews are here:\n\n- Krawczyk, B., Minku, L.L., Gama, J., Stefanowski, J., Wozniak, M. (2017). \"Ensemble Learning for Data Stream Analysis: a survey\", Information Fusion, Vol 37, pp. 132–156, Access\n- Dal Pozzolo, A., Boracchi, G., Caelen, O., Alippi, C., & Bontempi, G. (2015). Credit card fraud detection and concept-drift adaptation with delayed supervised information. In 2015 International Joint Conference on Neural Networks (IJCNN) (pp. 1–8). IEEE. PDF\n- C.Alippi, \"Learning in Nonstationary and Evolving Environments\", Chapter in \"Intelligence for Embedded Systems.\" Springer, 2014, 283pp, .\n- Gama, J., Žliobaitė, I., Bifet, A., Pechenizkiy, M. and Bouchachia, A., 2014. A survey on concept drift adaptation. \"ACM computing surveys (CSUR)\", \"46\"(4), p.44. PDF\n- C.Alippi, R.Polikar, Special Issue on Learning In Nonstationary and Evolving Environments, IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 25, NO. 1, JANUARY 2014\n- Dal Pozzolo, A., Caelen, O., Le Borgne, Y. A., Waterschoot, S., & Bontempi, G. (2014). Learned lessons in credit card fraud detection from a practitioner perspective. Expert systems with applications, 41(10), 4915–4928. PDF\n- Zliobaite, I., Learning under Concept Drift: an Overview. Technical Report. 2009, Faculty of Mathematics and Informatics, Vilnius University: Vilnius, Lithuania. PDF\n- Jiang, J., A Literature Survey on Domain Adaptation of Statistical Classifiers. 2008. PDF\n- Kuncheva L.I. Classifier ensembles for detecting concept change in streaming data: Overview and perspectives, Proc. 2nd Workshop SUEMA 2008 (ECAI 2008), Patras, Greece, 2008, 5–10, PDF\n- Gaber, M, M., Zaslavsky, A., and Krishnaswamy, S., Mining Data Streams: A Review, in ACM SIGMOD Record, Vol. 34, No. 1, June 2005,\n- Kuncheva L.I., Classifier ensembles for changing environments, Proceedings 5th International Workshop on Multiple Classifier Systems, MCS2004, Cagliari, Italy, in F. Roli, J. Kittler and T. Windeatt (Eds.), Lecture Notes in Computer Science, Vol 3077, 2004, 1–15, PDF.\n- Tsymbal, A., The problem of concept drift: Definitions and related work. Technical Report. 2004, Department of Computer Science, Trinity College: Dublin, Ireland. PDF\n\n", "related": "\n- Data stream mining\n- Data mining\n- Machine learning\n"}
{"id": "6968451", "url": "https://en.wikipedia.org/wiki?curid=6968451", "title": "Concept learning", "text": "Concept learning\n\nConcept learning, also known as category learning, concept attainment, and concept formation, is defined by Bruner, Goodnow, & Austin (1967) as \"the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories\". More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features. \n\nIn a concept learning task, a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels. The learner simplifies what has been observed by condensing it in the form of an example. This simplified version of what has been learned is then applied to future examples. Concept learning may be simple or complex because learning takes place over many areas. When a concept is difficult, it is less likely that the learner will be able to simplify, and therefore will be less likely to learn. Colloquially, the task is known as \"learning from examples.\" Most theories of concept learning are based on the storage of exemplars and avoid summarization or overt abstraction of any kind.\n\n- Concept Learning: Inferring a Boolean-valued function from training examples of its input and output.\n- A concept is an idea of something formed by combining all its features or attributes which construct the given concept. Every concept has two components:\n- Attributes: features that one must look for to decide whether a data instance is a positive one of the concept.\n- A rule: denotes what conjunction of constraints on the attributes will qualify as a positive instance of the concept.\n\nConcept learning must be distinguished from learning by reciting something from memory (recall) or discriminating between two things that differ (discrimination). However, these issues are closely related, since memory recall of facts could be considered a \"trivial\" conceptual process where prior exemplars representing the concept are invariant. Similarly, while discrimination is not the same as initial concept learning, discrimination processes are involved in refining concepts by means of the repeated presentation of exemplars.\n\nConcrete or Perceptual Concepts vs Abstract Concepts\n\nDefined (or Relational) and Associated Concepts\n\nComplex Concepts. Constructs such as a schema and a script are examples of complex concepts. A schema is an organization of smaller concepts (or features) and is revised by situational information to assist in comprehension. A script on the other hand is a list of actions that a person follows in order to complete a desired goal. An example of a script would be the process of buying a CD. There are several actions that must occur before the actual act of purchasing the CD and a script provides a sequence of the necessary actions and proper order of these actions in order to be successful in purchasing the CD.\n\nDiscovery – Every baby discovers concepts for itself, such as discovering that each of its fingers can be individually controlled or that care givers are individuals. Although this is perception driven, formation of the concept is more than memorizing perceptions.\n\nExamples – Supervised or unsupervised generalizing from examples may lead to learning a new concept, but concept formation is more than generalizing from examples.\nWords – Hearing or reading new words leads to learning new concepts, but forming a new concept is more than learning a dictionary definition. A person may have previously formed a new concept before encountering the word or phrase for it.\n\nExemplars comparison and contrast – An efficient way to learn new categories and to induce new categorization rules is by comparing a few example objects while being informed about their categorical relation. Comparing two exemplars while being informed that the two are from the same category allows identifying the attributes shared by the category members, as it exemplifies variability within this category. On the other hand, contrasting two exemplars while being informed that the two are from different categories may allow identifying attributes with diagnostic value. Within category comparison and between categories contrast are not similarly useful for category learning (Hammer et al., 2008), and the capacity to use these two forms of comparison-based learning changes at childhood (Hammer et al., 2009).\n\nInvention – When prehistoric people who lacked tools used their fingernails to scrape food from killed animals or smashed melons, they noticed that a broken stone sometimes had a sharp edge like a fingernail and was therefore suitable for scraping food. Inventing a stone tool to avoid broken fingernails was a new concept.\n\nIn general, the theoretical issues underlying concept learning are those underlying induction. These issues are addressed in many diverse publications, including literature on subjects like Version Spaces, Statistical Learning Theory, PAC Learning, Information Theory, and Algorithmic Information Theory. Some of the broad theoretical ideas are also discussed by Watanabe (1969,1985), Solomonoff (1964a,1964b), and Rendell (1986); see the reference list below.\n\nIt is difficult to make any general statements about human (or animal) concept learning without already assuming a particular psychological theory of concept learning. Although the classical views of concepts and concept learning in philosophy speak of a process of abstraction, data compression, simplification, and summarization, currently popular psychological theories of concept learning diverge on all these basic points. The history of psychology has seen the rise and fall of many theories about concept learning. Classical conditioning (as defined by Pavlov) created the earliest experimental technique. Reinforcement learning as described by Watson and elaborated by Clark Hull created a lasting paradigm in behavioral psychology. Cognitive psychology emphasized a computer and information flow metaphor for concept formation. Neural network models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as George Miller's Wordnet. Neural networks are based on computational models of learning using factor analysis or convolution. Neural networks also are open to neuroscience and psychophysiological models of learning following Karl Lashley and Donald Hebb.\n\nRule-based theories of concept learning began with cognitive psychology and early computer models of learning that might be implemented in a high level computer language with computational statements such as if:then production rules. They take classification data and a rule-based theory as input which are the result of a rule-based learner with the hopes of producing a more accurate model of the data (Hekenaho 1997). The majority of rule-based models that have been developed are heuristic, meaning that rational analyses have not been provided and the models are not related to statistical approaches to induction. A rational analysis for rule-based models could presume that concepts are represented as rules, and would then ask to what degree of belief a rational agent should be in agreement with each rule, with some observed examples provided (Goodman, Griffiths, Feldman, and Tenenbaum). Rule-based theories of concept learning are focused more so on perceptual learning and less on definition learning. Rules can be used in learning when the stimuli are confusable, as opposed to simple. When rules are used in learning, decisions are made based on properties alone and rely on simple criteria that do not require a lot of memory ( Rouder and Ratcliff, 2006).\n\nExample of rule-based theory:\n\n\"A radiologist using rule-based categorization would observe\nwhether specific properties of an X-ray image meet certain\ncriteria; for example, is there an extreme difference in brightness\nin a suspicious region relative to other regions? A decision is\nthen based on this property alone.\" (see Rouder and Ratcliff 2006)\n\nThe prototype view of concept learning holds that people abstract out the central tendency (or prototype) of the examples experienced and use this as a basis for their categorization decisions.\n\nThe prototype view of concept learning holds that people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples. This implies that people do not categorize based on a list of things that all correspond to a definition, but rather on a hierarchical inventory based on semantic similarity to the central example(s).\n\nTo illustrate, imagine the following mental representations of the category: Sports\n\nThe first illustration demonstrates a mental representation if we were to categorize by definition:\nDefinition of Sports: an athletic activity requiring skill or physical prowess and often of a competitive nature.\n\nThe second illustration demonstrates a mental representation that prototype theory would predict:\n\n1. Baseball<br>\n2. Football<br>\n3. Basketball<br>\n4. Soccer<br>\n5. Hockey<br>\n6. Tennis<br>\n7. Golf<br>\n...<br>\n15. Bike-racing<br>\n16. Weightlifting<br>\n17. Skateboarding<br>\n18. Snowboarding<br>\n19. Boxing<br>\n20. Wrestling<br>\n...<br>\n32. Fishing<br>\n33. Hunting<br>\n34. Hiking<br>\n35. Sky-diving<br>\n36. Bungee-jumping<br>\n...<br>\n62. Cooking<br>\n63. Walking<br>\n...<br>\n82. Gatorade<br>\n83. Water<br>\n84. Protein<br>\n85. Diet\n\nIt is evident that prototype theory hypothesizes a more continuous (less discrete) way of categorization in which the list of things that match the category's definition is not limited.\n\nExemplar theory is the storage of specific instances (exemplars), with new objects evaluated only with respect to how closely they resemble specific known members (and nonmembers) of the category. This theory hypothesizes that learners store examples \"verbatim\". This theory views concept learning as highly simplistic. Only individual properties are represented. These individual properties are not abstract and they do not create rules. An example of what exemplar theory might look like is, \"water is wet\". It is simply known that some (or one, or all) stored examples of water have the property wet. Exemplar based theories have become more empirically popular over the years with some evidence suggesting that human learners use exemplar based strategies only in early learning, forming prototypes and generalizations later in life. An important result of exemplar models in psychology literature has been a de-emphasis of complexity in concept learning. One of the best known exemplar theories of concept learning is the Generalized Context Model (GCM).\n\nA problem with exemplar theory is that exemplar models critically depend on two measures: similarity between exemplars, and having a rule to determine group membership. Sometimes it is difficult to attain or distinguish these measures.\n\nMore recently, cognitive psychologists have begun to explore the idea that the prototype and exemplar models form two extremes. It has been suggested that people are able to form a multiple prototype representation, besides the two extreme representations. For example, consider the category 'spoon'. There are two distinct subgroups or conceptual clusters: spoons tend to be either large and wooden, or small and made of metal. The prototypical spoon would then be a medium-size object made of a mixture of metal and wood, which is clearly an unrealistic proposal. A more natural representation of the category 'spoon' would instead consist of multiple (at least two) prototypes, one for each cluster. A number of different proposals have been made in this regard (Anderson, 1991; Griffiths, Canini, Sanborn & Navarro, 2007; Love, Medin & Gureckis, 2004; Vanpaemel & Storms, 2008). These models can be regarded as providing a compromise between exemplar and prototype models.\n\nThe basic idea of explanation-based learning suggests that a new concept is acquired by experiencing examples of it and forming a basic outline. Put simply, by observing or receiving the qualities of a thing the mind forms a concept which possesses and is identified by those qualities.\n\nThe original theory, proposed by Mitchell, Keller, and Kedar-Cabelli in 1986 and called explanation-based generalization, is that learning occurs through progressive generalizing. This theory was first developed to program machines to learn. When applied to human cognition, it translates as follows: the mind actively separates information that applies to more than one thing and enters it into a broader description of a category of things. This is done by identifying sufficient conditions for something to fit in a category, similar to schematizing.\n\nThe revised model revolves around the integration of four mental processes – generalization, chunking, operationalization, and analogy.\n\n- Generalization is the process by which the characteristics of a concept which are fundamental to it are recognized and labeled. For example, birds have feathers and wings. Anything with feathers and wings will be identified as ‘bird’.\n- When information is grouped mentally, whether by similarity or relatedness, the group is called a chunk. Chunks can vary in size from a single item with parts or many items with many parts.\n- A concept is operationalized when the mind is able to actively recognize examples of it by characteristics and label it appropriately.\n- Analogy is the recognition of similarities among potential examples.\n\nThis particular theory of concept learning is relatively new and more research is being conducted to test it.\n\nTaking a mathematical approach to concept learning, Bayesian theories propose that the human mind produces \"probabilities\" for a certain concept definition, based on examples it has seen of that concept. The Bayesian concept of Prior Probability stops learners' hypotheses being overly specific, while the likelihood of a hypothesis ensures the definition is not too broad.\n\nFor example- say a child is shown three horses by a parent and told these are called \"horses\"- she needs to work out exactly what the adult means by this word. She is much more likely to define the word \"horses\" as referring to either this \"type of animal\" or \"all animals\", rather than an oddly specific example like \"all horses except Clydedales\", which would be an unnatural concept. Meanwhile, the likelihood of 'horses' meaning 'all animals' when the three animals shown are all very similar is low. The hypothesis that the word \"horse\" refers to all \"animals of this species\" is most likely of the three possible definitions, as it has both a reasonable prior probability and likelihood given examples.\n\nBayes' theorem is important because it provides a powerful tool for understanding, manipulating and controlling data that takes a larger view that is not limited to data analysis alone. The approach is subjective, and this requires the assessment of prior probabilities, making it also very complex. However, if Bayesians show that the accumulated evidence and the application of Bayes' law are sufficient, the work will overcome the subjectivity of the inputs involved. Bayesian inference can be used for any honestly collected data and has a major advantage because of its scientific focus.\n\nOne model that incorporates the Bayesian theory of concept learning is the ACT-R model, developed by John R. Anderson. The ACT-R model is a programming language that defines the basic cognitive and perceptual operations that enable the human mind by producing a step-by-step simulation of human behavior. This theory exploits the idea that each task humans perform consists of a series of discrete operations. The model has been applied to learning and memory, higher level cognition, natural language, perception and attention, human-computer interaction, education, and computer generated forces.\n\nIn addition to John R. Anderson, Joshua Tenenbaum has been a contributor to the field of concept learning; he studied the computational basis of human learning and inference using behavioral testing of adults, children, and machines from Bayesian statistics and probability theory, but also from geometry, graph theory, and linear algebra. Tenenbaum is working to achieve a better understanding of human learning in computational terms and trying to build computational systems that come closer to the capacities of human learners.\n\nM. D. Merrill's component display theory (CDT) is a cognitive matrix that focuses on the interaction between two dimensions: the level of performance expected from the learner and the types of content of the material to be learned. Merrill classifies a learner's level of performance as: find, use, remember, and material content as: facts, concepts, procedures, and principles. The theory also calls upon four primary presentation forms and several other secondary presentation forms. The primary presentation forms include: rules, examples, recall, and practice. Secondary presentation forms include: prerequisites, objectives, helps, mnemonics, and feedback. A complete lesson includes a combination of primary and secondary presentation forms, but the most effective combination varies from learner to learner and also from concept to concept. Another significant aspect of the CDT model is that it allows for the learner to control the instructional strategies used and adapt them to meet his or her own learning style and preference. A major goal of this model was to reduce three common errors in concept formation: over-generalization, under-generalization and misconception.\n\n", "related": "\n- Sample exclusion dimension\n\n- Bruner, J., Goodnow, J. J., & Austin, G. A. (1967). A study of thinking. New York: Science Editions.\n"}
{"id": "3290880", "url": "https://en.wikipedia.org/wiki?curid=3290880", "title": "Robot learning", "text": "Robot learning\n\nRobot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).\n\nExample of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization, as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.\n\nRobot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills.\nWhile machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as \"robot learning\".\n\nMaya Cakmak, assistant professor of computer science and engineering at the University of Washington, is trying to create a robot that learns by imitating - a technique called \"programming by demonstration\". A researcher shows it a cleaning technique for the robot's vision system and it generalizes the cleaning motion from the human demonstration as well as identifying the \"state of dirt\" before and after cleaning.\n\nSimilarly the Baxter industrial robot can be taught how to do something by grabbing its arm and showing it the desired movements. It can also use deep learning to teach itself to grasp an unknown object.\n\nIn Tellex's \"Million Object Challenge,\" the goal is robots that learn how to spot and handle simple items and upload their data to the cloud to allow other robots to analyze and use the information.\n\nRoboBrain is a knowledge engine for robots which can be freely accessed by any device wishing to carry out a task. The database gathers new information about tasks as robots perform them, by searching the Internet, interpreting natural language text, images, and videos, object recognition as well as interaction. The project is led by Ashutosh Saxena at Stanford University.\n\nRoboEarth is a project that has been described as a \"World Wide Web for robots\" − it is a network and database repository where robots can share information and learn from each other and a cloud for outsourcing heavy computation tasks. The project brings together researchers from five major universities in Germany, the Netherlands and Spain and is backed by the European Union.\n\nGoogle Research, DeepMind, and Google X have decided to allow their robots share their experiences.\n\n", "related": "\n- Developmental robotics\n- Cognitive robotics\n- Evolutionary robotics\n\n- IEEE RAS Technical Committee on Robot Learning (official IEEE website)\n- IEEE RAS Technical Committee on Robot Learning (TC members website)\n- Robot Learning at the Max Planck Institute for Intelligent Systems and the Technical University Darmstadt\n- Robot Learning at the Computational Learning and Motor Control lab\n- Humanoid Robot Learning at the Advanced Telecommunication Research Center (ATR)\n- Learning Algorithms and Systems Laboratory at EPFL (LASA)\n- Robot Learning at the Cognitive Robotics Lab of Juergen Schmidhuber at IDSIA and Technical University of Munich\n- The Humanoid Project: Peter Nordin, Chalmers University of Technology\n- Inria and Ensta ParisTech FLOWERS team, France: Autonomous lifelong learning in developmental robotics\n- CITEC at University of Bielefeld, Germany\n- Asada Laboratory, Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan\n- The Laboratory for Perceptual Robotics, University of Massachusetts Amherst Amherst, USA\n- Centre for Robotics and Neural Systems, Plymouth University Plymouth, United Kingdom\n- Robot Learning Lab at Carnegie Mellon University\n- Project Learning Humanoid Robots at University of Bonn\n- Skilligent Robot Learning and Behavior Coordination System (commercial product)\n- Robot Learning class at Cornell University\n- Robot Learning and Interaction Lab at Italian Institute of Technology\n- Reinforcement learning for robotics at Delft University of Technology\n"}
{"id": "7578809", "url": "https://en.wikipedia.org/wiki?curid=7578809", "title": "Version space learning", "text": "Version space learning\n\nVersion space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction\n\n(i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through ). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example , the hypotheses that are inconsistent with are removed from the space. This iterative refining of the hypothesis space is called the candidate elimination algorithm, the hypothesis space maintained inside the algorithm its \"version space\".\n\nIn settings where there is a generality-ordering on hypotheses, it is possible to represent the version space by two sets of hypotheses: (1) the most specific consistent hypotheses, and (2) the most general consistent hypotheses, where \"consistent\" indicates agreement with observed data. \n\nThe most specific hypotheses (i.e., the specific boundary SB) cover the observed positive training examples, and as little of the remaining feature space as possible. These hypotheses, if reduced any further, \"exclude\" a \"positive\" training example, and hence become inconsistent. These minimal hypotheses essentially constitute a (pessimistic) claim that the true concept is defined just by the \"positive\" data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be negative. (I.e., if data has not previously been ruled in, then it's ruled out.) \n\nThe most general hypotheses (i.e., the general boundary GB) cover the observed positive training examples, but also cover as much of the remaining feature space without including any negative training examples. These, if enlarged any further, \"include\" a \"negative\" training example, and hence become inconsistent. These maximal hypotheses essentially constitute a (optimistic) claim that the true concept is defined just by the \"negative\" data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be positive. (I.e., if data has not previously been ruled out, then it's ruled in.) \n\nThus, during learning, the version space (which itself is a set – possibly infinite – containing \"all\" consistent hypotheses) can be represented by just its lower and upper bounds (maximally general and maximally specific hypothesis sets), and learning operations can be performed just on these representative sets.\n\nAfter learning, classification can be performed on unseen examples by testing the hypothesis learned by the algorithm. If the example is consistent with multiple hypotheses, a majority vote rule can be applied.\n\nThe notion of version spaces was introduced by Mitchell in the early 1980s as a framework for understanding the basic problem of supervised learning within the context of solution search. Although the basic \"candidate elimination\" search method that accompanies the version space framework is not a popular learning algorithm, there are some practical implementations that have been developed (e.g., Sverdlik & Reynolds 1992, Hong & Tsang 1997, Dubois & Quafafou 2002).\n\nA major drawback of version space learning is its inability to deal with noise: any pair of inconsistent examples can cause the version space to \"collapse\", i.e., become empty, so that classification becomes impossible. One solution of this problem is proposed by Dubois and Quafafou that proposed the Rough Version Space, where rough sets based approximations are used to learn certain and possible hypothesis in the presence of inconsistent data.\n\n", "related": "\n- Formal concept analysis\n- Inductive logic programming\n- Rough set. [The rough set framework focuses on the case where ambiguity is introduced by an impoverished feature set. That is, the target concept cannot be decisively described because the available feature set fails to disambiguate objects belonging to different categories. The version space framework focuses on the (classical induction) case where the ambiguity is introduced by an impoverished data set. That is, the target concept cannot be decisively described because the available data fails to uniquely pick out a hypothesis. Naturally, both types of ambiguity can occur in the same learning problem.]\n- provides an interesting discussion of the general problem of induction.\n- Mill (1843/2002) is the classic source on inductive logic.\n\n"}
{"id": "8416103", "url": "https://en.wikipedia.org/wiki?curid=8416103", "title": "Evolvability (computer science)", "text": "Evolvability (computer science)\n\nThe term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries.\n\nLet formula_1 and formula_2 be collections of functions on formula_3 variables. Given an \"ideal function\" formula_4, the goal is to find by local search a \"representation\" formula_5 that closely approximates formula_6. This closeness is measured by the \"performance\" formula_7 of formula_8 with respect to formula_6.\n\nAs is the case in the biological world, there is a difference between genotype and phenotype. In general, there can be multiple representations (genotypes) that correspond to the same function (phenotype). That is, for some formula_10, with formula_11, still formula_12 for all formula_13. However, this need not be the case. The goal then, is to find a representation that closely matches the phenotype of the ideal function, and the spirit of the local search is to allow only small changes in the genotype. Let the \"neighborhood\" formula_14 of a representation formula_8 be the set of possible mutations of formula_8.\n\nFor simplicity, consider Boolean functions on formula_17, and let formula_18 be a probability distribution on formula_19. Define the performance in terms of this. Specifically,\nNote that formula_21 In general, for non-Boolean functions, the performance will not correspond directly to the probability that the functions agree, although it will have some relationship.\n\nThroughout an organism's life, it will only experience a limited number of environments, so its performance cannot be determined exactly. The \"empirical performance\" is defined by\nformula_22\nwhere formula_23 is a multiset of formula_24 independent selections from formula_19 according to formula_18. If formula_24 is large enough, evidently formula_28 will be close to the actual performance formula_7.\n\nGiven an ideal function formula_4, initial representation formula_5, \"sample size\" formula_24, and \"tolerance\" formula_33, the \"mutator\" formula_34 is a random variable defined as follows. Each formula_35 is classified as beneficial, neutral, or deleterious, depending on its empirical performance. Specifically,\n- formula_36 is a beneficial mutation if formula_37;\n- formula_36 is a neutral mutation if formula_39;\n- formula_36 is a deleterious mutation if formula_41.\n\nIf there are any beneficial mutations, then formula_34 is equal to one of these at random. If there are no beneficial mutations, then formula_34 is equal to a random neutral mutation. In light of the similarity to biology, formula_8 itself is required to be available as a mutation, so there will always be at least one neutral mutation.\n\nThe intention of this definition is that at each stage of evolution, all possible mutations of the current genome are tested in the environment. Out of the ones who thrive, or at least survive, one is chosen to be the candidate for the next stage. Given formula_45, we define the sequence formula_46 by formula_47. Thus formula_48 is a random variable representing what formula_49 has evolved to after formula_50 \"generations\".\n\nLet formula_51 be a class of functions, formula_52 be a class of representations, and formula_53 a class of distributions on formula_54. We say that formula_51 is \"evolvable by formula_52 over formula_53\" if there exists polynomials formula_58, formula_59, formula_60, and formula_61 such that for all formula_3 and all formula_63, for all ideal functions formula_4 and representations formula_45, with probability at least formula_66,\nwhere the sizes of neighborhoods formula_14 for formula_69 are at most formula_70, the sample size is formula_71, the tolerance is formula_72, and the generation size is formula_73.\n\nformula_51 is \"evolvable over formula_53\" if it is evolvable by some formula_52 over formula_53.\n\nformula_51 is \"evolvable\" if it is evolvable over all distributions formula_53.\n\nThe class of conjunctions and the class of disjunctions are evolvable over the uniform distribution for short conjunctions and disjunctions, respectively.\n\nThe class of parity functions (which evaluate to the parity of the number of true literals in a given subset of literals) are not evolvable, even for the uniform distribution.\n\nEvolvability implies PAC learnability.\n", "related": "NONE"}
{"id": "6881120", "url": "https://en.wikipedia.org/wiki?curid=6881120", "title": "Prior knowledge for pattern recognition", "text": "Prior knowledge for pattern recognition\n\nPattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs formula_1 that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.\n\nPrior knowledge refers to all information about the problem available in addition to the training data. However, in this most general form, determining a model from a finite set of samples without prior knowledge is an ill-posed problem, in the sense that a unique model may not exist. Many classifiers incorporate the general smoothness assumption that a test pattern similar to one of the training samples tends to be assigned to the same class.\n\nThe importance of prior knowledge in machine learning is suggested by its role in search and optimization. Loosely, the no free lunch theorem states that all search algorithms have the same average performance over all problems, and thus implies that to gain in performance on a certain application one must use a specialized algorithm that includes some prior knowledge about the problem. \n\nThe different types of prior knowledge encountered in pattern recognition are now regrouped under two main categories: class-invariance and knowledge on the data.\n\nA very common type of prior knowledge in pattern recognition is the invariance of the class (or the output of the classifier) to a transformation of the input pattern. This type of knowledge is referred to as transformation-invariance. The mostly used transformations used in image recognition are:\n\n- translation;\n- rotation;\n- skewing;\n- scaling.\n\nIncorporating the invariance to a transformation formula_2 parametrized in formula_3 into a classifier of output formula_4 for an input pattern formula_5 corresponds to enforcing the equality\n\nLocal invariance can also be considered for a transformation centered at formula_7, so that formula_8, by using the constraint\n\nThe function formula_10 in these equations can be either the decision function of the classifier or its real-valued output.\n\nAnother approach is to consider class-invariance with respect to a \"domain of the input space\" instead of a transformation. In this case, the problem becomes finding formula_10 so that\n\nwhere formula_13 is the membership class of the region formula_14 of the input space.\n\nA different type of class-invariance found in pattern recognition is permutation-invariance, i.e. invariance of the class to a permutation of elements in a structured input. A typical application of this type of prior knowledge is a classifier invariant to permutations of rows of the matrix inputs.\n\nOther forms of prior knowledge than class-invariance concern the data more specifically and are thus of particular interest for real-world applications. The three particular cases that most often occur when gathering data are:\n- Unlabeled samples are available with supposed class-memberships;\n- Imbalance of the training set due to a high proportion of samples of a class;\n- Quality of the data may vary from a sample to another.\n\nPrior knowledge of these can enhance the quality of the recognition if included in the learning. Moreover, not taking into account the poor quality of some data or a large imbalance between the classes can mislead the decision of a classifier.\n\n- E. Krupka and N. Tishby, \"Incorporating Prior Knowledge on Features into Learning\", Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS 07)\n", "related": "NONE"}
{"id": "1041204", "url": "https://en.wikipedia.org/wiki?curid=1041204", "title": "Granular computing", "text": "Granular computing\n\nGranular computing (GrC) is an emerging computing paradigm of information processing that concerns the processing of complex information entities called \"information granules\", which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.\n\nAt present, granular computing is more a \"theoretical perspective\" than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.\n\n \nAs mentioned above, \"granular computing\" is not an algorithm or process; there is no particular method that is called \"granular computing\". It is rather an approach to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in satellite images of greater or lesser resolution. On a low-resolution satellite image, for example, one might notice interesting cloud patterns representing cyclones or other large-scale weather phenomena, while in a higher-resolution image, one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of Manhattan. The same is generally true of all data: At different resolutions or granularities, different features and relationships emerge. The aim of granular computing is to try to take advantage of this fact in designing more effective machine-learning and reasoning systems.\n\nThere are several types of granularity that are often encountered in data mining and machine learning, and we review them below:\n\nOne type of granulation is the quantization of variables. It is very common that in data mining or machine-learning applications the resolution of variables needs to be \"decreased\" in order to extract meaningful regularities. An example of this would be a variable such as \"outside temperature\" (formula_1), which in a given application might be recorded to several decimal places of precision (depending on the sensing apparatus). However, for purposes of extracting relationships between \"outside temperature\" and, say, \"number of health-club applications\" (formula_2), it will generally be advantageous to quantize \"outside temperature\" into a smaller number of intervals.\n\nThere are several interrelated reasons for granulating variables in this fashion:\n- Based on prior domain knowledge, there is no expectation that minute variations in temperature (e.g., the difference between ) could have an influence on behaviors driving the number of health-club applications. For this reason, any \"regularity\" which our learning algorithms might detect at this level of resolution would have to be \"spurious\", as an artifact of overfitting. By coarsening the temperature variable into intervals the difference between which we \"do\" anticipate (based on prior domain knowledge) might influence number of health-club applications, we eliminate the possibility of detecting these spurious patterns. Thus, in this case, reducing resolution is a method of controlling overfitting.\n- By reducing the number of intervals in the temperature variable (i.e., increasing its \"grain size\"), we increase the amount of sample data indexed by each interval designation. Thus, by coarsening the variable, we increase sample sizes and achieve better statistical estimation. In this sense, increasing granularity provides an antidote to the so-called \"curse of dimensionality\", which relates to the exponential decrease in statistical power with increase in number of dimensions or variable cardinality.\n- Independent of prior domain knowledge, it is often the case that meaningful regularities (i.e., which can be detected by a given learning methodology, representational language, etc.) may exist at one level of resolution and not at another.\nFor example, a simple learner or pattern recognition system may seek to extract regularities satisfying a conditional probability threshold such as formula_3. In the special case where formula_4, this recognition system is essentially detecting \"logical implication\" of the form formula_5 or, in words, \"if formula_6, then formula_7\". The system's ability to recognize such implications (or, in general, conditional probabilities exceeding threshold) is partially contingent on the resolution with which the system analyzes the variables.\n\nAs an example of this last point, consider the feature space shown to the right. The variables may each be regarded at two different resolutions. Variable formula_8 may be regarded at a high (quaternary) resolution wherein it takes on the four values formula_9 or at a lower (binary) resolution wherein it takes on the two values formula_10. Similarly, variable formula_11 may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values formula_12 or formula_13, respectively. At the high resolution, there are no detectable implications of the form formula_5, since every formula_15 is associated with more than one formula_16, and thus, for all formula_15, formula_18. However, at the low (binary) variable resolution, two bilateral implications become detectable: formula_19 and formula_20, since every formula_21 occurs \"iff\" formula_22 and formula_23 occurs \"iff\" formula_24. Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the higher quaternary variable resolution.\n\nIt is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results. Instead, the feature space must be preprocessed (often by an entropy analysis of some kind) so that some guidance can be given as to how the discretization process should proceed. Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover.\n\nA sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, is as follows: , , , , , , , , , , , , , , , , \n\nVariable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements. We briefly describe some of the ideas here, and present pointers to the literature.\n\nA number of classical methods, such as principal component analysis, multidimensional scaling, factor analysis, and structural equation modeling, and their relatives, fall under the genus of \"variable transformation.\" Also in this category are more modern areas of study such as dimensionality reduction, projection pursuit, and independent component analysis. The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space. These dimensionality reduction methods are all reviewed in the standard texts, such as , , and .\n\nA different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods. It was noted fairly early that one may consider \"clustering\" related variables in just the same way that one considers clustering related data. In data clustering, one identifies a group of similar entities (using a \"measure of similarity\" suitable to the domain — ), and then in some sense \"replaces\" those entities with a prototype of some kind. The prototype may be the simple average of the data in the identified cluster, or some other representative measure. But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to \"stand in\" for the much larger set of exemplars. These prototypes are generally such as to capture most of the information of interest concerning the entities.\nSimilarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of \"prototype\" variables that capture the most salient relationships between the variables. Although variable clustering methods based on linear correlation have been proposed (;), more powerful methods of variable clustering are based on the mutual information between variables. Watanabe has shown (;) that for any set of variables one can construct a \"polytomic\" (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate \"total\" correlation among the complete variable set is the sum of the \"partial\" correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts \"... as if they were looking for a natural division or a hidden crack.\"\n\nOne practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information . The product of each agglomeration is a new (constructed) variable that reflects the local joint distribution of the two agglomerating variables, and thus possesses an entropy equal to their joint entropy.\n(From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table—representing the two agglomerating variables—with a single column that has a unique value for every unique combination of values in the replaced columns . No information is lost by such an operation; however, if one is exploring the data for inter-variable relationships, it would generally \"not\" be desirable to merge redundant variables in this way, since in such a context it is likely to be precisely the redundancy or \"dependency\" between variables that is of interest; and once redundant variables are merged, their relationship to one another can no longer be studied.\n\nIn database systems, aggregations (see e.g. OLAP aggregation and Business intelligence systems) result in transforming original data tables (often called information systems) into the tables with different semantics of rows and columns, wherein the rows correspond to the groups (granules) of original tuples and the columns express aggregated information about original values within each of the groups. Such aggregations are usually based on SQL and its extensions. The resulting granules usually correspond to the groups of original tuples with the same values (or ranges) over some pre-selected original columns.\n\nThere are also other approaches wherein the groups are defined basing on, e.g., physical adjacency of rows. For example, Infobright implemented a database engine wherein data was partitioned onto \"rough rows\", each consisting of 64K of physically consecutive (or almost consecutive) rows. Rough rows were automatically labeled with compact information about their values on data columns, often involving multi-column and multi-table relationships. It resulted in a higher layer of granulated information where objects corresponded to rough rows and attributes - to various aspects of rough information. Database operations could be efficiently supported within such a new framework, with an access to the original data pieces still available .\n\nThe origins of the \"granular computing\" ideology are to be found in the rough sets and fuzzy sets literatures. One of the key insights of rough set research—although by no means unique to it—is that, in general, the selection of different sets of features or variables will yield different \"concept\" granulations. Here, as in elementary rough set theory, by \"concept\" we mean a set of entities that are \"indistinguishable\" or \"indiscernible\" to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept). To put it in other words, by projecting a data set (value-attribute system) onto different sets of variables, we recognize alternative sets of equivalence-class \"concepts\" in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities.\n\nWe illustrate with an example. Consider the attribute-value system below:\n\nWhen the full set of attributes formula_25 is considered, we see that we have the following seven equivalence classes or primitive (simple) concepts:\n\nThus, the two objects within the first equivalence class, formula_27, cannot be distinguished from one another based on the available attributes, and the three objects within the second equivalence class, formula_28, cannot be distinguished from one another based on the available attributes. The remaining five objects are each discernible from all other objects. Now, let us imagine a projection of the attribute value system onto attribute formula_29 alone, which would represent, for example, the view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.\n\nThis is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size). Just as in the case of value granulation (discretization/quantization), it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another. As an example of this, we can consider the effect of concept granulation on the measure known as \"attribute dependency\" (a simpler relative of the mutual information).\n\nTo establish this notion of dependency (see also rough sets), let formula_31 represent a particular concept granulation, where each formula_32 is an equivalence class from the concept structure induced by attribute set formula_33. For example, if the attribute set formula_33 consists of attribute formula_29 alone, as above, then the concept structure formula_36 will be composed of formula_37, formula_38, and formula_39. The dependency of attribute set formula_33 on another attribute set formula_41, formula_42, is given by\n\nThat is, for each equivalence class formula_32 in formula_36, we add up the size of its \"lower approximation\" (see rough sets) by the attributes in formula_41, i.e., formula_47. More simply, this approximation is the number of objects which on attribute set formula_41 can be positively identified as belonging to target set formula_32. Added across all equivalence classes in formula_36, the numerator above represents the total number of objects which—based on attribute set formula_41—can be positively categorized according to the classification induced by attributes formula_33. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the \"synchronization\" of the two concept structures formula_36 and formula_54. The dependency formula_42 \"can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in formula_41 to determine the values of attributes in formula_33\" (Ziarko & Shan 1995).\n\nHaving gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes. Consider again the attribute value table from above:\n\nConsider the dependency of attribute set formula_58\non attribute set formula_59. That is, we wish to know what proportion of objects can be correctly classified into classes of formula_36 based on knowledge of formula_54. The equivalence classes of formula_36 and of formula_54 are shown below.\n\nThe objects that can be \"definitively\" categorized according to concept structure formula_36 based on formula_54 are those in the set formula_66, and since there are six of these, the dependency of formula_33 on formula_41, formula_69. This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired.\n\nWe might then consider the dependency of the smaller attribute set formula_70\non the attribute set formula_59. The move from formula_58 to formula_70 induces a coarsening of the class structure formula_36, as will be seen shortly. We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of formula_36 based on knowledge of formula_54. The equivalence classes of the new formula_36 and of formula_54 are shown below.\n\nClearly, formula_36 has a coarser granularity than it did earlier. The objects that can now be \"definitively\" categorized according to the concept structure formula_36 based on formula_54 constitute the complete universe formula_82, and thus the dependency of formula_33 on formula_41, formula_85. That is, knowledge of membership according to category set formula_54 is adequate to determine category membership in formula_36 with complete certainty; In this case we might say that formula_88. Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency. However, we also note that the classes induced in formula_36 from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of formula_36.\n\nIn general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence. Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and Lotfi Zadeh listed in the #References below.\n\nAnother perspective on concept granulation may be obtained from work on parametric models of categories. In mixture model learning, for example, a set of data is explained as a mixture of distinct Gaussian (or other) distributions. Thus, a large amount of data is \"replaced\" by a small number of distributions. The choice of the number of these distributions, and their size, can again be viewed as a problem of \"concept granulation\". In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately \"coarsening\" the concept resolution. Finding the \"right\" concept resolution is a tricky problem for which many methods have been proposed (e.g., AIC, BIC, MDL, etc.), and these are frequently considered under the rubric of \"model regularization\".\n\nGranular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving. In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation. By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving.\n\nIn a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure. Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems.\n\n", "related": "\n- Rough Sets, Discretization\n- Type-2 Fuzzy Sets and Systems\n\n- Bargiela, A. and Pedrycz, W. (2003) \"Granular Computing. An introduction\", Kluwer Academic Publishers\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- Yao, Y.Y. (2004) \"A Partition Model of Granular Computing\", Lecture Notes in Computer Science (to appear)\n- Zadeh, L.A. (1997) \"Toward a Theory of Fuzzy Information Granulation and its Centrality in Human Reasoning and Fuzzy Logic\"\", Fuzzy Sets and Systems\", 90:111-127\n"}
{"id": "9731945", "url": "https://en.wikipedia.org/wiki?curid=9731945", "title": "Probability matching", "text": "Probability matching\n\nProbability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a \"probability-matching\" strategy will predict (for unlabeled examples) a class label of \"positive\" on 60% of instances, and a class label of \"negative\" on 40% of instances. \n\nThe optimal Bayesian decision strategy (to maximize the number of correct predictions, see ) in such a case is to always predict \"positive\" (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning (where \"p\" is the probability of positive realization, the result of matching would be formula_1, here formula_2). The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling).\n\nThe only case when probability matching will yield same results as Bayesian decision strategy mentioned above is when all class base rates are the same. So, if in the training set positive examples are observed 50% of the time, then the Bayesian strategy would yield 50% accuracy (1 × .5), just as probability matching (.5 ×.5 + .5 × .5). \n\n- Shanks, D. R., Tunney, R. J., & McCarthy, J. D. (2002). A re‐examination of probability matching and rational choice. \"Journal of Behavioral Decision Making\", 15(3), 233-250.\n", "related": "NONE"}
{"id": "10704974", "url": "https://en.wikipedia.org/wiki?curid=10704974", "title": "Structural risk minimization", "text": "Structural risk minimization\n\nStructural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.\n\nIn practical terms, Structural Risk Minimization is implemented by minimizing formula_1, where formula_2 is the train error, the function formula_3 is called a regularization function, and formula_4 is a constant. formula_3 is chosen such that it takes large values on parameters formula_6 that belong to high-capacity subsets of the parameter space. Minimizing formula_3 in effect limits the capacity of the accessible subsets of the parameter space, thereby controlling the trade-off between minimizing the training error and minimizing the expected gap between the training error and test error.\n\nThe SRM principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension.\n\n", "related": "\n- Vapnik–Chervonenkis theory\n- Support vector machines\n- Model selection\n- Occam Learning\n- Empirical risk minimization\n\n- Structural risk minimization at the support vector machines website.\n"}
{"id": "10747879", "url": "https://en.wikipedia.org/wiki?curid=10747879", "title": "Lazy learning", "text": "Lazy learning\n\nIn machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. \n\nThe primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online recommendation systems (\"people who viewed/purchased/listened to this movie/item/tune also ...\") is that the data set is continuously updated with new entries (e.g., new items for sale at Amazon, new movies to view at Netflix, new clips at Youtube, new music at Spotify or Pandora). Because of the continuous update, the \"training data\" would be rendered obsolete in a relatively short time especially in areas like books and movies, where new best-sellers or hit movies/music are published/released continuously. Therefore, one cannot really talk of a \"training phase\".\n\nLazy classifiers are most useful for large, continuously changing datasets with few attributes that are commonly queried. Specifically, even if a large set of attributes exist - for example, books have a year of publication, author/s, publisher, title, edition, ISBN, selling price, etc. - recommendation queries rely on far fewer attributes - e.g., purchase or viewing co-occurrence data, and user ratings of items purchased/viewed. \n\nThe main advantage gained in employing a lazy learning method is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain. It is said that the advantage of this system is achieved if the predictions using a single training set are only developed for few objects. This can be demonstrated in the case of the k-NN technique, which is instance-based and function is only estimated locally.\n\nTheoretical disadvantages with lazy learning include:\n\n- The large space requirement to store the entire training dataset. In practice, this is not an issue because of advances in hardware and the relatively small number of attributes (e.g.,as co-occurrence frequency) that need to be stored.\n- Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. In practice, as stated earlier, lazy learning is applied to situations where any learning performed in advance soon becomes obsolete because of changes in the data. Also, for the problems for which lazy learning is optimal, \"noisy\" data does not really occur - the purchaser of a book has either bought another book or hasn't.\n- Lazy learning methods are usually slower to evaluate. In practice, for very large databases with high concurrency loads, the queries are \"not\" postponed until actual query time, but recomputed in advance on a periodic basis - e.g., nightly, in anticipation of future queries, and the answers stored. This way, the next time new queries are asked about existing entries in the database, the answers are merely looked up rapidly instead of having to be computed on the fly, which would almost certainly bring a high-concurrency multi-user system to its knees.\n- Larger training data also entail increased cost. Particularly, there is the fixed amount of computational cost, where a processor can only process a limited amount of training data points.\n\nThere are standard techniques to improve re-computation efficiency so that a particular answer is not recomputed unless the data that impact this answer have changed (e.g., new items, new purchases, new views). In other words, the stored answers are updated incrementally.\n\nThis approach, used by large e-commerce or media sites, has long been used in the Entrez portal of the National Center for Biotechnology Information (NCBI) to precompute similarities between the different items in its large datasets: biological sequences, 3-D protein structures, published-article abstracts, etc. Because \"find similar\" queries are asked so frequently, the NCBI uses highly parallel hardware to perform nightly recomputation. The recomputation is performed only for new entries in the datasets against each other and against existing entries: the similarity between two existing entries need not be recomputed.\n\n- K-nearest neighbors, which is a special case of instance-based learning.\n- Local regression.\n- Lazy naive Bayes rules, which are extensively used in commercial spam detection software. Here, the spammers keep getting smarter and revising their spamming strategies, and therefore the learning rules must also be continually updated.\n\n- lazy: Lazy Learning for Local Regression, R package with reference manual\n- Webb G.I. (2011) Lazy Learning. In: Sammut C., Webb G.I. (eds) Encyclopedia of Machine Learning. Springer, Boston, MA\n", "related": "NONE"}
{"id": "10747995", "url": "https://en.wikipedia.org/wiki?curid=10747995", "title": "Eager learning", "text": "Eager learning\n\nIn artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. \nThe main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.\n\nThe main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function.\n", "related": "NONE"}
{"id": "12155912", "url": "https://en.wikipedia.org/wiki?curid=12155912", "title": "Discriminative model", "text": "Discriminative model\n\nDiscriminative models, also referred to as conditional models, are a class of models used in statistical classification, especially in supervised machine learning. A discriminative classifier tries to model by just depending on the observed data while learning how to do the classification from the given statistics.\n\nThe approaches used in supervised learning can be categorized into discriminative models or generative models. Compared to generative models, discriminative models make fewer assumptions about distributions and rely more on data quality. For example, given a set of labeled pictures of dog and rabbit, discriminative models will be matching a new, unlabeled picture to a most similar labeled picture and then give out the label class, a dog or a rabbit. However, generative will develop a model which should be able to output a class label to the unlabeled picture from the assumption they made, like all rabbits have red eyes.\n\nTypical discriminative learning approaches include logistic regression (LR), support vector machines (SVM), conditional random fields (CRFs) (specified over an undirected graph), and others. Typical generative model approaches include naive Bayes classifiers, Gaussian mixture models, and others.\n\nUnlike generative modelling, which studies from the joint probability formula_1, discriminative modeling studies the formula_2 or the direct maps the given unobserved variable (target) formula_3 a class label formula_4 depended on the observed variables (training samples). For example, in object recognition, formula_3 is likely to be a vector of raw pixels (or features extracted from the raw pixels of the image). Within a probabilistic framework, this is done by modeling the conditional probability distribution formula_2, which can be used for predicting formula_4 from formula_3. Note that there is still distinction between the conditional model and the discriminative model, though more often they are simply categorised as discriminative model.\n\nA \"conditional model\" models the conditional probability distribution, while the traditional discriminative model aims to optimize on mapping the input around the most similar trained samples.\n\nThe following approach is based on the assumption that it is given the training data-set formula_9, where formula_10is the corresponding output for the input formula_11.\n\nWe intend to use the function formula_12to simulate the behavior of what we observed from the training data-set by the linear classifier method. Using the joint feature vector formula_13, the decision function is defined as:\nAccording to Memisevic's interpretation, formula_15, which is also formula_16, computes a score which measures the computability of the input formula_3with the potential output formula_4. Then the formula_19 determines the class with the highest score.\n\nSince the 0-1 loss function is a commonly used one in the decision theory, the conditional probability distribution formula_20, where formula_21 is a parameter vector for optimizing the training data, could be reconsidered as following for the logistics regression model:\nThe equation above represents logistic regression. Notice that a major distinction between models is their way of introducing posterior probability. Posterior probability is inferred from the parametric model. We then can maximize the parameter by following equation:\nIt could also be replaced by the log-loss equation below:\nSince the log-loss is differentiable, a gradient-based method can be used to optimize the model. A global optimum is guaranteed because the objective function is convex. The gradient of log likelihood is represented by:\nwhere formula_27is the expectation of formula_28.\n\nThe above method will provide efficient computation for the relative small number of classification.\n\nAnother continuous (but not differentiable) alternative to the 0/1-loss is the ’hinge-loss’, which can be defined as the following equation\nThe hinge loss measures the difference between the maximal confidence that the classifier has over all classes and the confidence it has in the correct class. In computing this maximum, all wrong classes get a ’head start’ by adding 1 to the confidence. As a result, the hinge loss is 0 if the confidence in the correct class is by at least 1 greater than the confidence in the closest follow-up. Even though the hinge-loss is not differentiable, it can also give rise to a tractable variant of the 0/1- loss based learning problem, since the hinge-loss allows it to recast to the equivalent constrained optimization problem.\n\nLet's say we are given the formula_30 class labels (classification) and formula_31 feature variables, formula_32, as the training samples.\n\nA generative model takes the joint probability formula_1, where formula_3 is the input and formula_4 is the label, and predicts the most possible known label formula_36 for the unknown variable formula_37 using Bayes' theorem.\n\nDiscriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of observed and target variables. However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance (in part because they have fewer variables to compute). On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily support unsupervised learning. Application-specific details ultimately dictate the suitability of selecting a discriminative versus generative model.\n\nDiscriminative models and generative models also differ in introducing the posterior possibility. To maintain the least expected loss, the minimization of result's misclassification should be acquired. In the discriminative model, the posterior probabilities, formula_38, is inferred from a parametric model, where the parameters come from the training data. Points of estimation of the parameters are obtained from the maximization of likelihood or distribution computation over the parameters. On the other hand, considering that the generative models focus on the joint probability, the class posterior possibility formula_39 is considered in Bayes' theorem, which is\n\nIn the repeated experiments, logistic regression and naive Bayes are applied here for different models on binary classification task, discriminative learning results in lower asymptotic errors, while generative one results in higher asymptotic errors faster. However, in Ulusoy and Bishop's joint work, \"Comparison of Generative and Discriminative Techniques for Object Detection and Classification\", they state that the above statement is true only when the model is the appropriate one for data (i.e.the data distribution is correctly modeled by the generative model).\n\nSignificant advantages of using discriminative modeling are:\n\n- Higher accuracy, which mostly leads to better learning result.\n- Allows simplification of the input and provides a direct approach to formula_2\n- Saves calculation resource\n- Generates lower asymptotic errors\n\nCompared with the advantages of using generative modeling:\n\n- Takes all data into consideration, which could result in slower processing as a disadvantage\n- Requires less training samples\n- A flexible framework that could easily cooperate with other needs of the application\n\n- Training method usually requires multiple numerical optimization techniques\n- Similarly by the definition, the discriminative model will need the combination of multiple subtasks for a solving complex real-world problem\n\nSince both advantages and disadvantages present on the two way of modeling, combining both approaches will be a good modeling in practice. For example, in Marras' article \"A Joint Discriminative Generative Model for Deformable Model Construction and Classification\", he and his coauthors apply the combination of two modelings on face classification of the models, and receive a higher accuracy than the traditional approach.\n\nSimilarly, Kelm also proposed the combination of two modelings for pixel classification in his article \"Combining Generative and Discriminative Methods for Pixel Classification with Multi-Conditional Learning\".\n\nDuring the process of extracting the discriminative features prior to the clustering, Principal component analysis (PCA), though commonly used, is not a necessarily discriminative approach. In contrast, LDA is a discriminative one. Linear discriminant analysis (LDA), provides an efficient way of eliminating the disadvantage we list above. As we know, the discriminative model needs a combination of multiple subtasks before classification, and LDA provides appropriate solution towards this problem by reducing dimension.\n\nIn \"Beyerlein\"'s paper, \"DISCRIMINATIVE MODEL COMBINATION\", the discriminative model combination provides a new approach in auto speech recognition. It not only helps to optimize the integration of various kinds of models into one log-linear posterior probability distribution. The combination also aims at minimizing the empirical word error rate of training samples.\n\nIn the article, A Unified and Discriminative Model for Query Refinement, Guo and his partners use a unified discriminative model in query refinement using linear classifier, and successfully obtain a much higher accuracy rate. The experiment they design also consider generative model as a comparison with the unified model. Just as expected in the real-world application, the generative model perform the poorest comparing to the other models, including the models without their improvement.\n\nExamples of discriminative models used in machine learning include:\n- Logistic regression, a type of generalized linear regression used for predicting binary or categorical outputs (also known as maximum entropy classifiers)\n- Support vector machines\n- Boosting (meta-algorithm)\n- Conditional random fields\n- Linear regression\n- Neural networks\n- Random forests\n- Perceptrons\n\n", "related": "\n- Generative model\n"}
{"id": "12386904", "url": "https://en.wikipedia.org/wiki?curid=12386904", "title": "Data pre-processing", "text": "Data pre-processing\n\nData preprocessing is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis. \nOften, data preprocessing is the most important phase of a machine learning project, especially in computational biology.\n\nIf there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data preprocessing includes cleaning, Instance selection, normalization, transformation, feature extraction and selection, etc. The product of data preprocessing is the final training set.\n\nData pre-processing may affect the way in which outcomes of the final data processing can be interpreted. This aspect should be carefully considered when interpretation of the results is a key point, such in the multivariate processing of chemical data (chemometrics).\n\n- Data cleansing\n- Data editing\n- Data reduction\n- Data wrangling\n\nThe origins of data preprocessing are located in data mining. The idea is to aggregate existing information and search in the content. Later it was recognized, that for machine learning and neural networks a data preprocessing step is needed too. So it has become to a universal technique which is used in computing in general.\n\nFrom a users perspective, data preprocessing is equal to put existing comma-separated values files together. Data are usually stored in files. The CSV format was mentioned already but it's possible that the data are stored in a Microsoft Excel sheet or in a json file. A self-created script is applied to the file. From a technical side the script can be written in Python and in R (programming language).\n\nThe reason why a user transforms existing files into a new one is because of many reasons. Data preprocessing has the objective to add missing values, aggregate information, label data with categories (Data binning) and smooth a trajectory. More advanced techniques like principle component analysis and feature selection are working with statistical formulas and are applied to complex datasets which are recorded by GPS trackers and motion capture devices.\n\nComplex problems are asking for more elaborated analyzing techniques of existing information. Instead of creating a simple script for aggregating different numerical values into one, it make sense to focus on semantic based data preprocessing. Here is the idea to build a dedicated ontology which explains on a higher level what the problem is about. The Protégé (software) is the standard tool for this purpose. A second more advanced technique is Fuzzy preprocessing. Here is the idea to ground numerical values with linguistic information. Raw data are transformed into natural language.\n\n- Online Data Processing Compendium\n- Data preprocessing in predictive data mining. Knowledge Eng. Review 34: e1 (2019)\n", "related": "NONE"}
{"id": "11360852", "url": "https://en.wikipedia.org/wiki?curid=11360852", "title": "Predictive state representation", "text": "Predictive state representation\n\nIn computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the test's observation-sequence happening if the test's action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.\n", "related": "NONE"}
{"id": "14923880", "url": "https://en.wikipedia.org/wiki?curid=14923880", "title": "Expectation propagation", "text": "Expectation propagation\n\nExpectation propagation (EP) is a technique in Bayesian machine learning.\n\nEP finds approximations to a probability distribution. It uses an iterative approach that leverages the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as variational Bayesian methods.\n\nMore specifically, suppose we wish to approximate an intractable probability distribution formula_1 with a tractable distribution formula_2. Expectation propagation achieves this approximation by minimizing the Kullback-Leibler divergence formula_3. Variational Bayesian methods minimize formula_4 instead.\n\nIf formula_2 is a Gaussian formula_6, then formula_3 is minimized with formula_8 and formula_9 being equal to the mean of formula_1 and the covariance of formula_1, respectively; this is called moment matching.\n\nExpectation propagation via moment matching plays a vital role in approximation for indicator functions that appear when deriving the message passing equations for TrueSkill.\n\n- Minka's EP papers\n- List of papers using EP.\n", "related": "NONE"}
{"id": "5077439", "url": "https://en.wikipedia.org/wiki?curid=5077439", "title": "Ugly duckling theorem", "text": "Ugly duckling theorem\n\nThe Ugly Duckling theorem is an argument showing that classification is not really possible without some sort of bias. More particularly, it assumes finitely many properties combinable by logical connectives, and finitely many objects; it asserts that any two different objects share the same number of (extensional) properties. The theorem is named after Hans Christian Andersen's story \"The Ugly Duckling\", because it shows that a duckling is just as similar to a swan as two duckling are to each other. It was proposed by Satosi Watanabe in 1969.\n\nSuppose there are n things in the universe, and one wants to put them into classes or categories. One has no preconceived ideas or biases about what sorts of categories are \"natural\" or \"normal\" and what are not. So one has to consider all the possible classes that could be, all the possible ways of making sets out of the n objects. There are formula_1 such ways, the size of the power set of n objects. One can use that to measure the similarity between two objects: and one would see how many sets they have in common. However one can not. Any two objects have exactly the same number of classes in common if we can form any possible class, namely formula_2 (half the total number of classes there are). To see this is so, one may imagine each class is a represented by an n-bit string (or binary encoded integer), with a zero for each element not in the class and a one for each element in the class. As one finds, there are formula_1 such strings.\n\nAs all possible choices of zeros and ones are there, any two bit-positions will agree exactly half the time. One may pick two elements and reorder the bits so they are the first two, and imagine the numbers sorted lexicographically. The first formula_4 numbers will have bit #1 set to zero, and the second formula_4 will have it set to one. Within each of those blocks, the top formula_6 will have bit #2 set to zero and the other formula_6 will have it as one, so they agree on two blocks of formula_6 or on half of all the cases. No matter which two elements one picks. So if we have no preconceived bias about which categories are better, everything is then equally similar (or equally dissimilar). The number of predicates simultaneously satisfied by two non-identical elements is constant over all such pairs and is the same as the number of those satisfied by one. Thus, some kind of inductive bias is needed to make judgements; i.e. to prefer certain categories over others.\n\nLet formula_9 be a set of vectors of formula_10 booleans each. The ugly duckling is the vector which is least like the others. Given the booleans, this can be computed using Hamming distance.\n\nHowever, the choice of boolean features to consider could have been somewhat arbitrary. Perhaps there were features derivable from the original features that were important for identifying the ugly duckling. The set of booleans in the vector can be extended with new features computed as boolean functions of the formula_10 original features. The only canonical way to do this is to extend it with \"all\" possible Boolean functions. The resulting completed vectors have formula_12 features. The Ugly Duckling Theorem states that there is no ugly duckling because any two completed vectors will either be equal or differ in exactly half of the features.\n\nProof. Let x and y be two vectors. If they are the same, then their completed vectors must also be the same because any Boolean function of x will agree with the same Boolean function of y. If x and y are different, then there exists a coordinate formula_13 where the formula_13-th coordinate of formula_15 differs from the formula_13-th coordinate of formula_17. Now the completed features contain every Boolean function on formula_10 Boolean variables, with each one exactly once. Viewing these Boolean functions as polynomials in formula_10 variables over GF(2), segregate the functions into pairs formula_20 where formula_21 contains the formula_13-th coordinate as a linear term and formula_23 is formula_21 without that linear term. Now, for every such pair formula_20, formula_15 and formula_17 will agree on exactly one of the two functions. \nIf they agree on one, they must disagree on the other and vice versa. (This proof is believed to be due to Watanabe.)\n\n would be to introduce a constraint on how similarity is measured by limiting the properties involved in classification, say between A and B. However Medin et al. (1993) point out that this does not actually resolve the arbitrariness or bias problem since in what respects A is similar to B: “varies with the stimulus context and task, so that there is no unique answer, to the question of how similar is one object to another”. For example, \"a barberpole and a zebra would be more similar than a horse and a zebra if the feature \"striped\" had sufficient weight. Of course, if these feature weights were fixed, then these similarity relations would be constrained\". Yet the property \"striped\" as a weight 'fix' or constraint is arbitrary itself, meaning: \"unless one can specify such criteria, then the claim that categorization is based on attribute matching is almost entirely vacuous\".\n\nStamos (2003) has attempted to solve the Ugly Ducking Theorem by showing some judgments of overall similarity are non-arbitrary in the sense they are useful:\n\nUnless some properties are considered more salient, or ‘weighted’ more important than others, everything will appear equally similar, hence Watanabe (1986) wrote: “any objects, in so far as they are distinguishable, are equally similar\". \nIn a weaker setting that assumes infinitely many properties, Murphy and Medin (1985) give an example of two putative classified things, plums and lawnmowers:\n\n", "related": "\n- No free lunch in search and optimization\n- No free lunch theorem\n"}
{"id": "14529261", "url": "https://en.wikipedia.org/wiki?curid=14529261", "title": "Rademacher complexity", "text": "Rademacher complexity\n\nIn computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution.\n\nGiven a set formula_1, the Rademacher complexity of \"A\" is defined as follows:\n\nwhere formula_3 are independent random variables drawn from the Rademacher distribution i.e. formula_4 for formula_5, and formula_6. Some authors take the absolute value of the sum before taking the supremum, but if formula_7 is symmetric this makes no difference.\n\nGiven a sample formula_8, and a class formula_9 of real-valued functions defined on a domain space formula_10, the empirical Rademacher complexity of formula_9 given formula_12 is defined as:\n\nThis can also be written using the previous definition:\nwhere formula_15 denotes function composition, i.e.:\n\nLet formula_17 be a probability distribution over formula_10. \nThe Rademacher complexity of the function class formula_9 with respect to formula_17 for sample size formula_21 is:\n\nwhere the above expectation is taken over an identically independently distributed (i.i.d.) sample formula_23 generated according to formula_17.\n\n1. formula_7 contains a single vector, e.g., formula_26. Then:\nThe same is true for every singleton hypothesis class.\n\n2. formula_7 contains two vectors, e.g., formula_29. Then:\n\nThe Rademacher complexity can be used to derive data-dependent upper-bounds on the learnability of function classes. Intuitively, a function-class with smaller Rademacher complexity is easier to learn.\n\nIn machine learning, it is desired to have a training set that represents the true distribution of some sample data formula_12. This can be quantified using the notion of representativeness. Denote by formula_17 the probability distribution from which the samples are drawn. Denote by formula_33 the set of hypotheses (potential classifiers) and denote by formula_9 the corresponding set of error functions, i.e., for every hypothesis formula_35, there is a function formula_36, that maps each training sample (features,label) to the error of the classifier formula_37 (note in this case hypothesis and classifier are used interchangeably). For example, in the case that formula_37 represents a binary classifier, the error function is a 0–1 loss function, i.e. the error function formula_39 returns 1 if formula_37 correctly classifies a sample and 0 else. We omit the index and write formula_41 instead of formula_39 when the unterlying hypothesis is irrelevant. Define:\nThe representativeness of the sample formula_12, with respect to formula_17 and formula_9, is defined as:\n\nSmaller representativeness is better, since it provides a way to avoid overfitting: it means that the true error of a classifier is not much higher than its estimated error, and so selecting a classifier that has low estimated error will ensure that the true error is also low. Note however that the concept of representativeness is relative and hence can not be compared across distinct samples.\n\nThe expected representativeness of a sample can be bounded above by the Rademacher complexity of the function class:\n\nWhen the Rademacher complexity is small, it is possible to learn the hypothesis class H using empirical risk minimization.\n\nFor example, (with binary error function), for every formula_54, with probability at least formula_55, for every hypothesis formula_35:\n\nSince smaller Rademacher complexity is better, it is useful to have upper bounds on the Rademacher complexity of various function sets. The following rules can be used to upper-bound the Rademacher complexity of a set formula_58.\n\n1. If all vectors in formula_7 are translated by a constant vector formula_60, then Rad(\"A\") does not change.\n\n2. If all vectors in formula_7 are multiplied by a scalar formula_62, then Rad(\"A\") is multiplied by formula_63.\n\n3. Rad(\"A\" + \"B\") = Rad(\"A\") + Rad(\"B\").\n\n4. (Kakade & Tewari Lemma) If all vectors in formula_7 are operated by a Lipschitz function, then Rad(\"A\") is (at most) multiplied by the Lipschitz constant of the function. In particular, if all vectors in formula_7 are operated by a contraction mapping, then Rad(\"A\") strictly decreases.\n\n5. The Rademacher complexity of the convex hull of formula_7 equals Rad(\"A\").\n\n6. (Massart Lemma) The Rademacher complexity of a finite set grows logarithmically with the set size. Formally, let formula_7 be a set of formula_68 vectors in formula_69, and let formula_70 be the mean of the vectors in formula_7. Then:\nIn particular, if formula_7 is a set of binary vectors, the norm is at most formula_74, so:\n\nLet formula_33 be a set family whose VC dimension is formula_77. It is known that the growth function of formula_33 is bounded as:\nThis means that, for every set formula_37 with at most formula_21 elements, formula_83. The set-family formula_84 can be considered as a set of binary vectors over formula_69. Substituting this in Massart's lemma gives:\n\nWith more advanced techniques (Dudley's entropy bound and Haussler's upper bound) one can show, for example, that there exists a constant formula_87, such that any class of formula_88-indicator functions with Vapnik–Chervonenkis dimension formula_77 has Rademacher complexity upper-bounded by formula_90.\n\nThe following bounds are related to linear operations on formula_12 – a constant set of formula_21 vectors in formula_93.\n\n1. Define formula_94 the set of dot-products of the vectors in formula_12 with vectors in the unit ball. Then:\n\n2. Define formula_97 the set of dot-products of the vectors in formula_12 with vectors in the unit ball of the 1-norm. Then:\n\nThe following bound relates the Rademacher complexity of a set formula_7 to its external covering number – the number of balls of a given radius formula_101 whose union contains formula_7. The bound is attributed to Dudley.\n\nSuppose formula_103 is a set of vectors whose length (norm) is at most formula_104. Then, for every integer formula_105:\n\nIn particular, if formula_7 lies in a \"d\"-dimensional subspace of formula_69, then:\nSubstituting this in the previous bound gives the following bound on the Rademacher complexity:\n\nGaussian complexity is a similar complexity with similar physical meanings, and can be obtained from the Rademacher complexity using the random variables formula_111 instead of formula_112, where formula_111 are Gaussian i.i.d. random variables with zero-mean and variance 1, i.e. formula_114. Gaussian and Rademacher complexities are known to be equivalent up to logarithmic factors.\n\n- Peter L. Bartlett, Shahar Mendelson (2002) \"Rademacher and Gaussian Complexities: Risk Bounds and Structural Results\". Journal of Machine Learning Research 3 463–482\n- Giorgio Gnecco, Marcello Sanguineti (2008) \"Approximation Error Bounds via Rademacher's Complexity\". Applied Mathematical Sciences, Vol. 2, 2008, no. 4, 153–176\n", "related": "NONE"}
{"id": "787776", "url": "https://en.wikipedia.org/wiki?curid=787776", "title": "Curse of dimensionality", "text": "Curse of dimensionality\n\nThe curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic programming.\n\nCursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.\n\nIn some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the combinatorial explosion. Even in the simplest case of formula_1 binary variables, the number of possible combinations already is formula_2, exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.\n\nThere is an exponential increase in volume associated with adding extra dimensions to a mathematical space. For example, 10=100 evenly spaced sample points suffice to sample a unit interval (a \"1-dimensional cube\") with no more than 10=0.01 distance between points; an equivalent sampling of a 10-dimensional unit hypercube with a lattice that has a spacing of 10=0.01 between adjacent points would require 10=[(10)] sample points. In general, with a spacing distance of 10 the 10-dimensional hypercube appears to be a factor of 10=[(10)/(10)] \"larger\" than the 1-dimensional hypercube, which is the unit interval. In the above example n=2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 10 \"larger\" than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.\n\nWhen solving dynamic optimization problems by numerical backward induction, the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the \"state variable\" is large.\n\nIn machine learning problems that involve learning a \"state-of-nature\" from a finite number of data samples in a high-dimensional feature space with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. A typical rule of thumb is that there should be at least 5 training examples for each dimension in the representation. With a fixed number of training samples, the predictive power of a classifier or regressor first increases as number of dimensions or features used is increased but then decreases, which is known as \"Hughes phenomenon\" or \"peaking phenomena\".\n\nWhen a measure such as a Euclidean distance is defined using many coordinates, there is little difference in the distances between different pairs of samples.\n\nOne way to illustrate the \"vastness\" of high-dimensional Euclidean space is to compare the proportion of an inscribed hypersphere with radius formula_3 and dimension formula_1, to that of a hypercube with edges of length formula_5.\nThe volume of such a sphere is formula_6, where formula_7 is the gamma function, while the volume of the cube is formula_8.\nAs the dimension formula_1 of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be by comparing the proportions as the dimension formula_1 goes to infinity:\n\nFurthermore, the distance between the center and the corners is formula_13, which increases without bound for fixed r.\nIn this sense, nearly all of the high-dimensional space is \"far away\" from the centre. To put it another way, the high-dimensional unit hypercube can be said to consist almost entirely of the \"corners\" of the hypercube, with almost no \"middle\".\n\nThis also helps to understand the chi-squared distribution. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval [-1, 1] is the same as the distribution of the length-squared of a random point in the \"d\"-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around \"d\" times the standard deviation squared (σ) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the \"d\"-cube concentrates near the surface of a sphere of radius formula_14.\n\nA further development of this phenomenon is as follows. Any fixed distribution on \"ℝ\" induces a product distribution on points in \"ℝ\". For any fixed \"n\", it turns out that the minimum and the maximum distance between a random reference point \"Q\" and a list of \"n\" random data points \"P...,P\" become indiscernible compared to the minimum distance:\nThis is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions \"ℝ\" are independent and identically distributed. When attributes are correlated, data can become easier and provide higher distance contrast and the signal-to-noise ratio was found to play an important role, thus feature selection should be used.\n\nThe effect complicates nearest neighbor search in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.\n\nHowever, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties, since \"relevant\" additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant (\"noise\") dimensions, however, reduce the contrast in the manner described above. In time series analysis, where the data are inherently high-dimensional, distance functions also work reliably as long as the signal-to-noise ratio is high enough.\n\nAnother effect of high dimensionality on distance functions concerns \"k\"-nearest neighbor (\"k\"-NN) graphs constructed from a data set using a distance function. As the dimension increases, the indegree distribution of the \"k\"-NN digraph becomes skewed with a peak on the right because of the emergence of a disproportionate number of hubs, that is, data-points that appear in many more \"k\"-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for classification (including the \"k\"-NN classifier), semi-supervised learning, and clustering, and it also affects information retrieval.\n\nIn a 2012 survey, Zimek et al. identified the following problems when searching for anomalies in high-dimensional data:\n\n1. Concentration of scores and distances: derived values such as distances become numerically similar\n2. Irrelevant attributes: in high dimensional data, a significant number of attributes may be irrelevant\n3. Definition of reference sets: for local methods, reference sets are often nearest-neighbor based\n4. Incomparable scores for different dimensionalities: different subspaces produce incomparable scores\n5. Interpretability of scores: the scores often no longer convey a semantic meaning\n6. Exponential search space: the search space can no longer be systematically scanned\n7. Data snooping bias: given the large search space, for every desired significance a hypothesis can be found\n8. Hubness: certain objects occur more frequently in neighbor lists than others.\n\nMany of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions.\n\nSurprisingly and despite the expected \"curse of dimensionality\" difficulties, common-sense heuristics based on the most straightforward methods \"can yield results which are almost surely optimal\" for high-dimensional problems. The term \"blessing of dimensionality\" was introduced in the late 1990s. Donoho in his \"Millennium manifesto\" clearly explained why the \"blessing of dimensionality\" will form a basis of future data mining. The effects of the blessing of dimensionality were discovered in many applications and found their foundation in the concentration of measure phenomena. One example of the blessing of dimensionality phenomena is linear separability of a random point from a large finite random set with high probability even if this set is exponentially large: the number of elements in this random set can grow exponentially with dimension. Moreover, this linear functional can be selected in the form of the simplest linear Fisher discriminant. This separability theorem was proven for a wide class of probability distributions: general uniformly log-concave distributions, product distributions in a cube and many other families (reviewed recently in ). \n\n\"The blessing of dimensionality and the curse of dimensionality are two sides of the same coin.\" For example, the typical property of essentially high-dimensional probability distributions in a high-dimensional space is: the squared distance of random points to a selected point is, with high probability, close to the average (or median) squared distance. This property significantly simplifies the expected geometry of data and indexing of high-dimensional data (blessing), but, at the same time, it makes the similarity search in high dimensions difficult and even useless (curse).\n\nZimek et al. noted that while the typical formalizations of the curse of dimensionality affect i.i.d. data, having data that is separated in each attribute becomes easier even in high dimensions, and argued that the signal-to-noise ratio matters: data becomes easier with each attribute that adds signal, and harder with attributes that only add noise (irrelevant error) to the data. In particular for unsupervised data analysis this effect is known as swamping.\n\n", "related": "\n- Bellman equation\n- Clustering high-dimensional data\n- Concentration of measure\n- Dimension reduction\n- Model Order Reduction\n- Dynamic programming\n- Fourier-related transforms\n- Linear least squares\n- Multilinear PCA\n- Multilinear subspace learning\n- Principal component analysis\n- Singular value decomposition\n"}
{"id": "19058043", "url": "https://en.wikipedia.org/wiki?curid=19058043", "title": "Uncertain data", "text": "Uncertain data\n\nIn computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.\n\nUncertain data is found in the area of sensor networks; text where noisy text is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the mathematical model may only be an approximation of the actual process. When representing such data in a database, some indication of the probability of the correctness of the various values also needs to be estimated.\n\nThere are three main models of uncertain data in databases. In attribute uncertainty, each uncertain attribute in a tuple is subject to its own independent probability distribution. For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other.\n\nIn correlated uncertainty, multiple attributes may be described by a joint probability distribution. For example, if readings are taken of the position of an object, and the \"x\"- and \"y\"-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not independent.\n\nIn tuple uncertainty, all the attributes of a tuple are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one. For example, assume we have the following tuple from a probabilistic database:\nThen, the tuple has 10% chance of not existing in the database.\n\n", "related": "NONE"}
{"id": "4144848", "url": "https://en.wikipedia.org/wiki?curid=4144848", "title": "Knowledge integration", "text": "Knowledge integration\n\nKnowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation).\n\nCompared to information integration, which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.\n\nFor example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.\n\nThe Web-based Inquiry Science Environment (WISE), from the University of California at Berkeley has been developed along the lines of knowledge integration theory.\n\nKnowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach. This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.\n\nA learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps. By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.\n\nThe machine learning program KI, developed by Murray and Porter at the University of Texas at Austin, was created to study the use of automated and semi-automated knowledge integration to assist knowledge engineers constructing a large knowledge base.\n\nA possible technique which can be used is semantic matching. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on Minimal Mappings. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).\n\nThe University of Waterloo operates a Bachelor of Knowledge Integration undergraduate degree program as an academic major or minor. The program started in 2008.\n\n", "related": "\n- Knowledge value chain\n\n- Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In \"The Cambridge Handbook of the Learning Sciences.\" Cambridge, MA. Cambridge University Press\n- Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence\n- Murray, K. S. (1995) Learning as Knowledge Integration, Technical Report TR-95-41, The University of Texas at Austin\n- Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society\n- Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33\n- Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference\n- Shen, J., Sung, S., & Zhang, D.M. (2016) Toward an analytic framework of interdisciplinary reasoning and communication (IRC) processes in science. International Journal of Science Education, 37 (17), 2809-2835.\n- Shen, J., Liu, O., & Sung, S. (2014). Designing interdisciplinary assessments in science for college students: An example on osmosis. International Journal of Science Education, 36 (11), 1773-1793.\n"}
{"id": "10748030", "url": "https://en.wikipedia.org/wiki?curid=10748030", "title": "Offline learning", "text": "Offline learning\n\nIn machine learning, systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed. These systems are also typically examples of eager learning.\n\nWhile in online learning, only the set of possible elements is known, in offline learning, the identity of the elements as well as the order in which they are presented is known to the learner.\n\nThe ability of robots to learn is equal to create a table (information) which is filled with values. One option for doing so is programming by demonstration. Here, the table is filled with values by a human teacher. The demonstration is provided either as direct numerical control policy which is equal to a trajectory, or as an indirect objective function which is given in advance.\n\nOffline learning is working in batch mode. In step 1 the task is demonstrated and stored in the table, and in step 2 the task is reproduced by the robot. The pipeline is slow and inefficient because a time lag is there between behavior demonstration and skill replay.\n\nA short example will help to understand the idea. Suppose the robot should learn a wall following task and the internal table of the robot is empty. Before the robot gets activated in the replay mode, the human demonstrator has to teach the behavior. He is controlling the robot with teleoperation and during the learning step the skill table is generated. The process is called offline, because the robot control software is doing nothing but the device is utilized by the human operator as a pointing device for driving along the wall.\n\n", "related": "\n- Online learning, the opposite model\n- Incremental learning, a learning model for the incremental extension of knowledge\n"}
{"id": "19208664", "url": "https://en.wikipedia.org/wiki?curid=19208664", "title": "Neural modeling fields", "text": "Neural modeling fields\n\nNeural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS).\n\nThis framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.\n\nIn the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level. In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as aesthetic emotions.\n\nEach hierarchical level consists of N \"neurons\" enumerated by index n=1,2..N. These neurons receive input, bottom-up signals, X(n), from lower levels in the processing hierarchy. X(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level. Each neuron has a number of synapses; for generality, each neuron activation is described as a set of numbers, \n\n, where D is the number or dimensions necessary to describe individual neuron's activation. \n\nTop-down, or priming signals to these neurons are sent by concept-models, M(S,n) \n\n, where M is the number of models. Each model is characterized by its parameters, S; in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers, \n\n, where A is the number of dimensions necessary to describe individual model.\n\nModels represent signals in the following way. Suppose that signal X(\"n\") is coming from sensory neurons n activated by object m, which is characterized by parameters S. These parameters may include position, orientation, or lighting of an object m. Model M(S,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal M(S,n) from an object-concept-model \"m\". Neuron \"n\" is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.\n\nLearning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals, L({X},{M}). The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles:\n\nTherefore, the similarity measure is constructed so that it accounts for all bottom-up signals, \"X\"(\"n\"),\n\nThis expression contains a product of partial similarities, l(X(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this is a reflection of the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal. Its constituent elements are conditional partial similarities between signal X(n) and model M, l(X(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows: \n\nThe structure of the expression above follows standard principles of the probability theory: a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name “conditional partial similarity” for l(X(n)|m) (or simply l(n|m)) follows the probabilistic terminology. If learning is successful, l(n|m) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m. Then L is a total likelihood of observing signals {X(n)} coming from objects described by concept-model {M}. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values; their true values are usually unknown and should be learned, like other parameters S.\n\nNote that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals X(n). There is a dependence among signals due to concept-models: each model M(S,n) predicts expected signal values in many neurons n. \n\nDuring the learning process, concept-models are constantly modified. Usually, the functional forms of models, M(S,n), are all fixed and learning-adaptation involves only model parameters, S. From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L; The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a “skeptic penalty function,” (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-N/2), where N is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references).\n\nThe learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in M items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic. An important aspect of dynamic logic is \"matching vagueness or fuzziness of similarity measures to the uncertainty of models\". Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases. \n\nThe maximization of similarity L is done as follows. First, the unknown parameters {S} are randomly initialized. Then the association variables f(m|n) are computed,\n\nEquation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows:\n\nThe following theorem has been proved (Perlovsky 2001):\n\n\"Theorem\". Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{S}L.\n\nIt follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {S} are asymptotically unbiased and efficient estimates of these parameters. The computational complexity of dynamic logic is linear in N. \n\nPractically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5).\n\nThe proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning.\n\nFinding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy ‘smile’ and ‘frown’ patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, M ~ 10. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 10 to 10 operations, still a prohibitive computational complexity.\nTo apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in: a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for ‘smiles’ and ‘frowns’. The number of computer operations in this example was about 10. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.\n\nDuring an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the ‘best’ fit. \n\nThere are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models; their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing.\n\nAbove, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions; actions include adaptation, behavior satisfying the knowledge instinct – maximization of similarity. An input to each level is a set of signals X(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n; these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level.\n\nThe activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, a, defined as\n\na = ∑ f(m|n). \n\nThe hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its “mental” meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model “chair.” It has a “behavioral” purpose of initiating sitting behavior (if sitting is required by the body), this is the “bodily” purpose at the same hierarchical level. In addition, it has a “purely mental” purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a “concert hall,” a model of which contains rows of chairs. \n\nFrom time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data; therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can “grab” every signal that does not fit into more specific, less fuzzy, active models. When the activation signal a for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level { a } form a “neuronal field,” which serve as input signals to the next level, where more abstract and more general concepts are formed.\n\n- Leonid Perlovsky\n", "related": "NONE"}
{"id": "14271782", "url": "https://en.wikipedia.org/wiki?curid=14271782", "title": "Semantic analysis (machine learning)", "text": "Semantic analysis (machine learning)\n\nIn machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.\n\nLatent semantic analysis (sometimes latent semantic indexing), is a class of techniques where documents are represented as vectors in term space. A prominent example is PLSI.\n\nLatent Dirichlet allocation involves attributing document terms to topics.\n\nn-grams and hidden Markov models work by representing the term stream as a markov chain where each term is derived from the few terms before it.\n\n", "related": "\n- Information extraction\n- Semantic similarity\n- Ontology learning\n"}
{"id": "20890511", "url": "https://en.wikipedia.org/wiki?curid=20890511", "title": "Algorithmic inference", "text": "Algorithmic inference\n\nAlgorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability .\nThe main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.\n\nConcerning the identification of the parameters of a distribution law, the mature reader may recall lengthy disputes in the mid 20th century about the interpretation of their variability in terms of fiducial distribution , structural probabilities , priors/posteriors , and so on. From an epistemology viewpoint, this entailed a companion dispute as to the nature of probability: is it a physical feature of phenomena to be described through random variables or a way of synthesizing data about a phenomenon? Opting for the latter, Fisher defines a \"fiducial distribution\" law of parameters of a given random variable that he deduces from a sample of its specifications. With this law he computes, for instance “the probability that μ (mean of a Gaussian variable – our note) is less than any assigned value, or the probability that it lies between any assigned values, or, in short, its probability distribution, in the light of the sample observed”.\n\nFisher fought hard to defend the difference and superiority of his notion of parameter distribution in comparison to \nanalogous notions, such as Bayes' posterior distribution, Fraser's constructive probability and Neyman's confidence intervals. For half a century, Neyman's confidence intervals won out for all practical purposes, crediting the phenomenological nature of probability. With this perspective, when you deal with a Gaussian variable, its mean μ is fixed by the physical features of the phenomenon you are observing, where the observations are random operators, hence the observed values are specifications of a random sample. Because of their randomness, you may compute from the sample specific intervals containing the fixed μ with a given probability that you denote \"confidence\".\n\nLet \"X\" be a Gaussian variable with parameters formula_1 and formula_2 \nand formula_3 a sample drawn from it. Working with statistics\n\nand\n\nis the sample mean, we recognize that\n\nfollows a Student's t distribution with parameter (degrees of freedom) \"m\" − 1, so that\n\nGauging \"T\" between two quantiles and inverting its expression as a function of formula_1 you obtain confidence intervals for formula_1.\n\nWith the sample specification:\n\nhaving size \"m\" = 10, you compute the statistics formula_11 and formula_12, and obtain a 0.90 confidence interval for formula_1 with extremes (3.03, 5.65).\nFrom a modeling perspective the entire dispute looks like a chicken-egg dilemma: either fixed data by first and probability distribution of their properties as a consequence, or fixed properties by first and probability distribution of the observed data as a corollary.\nThe classic solution has one benefit and one drawback. The former was appreciated particularly back when people still did computations with sheet and pencil. Per se, the task of computing a Neyman confidence interval for the fixed parameter θ is hard: you don’t know θ, but you look for disposing around it an interval with a possibly very low probability of failing. The analytical solution is allowed for a very limited number of theoretical cases. \"Vice versa\" a large variety of instances may be quickly solved in an \"approximate way\" via the central limit theorem in terms of confidence interval around a Gaussian distribution – that's the benefit. \nThe drawback is that the central limit theorem is applicable when the sample size is sufficiently large. Therefore, it is less and less applicable with the sample involved in modern inference instances. The fault is not in the sample size on its own part. Rather, this size is not sufficiently large because of the complexity of the inference problem.\n\nWith the availability of large computing facilities, scientists refocused from isolated parameters inference to complex functions inference, i.e. re sets of highly nested parameters identifying functions. In these cases we speak about \"learning of functions\" (in terms for instance of regression, neuro-fuzzy system or computational learning) on the basis of highly informative samples. A first effect of having a complex structure linking data is the reduction of the number of sample degrees of freedom, i.e. the burning of a part of sample points, so that the effective sample size to be considered in the central limit theorem is too small. Focusing on the sample size ensuring a limited learning error with a given confidence level, the consequence is that the lower bound on this size grows with complexity indices such as VC dimension or detail of a class to which the function we want to learn belongs.\n\nA sample of 1,000 independent bits is enough to ensure an absolute error of at most 0.081 on the estimation of the parameter \"p\" of the underlying Bernoulli variable with a confidence of at least 0.99. The same size cannot guarantee a threshold less than 0.088 with the same confidence 0.99 when the error is identified with the probability that a 20-year-old man living in New York does not fit the ranges of height, weight and waistline observed on 1,000 Big Apple inhabitants. The accuracy shortage occurs because both the VC dimension and the detail of the class of parallelepipeds, among which the one observed from the 1,000 inhabitants' ranges falls, are equal to 6.\nWith insufficiently large samples, the approach: \"fixed sample – random properties\" suggests inference procedures in three steps:\nFor a random variable and a sample drawn from it a \"compatible distribution\" is a distribution having the same sampling mechanism formula_14 of \"X\" with a value formula_15 of the random parameter formula_16 derived from a master equation rooted on a well-behaved statistic \"s\".\n\nYou may find the distribution law of the Pareto parameters \"A\" and \"K\" as an implementation example of the population bootstrap method as in the figure on the left.\n\nImplementing the twisting argument method, you get the distribution law formula_17 of the mean \"M\" of a Gaussian variable \"X\" on the basis of the statistic formula_18 when formula_19 is known to be equal to formula_2 . Its expression is:\n\nshown in the figure on the right, where formula_22 is the cumulative distribution function of a standard normal distribution.\n\nThe Achilles heel of Fisher's approach lies in the joint distribution of more than one parameter, say mean and variance of a Gaussian distribution. On the contrary, with the last approach (and above-mentioned methods: population bootstrap and twisting argument) we may learn the joint distribution of many parameters. For instance, focusing on the distribution of two or many more parameters, in the figures below we report two confidence regions where the function to be learnt falls with a confidence of 90%. The former concerns the probability with which an extended support vector machine attributes a binary label 1 to the points of the formula_25 plane. The two surfaces are drawn on the basis of a set of sample points in turn labelled according to a specific distribution law . The latter concerns the confidence region of the hazard rate of breast cancer recurrence computed from a censored sample .\n\n", "related": "NONE"}
{"id": "19317802", "url": "https://en.wikipedia.org/wiki?curid=19317802", "title": "Decision list", "text": "Decision list\n\nDecision lists are a representation for Boolean functions which can be easily learnable from examples. Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.\n\nThe language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.\n\nLearning decision lists can be used for attribute efficient learning.\n\nA decision list (DL) of length is of the form:\n\nwhere is the th formula and is the th boolean for formula_1. The last if-then-else is the default case, which means formula is always equal to true. A -DL is a decision list where all of formulas have at most terms. Sometimes \"decision list\" is used to refer to a 1-DL, where all of the formulas are either a variable or its negation.\n\n", "related": "\n- Decision stump\n"}
{"id": "7517319", "url": "https://en.wikipedia.org/wiki?curid=7517319", "title": "Rule induction", "text": "Rule induction\n\nRule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.\n\nSome major rule induction paradigms are:\n- Association rule learning algorithms (e.g., Agrawal)\n- Decision rule algorithms (e.g., Quinlan 1987)\n- Hypothesis testing algorithms (e.g., RULEX)\n- Horn clause induction\n- Version spaces\n- Rough set rules\n- Inductive Logic Programming\n- Boolean decomposition (Feldman)\n\nSome rule induction algorithms are:\n- Charade\n- Rulex\n- Progol\n- CN2\n", "related": "NONE"}
{"id": "22589574", "url": "https://en.wikipedia.org/wiki?curid=22589574", "title": "Instance-based learning", "text": "Instance-based learning\n\nIn machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory.\n\nIt is called instance-based because it constructs hypotheses directly from the training instances themselves.\nThis means that the hypothesis complexity can grow with the data: in the worst case, a hypothesis is a list of \"n\" training items and the computational complexity of classifying a single new instance is \"O\"(\"n\"). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away.\n\nExamples of instance-based learning algorithm are the \"k\"-nearest neighbors algorithm, kernel machines and RBF networks. These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision.\n\nTo battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, \"instance reduction\" algorithms have been proposed.\n\nGagliardi applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. \nOne of these classifiers (called \"Prototype exemplar learning classifier\" (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases.\n\n", "related": "\n- Analogical modeling\n"}
{"id": "173332", "url": "https://en.wikipedia.org/wiki?curid=173332", "title": "Overfitting", "text": "Overfitting\n\nIn statistics, overfitting is \"the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably\". An overfitted model is a statistical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e. the noise) as if that variation represented underlying model structure.\n\nUnderfitting occurs when a statistical model cannot adequately capture the underlying structure of the data. An underfitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.\n\nOverfitting and underfitting can occur in machine learning, in particular. In machine learning, the phenomena are sometimes called \"overtraining\" and \"undertraining\". \n\nThe possibility of overfitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then overfitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend. \n\nAs an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. \n\nThe potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as \"shrinkage\"). In particular, the value of the coefficient of determination will shrink relative to the original data.\n\nTo lessen the chance of, or amount of, overfitting, several techniques are available (e.g. model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.\n\nIn statistics, an inference is drawn from a statistical model, which has been selected via some procedure. Burnham & Anderson, in their much-cited text on model selection, argue that to avoid overfitting, we should adhere to the \"Principle of Parsimony\". The authors also state the following.\nOverfitting is more likely to be a serious concern when there is little theory available to guide the analysis, in part because then there tend to be a large number of models to select from. The book \"Model Selection and Model Averaging\" (2008) puts it this way.\nIn regression analysis, overfitting occurs frequently. As an extreme example, if there are \"p\" variables in a linear regression with \"p\" data points, the fitted line can go exactly through every point. For logistic regression or Cox proportional hazards models, there are a variety of rules of thumb (e.g. 5–9, 10 and 10–15 — the guideline of 10 observations per independent variable is known as the \"one in ten rule\"). In the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of the regression function. The bias–variance tradeoff is often used to overcome overfit models.\n\nWith a large set of explanatory variables that actually have no relation to the dependent variable being predicted, some variables will in general be falsely found to be statistically significant and the researcher may thus retain them in the model, thereby overfitting the model. This is known as Freedman's paradox.\n\nUsually a learning algorithm is trained using some set of \"training data\": exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed \"validation data\" that was not encountered during its training.\n\nOverfitting is the use of models or procedures that violate Occam's razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for can be adequately predicted by a linear function of two independent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two independent variables, carries a risk: Occam's razor implies that any given complex function is \"a priori\" less probable than any given simple function. If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training-data fit to offset the complexity increase, then the new complex function \"overfits\" the data, and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset.\n\nWhen comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with parameters to a regression model with parameters.\n\nOverfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse.\n\nAs a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. It's easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes, but this model will not generalize at all to new data, because those past times will never occur again.\n\nGenerally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future, and irrelevant information (\"noise\"). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the chance of fitting noise is called \"robust.\"\n\nThe most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include:\n\n- A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data-entry.\n- A more complex, overfitted function is likely to be less portable than a simple one. At one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. At the other extreme are models that can be reproduced only by exactly duplicating the original modeler's entire setup, making reuse or scientific reproduction difficult.\n\nThe optimal function usually needs verification on bigger or completely new datasets. There are, however, methods like minimum spanning tree or life-time of correlation that applies the dependence between correlation coefficients and time-series (window width). Whenever the window width is big enough, the correlation coefficients are stable and don't depend on the window width size anymore. Therefore, a correlation matrix can be created by calculating a coefficient of correlation between investigated variables. This matrix can be represented topologically as a complex network where direct and indirect influences between variables are visualized.\n\nUnderfitting occurs when a statistical model or machine learning algorithm cannot adequately capture the underlying structure of the data. It occurs when the model or algorithm does not fit the data enough. Underfitting occurs if the model or algorithm shows low variance but high bias (to contrast the opposite, overfitting from high variance and low bias). It is often a result of an excessively simple model. \n\nBurnham & Anderson state the following.\n\n", "related": "\n- Bias–variance tradeoff\n- Curve fitting\n- Data dredging\n- Feature selection\n- Freedman's paradox\n- Generalization error\n- Goodness of fit\n- Life-time of correlation\n- Model selection\n- Occam's razor\n- VC dimension – larger VC dimension implies larger risk of overfitting\n\n- \"Tip 7: Minimize overfitting\".\n\n- Overfitting: when accuracy measure goes wrong (an introductory video tutorial)\n- The Problem of Overfitting Data —Stony Brook University\n- What is \"overfitting,\" exactly? —Andrew Gelman blog\n- CSE546: Linear Regression Bias / Variance Tradeoff —University of Washington\n"}
{"id": "22999791", "url": "https://en.wikipedia.org/wiki?curid=22999791", "title": "Uniform convergence in probability", "text": "Uniform convergence in probability\n\nUniform convergence in probability is a form of convergence in probability in statistical asymptotic theory and probability theory. It means that, under certain conditions, the \"empirical frequencies\" of all events in a certain event-family converge to their \"theoretical probabilities\". Uniform convergence in probability has applications to statistics as well as machine learning as part of statistical learning theory.\n\nThe law of large numbers says that, for each \"single\" event, its empirical frequency in a sequence of independent trials converges (with high probability) to its theoretical probability. But in some applications, we are interested not in a single event but in a whole \"family of events\". We would like to know whether the empirical frequency of every event in the family converges to its theoretical probability \"simultaneously\". The Uniform Convergence Theorem gives a sufficient condition for this convergence to hold. Roughly, if the event-family is sufficiently simple (its VC dimension is sufficiently small) then uniform convergence holds.\n\nFor a class of predicates formula_1 defined on a set formula_2 and a set of samples formula_3, where formula_4, the \"empirical frequency\" of formula_5 on formula_6 is\n\nThe \"theoretical probability\" of formula_5 is defined as formula_9\n\nThe Uniform Convergence Theorem states, roughly, that if formula_1 is \"simple\" and we draw samples independently (with replacement) from formula_2 according to any distribution formula_12, then with high probability, the empirical frequency will be close to its expected value, which is the theoretical probability.\n\nHere \"simple\" means that the Vapnik–Chervonenkis dimension of the class formula_1 is small relative to the size of the sample. In other words, a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole.\n\nThe Uniform Convergence Theorem was first proved by Vapnik and Chervonenkis using the concept of growth function.\n\nThe statement of the uniform convergence theorem is as follows:\n\nIf formula_1 is a set of formula_15-valued functions defined on a set formula_2 and formula_12 is a probability distribution on formula_2 then for formula_19 and formula_20 a positive integer, we have:\n\nThe maximum is guaranteed to exist since there is only a finite set of values that probability under a random permutation can take.\n\nLemma: Basing on the previous lemma,\n\nProof:\nLet us define formula_23 and formula_24 which is at most formula_25. This means there are functions formula_26 such that for any formula_27 between formula_28 and formula_29 with formula_30 for formula_31\n\nWe see that formula_32 iff for some formula_33 in formula_1 satisfies,\nformula_35. \nHence if we define formula_36 if formula_37 and formula_38 otherwise.\n\nFor formula_39 and formula_40, we have that formula_32 iff for some formula_42 in formula_43 satisfies formula_44. By union bound we get\n\nSince, the distribution over the permutations formula_47 is uniform for each formula_48, so formula_49 equals formula_50, with equal probability.\n\nThus,\n\nwhere the probability on the right is over formula_52 and both the possibilities are equally likely. By Hoeffding's inequality, this is at most formula_53.\n\nFinally, combining all the three parts of the proof we get the Uniform Convergence Theorem.\n", "related": "NONE"}
{"id": "17114678", "url": "https://en.wikipedia.org/wiki?curid=17114678", "title": "Center for Biological and Computational Learning", "text": "Center for Biological and Computational Learning\n\nThe Center for Biological & Computational Learning is a research lab at the Massachusetts Institute of Technology.\n\nCBCL was established in 1992 with support from the National Science Foundation. It is based in the Department of Brain & Cognitive Sciences at MIT, and is associated with the McGovern Institute for Brain Research, and the MIT Computer Science and Artificial Intelligence Laboratory.\n\nIt was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial. Learning is thus the gateway to understanding how the human brain works and for making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning. \n\nResearch is focused on the problem of learning in theory, engineering applications, and neuroscience.\n\nIn computational neuroscience, the center has developed a model of the ventral stream in the visual cortex which accounts for much of the physiological data, and psychophysical experiments in difficult object recognition tasks. The model performs at the level of the best computer vision systems.\n\nSection::::See also.\nTomaso Poggio director of CBCL\n\n- The Center for Biological and Computational Learning (CBCL)\n- BBC: Visions of the Future - February 29, 2008 - This is part of the excellent BBC series entitled \"visions of the future\". This short clip (3min) here shows work performed at CBCL (MIT) about a computational neuroscience model of the ventral stream of the visual cortex. The story here focuses on recent work by Serre, Oliva and Poggio on comparing the performance of the model to human observers during a rapid object categorization task.\n- THE DISCOVERY CHANNEL [Toronto, Canada] by Jennifer Scott (June 17, 2002): Video:Science, Lies & Videotape - Tony Ezzat and Tomaso Poggio.\n- NBC TODAY SHOW with Katie Couric (May 20, 2002): Video:* (100 kbit/s) (300 kbit/s) - Tony Ezzat and Tomaso Poggio.\n", "related": "NONE"}
{"id": "12306500", "url": "https://en.wikipedia.org/wiki?curid=12306500", "title": "Matthews correlation coefficient", "text": "Matthews correlation coefficient\n\nThe Matthews correlation coefficient (MCC) is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. Although the MCC is equivalent to Karl Pearson's phi coefficient, which was developed decades earlier, the term MCC is widely used in the field of bioinformatics. \n\nThe coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table\n\nwhere \"n\" is the total number of observations.\n\nWhile there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.\n\nThe MCC can be calculated directly from the confusion matrix using the formula:\n\nIn this equation, \"TP\" is the number of true positives, \"TN\" the number of true negatives, \"FP\" the number of false positives and \"FN\" the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n\nThe MCC can be calculated with the formula:\nusing the positive predictive value, the true positive rate, the true negative rate, the negative predictive value, the false discovery rate, the false negative rate, the false positive rate, and the false omission rate.\n\nThe original formula as given by Matthews was:\n\nThis is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness (Δp) and Youden's J statistic (Informedness or Δp'). Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the formula_8p statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.\n\nSome scientists claim the Matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context.\n\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 \"contingency table\" or \"confusion matrix\", as follows:\n\nThe Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the formula_9 statistic (for K different classes) by the author, and defined in terms of a formula_10 confusion matrix formula_11\n\nWhen there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1.\n\nAs explained by Davide Chicco in his paper \"Ten quick tips for machine learning in computational biology\" (BioData Mining, 2017) and by Giuseppe Jurman in his paper \"The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation\" (BMC Genomics, 2020), the Matthews correlation coefficient is more informative than F1 score and accuracy in evaluating binary classification problems, because it takes into account the balance ratios of the four confusion matrix categories (true positives, true negatives, false positives, false negatives)\n\nThe former article explains, for \"Tip 8\":\n\nNote that the F1 score depends on which class is defined as the positive class. In the first example above, the F1 score is high because the majority class is defined as the positive class. Inverting the positive and negative classes results in the following confusion matrix:\n\nTP = 0, FP = 0; TN = 5, FN = 95\n\nThis gives an F1 score = 0%.\n\nThe MCC doesn't depend on which class is the positive one, which has the advantage over the F1 score to avoid incorrectly defining the positive class.\n\n", "related": "\n- Cohen's kappa\n- Cramér's V, a similar measure of association between nominal variables.\n- F1 score\n- Phi coefficient\n"}
{"id": "23864530", "url": "https://en.wikipedia.org/wiki?curid=23864530", "title": "Learning with errors", "text": "Learning with errors\n\nLearning with errors (LWE) is the computational problem of inferring a linear formula_1-ary function formula_2 over a finite ring from given samples formula_3 some of which may be erroneous.\nThe LWE problem is conjectured to be hard to solve, and thus be useful in cryptography.\n\nMore precisely, the LWE problem is defined as follows. Let formula_4 denote the ring of integers modulo formula_5 and let\nformula_6 denote the set of formula_1-vectors over formula_4. There exists a certain unknown linear function formula_9, and the input to the LWE problem is a sample of pairs formula_10, where formula_11 and formula_12, so that with high probability formula_13. Furthermore, the deviation from the equality is according to some known noise model. The problem calls for finding the function formula_2, or some close approximation thereof, with high probability.\n\nThe LWE problem was introduced by Oded Regev in 2005 (who won the 2018 Gödel Prize for this work), it is a generalization of the parity learning problem. Regev showed that the LWE problem is as hard to solve as several worst-case lattice problems. Subsequently, the LWE problem has been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert.\n\nDenote by formula_15 the additive group on reals modulo one. \nLet formula_16 be a fixed vector.\nLet formula_17 be a fixed probability distribution over formula_18.\nDenote by formula_19 the distribution on formula_20 obtained as follows.\n1. Pick a vector formula_21 from the uniforms distribution over formula_21,\n2. Pick a number formula_23 from the distribution formula_17,\n3. Evaluate formula_25, where formula_26 is the standard inner product in formula_27, the division is done in the field of reals (or more formally, this \"division by formula_5\" is notation for the group homomorphism formula_29 mapping formula_30 to formula_31), and the final addition is in formula_18.\n4. Output the pair formula_33.\n\nThe learning with errors problem formula_34 is to find formula_16, given access to polynomially many samples of choice from formula_19.\n\nFor every formula_37, denote by formula_38 the one-dimensional Gaussian with zero mean and variance\nformula_39, that is, the density function is formula_40 where formula_41, and let formula_42 be the distribution on formula_18 obtained by considering formula_38 modulo one. The version of LWE considered in most of the results would be formula_45\n\nThe LWE problem described above is the \"search\" version of the problem. In the \"decision\" version (DLWE), the goal is to distinguish between noisy inner products and uniformly random samples from formula_20 (practically, some discretized version of it). Regev showed that the \"decision\" and \"search\" versions are equivalent when formula_5 is a prime bounded by some polynomial in formula_1.\n\nIntuitively, if we have a procedure for the search problem, the decision version can be solved easily: just feed the input samples for the decision problem to the solver for the search problem. Denote the given samples by formula_49. If the solver returns a candidate formula_50, for all formula_51, calculate formula_52. If the samples are from an LWE distribution, then the results of this calculation will be distributed according formula_53, but if the samples are uniformly random, these quantities will be distributed uniformly as well.\n\nFor the other direction, given a solver for the decision problem, the search version can be solved as follows: Recover formula_50 one coordinate at a time. To obtain the first coordinate, formula_55, make a guess formula_56, and do the following. Choose a number formula_57 uniformly at random. Transform the given samples formula_49 as follows. Calculate formula_59. Send the transformed samples to the decision solver.\n\nIf the guess formula_60 was correct, the transformation takes the distribution formula_61 to itself, and otherwise, since formula_5 is prime, it takes it to the uniform distribution. So, given a polynomial-time solver for the decision problem that errs with very small probability, since formula_5 is bounded by some polynomial in formula_1, it only takes polynomial time to guess every possible value for formula_60 and use the solver to see which one is correct.\n\nAfter obtaining formula_55, we follow an analogous procedure for each other coordinate formula_67. Namely, we transform our formula_68 samples the same way, and transform our formula_69 samples by calculating formula_70, where the formula_71 is in the formula_72 coordinate.\n\nPeikert showed that this reduction, with a small modification, works for any formula_5 that is a product of distinct, small (polynomial in formula_1) primes. The main idea is if formula_75, for each formula_76, guess and check to see if formula_67 is congruent to formula_78, and then use the Chinese remainder theorem to recover formula_67.\n\nRegev showed the random self-reducibility of the LWE and DLWE problems for arbitrary formula_5 and formula_53. Given samples formula_82 from formula_61, it is easy to see that formula_84 are samples from formula_85.\n\nSo, suppose there was some set formula_86 such that formula_87, and for distributions formula_88, with formula_89, DLWE was easy.\n\nThen there would be some distinguisher formula_90, who, given samples formula_91, could tell whether they were uniformly random or from formula_88. If we need to distinguish uniformly random samples from formula_61, where formula_50 is chosen uniformly at random from formula_27, we could simply try different values formula_96 sampled uniformly at random from formula_27, calculate formula_84 and feed these samples to formula_90. Since formula_100 comprises a large fraction of formula_27, with high probability, if we choose a polynomial number of values for formula_102, we will find one such that formula_103, and formula_90 will successfully distinguish the samples.\n\nThus, no such formula_100 can exist, meaning LWE and DLWE are (up to a polynomial factor) as hard in the average case as they are in the worst case.\n\nFor a \"n\"-dimensional lattice formula_106, let \"smoothing parameter\" formula_107 denote the smallest formula_108 such that formula_109 where formula_110 is the dual of formula_106 and formula_41 is extended to sets by summing over function values at each element in the set. Let formula_113 denote the discrete Gaussian distribution on formula_106 of width formula_71 for a lattice formula_106 and real formula_117. The probability of each formula_118 is proportional to formula_119.\n\nThe \"discrete Gaussian sampling problem\"(DGS) is defined as follows: An instance of formula_120 is given by an formula_1-dimensional lattice formula_106 and a number formula_123. The goal is to output a sample from formula_113. Regev shows that there is a reduction from formula_125 to formula_126 for any function formula_127.\n\nRegev then shows that there exists an efficient quantum algorithm for formula_128 given access to an oracle for formula_45 for integer formula_5 and formula_131 such that formula_132. This implies the hardness for LWE. Although the proof of this assertion works for any formula_5, for creating a cryptosystem, the formula_5 has to be polynomial in formula_1.\n\nPeikert proves that there is a probabilistic polynomial time reduction from the formula_136 problem in the worst case to solving formula_45 using formula_138 samples for parameters formula_131, formula_140, formula_141 and formula_142.\n\nThe LWE problem serves as a versatile problem used in construction of several cryptosystems. In 2005, Regev showed that the decision version of LWE is hard assuming quantum hardness of the lattice problems formula_143 (for formula_144 as above) and formula_145 with formula_146). In 2009, Peikert proved a similar result assuming only the classical hardness of the related problem formula_147. The disadvantage of Peikert's result is that it bases itself on a non-standard version of an easier (when compared to SIVP) problem GapSVP.\n\nRegev proposed a public-key cryptosystem based on the hardness of the LWE problem. The cryptosystem as well as the proof of security and correctness are completely classical. The system is characterized by formula_148 and a probability distribution formula_53 on formula_18. The setting of the parameters used in proofs of correctness and security is\n- formula_151, usually a prime number between formula_152 and formula_153.\n- formula_154 for an arbitrary constant formula_155\n- formula_156 for formula_157, where formula_158 is a probability distribution obtained by sampling a normal variable with mean formula_159 and standard variation formula_160 and reducing the result modulo formula_161.\n\nThe cryptosystem is then defined by:\n- \"Private key\": Private key is an formula_162 chosen uniformly at random.\n- \"Public key\": Choose formula_163 vectors formula_164 uniformly and independently. Choose error offsets formula_165 independently according to formula_53. The public key consists of formula_167\n- \"Encryption\": The encryption of a bit formula_168 is done by choosing a random subset formula_169 of formula_170 and then defining formula_171 as\n- \"Decryption\": The decryption of formula_173 is formula_159 if formula_175 is closer to formula_159 than to formula_177, and formula_161 otherwise.\n\nThe proof of correctness follows from choice of parameters and some probability analysis. The proof of security is by reduction to the decision version of LWE: an algorithm for distinguishing between encryptions (with above parameters) of formula_159 and formula_161 can be used to distinguish between formula_181 and the uniform distribution over formula_182\n\nPeikert proposed a system that is secure even against any chosen-ciphertext attack.\n\nThe idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper appeared in 2012 after a provisional patent application was filed in 2012.\n\nThe security of the protocol is proven based on the hardness of solving the LWE problem. In 2014, Peikert presented a key-transport scheme following the same basic idea of Ding's, where the new idea of sending an additional 1-bit signal for rounding in Ding's construction is also used. The \"new hope\" implementation selected for Google's post-quantum experiment, uses Peikert's scheme with variation in the error distribution.\n\n", "related": "\n- Post-quantum cryptography\n- Lattice-based cryptography\n- Ring learning with errors key exchange\n- Short integer solution (SIS) problem\n"}
{"id": "22795783", "url": "https://en.wikipedia.org/wiki?curid=22795783", "title": "CIML community portal", "text": "CIML community portal\n\nThe computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site.\n\nThe CIML community portal was created to facilitate an online virtual scientific community wherein anyone interested in CIML can share research, obtain resources, or simply learn more. The effort is currently led by Jacek Zurada (principal investigator), with Rammohan Ragade and Janusz Wojtusiak, aided by a team of 25 volunteer researchers from 13 different countries.\n\nThe ultimate goal of the CIML community portal is to accommodate and cater to a broad range of users, including experts, students, the public, and outside researchers interested in using CIML methods and software tools. Each community member and user will be guided through the portal resources and tools based on their respective CIML experience (e.g. expert, student, outside researcher) and goals (e.g. collaboration, education). A preliminary version of the community's portal, with limited capabilities, is now operational and available for users. All electronic resources on the portal are peer-reviewed to ensure high quality and cite-ability for literature.\n\n- Jacek M. Zurada, Janusz Wojtusiak, Fahmida Chowdhury, James E. Gentle, Cedric J. Jeannot, and Maciej A. Mazurowski, Computational Intelligence Virtual Community: Framework and Implementation Issues, Proceedings of the IEEE World Congress on Computational Intelligence, Hong Kong, June 1–6, 2008.\n- Jacek M. Zurada, Janusz Wojtusiak, Maciej A. Mazurowski, Devendra Mehta, Khalid Moidu, Steve Margolis, Toward Multidisciplinary Collaboration in the CIML Virtual Community, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 62–66\n- Chris Boyle, Artur Abdullin, Rammohan Ragade, Maciej A. Mazurowski, Janusz Wojtusiak, Jacek M. Zurada, Workflow considerations in the emerging CI-ML virtual organization, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 67–70\n\n", "related": "\n- Artificial Intelligence\n- Computational Intelligence\n- Machine Learning\n- National Science Foundation\n"}
{"id": "25050663", "url": "https://en.wikipedia.org/wiki?curid=25050663", "title": "Learning to rank", "text": "Learning to rank\n\nLearning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. \"relevant\" or \"not relevant\") for each item. The ranking model purposes to rank, i.e. producing a permutation of items in new, unseen lists in a similar way to rankings in the training data.\n\nRanking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising.\n\nA possible architecture of a machine-learned search engine is shown in the accompanying figure.\n\nTraining data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human \"assessors\" (or \"raters\", as Google calls them),\n\nwho check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. Alternatively, training data may be derived automatically by analyzing \"clickthrough logs\" (i.e. search results which got clicks from users), \"query chains\", or such search engines' features as Google's SearchWiki.\n\nTraining data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries.\n\nTypically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, boolean model, weighted AND, or BM25. This phase is called \"top-formula_1 document retrieval\" and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes. In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.\n\nLearning to rank algorithms have been applied in areas other than information retrieval:\n- In machine translation for ranking a set of hypothesized translations;\n- In computational biology for ranking candidate 3-D structures in protein structure prediction problem.\n- In recommender systems for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.\n- In software engineering, learning-to-rank methods have been used for fault localization.\n\nFor the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called \"feature vectors\". Such an approach is sometimes called \"bag of features\" and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents.\n\nComponents of such vectors are called \"features\", \"factors\" or \"ranking signals\". They may be divided into three groups (features from document retrieval are shown as examples):\n- \"Query-independent\" or \"static\" features — those features, which depend only on the document, but not on the query. For example, PageRank or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's \"static quality score\" (or \"static rank\"), which is often used to speed up search query evaluation.\n- \"Query-dependent\" or \"dynamic\" features — those features, which depend both on the contents of the document and the query, such as TF-IDF score or other non-machine-learned ranking functions.\n- \"Query level features\" or \"query features\", which depend only on the query. For example, the number of words in a query. \"Further information: query level feature\"\n\nSome examples of features, which were used in the well-known LETOR dataset:\n- TF, TF-IDF, BM25, and language modeling scores of document's zones (title, body, anchors text, URL) for a given query;\n- Lengths and IDF sums of document's zones;\n- Document's PageRank, HITS ranks and their variants.\n\nSelecting and designing good features is an important area in machine learning, which is called feature engineering.\n\nThere are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.\n\nExamples of ranking quality measures:\n- Mean average precision (MAP);\n- DCG and NDCG;\n- Precision@\"n\", NDCG@\"n\", where \"@\"n\"\" denotes that the metrics are evaluated only on top \"n\" documents;\n- Mean reciprocal rank;\n- Kendall's tau;\n- Spearman's rho.\n\nDCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. Other metrics such as MAP, MRR and precision, are defined only for binary judgments.\n\nRecently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:\n- Expected reciprocal rank (ERR);\n- Yandex's pfound.\nBoth of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.\n\nTie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his paper \"Learning to Rank for Information Retrieval\". He categorized them into three groups by their input representation and loss function: the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets.\n\nIn this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.\n\nA number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values.\n\nIn this case, the learning-to-rank problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents. The goal is to minimize the average number of inversions in ranking.\n\nThese algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.\n\nA partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:\n\nNote: as most supervised learning algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.\n\nNorbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation; a specific variant of this approach (using polynomial regression) had been published by him three years earlier. Bill Cooper proposed logistic regression for the same purpose in 1992 and used it with his Berkeley research group to train a successful ranking function for TREC. Manning et al. suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.\n\nSeveral conferences, such as NIPS, SIGIR and ICML had workshops devoted to the learning-to-rank problem since mid-2000s (decade).\n\nCommercial web search engines began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.\n\nBing's search is said to be powered by RankNet algorithm, which was invented at Microsoft Research in 2005.\n\nIn November 2009 a Russian search engine Yandex announced that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees. Recently they have also sponsored a machine-learned ranking competition \"Internet Mathematics 2009\" based on their own search engine's production data. Yahoo has announced a similar competition in 2010.\n\nAs of 2008, Google's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking. Cuil's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models \"learn what people say they like, not what people actually like\".\n\nIn January 2017 the technology was included in the open source search engine Apache Solr™, thus making machine learned search rank widely accessible also for enterprise search.\n\n- Competitions and public datasets\n- LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval\n- Yandex's Internet Mathematics 2009\n- Yahoo! Learning to Rank Challenge\n- Microsoft Learning to Rank Datasets\n\n- Open Source code\n- Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011\n- C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking\n- C++ and Python tools for using the SVM-Rank algorithm\n- Java implementation in the Apache Solr search engine\n", "related": "NONE"}
{"id": "960361", "url": "https://en.wikipedia.org/wiki?curid=960361", "title": "Transduction (machine learning)", "text": "Transduction (machine learning)\n\nIn logic, statistical inference, and supervised learning,\ntransduction or transductive inference is reasoning from\nobserved, specific (training) cases to specific (test) cases. In contrast,\ninduction is reasoning from observed training cases\nto general rules, which are then applied to the test cases. The distinction is\nmost interesting in cases where the predictions of the transductive model are\nnot achievable by any inductive model. Note that this is caused by transductive\ninference on different test sets producing mutually inconsistent predictions.\n\nTransduction was introduced by Vladimir Vapnik in the 1990s, motivated by\nhis view that transduction is preferable to induction since, according to him, induction requires\nsolving a more general problem (inferring a function) before solving a more\nspecific problem (computing outputs for new cases): \"When solving a problem of\ninterest, do not solve a more general problem as an intermediate step. Try to\nget the answer that you really need but not a more general one.\" A similar\nobservation had been made earlier by Bertrand Russell:\n\"we shall reach the conclusion that Socrates is mortal with a greater approach to \ncertainty if we make our argument purely inductive than if we go by way of 'all men are mortal' and then use \ndeduction\" (Russell 1912, chap VII).\n\nAn example of learning which is not inductive would be in the case of binary\nclassification, where the inputs tend to cluster in two groups. A large set of\ntest inputs may help in finding the clusters, thus providing useful information\nabout the classification labels. The same predictions would not be obtainable\nfrom a model which induces a function based only on the training cases. Some\npeople may call this an example of the closely related semi-supervised learning, since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).\n\nA third possible motivation which leads to transduction arises through the need\nto approximate. If exact inference is computationally prohibitive, one may at\nleast try to make sure that the approximations are good at the test inputs. In\nthis case, the test inputs could come from an arbitrary distribution (not\nnecessarily related to the distribution of the training inputs), which wouldn't\nbe allowed in semi-supervised learning. An example of an algorithm falling in\nthis category is the Bayesian Committee Machine (BCM).\n\nThe following example problem contrasts some of the unique properties of transduction against induction.\n\nA collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.\n\nThe inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to build a model that captures the structure of this data. For example, if a nearest-neighbor algorithm is used, then the points near the middle will be labeled \"A\" or \"C\", even though it is apparent that they belong to the same cluster as the point labeled \"B\".\n\nTransduction has the advantage of being able to consider all of the points, not just the labeled points, while performing the labeling task. In this case, transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong. The points in the middle, therefore, would most likely be labeled \"B\", because they are packed very close to that cluster.\n\nAn advantage of transduction is that it may be able to make better predictions with fewer labeled points, because it uses the natural breaks found in the unlabeled points. One disadvantage of transduction is that it builds no predictive model. If a previously unknown point is added to the set, the entire transductive algorithm would need to be repeated with all of the points in order to predict a label. This can be computationally expensive if the data is made available incrementally in a stream. Further, this might cause the predictions of some of the old points to change (which may be good or bad, depending on the application). A supervised learning algorithm, on the other hand, can label new points instantly, with very little computational cost.\n\nTransduction algorithms can be broadly divided into two categories: those that seek to assign discrete labels to unlabeled points, and those that seek to regress continuous labels for unlabeled points. Algorithms that seek to predict discrete labels tend to be derived by adding partial supervision to a clustering algorithm. These can be further subdivided into two categories: those that cluster by partitioning, and those that cluster by agglomerating. Algorithms that seek to predict continuous labels tend to be derived by adding partial supervision to a manifold learning algorithm.\n\nPartitioning transduction can be thought of as top-down transduction. It is a semi-supervised extension of partition-based clustering. It is typically performed as follows:\n\nOf course, any reasonable partitioning technique could be used with this algorithm. Max flow min cut partitioning schemes are very popular for this purpose.\n\nAgglomerative transduction can be thought of as bottom-up transduction. It is a semi-supervised extension of agglomerative clustering. It is typically performed as follows:\n\nManifold-learning-based transduction is still a very young field of research.\n\n", "related": "\n- Epilogism\n\n- V. N. Vapnik. \"Statistical learning theory\". New York: Wiley, 1998. \"(See pages 339-371)\"\n- V. Tresp. \"A Bayesian committee machine\", Neural Computation, 12, 2000, pdf.\n- B. Russell. \"The Problems of Philosophy\", Home University Library, 1912. .\n\n- A Gammerman, V. Vovk, V. Vapnik (1998). \"Learning by Transduction.\" An early explanation of transductive learning.\n- \"A Discussion of Semi-Supervised Learning and Transduction,\" Chapter 25 of \"Semi-Supervised Learning,\" Olivier Chapelle, Bernhard Schölkopf and Alexander Zien, eds. (2006). MIT Press. A discussion of the difference between SSL and transduction.\n- Waffles is an open source C++ library of machine learning algorithms, including transduction algorithms, also Waffles.\n- SVMlight is a general purpose SVM package that includes the transductive SVM option.\n"}
{"id": "416612", "url": "https://en.wikipedia.org/wiki?curid=416612", "title": "Cross-validation (statistics)", "text": "Cross-validation (statistics)\n\nCross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of \"known data\" on which training is run (\"training dataset\"), and a dataset of \"unknown data\" (or \"first seen\" data) against which the model is tested (called the validation dataset or \"testing set\"). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).\n\nOne round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the \"training set\"), and validating the analysis on the other subset (called the \"validation set\" or \"testing set\"). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.\n\nIn summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.\n\nSuppose we have a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of validation data from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.\n\nIn linear regression we have real \"response values\" \"y\", ..., \"y\", and \"n\" \"p\"-dimensional vector \"covariates\" x, ..., x. The components of the vector x are denoted \"x\", ..., \"x\". If we use least squares to fit a function in the form of a hyperplane \"y\" = \"a\" + βx to the data (x, \"y\"), we could then assess the fit using the mean squared error (MSE). The MSE for given estimated parameter values \"a\" and β on the training set (x, \"y\") is\n\nIf the model is correctly specified, it can be shown under mild assumptions that the expected value of the MSE for the training set is (\"n\" − \"p\" − 1)/(\"n\" + \"p\" + 1) < 1 times the expected value of the MSE for the validation set (the expected value is taken over the distribution of training sets). Thus if we fit the model and compute the MSE on the training set, we will get an optimistically biased assessment of how well the model will fit an independent data set. This biased estimate is called the \"in-sample\" estimate of the fit, whereas the cross-validation estimate is an \"out-of-sample\" estimate.\n\nSince in linear regression it is possible to directly compute the factor (\"n\" − \"p\" − 1)/(\"n\" + \"p\" + 1) by which the training MSE underestimates the validation MSE under the assumption that the model specification is valid, cross-validation can be used for checking whether the model has been overfitted, in which case the MSE in the validation set will substantially exceed its anticipated value. (Cross-validation in the context of linear regression is also useful in that it can be used to select an optimally regularized cost function).\nIn most other regression procedures (e.g. logistic regression), there is no simple formula to compute the expected out-of-sample fit. Cross-validation is, thus, a generally applicable way to predict the performance of a model on unavailable data using numerical computation in place of theoretical analysis.\n\nTwo types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation.\n\nExhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.\n\nLeave-\"p\"-out cross-validation (LpO CV) involves using \"p\" observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of \"p\" observations and a training set.\n\nLpO cross-validation requires training and validating the model formula_2 times, where \"n\" is the number of observations in the original sample, and where formula_2 is the binomial coefficient. For \"p\" > 1 and for even moderately large \"n\", LpO CV can become computationally infeasible. For example, with \"n\" = 100 and \"p\" = 30 = 30 percent of 100formula_4\n\nA variant of LpO cross-validation with p=2 known as leave-pair-out cross-validation has been recommended as a nearly unbiased method for estimating the area under ROC curve of binary classifiers.\n\nLeave-\"one\"-out cross-validation (LOOCV) is a particular case of leave-\"p\"-out cross-validation with \"p\" = 1.The process looks similar to jackknife; however, with cross-validation one computes a statistic on the left-out sample(s), while with jackknifing one computes a statistic from the kept samples only.\n\nLOO cross-validation requires less computation time than LpO cross-validation because there are only formula_5 passes rather than formula_2. However, formula_7 passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate.\n\nPseudo-Code-Algorithm:\n\nInput:\n\nOutput:\n\nSteps:\n\nNon-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-\"p\"-out cross-validation.\n\nIn \"k\"-fold cross-validation, the original sample is randomly partitioned into \"k\" equal sized subsamples. Of the \"k\" subsamples, a single subsample is retained as the validation data for testing the model, and the remaining \"k\" − 1 subsamples are used as training data. The cross-validation process is then repeated \"k\" times, with each of the \"k\" subsamples used exactly once as the validation data. The \"k\" results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general \"k\" remains an unfixed parameter.\n\nFor example, setting \"k\" = \"2\" results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets \"d\" and \"d\", so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on \"d\" and validate on \"d\", followed by training on \"d\" and validating on \"d\".\n\nWhen \"k\" = \"n\" (the number of observations), \"k\"-fold cross-validation is equivalent to leave-one-out cross-validation.\n\nIn \"stratified\" \"k\"-fold cross-validation, the partitions are selected so that the mean response value is approximately equal in all the partitions. In the case of binary classification, this means that each partition contains roughly the same proportions of the two types of class labels.\n\nIn \"repeated\" cross-validation the data is randomly split into \"k\" partitions several times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice.\n\nIn the holdout method, we randomly assign data points to two sets \"d\" and \"d\", usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train (build a model) on \"d\" and test (evaluate its performance) on \"d\".\n\nIn typical cross-validation, results of multiple runs of model-testing are averaged together; in contrast, the holdout method, in isolation, involves a single run. It should be used with caution because without such averaging of multiple runs, one may achieve highly misleading results. One's indicator of predictive accuracy (F*) will tend to be unstable since it will not be smoothed out by multiple iterations (see below). Similarly, indicators of the specific role played by various predictor variables (e.g., values of regression coefficients) will tend to be unstable.\n\nWhile the holdout method can be framed as \"the simplest kind of cross-validation\", many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation.\n\nThis method, also known as Monte Carlo cross-validation, creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over \"k\"-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (i.e., the number of partitions). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.\n\nAs the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation.\n\nIn a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data.\n\nWhen cross-validation is used simultaneously for selection of the best set of hyperparameters and for error estimation (and assessment of generalization capacity), a nested cross-validation is required. Many variants exist. At least two variants can be distinguished:\n\nThis is a truly nested variant (for instance used by ), which contains an outer loop of \"k\" folds and an inner loop of \"l\" folds. The total data set is split in \"k\" sets. One by one, a set is selected as (outer) test set and the \"k-1\" other sets are combined into the corresponding outer training set. This is repeated for each of the k sets. Each outer training set is further sub-divided into \"l\" sets. One by one, a set is selected as inner test (validation) set and the \"l-1\" other sets are combined into the corresponding inner training set. This is repeated for each of the l sets. The inner training sets are used to fit model parameters, while the outer test set is used as a validation set to provides an unbiased evaluation of the model fit. Typically, this is repeated for many different hyperparameters (or even different model types) and the validation set is used to determine the best hyperparameter set (and model type) for this inner training set. After this, a new model is fit on the entire outer training set, using the best set of hyperparameters from the inner cross-validation. The performance of this model is then evaluated using the outer test set.\n\nThis is a type of k*l-fold cross-validation when l=k-1. A single k-fold cross-validation is used with both a validation and test set. The total data set is split in \"k\" sets. One by one, a set is selected as test set. Then, one by one, one of the remaining sets is used as a validation sets and the other \"k-2\" sets are used as training sets until all possible combinations have been evaluated. Similar to the k*l-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. Finally, for the selected parameter set, the test set is used to evaluate the model with the best parameter set. Here, two variants are possible: either evaluating the model that was trained on the training set or evaluating a new model that was fit on the combination of the train and the validation set.\n\nThe goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures like positive predictive value could also be used. When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors.\n\nWhen users apply cross-validation to select a good configuration formula_8, then they might want to balance the cross-validated choice with their own estimate of the configuration. In this way, they can attempt to counter the volatility of cross-validation when the sample size is small and include relevant information from previous research. In a forecasting combination exercise, for instance, cross-validation can be applied to estimate the weights that are assigned to each forecast. Since a simple equal-weighted forecast is difficult to beat, a penalty can be added for deviating from equal weights. Or, if cross-validation is applied to assign individual weights to observations, then one can penalize deviations from equal weights to avoid wasting potentially relevant information. Hoornweg (2018) shows how a tuning parameter formula_9 can be defined so that a user can intuitively balance between the accuracy of cross-validation and the simplicity of sticking to a reference parameter formula_10 that is defined by the user.\n\nIf formula_11 denotes the formula_12 candidate configuration that might be selected, then the loss function that is to be minimized can be defined as\nRelative accuracy can be quantified as formula_14, so that the mean squared error of a candidate formula_11 is made relative to that of a user-specified formula_10. The relative simplicity term measures the amount that formula_11 deviates from formula_10 relative to the maximum amount of deviation from formula_10. Accordingly, relative simplicity can be specified as formula_20, where formula_21 corresponds to the formula_8 value with the highest permissible deviation from formula_10. With formula_24, the user determines how high the influence of the reference parameter is relative to cross-validation.\n\nOne can add relative simplicity terms for multiple configurations formula_25 by specifying the loss function as\nHoornweg (2018) shows that a loss function with such an accuracy-simplicity tradeoff can also be used to intuitively define shrinkage estimators like the (adaptive) lasso and Bayesian / ridge regression. Click on the lasso for an example.\n\nSuppose we choose a measure of fit \"F\", and use cross-validation to produce an estimate \"F\" of the expected fit \"EF\" of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for \"F\" will vary. The statistical properties of \"F\" result from this variation.\n\nThe cross-validation estimator \"F\" is very nearly unbiased for \"EF\" . The reason that it is slightly biased is that the training set in cross-validation is slightly smaller than the actual data set (e.g. for LOOCV the training set size is \"n\" − 1 when there are \"n\" observed cases). In nearly all situations, the effect of this bias will be conservative in that the estimated fit will be slightly biased in the direction suggesting a poorer fit. In practice, this bias is rarely a concern.\n\nThe variance of \"F\" can be large. For this reason, if two statistical procedures are compared based on the results of cross-validation, the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of \"EF\"). Some progress has been made on constructing confidence intervals around cross-validation estimates, but this is considered a difficult problem.\n\nMost forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available. In particular, the prediction method can be a \"black box\" – there is no need to have access to the internals of its implementation. If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly. In some cases such as least squares and kernel regression, cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast \"updating rules\" such as the Sherman–Morrison formula. However one must be careful to preserve the \"total blinding\" of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the \"prediction residual error sum of squares\" (PRESS).\n\nCross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled.\n\nIn many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is \"non-stationary\"). Both of these can introduce systematic differences between the training and validation sets. For example, if a model for predicting stock values is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year. If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance.\n\nIn many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor. New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity. As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another. When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation.\n\nThe reason for the success of the swapped sampling is a built-in control for human biases in model building. In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused:\n\n- By performing an initial analysis to identify the most informative features using the entire data set – if feature selection or model tuning is required by the modeling procedure, this must be repeated on every training set. Otherwise, predictions will certainly be upwardly biased. If cross-validation is used to decide which features to use, an \"inner cross-validation\" to carry out the feature selection on every training set must be performed.\n- By allowing some of the training data to also be included in the test set – this can happen due to \"twinning\" in the data set, whereby some exactly identical or nearly identical samples are present in the data set. To some extent twinning always takes place even in perfectly independent training and validation samples. This is because some of the training sample observations will have nearly identical values of predictors as validation sample observations. And some of these will correlate with a target at better than chance levels in the same direction in both training and validation when they are actually driven by confounded predictors with poor external validity. If such a cross-validated model is selected from a \"k\"-fold set, human confirmation bias will be at work and determine that such a model has been validated. This is why traditional cross-validation needs to be supplemented with controls for human bias and confounded model specification like swap sampling and prospective studies.\n\nSince the order of the data is important, cross-validation might be problematic for time-series models. A more appropriate approach might be to use rolling cross-validation.\n\nHowever, if performance is described by a single summary statistic, it is possible that the approach described by Politis and Romano as a will work. The statistic of the bootstrap needs to accept an interval of the time series and return the summary statistic on it. The call to the stationary bootstrap needs to specify an appropriate mean interval length.\n\nCross-validation can be used to compare the performances of different predictive modeling procedures. For example, suppose we are interested in optical character recognition, and we are considering using either support vector machines (SVM) or \"k\"-nearest neighbors (KNN) to predict the true character from an image of a handwritten character. Using cross-validation, we could objectively compare these two methods in terms of their respective fractions of misclassified characters. If we simply compared the methods based on their in-sample error rates, the KNN method would likely appear to perform better, since it is more flexible and hence more prone to overfitting compared to the SVM method.\n\nCross-validation can also be used in \"variable selection\". Suppose we are using the expression levels of 20 proteins to predict whether a cancer patient will respond to a drug. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative.\n\nA recent development in medical statistics is its use in meta-analysis. It forms the basis of the validation statistic, Vn which is used to test the statistical validity of meta-analysis summary estimates. It has also been used in a more conventional sense in meta-analysis to estimate the likely prediction error of meta-analysis results.\n\n", "related": "\n- Boosting (machine learning)\n- Bootstrap aggregating (bagging)\n- Bootstrapping (statistics)\n- Leakage (machine learning)\n- Model selection\n- Resampling (statistics)\n- Stability (learning theory)\n- Validity (statistics)\n"}
{"id": "2291650", "url": "https://en.wikipedia.org/wiki?curid=2291650", "title": "Predictive learning", "text": "Predictive learning\n\nPredictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by Piaget's account of how children construct knowledge of the world by interacting with it. Gary Drescher's book 'Made-up Minds' was seminal for the area.\n\nAnother more recent predictive learning theory is Jeff Hawkins' memory-prediction framework, which is laid out in his On Intelligence.\n", "related": "NONE"}
{"id": "1455062", "url": "https://en.wikipedia.org/wiki?curid=1455062", "title": "Empirical risk minimization", "text": "Empirical risk minimization\n\nEmpirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true \"risk\") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the \"empirical\" risk). \n\nConsider the following situation, which is a general setting of many supervised learning problems. We have two spaces of objects formula_1 and formula_2 and would like to learn a function formula_3 (often called \"hypothesis\") which outputs an object formula_4, given formula_5. To do so, we have at our disposal a \"training set\" of formula_6 examples formula_7 where formula_8 is an input and formula_9 is the corresponding response that we wish to get from formula_10.\n\nTo put it more formally, we assume that there is a joint probability distribution formula_11 over formula_1 and formula_2, and that the training set consists of formula_6 instances formula_7 drawn i.i.d. from formula_11. Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because formula_17 is not a deterministic function of but rather a random variable with conditional distribution formula_18 for a fixed formula_19.\n\nWe also assume that we are given a non-negative real-valued loss function formula_20 which measures how different the prediction formula_21 of a hypothesis is from the true outcome formula_22 The risk associated with hypothesis formula_23 is then defined as the expectation of the loss function:\n\nA loss function commonly used in theory is the 0-1 loss function: formula_25.\n\nThe ultimate goal of a learning algorithm is to find a hypothesis formula_26 among a fixed class of functions formula_27 for which the risk formula_28 is minimal:\n\nIn general, the risk formula_28 cannot be computed because the distribution formula_11 is unknown to the learning algorithm (this situation is referred to as agnostic learning). However, we can compute an approximation, called \"empirical risk\", by averaging the loss function on the training set:\n\nThe empirical risk minimization principle states that the learning algorithm should choose a hypothesis formula_33 which minimizes the empirical risk:\nThus the learning algorithm defined by the ERM principle consists in solving the above optimization problem.\n\nEmpirical risk minimization for a classification problem with a 0-1 loss function is known to be an NP-hard problem even for such a relatively simple class of functions as linear classifiers. Though, it can be solved efficiently when the minimal empirical risk is zero, i.e. data is linearly separable.\n\nIn practice, machine learning algorithms cope with that either by employing a convex approximation to the 0-1 loss function (like hinge loss for SVM), which is easier to optimize, or by imposing assumptions on the distribution formula_11 (and thus stop being agnostic learning algorithms to which the above result applies).\n\n", "related": "\n- Maximum likelihood estimation\n- M-estimator\n"}
{"id": "24825162", "url": "https://en.wikipedia.org/wiki?curid=24825162", "title": "Product of experts", "text": "Product of experts\n\nProduct of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions.\nIt was proposed by Geoff Hinton, along with an algorithm for training the parameters of such a system.\n\nThe core idea is to combine several probability distributions (\"experts\") by multiplying their density functions—making the PoE classification similar to an \"and\" operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem.\n\nThis is related to (but quite different from) a mixture model, where several probability distributions are combined via an \"or\" operation, which is a weighted sum of their density functions.\n", "related": "NONE"}
{"id": "233497", "url": "https://en.wikipedia.org/wiki?curid=233497", "title": "Unsupervised learning", "text": "Unsupervised learning\n\nUnsupervised learning is a type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels and with a minimum of human supervision. In contrast to supervised learning that usually makes use of human-labeled data, unsupervised learning, also known as self-organization allows for modeling of probability densities over inputs. It forms one of the three main categories of machine learning, along with supervised and reinforcement learning. Semi-supervised learning, a related variant, makes use of supervised and unsupervised techniques.\n\nTwo of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n\nA central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It could be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution formula_1 conditioned on the label formula_2 of input data; unsupervised learning intends to infer an a priori probability distribution formula_3.\n\nGenerative adversarial networks can also be used with supervised learning, though they can also be applied to unsupervised and reinforcement techniques.\n\nSome of the most common algorithms used in unsupervised learning include: 1) Clustering (2) Anomaly detection (3) Neural Networks (4) Approaches for learning latent variable models.\nEach approach again uses several methods as follow:\n- Clustering\n- hierarchical clustering,\n- k-means\n- mixture models\n- DBSCAN\n- OPTICS algorithm\n- Anomaly detection\n- Local Outlier Factor\n- Neural Networks\n- Autoencoders\n- Deep Belief Nets\n- Hebbian Learning\n- Generative adversarial networks\n- Self-organizing map\n- Approaches for learning latent variable models such as\n- Expectation–maximization algorithm (EM)\n- Method of moments\n- Blind signal separation techniques\n-  Principal component analysis\n-  Independent component analysis\n-  Non-negative matrix factorization\n-  Singular value decomposition\n\nThe classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n\nAmong neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.\n\nOne of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n\nIn particular, the method of moments is shown to be effective in learning the parameters of latent variable models.\nLatent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.\n\nThe Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\n\n", "related": "\n- Automated machine learning\n- Cluster analysis\n- Anomaly detection\n- Expectation–maximization algorithm\n- Generative topographic map\n- Meta-learning (computer science)\n- Multivariate analysis\n- Radial basis function network\n- Weak supervision\n\n- (This book focuses on unsupervised learning in neural networks)\n"}
{"id": "4615464", "url": "https://en.wikipedia.org/wiki?curid=4615464", "title": "Meta learning (computer science)", "text": "Meta learning (computer science)\n\nMeta learning\nis a subfield of machine learning where automatic learning algorithms are applied on metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.\n\nFlexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.\n\nBy using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987) and Yoshua Bengio et al.'s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta learning system using Genetic Programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.\n\nA proposed definition for a meta learning system combines three requirements:\n- The system must include a learning subsystem.\n- Experience is gained by exploiting meta knowledge extracted\n- in a previous learning episode on a single dataset, or\n- from different domains.\n- Learning bias must be chosen dynamically.\n\"Bias\" refers to the assumptions that influence the choice of explanatory hypotheses and not the notion of bias represented in the bias-variance dilemma. Meta learning is concerned with two aspects of learning bias.\n- Declarative bias specifies the representation of the space of hypotheses, and affects the size of the search space (e.g., represent hypotheses using linear functions only).\n- Procedural bias imposes constraints on the ordering of the inductive hypotheses (e.g., preferring smaller hypotheses).\n\nThere are three common approaches: 1)using (cyclic) networks with external or internal memory (model-based); 2)learning effective distance metrics (metrics-based); 3)explicitly optimizing model parameters for fast learning (optimization-based).\n\nModel-based meta-learning models updates its parameters rapidly with a few training steps, which can be achieved by its internal architecture or controlled by another meta-learner model.\n\nThe model is known as MANN short for Memory-Augmented Neural Networks, which is expected to encode new information fast and thus to adapt to new tasks after only a few samples, it fits well for meta-learning.\n\nMeta Networks (MetaNet) learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization.\n\nThe core idea in metric-based meta-learning is similar to nearest neighbors algorithms, which weight is generated by a kernel function. It aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving.\n\nSiamese neural network is composed of two twin networks whose output is jointly trained. There is a function above to learn the relationship between input data sample pairs. The two networks are the same, sharing the same weight and network parameters.\n\nMatching Networks learn a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types.\n\nThe Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting.\n\nPrototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve satisfied results.\n\nWhat optimization-based approach meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples.\n\nLSTM-based meta-learner is to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training.\n\nMAML, short for Model-Agnostic Meta-Learning, is a fairly general optimization algorithm, compatible with any model that learns through gradient descent.\n\nReptile is a remarkably simple meta-learning optimization algorithm, given that both rely on meta-optimization through gradient descent and both are model-agnostic.\n\nSome approaches which have been viewed as instances of meta learning:\n\n- Recurrent neural networks (RNNs) are universal computers. In 1993, Jürgen Schmidhuber showed how \"self-referential\" RNNs can in principle learn by backpropagation to run their own weight change algorithm, which may be quite different from backpropagation. In 2001, Sepp Hochreiter & A.S. Younger & P.R. Conwell built a successful supervised meta learner based on Long short-term memory RNNs. It learned through backpropagation a learning algorithm for quadratic functions that is much faster than backpropagation. Researchers at Deepmind (Marcin Andrychowicz et al.) extended this approach to optimization in 2017.\n\n- In the 1990s, Meta Reinforcement Learning or Meta RL was achieved in Schmidhuber's research group through self-modifying policies written in a universal programming language that contains special instructions for changing the policy itself. There is a single lifelong trial. The goal of the RL agent is to maximize reward. It learns to accelerate reward intake by continually improving its own learning algorithm which is part of the \"self-referential\" policy.\n- An extreme type of Meta Reinforcement Learning is embodied by the Gödel machine, a theoretical construct which can inspect and modify any part of its own software which also contains a general theorem prover. It can achieve recursive self-improvement in a provably optimal way.\n- \"Model-Agnostic Meta-Learning\" (MAML) was introduced in 2017 by Chelsea Finn et al. Given a sequence of tasks, the parameters of a given model are trained such that few iterations of gradient descent with few training data from a new task will lead to good generalization performance on that task. MAML \"trains the model to be easy to fine-tune.\" MAML was successfully applied to few-shot image classification benchmarks and to policy gradient-based reinforcement learning.\n- \"Discovering meta-knowledge\" works by inducing knowledge (e.g. rules) that expresses how each learning method will perform on different learning problems. The metadata is formed by characteristics of the data (general, statistical, information-theoretic... ) in the learning problem, and characteristics of the learning algorithm (type, parameter settings, performance measures...). Another learning algorithm then learns how the data characteristics relate to the algorithm characteristics. Given a new learning problem, the data characteristics are measured, and the performance of different learning algorithms are predicted. Hence, one can predict the algorithms best suited for the new problem.\n- \"Stacked generalisation\" works by combining multiple (different) learning algorithms. The metadata is formed by the predictions of those different algorithms. Another learning algorithm learns from this metadata to predict which combinations of algorithms give generally good results. Given a new learning problem, the predictions of the selected set of algorithms are combined (e.g. by (weighted) voting) to provide the final prediction. Since each algorithm is deemed to work on a subset of problems, a combination is hoped to be more flexible and able to make good predictions.\n- \"Boosting\" is related to stacked generalisation, but uses the same algorithm multiple times, where the examples in the training data get different weights over each run. This yields different predictions, each focused on rightly predicting a subset of the data, and combining those predictions leads to better (but more expensive) results.\n- \"Dynamic bias selection\" works by altering the inductive bias of a learning algorithm to match the given problem. This is done by altering key aspects of the learning algorithm, such as the hypothesis representation, heuristic formulae, or parameters. Many different approaches exist.\n- \"Inductive transfer\" studies how the learning process can be improved over time. Metadata consists of knowledge about previous learning episodes and is used to efficiently develop an effective hypothesis for a new task. A related approach is called learning to learn, in which the goal is to use acquired knowledge from one domain to help learning in other domains.\n- Other approaches using metadata to improve automatic learning are learning classifier systems, case-based reasoning and constraint satisfaction.\n- Some initial, theoretical work has been initiated to use \"Applied Behavioral Analysis\" as a foundation for agent-mediated meta-learning about the performances of human learners, and adjust the instructional course of an artificial agent.\n- AutoML such as Google Brain's \"AI building AI\" project, which according to Google briefly exceeded existing ImageNet benchmarks in 2017.\n\n- Metalearning article in Scholarpedia\n- Vilalta R. and Drissi Y. (2002). \"A perspective view and survey of meta-learning\", Artificial Intelligence Review, 18(2), 77—95.\n- Giraud-Carrier, C., & Keller, J. (2002). Dealing with the data flood, J. Meij (ed), chapter Meta-Learning. STT/Beweton, The Hague.\n- Brazdil P., Giraud-Carrier C., Soares C., Vilalta R. (2009) Metalearning: applications to data mining, chapter Metalearning: Concepts and Systems, Springer\n- Video courses about Meta-Learning with step-by-step explanation of MAML, Prototypical Networks, and Relation Networks.\n", "related": "NONE"}
{"id": "2854828", "url": "https://en.wikipedia.org/wiki?curid=2854828", "title": "Multi-armed bandit", "text": "Multi-armed bandit\n\nIn probability theory, the multi-armed bandit problem (sometimes called the \"K\"-<ref name=\"doi10.1023/A:1013689704352\"></ref> or \"N\"-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice. This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.\n\nIn the problem, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff and \"exploration\" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization like a science foundation or a pharmaceutical company. In early versions of the problem, the gambler begins with no initial knowledge about the machines.\n\nHerbert Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in \"some aspects of the sequential design of experiments\". A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward.\n\nThe multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called \"exploration\") and optimize their decisions based on existing knowledge (called \"exploitation\"). The agent attempts to balance these competing tasks in order to maximize their total value over the period of time considered. There are many practical applications of the bandit model, for example:\n\n- clinical trials investigating the effects of different experimental treatments while minimizing patient losses,\n- adaptive routing efforts for minimizing delays in a network,\n- financial portfolio design\n\nIn these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the \"exploitation vs. exploration tradeoff\" in machine learning.\n\nThe model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility.\n\nOriginally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.\n\nThe version of the problem now commonly analyzed was formulated by Herbert Robbins in 1952.\n\nThe multi-armed bandit (short: \"bandit\" or MAB) can be seen as a set of real distributions formula_1, each distribution being associated with the rewards delivered by one of the formula_2 levers. Let formula_3 be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon formula_4 is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state Markov decision process. The regret formula_5 after formula_6 rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards: \n\nformula_7,\n\nwhere formula_8 is the maximal reward mean, formula_9, and formula_10 is the reward in round \"t\".\n\nA \"zero-regret strategy\" is a strategy whose average regret per round formula_11 tends to zero with probability 1 when the number of played rounds tends to infinity. Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.\n\nA common formulation is the \"Binary multi-armed bandit\" or \"Bernoulli multi-armed bandit,\" which issues a reward of one with probability formula_12, and otherwise a reward of zero.\n\nAnother formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalisation called the \"restless bandit problem\", the states of non-played arms can also evolve over time. There has also been discussion of systems where the number of choices (about which arm to play) increases over time.\n\nComputer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite (asymptotic) time horizons for both stochastic and non-stochastic arm payoffs.\n\nA major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate to the population with highest mean) in the work described below.\n\nIn the paper \"Asymptotically efficient adaptive allocation rules\", Lai and Robbins (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family. Then, in Katehakis and Robbins simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and Katehakis in the paper \"Optimal adaptive policies for sequential allocation problems\", where index based policies with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions.\n\nLater in \"Optimal adaptive policies for Markov decision processes\" Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information, where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward, were constructed under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett, Ortner Filippi, Cappé, and Garivier, and Honda and Takemura.\n\nWhen optimal solutions to multi-arm bandit tasks are used to derive the value of animals' choices, the activity of neurons in the amygdala and ventral striatum encodes the values derived from these policies, and can be used to decode when the animals make exploratory versus exploitative choices. Moreover, optimal policies better predict animals' choice behavior than alternative strategies (described below). This suggests that the optimal solutions to multi-arm bandit problems are biologically plausible, despite being computationally demanding. \n\nMany strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below.\n\nSemi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a greedy behavior where the \"best\" lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.\n\n- Epsilon-greedy strategy: The best lever is selected for a proportion formula_13 of the trials, and a lever is selected at random (with uniform probability) for a proportion formula_14. A typical parameter value might be formula_15, but this can vary widely depending on circumstances and predilections.\n- Epsilon-first strategy: A pure exploration phase is followed by a pure exploitation phase. For formula_16 trials in total, the exploration phase occupies formula_17 trials and the exploitation phase formula_18 trials. During the exploration phase, a lever is randomly selected (with uniform probability); during the exploitation phase, the best lever is always selected.\n- Epsilon-decreasing strategy: Similar to the epsilon-greedy strategy, except that the value of formula_14 decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish.\n- Adaptive epsilon-greedy strategy based on value differences (VDBE): Similar to the epsilon-decreasing strategy, except that epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010). High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation); low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a softmax-weighted action selection in case of exploratory actions (Tokic & Palm, 2011).\n- Contextual-Epsilon-greedy strategy: Similar to the epsilon-greedy strategy, except that the value of formula_14 is computed regarding the situation in experiment processes, which lets the algorithm be Context-Aware. It is based on dynamic exploration/exploitation and can adaptively balance the two aspects by deciding which situation is most relevant for exploration or exploitation, resulting in highly explorative behavior when the situation is not critical and highly exploitative behavior at critical situation.\n\nProbability matching strategies reflect the idea that the number of pulls for a given lever should \"match\" its actual probability of being the optimal lever. Probability matching strategies are also known as Thompson sampling or Bayesian Bandits, and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative.\n\nProbability matching strategies also admit solutions to so-called contextual bandit problems.\n\nPricing strategies establish a \"price\" for each lever. For example, as illustrated with the POKER algorithm, the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled.\n\nThese strategies minimize the assignment of any patient to an inferior arm (\"physician's duty\"). In a typical case, they minimize expected successes lost (ESL), that is, the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior. Another version minimizes resources wasted on any inferior, more expensive, treatment.\n\nA particularly useful version of the multi-armed bandit is the contextual multi-armed bandit problem. In this problem, in each iteration an agent has to choose between arms. Before making the choice, the agent sees a d-dimensional feature vector (context vector),\nassociated with the current iteration. The learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play in\nthe current iteration. Over time, the learner's aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors.\n\nMany strategies exist that provide an approximate solution to the contextual bandit problem, and can be put into two broad categories detailed below.\n\n- LinUCB \"(Upper Confidence Bound)\" algorithm: the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors.\n- LinRel (Linear Associative Reinforcement Learning) algorithm: Similar to LinUCB, but utilizes Singular-value decomposition rather than Ridge regression to obtain an estimate of confidence.\n\n- UCBogram algorithm: The nonlinear reward functions are estimated using a piecewise constant estimator called a \"regressogram\" in Nonparametric regression. Then, UCB is employed on each constant piece. Successive refinements of the partition of the context space are scheduled or chosen adaptively.\n- Generalized linear algorithms: The reward distribution follows a generalized linear model, an extension to linear bandits.\n- NeuralBandit algorithm: In this algorithm several neural networks are trained to modelize the value of rewards knowing the context, and it uses a multi-experts approach to choose online the parameters of multi-layer perceptrons.\n- KernelUCB algorithm: a kernelized non-linear version of linearUCB, with efficient implementation and finite-time analysis.\n- Bandit Forest algorithm: a random forest is built and analyzed w.r.t the random forest built knowing the joint distribution of contexts and rewards.\n- Oracle-based algorithm: The algorithm reduces the contextual bandit problem into a series of supervised learning problem, and does not rely on typical realizability assumption on the reward function.\n\nIn practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that considers both the time and budget constraints in a multi-armed bandit setting.\nA. Badanidiyuru et al. first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a formula_21 regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient.\nA simple algorithm with logarithmic regret is proposed in:\n- UCB-ALP algorithm: The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems.\n\nAnother variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration, an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems.\n\nAn example often considered for adversarial bandits is the iterated prisoner's dilemma. In this example, each adversary has two arms to pull. They can either Deny or Confess. Standard stochastic bandit algorithms don't work very well with this iterations. For example, if the opponent cooperates in the first 100 rounds, defects for the next 200, then cooperate in the following 300, etc. Then algorithms such as UCB won't be able to react very quickly to these changes. This is because after a certain point sub-optimal arms are rarely pulled to limit exploration and focus on exploitation. When the environment changes the algorithm is unable to adapt or may not even detect the change.\n\n Parameters: Real formula_22\n\nExp3 chooses an arm at random with probability formula_33 it prefers arms with higher weights (exploit), it chooses with probability γ to uniformly randomly explore. After receiving the rewards the weights are updated. The exponential growth significantly increases the weight of good arms.\n\nThe (external) regret of the Exp3 algorithm is at most\nformula_34\n\n Parameters: Real formula_35\n\nWe follow the arm that we think has the best performance so far adding exponential noise to it to provide exploration.\n\nIn the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable formula_41. In the infinite armed case, introduced by Agarwal (1995), the \"arms\" are a continuous variable in formula_41 dimensions.\n\nGarivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB and Sliding-Window UCB.\n\nAnother work by Burtini et al. introduces a weighted least squares Thompson sampling approach (WLS-TS), which proves beneficial in both the known and unknown non-stationary cases. In the known non-stationary case, the authors in produce an alternative solution, a variant of UCB named Adjusted Upper Confidence Bound (A-UCB) which assumes a stochastic model and provide upper-bounds of the regret.\n\nMany variants of the problem have been proposed in recent years. \n\nThe dueling bandit variant was introduced by Yue et al. (2012) to model the exploration-versus-exploitation tradeoff for relative feedback.\nIn this variant the gambler is allowed to pull two levers at the same time, but they only get a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions.\nThe earliest algorithms for this problem are InterleaveFiltering, Beat-The-Mean.\nThe relative feedback of dueling bandits can also lead to voting paradoxes. A solution is to take the Condorcet winner as a reference.\n\nMore recently, researchers have generalized algorithms from traditional MAB to dueling bandits: Relative Upper Confidence Bounds (RUCB), Relative EXponential weighing (REX3), \nCopeland Confidence Bounds (CCB), Relative Minimum Empirical Divergence (RMED), and Double Thompson Sampling (DTS).\n\nThe collaborative filtering bandits (i.e., COFIBA) was introduced by Li and Karatzoglou and Gentile (SIGIR 2016), where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, they investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings. Their algorithm (COFIBA, pronounced as \"Coffee Bar\") takes into account the collaborative effects that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. They provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. They also provide a regret analysis within a standard linear stochastic noise setting.\n\nThe Combinatorial Multiarmed Bandit (CMAB) problem arises when instead of a single discrete variable to choose from, an agent needs to choose values for a set of variables. Assuming each variable is discrete, the number of possible choices per iteration is exponential in the number of variables. Several CMAB settings have been studied in the literature, from settings where the variables are binary to more general setting where each variable can take an arbitrary set of values.\n\n", "related": "\n- Gittins index – a powerful, general strategy for analyzing bandit problems.\n- Greedy algorithm\n- Optimal stopping\n- Search theory\n- Stochastic scheduling\n\n- .\n- .\n\n\n- MABWiser, open source Python implementation of bandit strategies that supports context-free, parametric and non-parametric contextual policies with built-in parallelization and simulation capability.\n- PyMaBandits, open source implementation of bandit strategies in Python and Matlab.\n- Contextual, open source R package facilitating the simulation and evaluation of both context-free and contextual Multi-Armed Bandit policies.\n- bandit.sourceforge.net Bandit project, open source implementation of bandit strategies.\n- Banditlib, Open-Source implementation of bandit strategies in C++.\n- Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration: The Single-State Case.\n- Tutorial: Introduction to Bandits: Algorithms and Theory. Part1. Part2.\n- Feynman's restaurant problem, a classic example (with known answer) of the exploitation vs. exploration tradeoff.\n- Bandit algorithms vs. A-B testing.\n- S. Bubeck and N. Cesa-Bianchi A Survey on Bandits.\n- A Survey on Contextual Multi-armed Bandits, a survey/tutorial for Contextual Bandits.\n- Blog post on multi-armed bandit strategies, with Python code.\n- Animated, interactive plots illustrating Epsilon-greedy, Thompson sampling, and Upper Confidence Bound exploration/exploitation balancing strategies.\n"}
{"id": "579867", "url": "https://en.wikipedia.org/wiki?curid=579867", "title": "Dimensionality reduction", "text": "Dimensionality reduction\n\nIn statistics, machine learning, and information theory, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration by obtaining a set of principal variables. Approaches can be divided into feature selection and feature extraction.\n\nFeature selection approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the \"filter\" strategy (e.g. information gain), the \"wrapper\" strategy (e.g. search guided by accuracy), and the \"embedded\" strategy (selected features add or are removed while building the model based on prediction errors).\n\nData analysis such as regression or classification can be done in the reduced space more accurately than in the original space.\n\nFeature projection (also called Feature extraction) transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.\n\nThe main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proven on a case-by-case basis as not all systems exhibit this behavior. . The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.\n\nNMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist. such as astronomy. NMF is well known since the multiplicative update rule by Lee & Seung, which has been continuously developed: the inclusion of uncertainties , the consideration of missing data and parallel computation , sequential construction which leads to the stability and linearity of NMF, as well as other updates.\n\nWith a stable component basis during construction, and a linear modeling process, sequential NMF is able to preserve the flux in direct imaging of circumstellar structures in astromony, as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks. In comparison with PCA, NMF does not remove the mean of the matrices which leads to unphysical non-negative fluxes, therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.\n\nPrincipal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled kernel PCA.\n\nOther prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis. These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.\n\nMore recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.\n\nAn alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.\n\nA different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer. The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.\n\nLinear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n\nGDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space. Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.\n\nAutoencoders can be used to learn non-linear dimension reduction functions and codings together with an inverse function from the coding to the original representation.\n\nT-distributed Stochastic Neighbor Embedding (t-SNE) is a non-linear dimensionality reduction technique useful for visualization of high-dimensional datasets.\n\nUniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.\n\nFor high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.\n\nFeature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.\n\nFor very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality sensitive hashing, random projection, \"sketches\" or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.\n\n1. It reduces the time and storage space required.\n2. Removal of multi-collinearity improves the interpretation of the parameters of the machine learning model.\n3. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D.\n4. It avoids the curse of dimensionality.\n\nA dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.\n\n", "related": "\n- Nearest neighbor search\n- MinHash\n- Information gain in decision trees\n- Semidefinite embedding\n- Multifactor dimensionality reduction\n- Multilinear subspace learning\n- Multilinear PCA\n- Random projection\n- Singular value decomposition\n- Latent semantic analysis\n- Semantic mapping\n- Topological data analysis\n- Locality sensitive hashing\n- Sufficient dimension reduction\n- Data transformation (statistics)\n- Weighted correlation network analysis\n- Hyperparameter optimization\n- CUR matrix approximation\n- Envelope model\n- Nonlinear dimensionality reduction\n- Sammon mapping\n- Johnson–Lindenstrauss lemma\n- Local tangent space alignment\n\n- JMLR Special Issue on Variable and Feature Selection\n- ELastic MAPs\n- Locally Linear Embedding\n- A Global Geometric Framework for Nonlinear Dimensionality Reduction\n"}
{"id": "29288159", "url": "https://en.wikipedia.org/wiki?curid=29288159", "title": "Sequence labeling", "text": "Sequence labeling\n\nIn machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the \"globally\" best set of labels for the entire sequence at once.\n\nAs an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described. Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right. For example, the word \"sets\" can be either a noun or verb. In a phrase like \"he sets the books down\", the word \"he\" is unambiguously a pronoun, and \"the\" unambiguously a determiner, and using either of these labels, \"sets\" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are. But in other cases, only one of the adjacent words is similarly helpful. In \"he sets and then knocks over the table\", only the word \"he\" to the left is helpful (cf. \"...picks up the sets and then knocks over...\"). Conversely, in \"... and also sets the table\" only the word \"the\" to the right is helpful (cf. \"... and also sets of books were ...\"). An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left.\n\nMost sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling. Other common models in use are the maximum entropy Markov model and conditional random field.\n\n", "related": "\n- Artificial intelligence\n- Bayesian networks (of which HMMs are an example)\n- Classification (machine learning)\n- Linear dynamical system, which applies to tasks where the \"label\" is actually a real number\n- Machine learning\n- Pattern recognition\n- Sequence mining\n\n- Erdogan H., . \"Sequence labeling: generative and discriminative approaches, hidden Markov models, conditional random fields and structured SVMs,\" ICMLA 2010 tutorial, Bethesda, MD (2010)\n"}
{"id": "871681", "url": "https://en.wikipedia.org/wiki?curid=871681", "title": "Mixture model", "text": "Mixture model\n\nIn statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with \"mixture distributions\" relate to deriving the properties of the overall population from those of the sub-populations, \"mixture models\" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.\n\nSome ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps.\n\nMixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.\n\nA typical finite-dimensional mixture model is a hierarchical model consisting of the following components:\n\n- \"N\" random variables that are observed, each distributed according to a mixture of \"K\" components, with the components belonging to the same parametric family of distributions (e.g., all normal, all Zipfian, etc.) but with different parameters\n- \"N\" random latent variables specifying the identity of the mixture component of each observation, each distributed according to a \"K\"-dimensional categorical distribution\n- A set of \"K\" mixture weights, which are probabilities that sum to 1.\n- A set of \"K\" parameters, each specifying the parameter of the corresponding mixture component. In many cases, each \"parameter\" is actually a set of parameters. For example, if the mixture components are Gaussian distributions, there will be a mean and variance for each component. If the mixture components are categorical distributions (e.g., when each observation is a token from a finite alphabet of size \"V\"), there will be a vector of \"V\" probabilities summing to 1.\n\nIn addition, in a Bayesian setting, the mixture weights and parameters will themselves be random variables, and prior distributions will be placed over the variables. In such a case, the weights are typically viewed as a \"K\"-dimensional random vector drawn from a Dirichlet distribution (the conjugate prior of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.\n\nMathematically, a basic parametric mixture model can be described as follows:\n\nIn a Bayesian setting, all parameters are associated with random variables, as follows:\n\nThis characterization uses \"F\" and \"H\" to describe arbitrary distributions over observations and parameters, respectively. Typically \"H\" will be the conjugate prior of \"F\". The two most common choices of \"F\" are Gaussian aka \"normal\" (for real-valued observations) and categorical (for discrete observations). Other common possibilities for the distribution of the mixture components are:\n- Binomial distribution, for the number of \"positive occurrences\" (e.g., successes, yes votes, etc.) given a fixed number of total occurrences\n- Multinomial distribution, similar to the binomial distribution, but for counts of multi-way occurrences (e.g., yes/no/maybe in a survey)\n- Negative binomial distribution, for binomial-type observations but where the quantity of interest is the number of failures before a given number of successes occurs\n- Poisson distribution, for the number of occurrences of an event in a given period of time, for an event that is characterized by a fixed rate of occurrence\n- Exponential distribution, for the time before the next event occurs, for an event that is characterized by a fixed rate of occurrence\n- Log-normal distribution, for positive real numbers that are assumed to grow exponentially, such as incomes or prices\n- Multivariate normal distribution (aka multivariate Gaussian distribution), for vectors of correlated outcomes that are individually Gaussian-distributed\n- Multivariate Student's-t distribution (aka multivariate t-distribution), for vectors of heavy-tailed correlated outcomes\n- A vector of Bernoulli-distributed values, corresponding, e.g., to a black-and-white image, with each value representing a pixel; see the handwriting-recognition example below\n\nA typical non-Bayesian Gaussian mixture model looks like this:\n\nA Bayesian version of a Gaussian mixture model is as follows:\n\nA Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions. In a multivariate distribution (i.e. one modelling a vector formula_6 with \"N\" random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by\nwhere the \"i\" vector component is characterized by normal distributions with weights formula_8, means formula_9 and covariance matrices formula_10. To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution formula_11 of the data formula_6 conditioned on the parameters formula_13 to be estimated. With this formulation, the posterior distribution formula_14 is \"also\" a Gaussian mixture model of the form \nwith new parameters formula_16 and formula_17 that are updated using the EM algorithm.\n\nSuch distributions are useful for assuming patch-wise shapes of images and clusters, for example. In the case of image representation, each Gaussian may be tilted, expanded, and warped according to the covariance matrices formula_10. One Gaussian distribution of the set is fit to each patch (usually of size 8x8 pixels) in the image. Notably, any distribution of points around a cluster (see \"k\"-means) may be accurately given enough Gaussian components, but scarcely over \"K\"=20 components are needed to accurately model a given image distribution or cluster of data.\n\nA typical non-Bayesian mixture model with categorical observations looks like this:\n\n- formula_20 as above\n- formula_21 as above\n- formula_22 as above\n- formula_23 dimension of categorical observations, e.g., size of word vocabulary\n- formula_24 probability for component formula_25 of observing item formula_26\n- formula_27 vector of dimension formula_28 composed of formula_29 must sum to 1\n\nThe random variables:\n\nA typical Bayesian mixture model with categorical observations looks like this:\n\n- formula_20 as above\n- formula_21 as above\n- formula_22 as above\n- formula_23 dimension of categorical observations, e.g., size of word vocabulary\n- formula_24 probability for component formula_25 of observing item formula_26\n- formula_27 vector of dimension formula_28 composed of formula_29 must sum to 1\n- formula_41 shared concentration hyperparameter of formula_42 for each component\n- formula_43 concentration hyperparameter of formula_44\n\nThe random variables:\n\nFinancial returns often behave differently in normal situations and during crisis times. A mixture model for return data seems reasonable. Sometimes the model used is a jump-diffusion model, or as a mixture of two normal distributions. See Financial economics#Challenges and criticism for further context.\n\nAssume that we observe the prices of \"N\" different houses. Different types of houses in different neighborhoods will have vastly different prices, but the price of a particular type of house in a particular neighborhood (e.g., three-bedroom house in moderately upscale neighborhood) will tend to cluster fairly closely around the mean. One possible model of such prices would be to assume that the prices are accurately described by a mixture model with \"K\" different components, each distributed as a normal distribution with unknown mean and variance, with each component specifying a particular combination of house type/neighborhood. Fitting this model to observed prices, e.g., using the expectation-maximization algorithm, would tend to cluster the prices according to house type/neighborhood and reveal the spread of prices in each type/neighborhood. (Note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow exponentially, a log-normal distribution might actually be a better model than a normal distribution.)\n\nAssume that a document is composed of \"N\" different words from a total vocabulary of size \"V\", where each word corresponds to one of \"K\" possible topics. The distribution of such words could be modelled as a mixture of \"K\" different \"V\"-dimensional categorical distributions. A model of this sort is commonly termed a topic model. Note that expectation maximization applied to such a model will typically fail to produce realistic results, due (among other things) to the excessive number of parameters. Some sorts of additional assumptions are typically necessary to get good results. Typically two sorts of additional components are added to the model:\n1. A prior distribution is placed over the parameters describing the topic distributions, using a Dirichlet distribution with a concentration parameter that is set significantly below 1, so as to encourage sparse distributions (where only a small number of words have significantly non-zero probabilities).\n2. Some sort of additional constraint is placed over the topic identities of words, to take advantage of natural clustering.\n\nThe following example is based on an example in Christopher M. Bishop, \"Pattern Recognition and Machine Learning\".\n\nImagine that we are given an \"N\"×\"N\" black-and-white image that is known to be a scan of a hand-written digit between 0 and 9, but we don't know which digit is written. We can create a mixture model with formula_46 different components, where each component is a vector of size formula_47 of Bernoulli distributions (one per pixel). Such a model can be trained with the expectation-maximization algorithm on an unlabeled set of hand-written digits, and will effectively cluster the images according to the digit being written. The same model could then be used to recognize the digit of another image simply by holding the parameters constant, computing the probability of the new image for each possible digit (a trivial calculation), and returning the digit that generated the highest probability.\n\nMixture models apply in the problem of directing multiple projectiles at a target (as in air, land, or sea defense applications), where the physical and/or statistical characteristics of the projectiles differ within the multiple projectiles. An example might be shots from multiple munitions types or shots from multiple locations directed at one target. The combination of projectile types may be characterized as a Gaussian mixture model. Further, a well-known measure of accuracy for a group of projectiles is the circular error probable (CEP), which is the number \"R\" such that, on average, half of the group of projectiles falls within the circle of radius \"R\" about the target point. The mixture model can be used to determine (or estimate) the value \"R\". The mixture model properly captures the different types of projectiles.\n\nThe financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.\n\nIn an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two normal distributions with different means may result in a density with two modes, which is not modeled by standard parametric distributions. Another example is given by the possibility of mixture distributions to model fatter tails than the basic Gaussian ones, so as to be a candidate for modeling more extreme events. When combined with dynamical consistency, this approach has been applied to financial derivatives valuation in presence of the volatility smile in the context of local volatility models. This defines our application.\n\nThe mixture model-based clustering is also predominantly used in identifying the state of the machine in predictive maintenance. Density plots are used to analyze the density of high dimensional features. If multi-model densities are observed, then it is assumed that a finite set of densities are formed by a finite set of normal mixtures. A multivariate Gaussian mixture model is used to cluster the feature data into k number of groups where k represents each state of the machine. The machine state can be a normal state, power off state, or faulty state. Each formed cluster can be diagnosed using techniques such as spectral analysis. In the recent years, this has also been widely used in other areas such as early fault detection.\n\nIn image processing and computer vision, traditional image segmentation models often assign to one pixel only one exclusive pattern. In fuzzy or soft segmentation, any pattern can have certain \"ownership\" over any single pixel. If the patterns are Gaussian, fuzzy segmentation naturally results in Gaussian mixtures. Combined with other analytic or geometric tools (e.g., phase transitions over diffusive boundaries), such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods.\n\nProbabilistic mixture models such as Gaussian mixture models (GMM) are used to resolve point set registration problems in image processing and computer vision fields. For pair-wise point set registration, one point set is regarded as the centroids of mixture models, and the other point set is regarded as data points (observations). State-of-the-art methods are e.g. coherent point drift (CPD) \nand Student's t-distribution mixture models (TMM). \nThe result of recent research demonstrate the superiority of hybrid mixture models \n(e.g. combining Student's t-Distritubtion and Watson distribution/Bingham distribution to model spatial positions and axes orientations separately) compare to CPD and TMM, in terms of inherent robustness, accuracy and discriminative capacity.\n\nIdentifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.\n\nLet \"J\" be the class of all binomial distributions with . Then a mixture of two members of \"J\" would have\n\nand . Clearly, given \"p\" and \"p\", it is not possible to determine the above mixture model uniquely, as there are three parameters (\"π\", \"θ\", \"θ\") to be determined.\n\nConsider a mixture of parametric distributions of the same class. Let\n\nbe the class of all component distributions. Then the convex hull \"K\" of \"J\" defines the class of all finite mixture of distributions in \"J\":\n\n\"K\" is said to be identifiable if all its members are unique, that is, given two members \"p\" and in \"K\", being mixtures of \"k\" distributions and distributions respectively in \"J\", we have if and only if, first of all, and secondly we can reorder the summations such that and for all \"i\".\n\nParametric mixture models are often used when we know the distribution \"Y\" and we can sample from \"X\", but we would like to determine the \"a\" and \"θ\" values. Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.\n\nIt is common to think of probability mixture modeling as a missing data problem. One way to understand this is to assume that the data points under consideration have \"membership\" in one of the distributions we are using to model the data. When we start, this membership is unknown, or missing. The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.\n\nA variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum \"a posteriori\" estimation (MAP). Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values. Some notable departures are the graphical methods as outlined in Tarter and Lock and more recently minimum message length (MML) techniques such as Figueiredo and Jain and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).\n\nExpectation maximization (EM) is seemingly the most popular technique used to determine the parameters of a mixture with an \"a priori\" given number of components. This is a particular way of implementing maximum likelihood estimation for this problem. EM is of particular appeal for finite normal mixtures where closed-form expressions are possible such as in the following iterative algorithm by Dempster \"et al.\" (1977)\n\nwith the posterior probabilities\n\nThus on the basis of the current estimate for the parameters, the conditional probability for a given observation \"x\" being generated from state \"s\" is determined for each ; \"N\" being the sample size. The parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample.\n\nDempster also showed that each successive EM iteration will not decrease the likelihood, a property not shared by other gradient based maximization techniques. Moreover, EM naturally embeds within it constraints on the probability vector, and for sufficiently large sample sizes positive definiteness of the covariance iterates. This is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values. Theoretically EM is a first-order algorithm and as such converges slowly to a fixed-point solution. Redner and Walker (1984) make this point arguing in favour of superlinear and second order Newton and quasi-Newton methods and reporting slow convergence in EM on the basis of their empirical tests. They do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not. The relative merits of EM and other algorithms vis-à-vis convergence have been discussed in other literature.\n\nOther common objections to the use of EM are that it has a propensity to spuriously identify local maxima, as well as displaying sensitivity to initial values. One may address these problems by evaluating EM at several initial points in the parameter space but this is computationally costly and other approaches, such as the annealing EM method of Udea and Nakano (1998) (in which the initial components are essentially forced to overlap, providing a less heterogeneous basis for initial guesses), may be preferable.\n\nFigueiredo and Jain note that convergence to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed when the number of model components exceeds the optimal/true one. On this basis they suggest a unified approach to estimation and identification in which the initial \"n\" is chosen to greatly exceed the expected optimal value. Their optimization routine is constructed via a minimum message length (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in \"n\" and consider estimation and identification jointly.\n\nThe Expectation-maximization algorithm can be used to compute the parameters of a parametric mixture model distribution (the \"a\" and \"θ\"). It is an iterative algorithm with two steps: an \"expectation step\" and a \"maximization step\". Practical examples of EM and Mixture Modeling are included in the SOCR demonstrations.\n\nWith initial guesses for the parameters of our mixture model, \"partial membership\" of each data point in each constituent distribution is computed by calculating expectation values for the membership variables of each data point. That is, for each data point \"x\" and distribution \"Y\", the membership value \"y\" is:\n\nWith expectation values in hand for group membership, plug-in estimates are recomputed for the distribution parameters.\n\nThe mixing coefficients \"a\" are the means of the membership values over the \"N\" data points.\n\nThe component model parameters \"θ\" are also calculated by expectation maximization using data points \"x\" that have been weighted using the membership values. For example, if \"θ\" is a mean \"μ\"\n\nWith new estimates for \"a\" and the \"θ\"'s, the expectation step is repeated to recompute new membership values. The entire procedure is repeated until model parameters converge.\n\nAs an alternative to the EM algorithm, the mixture model parameters can be deduced using posterior sampling as indicated by Bayes' theorem. This is still regarded as an incomplete data problem whereby membership of data points is the missing data. A two-step iterative procedure known as Gibbs sampling can be used.\n\nThe previous example of a mixture of two Gaussian distributions can demonstrate how the method works. As before, initial guesses of the parameters for the mixture model are made. Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a Bernoulli distribution (that is, it will be assigned to either the first or the second Gaussian). The Bernoulli parameter \"θ\" is determined for each data point on the basis of one of the constituent distributions. Draws from the distribution generate membership associations for each data point. Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.\n\nThe method of moment matching is one of the oldest techniques for determining the mixture parameters dating back to Karl Pearson's seminal work of 1894.\nIn this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value. In many instances extraction of solutions to the moment equations may present non-trivial algebraic or computational problems. Moreover, numerical analysis by Day has indicated that such methods may be inefficient compared to EM. Nonetheless there has been renewed interest in this method, e.g., Craigmile and Titterington (1998) and Wang.\n\nMcWilliam and Loh (2009) consider the characterisation of a hyper-cuboid normal mixture copula in large dimensional systems for which EM would be computationally prohibitive. Here a pattern analysis routine is used to generate multivariate tail-dependencies consistent with a set of univariate and (in some sense) bivariate moments. The performance of this method is then evaluated using equity log-return data with Kolmogorov–Smirnov test statistics suggesting a good descriptive fit.\n\nSome problems in mixture model estimation can be solved using spectral methods.\nIn particular it becomes useful if data points \"x\" are points in high-dimensional real space, and the hidden distributions are known to be log-concave (such as Gaussian distribution or Exponential distribution).\n\nSpectral methods of learning mixture models are based on the use of Singular Value Decomposition of a matrix which contains data points.\nThe idea is to consider the top \"k\" singular vectors, where \"k\" is the number of distributions to be learned. The projection\nof each data point to a linear subspace spanned by those vectors groups points originating from the same distribution\nvery close together, while points from different distributions stay far apart.\n\nOne distinctive feature of the spectral method is that it allows us to prove that if\ndistributions satisfy certain separation condition (e.g., not too close), then the estimated mixture will be very close to the true one with high probability.\n\nTarter and Lock describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intra-component variance. In this way one may more readily identify components having differing means. While this \"λ\"-method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure.\n\nSome of them can even probably learn mixtures of heavy-tailed distributions including those with\ninfinite variance (see links to papers below).\nIn this setting, EM based methods would not work, since the Expectation step would diverge due to presence of\noutliers.\n\nTo simulate a sample of size \"N\" that is from a mixture of distributions \"F\", \"i\"=1 to \"n\", with probabilities \"p\" (sum= \"p\" = 1):\n1. Generate \"N\" random numbers from a categorical distribution of size \"n\" and probabilities \"p\" for \"i\"= 1= to \"n\". These tell you which of the \"F\" each of the \"N\" values will come from. Denote by \"m\" the quantity of random numbers assigned to the \"i\" category.\n2. For each \"i\", generate \"m\" random numbers from the \"F\" distribution.\n\nIn a Bayesian setting, additional levels can be added to the graphical model defining the mixture model. For example, in the common latent Dirichlet allocation topic model, the observations are sets of words drawn from \"D\" different documents and the \"K\" mixture components represent topics that are shared across documents. Each document has a different set of mixture weights, which specify the topics prevalent in that document. All sets of mixture weights share common hyperparameters.\n\nA very common extension is to connect the latent variables defining the mixture component identities into a Markov chain, instead of assuming that they are independent identically distributed random variables. The resulting model is termed a hidden Markov model and is one of the most common sequential hierarchical models. Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.\n\nMixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan, 2000) although common reference is made to the work of Karl Pearson (1894) as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations. The motivation for this work was provided by the zoologist Walter Frank Raphael Weldon who had speculated in 1893 (in Tarter and Lock) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson's approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.\n\nWhile his work was successful in identifying two potentially distinct sub-populations and in demonstrating the flexibility of mixtures as a moment matching tool, the formulation required the solution of a 9th degree (nonic) polynomial which at the time posed a significant computational challenge.\n\nSubsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood (MLE) parameterisation techniques that research really took off. Since that time there has been a vast body of research on the subject spanning areas such as fisheries research, agriculture, botany, economics, medicine, genetics, psychology, palaeontology, electrophoresis, finance, geology and zoology.\n\n", "related": "\n- Mixture density\n- Mixture (probability)\n- Flexible Mixture Model (FMM)\n\n- Graphical model\n- Hierarchical Bayes model\n\n- RANSAC\n\n\n\n- The SOCR demonstrations of EM and Mixture Modeling\n- Mixture modelling page (and the Snob program for Minimum Message Length (MML) applied to finite mixture models), maintained by D.L. Dowe.\n- PyMix – Python Mixture Package, algorithms and data structures for a broad variety of mixture model based data mining applications in Python\n- sklearn.mixture – A Python package for learning Gaussian Mixture Models (and sampling from them), previously packaged with SciPy and now packaged as a SciKit\n- GMM.m Matlab code for GMM Implementation\n- GPUmix C++ implementation of Bayesian Mixture Models using EM and MCMC with 100x speed acceleration using GPGPU.\n- Matlab code for GMM Implementation using EM algorithm\n- jMEF: A Java open source library for learning and processing mixtures of exponential families (using duality with Bregman divergences). Includes a Matlab wrapper.\n- Very Fast and clean C implementation of the Expectation Maximization (EM) algorithm for estimating Gaussian Mixture Models (GMMs).\n- mclust is an R package for mixture modeling.\n- dpgmm Pure Python Dirichlet process Gaussian mixture model implementation (variational).\n"}
{"id": "1579244", "url": "https://en.wikipedia.org/wiki?curid=1579244", "title": "Statistical classification", "text": "Statistical classification\n\nIn machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the \"spam\" or \"non-spam\" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.\n\nIn the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.\n\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or \"features\". These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\n\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\n\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as \"instances\", the explanatory variables are termed \"features\" (grouped into a feature vector), and the possible categories to be predicted are \"classes\". Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis, i.e., a type of unsupervised learning, rather than the supervised learning described in this article.\n\nClassification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.\n\nA common subclass of classification is probabilistic classification. Algorithms of this nature use statistical inference to find the best class for a given instance. Unlike other algorithms, which simply output a \"best\" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes. The best class is normally then selected as the one with the highest probability. However, such an algorithm has numerous advantages over non-probabilistic classifiers:\n- It can output a confidence value associated with its choice (in general, a classifier that can do this is known as a \"confidence-weighted classifier\").\n- Correspondingly, it can \"abstain\" when its confidence of choosing any particular output is too low.\n- Because of the probabilities which are generated, probabilistic classifiers can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of \"error propagation\".\n\nEarly work on statistical classification was undertaken by Fisher, in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation. This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear. Later work for the multivariate normal distribution allowed the classifier to be nonlinear: several classification rules can be derived based on different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.\n\nUnlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the different groups within the overall population. Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.\n\nSome Bayesian procedures involve the calculation of group membership probabilities: these can be viewed as providing a more informative outcome of a data analysis than a simple attribution of a single group-label to each new observation.\n\nClassification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes. Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.\n\nMost algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent). Features may variously be binary (e.g. \"on\" or \"off\"); categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type); ordinal (e.g. \"large\", \"medium\" or \"small\"); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure). If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words. Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be \"discretized\" into groups (e.g. less than 5, between 5 and 10, or greater than 10).\n\nA large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category \"k\" by combining the feature vector of an instance with a vector of weights, using a dot product. The predicted category is the one with the highest score. This type of score function is known as a linear predictor function and has the following general form:\n\nwhere X is the feature vector for instance \"i\", β is the vector of weights corresponding to category \"k\", and score(X, \"k\") is the score associated with assigning instance \"i\" to category \"k\". In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person \"i\" choosing category \"k\".\n\nAlgorithms with this basic setup are known as linear classifiers. What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.\n\nExamples of such algorithms are\n- Logistic regression and Multinomial logistic regression\n- Probit regression\n- The perceptron algorithm\n- Support vector machines\n- Linear discriminant analysis.\n\nIn unsupervised learning, classifiers form the backbone of cluster analysis and in supervised or semi-supervised learning, classifiers are how the system characterizes and evaluates unlabeled data. In all cases though, classifiers have a specific set of dynamic rules, which includes an interpretation procedure to handle vague or unknown values, all tailored to the type of inputs being examined.\n\nSince no single form of classification is appropriate for all data sets, a large toolkit of classification algorithms have been developed. The most commonly used include:\n\n- Linear classifiers\n- Fisher's linear discriminant\n- Logistic regression\n- Naive Bayes classifier\n- Perceptron\n- Support vector machines\n- Least squares support vector machines\n- Quadratic classifiers\n- Kernel estimation\n- k-nearest neighbor\n- Boosting (meta-algorithm)\n- Decision trees\n- Random forests\n- Neural networks\n- Learning vector quantization\n\nClassifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.\n\nThe measures precision and recall are popular metrics used to evaluate the quality of a classification system. More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms.\n\nAs a performance metric, the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes.\n\nFurther, it will not penalize an algorithm for simply \"rearranging\" the classes.\n\nClassification has many applications. In some of these it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken.\n\n- Computer vision\n- Medical imaging and medical image analysis\n- Optical character recognition\n- Video tracking\n- Drug discovery and development\n- Toxicogenomics\n- Quantitative structure-activity relationship\n- Geostatistics\n- Speech recognition\n- Handwriting recognition\n- Biometric identification\n- Biological classification\n- Statistical natural language processing\n- Document classification\n- Internet search engines\n- Credit scoring\n- Pattern recognition\n- Recommender system\n- Micro-array classification\n\n", "related": "\n- Artificial intelligence\n- Binary classification\n- Class membership probabilities\n- Classification rule\n- Compound term processing\n- Data mining\n- Data warehouse\n- Fuzzy logic\n- Information retrieval\n- List of datasets for machine learning research\n- Machine learning\n- Recommender system\n"}
{"id": "19463198", "url": "https://en.wikipedia.org/wiki?curid=19463198", "title": "Apprenticeship learning", "text": "Apprenticeship learning\n\nIn artificial intelligence, apprenticeship learning (or learning from demonstration) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher.\n\nMapping methods try to mimic the expert by forming a direct mapping from the states to the actions. For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills.\n\nSystem models try to mimic the expert by modeling world dynamics.\n\nInverse reinforcement learning (IRL) is the process of deriving a reward function from observed behavior. While ordinary \"reinforcement learning\" involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a person's behavior to figure out what goal that behavior seems to be trying to achieve. The IRL problem can be defined as:\n\nGiven 1) measurements of an agent's behaviour over time, in a variety of circumstances; 2) measurements of the sensory inputs to that agent; 3) a model of the physical environment (including the agent's body): Determine the reward function that the agent is optimizing.\n\nIRL researcher Stuart J. Russell proposes that IRL might be used to observe humans and attempt to codify their complex \"ethical values\", in an effort to create \"ethical robots\" that might someday know \"not to cook your cat\" without needing to be explicitly told. The scenario can be modeled as a \"cooperative inverse reinforcement learning game\", where a \"person\" player and a \"robot\" player cooperate to secure the person's implicit goals, despite these goals not being explicitly known by either the person nor the robot.\n\nIn 2017, OpenAI and DeepMind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems.\n\nApprenticeship via inverse reinforcement learning (AIRP) was developed by in 2004 Pieter Abbeel, Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. AIRP deals with \"Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform\". AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.\n\nOne domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - \"Autonomous Helicopter Aerobatics through Apprenticeship Learning\"\n\nThe system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball\ncollection task.\n\nLearning from demonstration is often explained from a perspective that the working Robot-control-system is available and the human-demonstrator is using it. And indeed, if the software works, the Human operator takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 1:1. But that is not how the system works internally; it is only what the audience can observe. In reality, Learning from demonstration is much more complex.\n\nIn 1997, robotics expert Stefan Schaal was working on the Sarcos robot-arm. The goal was simple: solve the pendulum swingup task. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an Optimal control-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a Brute-force solver but record the movements of a human-demonstration. The angle of the pendulum is logged over the timeperiod of 3 seconds at the y-axis. This results into a diagram which produces a pattern.\n\nIn computer animation, the principle is called spline animation. That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases it's the position of an object. In the inverted pendulum it is the angle.\n\nThe overall task consists of two parts: recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called “Tracking control” or PID control. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle “steering behavior”, because the aim is to bring a robot to a given line.\n\n", "related": "\n- Inverse reinforcement learning\n"}
{"id": "3119546", "url": "https://en.wikipedia.org/wiki?curid=3119546", "title": "Subclass reachability", "text": "Subclass reachability\n\nIn computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).\n", "related": "NONE"}
{"id": "205393", "url": "https://en.wikipedia.org/wiki?curid=205393", "title": "Binary classification", "text": "Binary classification\n\nBinary or binomial classification is the task of classifying the elements of a given set into two groups (predicting which group each one belongs to) on the basis of a classification rule. Contexts requiring a decision as to whether or not an item has some qualitative property, some specified characteristic, or some typical binary classification include:\n- Medical testing to determine if a patient has certain disease or not – the classification property is the presence of the disease.\n- A \"pass or fail\" test method or quality control in factories, i.e. deciding if a specification has or has not been met – a Go/no go classification.\n- Information retrieval, namely deciding whether a page or an article should be in the result set of a search or not – the classification property is the relevance of the article, or the usefulness to the user.\n\nBinary classification is dichotomization applied to practical purposes, and in many practical binary classification problems, the two 2 groups are not symmetric – rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present).\n\nStatistical classification is a problem studied in machine learning. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification.\n\nSome of the methods commonly used for binary classification are:\n- Decision trees\n- Random forests\n- Bayesian networks\n- Support vector machines\n- Neural networks\n- Logistic regression\n- Probit model\nEach classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example, random forests perform better than SVM classifiers for 3D point clouds.\n\nThere are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.\n\nGiven a classification of a specific data set, there are four basic combinations of actual data category and assigned category: true positives TP (correct positive assignments), true negatives TN (correct negative assignments), false positives FP (incorrect positive assignments), and false negatives FN (incorrect negative assignments).\nThese can be arranged into a 2×2 contingency table, with columns corresponding to actual value – condition positive (CP) or condition negative (CN) – and rows corresponding to classification value – test outcome positive (OP) or test outcome negative (ON). There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form \"true positive row ratio\" or \"false negative column ratio\", though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.\n\nThe column ratios are True Positive Rate (TPR, aka Sensitivity or recall) (TP/(TP+FN)), with complement the False Negative Rate (FNR) (FN/(TP+FN)); and True Negative Rate (TNR, aka Specificity, SPC) (TN/(TN+FP)), with complement False Positive Rate (FPR) (FP/(TN+FP)). These are the proportion of the \"population with the condition\" (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence.\n\nThe row ratios are Positive Predictive Value (PPV, aka precision) (TP/(TP+FP)), with complement the False Discovery Rate (FDR) (FP/(TP+FP)); and Negative Predictive Value (NPV) (TN/(TN+FN)), with complement the False Omission Rate (FOR) (FN/(TN+FN)). These are the proportion of the \"population with a given test result\" for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence.\n\nIn diagnostic testing, the main ratios used are the true column ratios – True Positive Rate and True Negative Rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – Positive Predictive Value and True Positive Rate – where they are known as precision and recall.\n\nOne can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent.\n\nThere are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden's J statistic, the uncertainty coefficient, the Phi coefficient, and Cohen's kappa.\n\nTests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff.\n\nHowever, such conversion causes a loss of information, as the resultant binary classification does not tell \"how much\" above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as \"positive\" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as \"positive\" as the one of 52 mIU/ml.\n\n", "related": "\n- Examples of Bayesian inference\n- Classification rule\n- Confusion matrix\n- Detection theory\n- Kernel methods\n- Multiclass classification\n- Multi-label classification\n- One-class classification\n- Prosecutor's fallacy\n- Receiver operating characteristic\n- Thresholding (image processing)\n- Uncertainty coefficient, aka Proficiency\n- Qualitative property\n\n- Nello Cristianini and John Shawe-Taylor. \"An Introduction to Support Vector Machines and other kernel-based learning methods\". Cambridge University Press, 2000. \"( SVM Book)\"\n- John Shawe-Taylor and Nello Cristianini. \"Kernel Methods for Pattern Analysis\". Cambridge University Press, 2004. \"( Kernel Methods Book)\"\n- Bernhard Schölkopf and A. J. Smola: \"Learning with Kernels\". MIT Press, Cambridge, Massachusetts, 2002. \"(Partly available on line: .)\"\n"}
{"id": "21638340", "url": "https://en.wikipedia.org/wiki?curid=21638340", "title": "Explanation-based learning", "text": "Explanation-based learning\n\nExplanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory in order to make generalizations or form concepts from training examples.\n\nAn example of EBL using a perfect domain theory is a program that learns to play chess through example. A specific chess position that contains an important feature such as \"Forced loss of black queen in two moves\" includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization.\n\nA domain theory is \"perfect\" or \"complete\" if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle, it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to combinatoric explosion. EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice.\n\nIn essence, an EBL system works by finding a way to deduce each training example from the system's existing database of domain theory. Having a short proof of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly.\nThe main drawback of the method---the cost of applying the learned proof macros, as these become numerous---was analyzed by Minton.\nEBL software takes four inputs:\n\n- a hypothesis space (the set of all possible conclusions)\n- a domain theory (axioms about a domain of interest)\n- training examples (specific facts that rule out some possible hypothesis)\n- operationality criteria (criteria for determining which features in the domain are efficiently recognizable, e.g. which features are directly detectable using sensors)\n\nAn especially good application domain for an EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar---although neither perfect nor complete, is tuned to a particular application or particular language usage, using a treebank (training examples). Rayner pioneered this work. The first successful industrial application was to a commercial NL interface to relational databases. The method has been successfully applied to several large-scale natural language parsing systems, where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation.\nEBL-like techniques have also been applied to surface generation, the converse of parsing.\n\nWhen applying EBL to NLP, the operationality criteria can be hand-crafted, or can be\ninferred from the treebank using either the entropy of its or-nodes\nor a target coverage/disambiguation trade-off (= recall/precision trade-off = f-score).\nEBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars.\nNote how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase \"grammar specialization\"---quite the opposite of the original term \"explanation-based generalization.\" Perhaps the best name for this technique would be \"data-driven search space reduction.\"\nOther people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Sima'an.\n\n", "related": "\n- One-shot learning\n"}
{"id": "8964665", "url": "https://en.wikipedia.org/wiki?curid=8964665", "title": "Category utility", "text": "Category utility\n\nCategory utility is a measure of \"category goodness\" defined in and . It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as \"cue validity\" (; ) and \"collocation index\" . It provides a normative information-theoretic measure of the \"predictive advantage\" gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does \"not\" possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in .\n\nThe probability-theoretic definition of category utility given in and is as follows:\n\nwhere formula_2 is a size-formula_3 set of formula_4-ary features, and formula_5 is a set of formula_6 categories. The term formula_7 designates the marginal probability that feature formula_8 takes on value formula_9, and the term formula_10 designates the category-conditional probability that feature formula_8 takes on value formula_9 \"given\" that the object in question belongs to category formula_13.\n\nThe motivation and development of this expression for category utility, and the role of the multiplicand formula_14 as a crude overfitting control, is given in the above sources. Loosely , the term formula_15 is the expected number of attribute values that can be correctly guessed by an observer using a probability-matching strategy together with knowledge of the category labels, while formula_16 is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels. Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure.\n\nThe information-theoretic definition of category utility for a set of entities with size-formula_3 binary feature set formula_2, and a binary category formula_19 is given in as follows:\n\nwhere formula_21 is the prior probability of an entity belonging to the positive category formula_22 (in the absence of any feature information), formula_23 is the conditional probability of an entity having feature formula_8 given that the entity belongs to category formula_22, formula_26 is likewise the conditional probability of an entity having feature formula_8 given that the entity belongs to category formula_28, and formula_29 is the prior probability of an entity possessing feature formula_8 (in the absence of any category information).\n\nThe intuition behind the above expression is as follows: The term formula_31 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it is known that the objects to be described belong to category formula_22. Similarly, the term formula_33 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it is known that the objects to be described belong to category formula_28. The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, formula_35, represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available. The value of the category utility will, in the above formulation, be negative (???).\n\n and mention that the category utility is equivalent to the mutual information. Here is a simple demonstration of the nature of this equivalence. Assume a set of entities each having the same formula_36 features, i.e., feature set formula_2, with each feature variable having cardinality formula_38. That is, each feature has the capacity to adopt any of formula_38 distinct values (which need \"not\" be ordered; all variables can be nominal); for the special case formula_40 these features would be considered \"binary\", but more generally, for any formula_38, the features are simply \"m-ary\". For the purposes of this demonstration, without loss of generality, feature set formula_42 can be replaced with a single aggregate variable formula_43 that has cardinality formula_44, and adopts a unique value formula_45 corresponding to each feature combination in the Cartesian product formula_46. (Ordinality does \"not\" matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as formula_47 or simply formula_48 refers to the probability with which formula_43 adopts the particular value formula_50. (Using the aggregate feature variable formula_43 replaces multiple summations, and simplifies the presentation to follow.)\n\nFor this demonstration, also assume a single category variable formula_52, which has cardinality formula_53. This is equivalent to a classification system in which there are formula_53 non-intersecting categories. In the special case of formula_55 there are the two-category case discussed above. From the definition of mutual information for discrete variables, the mutual information formula_56 between the aggregate feature variable formula_43 and the category variable formula_52 is given by:\n\nwhere formula_48 is the prior probability of feature variable formula_43 adopting value formula_50, formula_63 is the marginal probability of category variable formula_52 adopting value formula_65, and formula_66 is the joint probability of variables formula_43 and formula_52 simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as\n\nIf the original definition of the category utility from above is rewritten with formula_19,\n\nThis equation clearly has the same form as the (blue) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum formula_72 in the category utility equation runs over independent binary variables formula_2, whereas the sum formula_74 in the mutual information runs over \"values\" of the single formula_44-ary variable formula_43. The two measures are actually equivalent then \"only\" when the features formula_77, are \"independent\" (and assuming that terms in the sum corresponding to formula_78 are also added).\n\nLike the mutual information, the category utility is not sensitive to any \"ordering\" in the feature or category variable values. That is, as far as the category utility is concerned, the category set codice_1 is not qualitatively different from the category set codice_2 since the formulation of the category utility does not account for any ordering of the class variable. Similarly, a feature variable adopting values codice_3 is not qualitatively different from a feature variable adopting values codice_4. As far as the category utility or \"mutual information\" are concerned, \"all\" category and feature variables are \"nominal variables.\" For this reason, category utility does not reflect any \"gestalt\" aspects of \"category goodness\" that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information.\n\nThis section provides some background on the origins of, and need for, formal measures of \"category goodness\" such as the category utility, and some of the history that lead to the development of this particular metric.\n\nAt least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of \"entity\" is a concept such as \"horse\"? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept \"horse\" therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals \"did\" indeed have a mind-independent existence. There was, therefore, always a \"fact to the matter\" about which concepts and universals exist in the world.\n\nIn the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: \"Why do we possess one set of concepts rather than another?\" What makes one set of concepts \"good\" and another set of concepts \"bad\"? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades.\n\nOne approach to answering such questions is to investigate the \"role\" or \"purpose\" of concepts in cognition. Thus the answer to \"What are concepts good for in the first place?\" by and many others is that classification (conception) is a precursor to \"induction\": By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (; ). As J.S. Mill puts it ,\n\nFrom this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of category utility:\n\nOne may compare this to the \"category utility hypothesis\" proposed by : \"A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category.\" Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object's class, and, simultaneously, the object class is maximally informative about the object's features. In other words, a useful classification scheme is one in which category knowledge can be used to accurately infer object properties, and property knowledge can be used to accurately infer object classes. One may also compare this idea to Aristotle's criterion of \"counter-predication\" for definitional predicates, as well as to the notion of concepts described in formal concept analysis.\n\nA variety of different measures have been suggested with an aim of formally capturing this notion of \"category goodness,\" the best known of which is probably the \"cue validity\". Cue validity of a feature formula_8 with respect to category formula_13 is defined as the conditional probability of the category given the feature (;;), formula_81, or as the deviation of the conditional probability from the category base rate (;), formula_82. Clearly, these measures quantify only inference from feature to category (i.e., \"cue validity\"), but not from category to feature, i.e., the \"category validity\" formula_83. Also, while the cue validity was originally intended to account for the demonstrable appearance of \"basic categories\" in human cognition—categories of a particular level of generality that are evidently preferred by human learners—a number of major flaws in the cue validity quickly emerged in this regard (;;, and others).\n\nOne attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by in defining the \"collocation index\" as the product formula_84, but this construction was fairly ad hoc (see ). The category utility was introduced as a more sophisticated refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure. As shown above, on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable. It has been suggested that categories having the greatest overall category utility are those that are not only those \"best\" in a normative sense, but also those human learners prefer to use, e.g., \"basic\" categories . Other related measures of category goodness are \"cohesion\" (;) and \"salience\" .\n\n- Category utilility is used as the category evaluation measure in the popular conceptual clustering algorithm called COBWEB .\n\n", "related": "\n- Abstraction\n- Concept learning\n- Universals\n- Unsupervised learning\n\n"}
{"id": "7309022", "url": "https://en.wikipedia.org/wiki?curid=7309022", "title": "Nearest neighbor search", "text": "Nearest neighbor search\n\nNearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set \"S\" of points in a space \"M\" and a query point \"q\" ∈ \"M\", find the closest point in \"S\" to \"q\". Donald Knuth in vol. 3 of \"The Art of Computer Programming\" (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a \"k\"-NN search, where we need to find the \"k\" closest points.\n\nMost commonly \"M\" is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, \"M\" is taken to be the \"d\"-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.\n\nThe nearest neighbour search problem arises in numerous fields of application, including:\n- Pattern recognition – in particular for optical character recognition\n- Statistical classification – see k-nearest neighbor algorithm\n- Computer vision\n- Computational geometry – see Closest pair of points problem\n- Databases – e.g. content-based image retrieval\n- Coding theory – see maximum likelihood decoding\n- Data compression – see MPEG-2 standard\n- Robotic sensing\n- Recommendation systems, e.g. see Collaborative filtering\n- Internet marketing – see contextual advertising and behavioral targeting\n- DNA sequencing\n- Spell checking – suggesting correct spelling\n- Plagiarism detection\n- Similarity scores for predicting career paths of professional athletes.\n- Cluster analysis – assignment of a set of observations into subsets (called clusters) so that observations in the same cluster are similar in some sense, usually based on Euclidean distance\n- Chemical similarity\n- Sampling-based motion planning\n\nVarious solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.\n\nThe simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the \"best so far\". This algorithm, sometimes referred to as the naive approach, has a running time of \"O\"(\"dN\"), where \"N\" is the cardinality of \"S\" and \"d\" is the dimensionality of \"M\". There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.\n\nSince the 1970s, the branch and bound methodology has been applied to the problem. In the case of Euclidean space this approach encompasses spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the k-d tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is \"O\"(log \"N\") in the case of randomly distributed points, worst case complexity is \"O\"(\"kN\"^(1-1/\"k\"))\nAlternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.\n\nIn the case of general metric space, the branch-and-bound approach is known as the metric tree approach. Particular examples include vp-tree and BK-tree methods.\n\nUsing a set of points taken from a 3-dimensional space and put into a BSP tree, and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm. (Strictly speaking, no such point may exist, because it may not be unique. But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.) The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point. This may not be the case, but it is a good heuristic. After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane. This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched. If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space. If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result. The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.\n\nAn approximate nearest neighbor search algorithm is allowed to return points, whose distance from the query is at most formula_1 times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one. In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter.\n\nProximity graph methods (such as HNSW) are considered the current state-of-the-art for the approximate nearest neighbors search.\n\nThe methods are based on greedy traversing in proximity neighborhood graphs formula_2in which every point formula_3 is uniquely associated with vertex formula_4. The search for the nearest neighbors to a query \"q\" in the set \"S\" takes the form of searching for the vertex in the graph formula_2.\nThe basic algorithm – greedy search – works as follows: search starts from an enter-point vertex formula_4 by computing the distances from the query q to each vertex of its neighborhood formula_7, and then finds a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new enter-point. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.\n\nThe idea of proximity neighborhood graphs was exploited in multiple publications, including the seminal paper by Arya and Mount, in the VoroNet system for the plane, in the RayNet system for the formula_8, and in the Metrized Small World and HNSW algorithms for the general case of spaces with a distance function. These works were preceded by a pioneering paper by Toussaint, in which he introduced the concept of a \"relative neighborhood\" graph.\n\nLocality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.\n\nThe cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is \"O\"(\"c\" log \"n\") where \"c\" is the expansion constant of the dataset.\n\nIn the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem.\nThis approach requires that the 3D data is organized by a projection to a two-dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries.\nThese assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general.\nIn practice this technique has an average search time of \"O\"(\"1\") or \"O\"(\"K\") for the \"k\"-nearest neighbor problem when applied to real world stereo vision data.\n\nIn high-dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.\n\nThe VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most \"promising\" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed. Also note the parallels between clustering and LSH.\n\nThere are numerous variants of the NNS problem and the two most well-known are the \"k\"-nearest neighbor search and the ε-approximate nearest neighbor search.\n\n\"k\"-nearest neighbor search identifies the top \"k\" nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. \"k\"-nearest neighbor graphs are graphs in which every point is connected to its \"k\" nearest neighbors.\n\nIn some applications it may be acceptable to retrieve a \"good guess\" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.\n\nAlgorithms that support the approximate nearest neighbor search include locality-sensitive hashing, best bin first and balanced box-decomposition tree based search.\n\nNearest neighbor distance ratio does not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a \"query by example\" using the similarity between local features. More generally it is involved in several matching problems.\n\nFixed-radius near neighbors is the problem where one wants to efficiently find all points given in Euclidean space within a given fixed distance from a specified point. The distance is assumed to be fixed, but the query point is arbitrary.\n\nFor some applications (e.g. entropy estimation), we may have \"N\" data-points and wish to know which is the nearest neighbor \"for every one of those N points\". This could, of course, be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these \"N\" queries to produce a more efficient search. As a simple example: when we find the distance from point \"X\" to point \"Y\", that also tells us the distance from point \"Y\" to point \"X\", so the same calculation can be reused in two different queries.\n\nGiven a fixed dimension, a semi-definite positive norm (thereby including every L norm), and \"n\" points in this space, the nearest neighbour of every point can be found in \"O\"(\"n\" log \"n\") time and the \"m\" nearest neighbours of every point can be found in \"O\"(\"mn\" log \"n\") time.\n\n", "related": "\n- Ball tree\n- Closest pair of points problem\n- Cluster analysis\n- Content-based image retrieval\n- Curse of dimensionality\n- Digital signal processing\n- Dimension reduction\n- Fixed-radius near neighbors\n- Fourier analysis\n- Instance-based learning\n- \"k\"-nearest neighbor algorithm\n- Linear least squares\n- Locality sensitive hashing\n- MinHash\n- Multidimensional analysis\n- Nearest-neighbor interpolation\n- Neighbor joining\n- Principal component analysis\n- Range search\n- Similarity learning\n- Singular value decomposition\n- Sparse distributed memory\n- Statistical distance\n- Time series\n- Voronoi diagram\n- Wavelet\n\n\n- Nearest Neighbors and Similarity Search – a website dedicated to educational materials, software, literature, researchers, open problems and events related to NN searching. Maintained by Yury Lifshits\n- Similarity Search Wiki – a collection of links, people, ideas, keywords, papers, slides, code and data sets on nearest neighbours\n"}
{"id": "18475546", "url": "https://en.wikipedia.org/wiki?curid=18475546", "title": "Multivariate adaptive regression spline", "text": "Multivariate adaptive regression spline\n\nIn statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.\n\nThe term \"MARS\" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open-source implementations of MARS are called \"Earth\".\n\nThis section introduces MARS using a few examples. We start with a set of data: a matrix of input variables \"x\", and a vector of the observed responses \"y\", with a response for each row in \"x\". For example, the data could be:\n\nHere there is only one independent variable, so the \"x\" matrix is just a single column. Given these measurements, we would like to build a model which predicts the expected \"y\" for a given \"x\".\nA linear model for the above data is\n\nThe hat on the formula_2 indicates that formula_2 is estimated from the data. The figure on the right shows a plot of this function: \na line giving the predicted formula_2 versus \"x\", with the original values of \"y\" shown as red dots.\n\nThe data at the extremes of \"x\" indicates that the relationship between \"y\" and \"x\" may be non-linear (look at the red dots relative to the regression line at low and high values of \"x\"). We thus turn to MARS to automatically build a model taking into account non-linearities. MARS software constructs a model from the given \"x\" and \"y\" as follows\n\nThe figure on the right shows a plot of this function: the predicted formula_2 versus \"x\", with the original values of \"y\" once again shown as red dots. The predicted response is now a better fit to the original \"y\" values.\n\nMARS has automatically produced a kink in the predicted \"y\" to take into account non-linearity. The kink is produced by \"hinge functions\". The hinge functions are the expressions starting with formula_7 (where formula_8 is formula_9 if formula_10, else formula_11). Hinge functions are described in more detail below.\n\nIn this simple example, we can easily see from the plot that \"y\" has a non-linear relationship with \"x\" (and might perhaps guess that y varies with the square of \"x\"). However, in general there will be multiple independent variables, and the relationship between \"y\" and these variables will be unclear and not easily visible by plotting. We can use MARS to discover that non-linear relationship.\n\nAn example MARS expression with multiple variables is\n\nThis expression models air pollution (the ozone level) as a function of the temperature and a few other variables. Note that the last term in the formula (on the last line) incorporates an interaction between formula_13 and formula_14.\n\nThe figure on the right plots the predicted formula_15 as formula_16 and formula_14 vary, with the other variables fixed at their median values. The figure shows that wind does not affect the ozone level unless visibility is low. We see that MARS can build quite flexible regression surfaces by combining hinge functions.\n\nTo obtain the above expression, the MARS model building procedure automatically selects which variables to use (some variables are important, others not), the positions of the kinks in the hinge functions, and how the hinge functions are combined.\n\nMARS builds models of the form\n\nThe model is a weighted sum of basis functions\nformula_19.\nEach formula_20 is a constant coefficient.\nFor example, each line in the formula for ozone above is one basis function\nmultiplied by its coefficient.\n\nEach basis function formula_19 takes one of the following three forms:\n\n1) a constant 1. There is just one such term, the intercept.\nIn the ozone formula above, the intercept term is 5.2.\n\n2) a \"hinge\" function. A hinge function has the form formula_22 or formula_23. MARS automatically selects variables and values of those variables for knots of the hinge functions. Examples of such basis functions can be seen in the middle three lines of the ozone formula.\n\n3) a product of two or more hinge functions.\nThese basis functions can model interaction between two or more variables.\nAn example is the last line of the ozone formula.\n\nHinge functions are a key part of MARS models. A hinge function takes the form\nor \nwhere formula_26 is a constant, called the \"knot\".\nThe figure on the right shows a mirrored pair of hinge functions with a knot at 3.1.\n\nA hinge function is zero for part of its range, so can be used to partition the data into disjoint regions, each of which can be treated independently. Thus for example a mirrored pair of hinge functions in the expression\ncreates the piecewise linear graph shown for the simple MARS model in the previous section.\n\nOne might assume that only piecewise linear functions can be formed from hinge functions, but hinge functions can be multiplied together to form non-linear functions.\n\nHinge functions are also called ramp, hockey stick, or rectifier functions. Instead of the formula_7 notation used in this article, hinge functions are often represented by formula_29 where formula_30 means take the positive part.\n\nMARS builds a model in two phases:\nthe forward and the backward pass.\nThis two-stage approach is the same as that used by \nrecursive partitioning trees.\n\nMARS starts with a model which consists of just the intercept term\n(which is the mean of the response values).\n\nMARS then repeatedly adds basis function in pairs to the model. At each step it finds the pair of basis functions that gives the maximum reduction in sum-of-squares residual error (it is a greedy algorithm). The two basis functions in the pair are identical except that a different side of a mirrored hinge function is used for each function. Each new basis function consists of a term already in the model (which could perhaps be the intercept term) multiplied by a new hinge function. A hinge function is defined by a variable and a knot, so to add a new basis function, MARS must search over all combinations of the following:\n\n1) existing terms (called \"parent terms\" in this context)\n\n2) all variables (to select one for the new basis function)\n\n3) all values of each variable (for the knot of the new hinge function).\n\nTo calculate the coefficient of each term MARS applies a linear regression over the terms.\n\nThis process of adding terms continues until the change in residual error is too small to continue or until the maximum number of terms is reached. The maximum number of terms is specified by the user before model building starts.\n\nThe search at each step is done in a brute-force fashion, but a key aspect of MARS is that because of the nature of hinge functions the search can be done relatively quickly using a fast least-squares update technique. Actually, the search is not quite brute force. The search can be sped up with a heuristic that reduces the number of parent terms to consider at each step (\"Fast MARS\").\n\nThe forward pass usually builds an overfit model. (An overfit model has a good fit to the data used to build the model but will not generalize well to new data.) To build a model with better generalization ability, the backward pass prunes the model. It removes terms one by one, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the GCV criterion described below.\n\nThe backward pass has an advantage over the forward pass: at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms.\n\nThe forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for formula_2 in the first MARS example above; there are no complete pairs retained in the ozone example.\n\nThe backward pass uses generalized cross validation (GCV) to compare the performance of model subsets in order to choose the best subset: lower values of GCV are better. The GCV is a form of regularization: it trades off goodness-of-fit against model complexity.\n\nThe formula for the GCV is\n\nwhere RSS is the residual sum-of-squares measured on the training data and \"N\" is the number of observations (the number of rows in the x matrix).\n\nThe \"EffectiveNumberOfParameters\" is defined in\nthe MARS context as\n\nwhere penalty is about 2 or 3 (the MARS software allows the user to preset penalty).\n\nNote that \n\nis the number of hinge-function knots, so the formula penalizes the addition of knots. Thus the GCV formula adjusts (i.e. increases) the training RSS to take into account the flexibility of the model. We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data.\n\nGeneralized cross-validation is so named because it uses a formula to approximate the error that would be determined by leave-one-out validation. It is just an approximation but works well in practice. GCVs were introduced by Craven and Wahba and extended by Friedman for MARS.\n\nOne constraint has already been mentioned: the user\ncan specify the maximum number of terms in the forward pass.\n\nA further constraint can be placed on the forward pass\nby specifying a maximum allowable degree of interaction.\nTypically only one or two degrees of interaction are allowed,\nbut higher degrees can be used when the data warrants it.\nThe maximum degree of interaction in the first MARS example\nabove is one (i.e. no interactions or an \"additive model\"); \nin the ozone example it is two.\n\nOther constraints on the forward pass are possible.\nFor example, the user can specify that interactions are allowed \nonly for certain input variables.\nSuch constraints could make sense because of knowledge\nof the process that generated the data.\n\nNo regression modeling technique is best for all situations.\nThe guidelines below are intended to give an idea of the pros and cons of MARS, \nbut there will be exceptions to the guidelines.\nIt is useful to compare MARS to recursive partitioning and this is done below.\n(Recursive partitioning is also commonly called \"regression trees\",\n\"decision trees\", or CART;\nsee the recursive partitioning article for details).\n\n- MARS models are more flexible than linear regression models.\n- MARS models are simple to understand and interpret. Compare the equation for ozone concentration above to, say, the innards of a trained neural network or a random forest.\n- MARS can handle both continuous and categorical data. MARS tends to be better than recursive partitioning for numeric data because hinges are more appropriate for numeric variables than the piecewise constant segmentation used by recursive partitioning.\n- Building MARS models often requires little or no data preparation. The hinge functions automatically partition the input data, so the effect of outliers is contained. In this respect MARS is similar to recursive partitioning which also partitions the data into disjoint regions, although using a different method. (Nevertheless, as with most statistical modeling techniques, known outliers should be considered for removal before training a MARS model.)\n- MARS (like recursive partitioning) does automatic variable selection (meaning it includes important variables in the model and excludes unimportant ones). However, there can be some arbitrariness in the selection, especially when there are correlated predictors, and this can affect interpretability\n- MARS models tend to have a good bias-variance trade-off. The models are flexible enough to model non-linearity and variable interactions (thus MARS models have fairly low bias), yet the constrained form of MARS basis functions prevents too much flexibility (thus MARS models have fairly low variance).\n- MARS is suitable for handling fairly large datasets. It is a routine matter to build a MARS model from an input matrix with, say, 100 predictors and 10 observations. Such a model can be built in about a minute on a 1 GHz machine, assuming the maximum degree of interaction of MARS terms is limited to one (i.e. additive terms only). A degree two model with the same data on the same 1 GHz machine takes longer—about 12 minutes. Be aware that these times are highly data dependent. Recursive partitioning is much faster than MARS.\n- With MARS models, as with any non-parametric regression, parameter confidence intervals and other checks on the model cannot be calculated directly (unlike linear regression models). Cross-validation and related techniques must be used for validating the model instead.\n- MARS models do not give as good fits as boosted trees, but can be built much more quickly and are more interpretable. (An 'interpretable' model is in a form that makes it clear what the effect of each predictor is.)\n- The codice_1, codice_2, and codice_3 implementations do not allow missing values in predictors, but free implementations of regression trees (such as codice_4 and codice_5) do allow missing values using a technique called surrogate splits.\n- MARS models can make predictions quickly. The prediction function simply has to evaluate the MARS model formula. Compare that to making a prediction with say a Support Vector Machine, where every variable has to be multiplied by the corresponding element of every support vector. That can be a slow process if there are many variables and many support vectors.\n- The resulting fitted function is not smooth (not differentiable along hinges).\n\n- Generalized linear models (GLMs) can be incorporated into MARS models by applying a link function after the MARS model is built. Thus, for example, MARS models can incorporate logistic regression to predict probabilities.\n- Non-linear regression is used when the underlying form of the function is known and regression is used only to estimate the parameters of that function. MARS, on the other hand, estimates the functions themselves, albeit with severe constraints on the nature of the functions. (These constraints are necessary because discovering a model from the data is an inverse problem that is not well-posed without constraints on the model.)\n- Recursive partitioning (commonly called CART). MARS can be seen as a generalization of recursive partitioning that allows the model to better handle numerical (i.e. non-categorical) data.\n- Generalized additive models. From the user's perspective GAMs are similar to MARS but (a) fit smooth loess or polynomial splines instead of MARS basis functions, and (b) do not automatically model variable interactions. The fitting method used internally by GAMs is very different from that of MARS. For models that do not require automatic discovery of variable interactions GAMs often compete favorably with MARS.\n- TSMARS. Time Series Mars is the term used when MARS models are applied in a time series context. Typically in this set up the predictors are the lagged time series values resulting in autoregressive spline models. These models and extensions to include moving average spline models are described in \"Univariate Time Series Modelling and Forecasting using TSMARS: A study of threshold time series autoregressive, seasonal and moving average models using TSMARS\".\n\n", "related": "\n- Linear regression\n- Local regression\n- Rational function modeling\n- Segmented regression\n- Spline interpolation\n- Spline regression\n\n- Hastie T., Tibshirani R., and Friedman J.H. (2009) \"The Elements of Statistical Learning\", 2nd edition. Springer, (has a section on MARS)\n- Faraway J. (2005) \"Extending the Linear Model with R\", CRC, (has an example using MARS with R)\n- Heping Zhang and Burton H. Singer (2010) \"Recursive Partitioning and Applications\", 2nd edition. Springer, (has a chapter on MARS and discusses some tweaks to the algorithm)\n- Denison D.G.T., Holmes C.C., Mallick B.K., and Smith A.F.M. (2004) \"Bayesian Methods for Nonlinear Classification and Regression\", Wiley,\n- Berk R.A. (2008) \"Statistical learning from a regression perspective\", Springer,\n\nSeveral free and commercial software packages are available for fitting MARS-type models.\n\n- Free software:\n- R packages:\n- codice_1 function in the codice_1 package\n- codice_8 function in the codice_2 package\n- codice_10 function in the codice_3 package. Not Friedman's MARS.\n- Matlab code:\n- ARESLab: Adaptive Regression Splines toolbox for Matlab\n- Python\n- Earth – Multivariate adaptive regression splines\n- py-earth\n\n- Commercial software:\n- MARS from Salford Systems. Based on Friedman's implementation.\n- STATISTICA Data Miner from StatSoft\n- ADAPTIVEREG from SAS.\n"}
{"id": "19667111", "url": "https://en.wikipedia.org/wiki?curid=19667111", "title": "Statistical relational learning", "text": "Statistical relational learning\n\nStatistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming. Significant contributions to the field have been made since the late 1990s.\n\nAs is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include \"statistical relational learning and reasoning\" (emphasizing the importance of reasoning) and \"first-order probabilistic languages\" (emphasizing the key properties of the languages with which models are represented).\n\nA number of canonical tasks are associated with statistical relational learning, the most common ones being\n\n- collective classification, i.e. the (simultaneous) prediction of the class of several objects given objects' attributes and their relations\n- link prediction, i.e. predicting whether or not two or more objects are related\n- link-based clustering, i.e. the grouping of similar objects, where similarity is determined according to the links of an object, and the related task of collaborative filtering, i.e. the filtering for information that is relevant to an entity (where a piece of information is considered relevant to an entity if it is known to be relevant to a similar entity).\n- social network modelling\n- object identification/entity resolution/record linkage, i.e. the identification of equivalent entries in two or more separate databases/datasets\n\nOne of the fundamental design goals of the representation formalisms developed in SRL is to abstract away from concrete entities and to represent instead general principles that are intended to be universally applicable. Since there are countless ways in which such principles can be represented, many representation formalisms have been proposed in recent years. In the following, some of the more common ones are listed in alphabetical order:\n\n- Bayesian logic program\n- BLOG model\n- Logic programs with annotated disjunctions\n- Markov logic networks\n- Multi-entity Bayesian network\n- Probabilistic relational model – a Probabilistic Relational Model (PRM) is the counterpart of a Bayesian network in statistical relational learning.\n- Probabilistic soft logic\n- Recursive random field\n- Relational Bayesian network\n- Relational dependency network\n- Relational Markov network\n- Relational Kalman filtering\n\n", "related": "\n- Association rule learning\n- Formal concept analysis\n- Fuzzy logic\n- Grammar induction\n\n- Brian Milch, and Stuart J. Russell: \"First-Order Probabilistic Languages: Into the Unknown\", Inductive Logic Programming, volume 4455 of Lecture Notes in Computer Science, page 10–24. Springer, 2006\n- Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth: \"A Survey of First-Order Probabilistic Models\", Innovations in Bayesian Networks, volume 156 of Studies in Computational Intelligence, Springer, 2008\n- Hassan Khosravi and Bahareh Bina: \"A Survey on Statistical Relational Learning\", Advances in Artificial Intelligence, Lecture Notes in Computer Science, Volume 6085/2010, 256–268, Springer, 2010\n- Ryan A. Rossi, Luke K. McDowell, David W. Aha, and Jennifer Neville: \"Transforming Graph Data for Statistical Relational Learning\", Journal of Artificial Intelligence Research (JAIR), Volume 45, page 363-441, 2012\n- Luc De Raedt, Kristian Kersting, Sriraam Natarajan and David Poole, \"Statistical Relational Artificial Intelligence: Logic, Probability, and Computation\", Synthesis Lectures on Artificial Intelligence and Machine Learning\" March 2016 .\n"}
{"id": "32402755", "url": "https://en.wikipedia.org/wiki?curid=32402755", "title": "Hyperparameter (machine learning)", "text": "Hyperparameter (machine learning)\n\nIn machine learning, a hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training.\n\nHyperparameters can be classified as model hyperparameters, that cannot be inferred while fitting the machine to the training set because they refer to the model selection task, or algorithm hyperparameters, that in principle have no influence on the performance of the model but affect the speed and quality of the learning process. An example of the first type is the topology and size of a neural network. An example of the second type is learning rate or mini-batch size.\n\nDifferent model training algorithms require different hyperparameters, some simple algorithms (such as ordinary least squares regression) require none. Given these hyperparameters, the training algorithm learns the parameters from the data. For instance, LASSO is an algorithm that adds a regularization hyperparameter to ordinary least squares regression, which has to be set before estimating the parameters through the training algorithm.\n\nThe time required to train and test a model can depend upon the choice of its hyperparameters. A hyperparameter is usually of continuous or integer type, leading to mixed-type optimization problems. The existence of some hyperparameters is conditional upon the value of others, e.g. the size of each hidden layer in a neural network can be conditional upon the number of layers.\n\nUsually, but not always, hyperparameters cannot be learned using well known gradient based methods (such as gradient descent, LBFGS) - which are commonly employed to learn parameters. These hyperparameters are those parameters describing a model representation that cannot be learned by common optimization methods but nonetheless affect the loss function. An example would be the tolerance hyperparameter for errors in support vector machines.\n\nSometimes, hyperparameters cannot be learned from the training data because they aggressively increase the capacity of a model and can push the loss function to a bad minimum - overfitting to, and picking up noise, in the data - as opposed to correctly mapping the richness of the structure in the data. For example - if we treat the degree of a polynomial equation fitting a regression model as a trainable parameter - this would just raise the degree up until the model perfectly fit the data, giving small training error - but bad generalization performance.\n\nMost performance variation can be attributed to just a few hyperparameters. The tunability of an algorithm, hyperparameter, or interacting hyperparameters is a measure of how much performance can be gained by tuning it. For an LSTM, while the learning rate followed by the network size are its most crucial hyperparameters, batching and momentum have no significant effect on its performance.\n\nAlthough some research has advocated the use of mini-batch sizes in the thousands, other work has found the best performance with mini-batch sizes between 2 and 32.\n\nAn inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance. Methods that are not robust to simple changes in hyperparameters, random seeds, or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification.\n\nReinforcement learning algorithms, in particular, require measuring their performance over a large number of random seeds, and also measuring their sensitivity to choices of hyperparameters. Their evaluation with a small number of random seeds does not capture performance adequately due to high variance. Some reinforcement learning methods, e.g. DDPG (Deep Deterministic Policy Gradient), are more sensitive to hyperparameter choices than others.\n\nHyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given test data. The objective function takes a tuple of hyperparameters and returns the associated loss.\n\nApart from tuning hyperparameters, machine learning involves storing and organizing the parameters and results, and making sure they are reproducible. In the absence of a robust infrastructure for this purpose, research code often evolves quickly and compromises essential aspects like bookkeeping and reproducibility. Online collaboration platforms for machine learning go further by allowing scientists to automatically share, organize and discuss experiments, data, and algorithms.\n\nA number of relevant services and open source software exist:\n\n", "related": "\n- Hyper-heuristic\n- Replication crisis\n"}
{"id": "13750669", "url": "https://en.wikipedia.org/wiki?curid=13750669", "title": "Elastic matching", "text": "Elastic matching\n\nElastic matching is one of the pattern recognition techniques in computer science. Elastic matching (EM) is also known as deformable template, flexible matching, or nonlinear template matching.\n\nElastic matching can be defined as an optimization problem of two-dimensional warping specifying corresponding pixels between subjected images.\n\n", "related": "\n- Dynamic time warping\n"}
{"id": "1331441", "url": "https://en.wikipedia.org/wiki?curid=1331441", "title": "Document classification", "text": "Document classification\n\nDocument classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done \"manually\" (or \"intellectually\") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.\n\nThe documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.\n\nDocuments may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.\n\nContent-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document.\n\nRequest-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230).\n\nRequest-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library. It is probably better, however, to understand request-oriented classification as \"policy-based classification\": The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.\n\nSometimes a distinction is made between assigning documents to classes (\"classification\") versus assigning subjects to documents (\"subject indexing\") but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. \"These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004; Broughton, 2008; Riesthuis & Bliedung, 1991). Therefore, is the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).\n\nAutomatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.\n\nAutomatic document classification techniques include:\n- Expectation maximization (EM)\n- Naive Bayes classifier\n- tf–idf\n- Instantaneously trained neural networks\n- Latent semantic indexing\n- Support vector machines (SVM)\n- Artificial neural network\n- K-nearest neighbour algorithms\n- Decision trees such as ID3 or C4.5\n- Concept Mining\n- Rough set-based classifier\n- Soft set-based classifier\n- Multiple-instance learning\n- Natural language processing approaches\n\nClassification techniques have been applied to\n- spam filtering, a process which tries to discern E-mail spam messages from legitimate emails\n- email routing, sending an email sent to a general address to a specific address or mailbox depending on topic\n- language identification, automatically determining the language of a text\n- genre classification, automatically determining the genre of a text\n- readability assessment, automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger text simplification system\n- sentiment analysis, determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.\n- health-related classification using social media in public health surveillance\n- article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.\n\n", "related": "\n- Categorization\n- Classification (disambiguation)\n- Compound term processing\n- Concept-based image indexing\n- Content-based image retrieval\n- Document\n- Supervised learning, unsupervised learning\n- Document retrieval\n- Document clustering\n- Information retrieval\n- Knowledge organization\n- Knowledge Organization System\n- Library classification\n- Machine learning\n- Native Language Identification\n- String metrics\n- Subject (documents)\n- Subject indexing\n- Text mining, web mining, concept mining\n- Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, 2002.\n- Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, 2010.\n\n- Introduction to document classification\n- Bibliography on Automated Text Categorization\n- Bibliography on Query Classification\n- Text Classification analysis page\n- Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python (available online)\n- TechTC - Technion Repository of Text Categorization Datasets\n- David D. Lewis's Datasets\n- BioCreative III ACT (article classification task) dataset\n"}
{"id": "30928751", "url": "https://en.wikipedia.org/wiki?curid=30928751", "title": "Multilinear principal component analysis", "text": "Multilinear principal component analysis\n\nMultilinear principal component analysis (MPCA) is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a \"data tensor\". N-way arrays may be decomposed, analyzed, or modeled by \n- linear tensor models such as CANDECOMP/Parafac, or\n- multilinear tensor models, such as multilinear principal component analysis (MPCA), or multilinear independent component analysis (MICA), etc.\nThe origin of MPCA can be traced back to the Tucker decomposition and Peter Kroonenberg's \"M-mode PCA/3-mode PCA\" work. In 2000, De Lathauwer et al. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled \"Multilinear Singular Value Decomposition\", (HOSVD) and in their paper \"On the Best Rank-1 and Rank-(R, R, ..., R ) Approximation of Higher-order Tensors\".\n\nCirca 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis. The power of the tensor framework was showcased by analyzing human motion joint angles, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures\n(CVPR 2001, ICPR 2002), face recognition – TensorFaces,\n(ECCV 2002, CVPR 2003, etc.) and computer graphics – TensorTextures (Siggraph 2004).\n\nHistorically, MPCA has been referred to as \"M-mode PCA\", a terminology which was coined by Peter Kroonenberg in 1980. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis.\n\nMultilinear PCA may be applied to compute the causal factors of data formation, or as signal processing tool on data tensors whose individual observation have either been vectorized, or whose observations are treated as matrix and concatenated into a data tensor.\n\nMPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD. This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis).\n\nThe MPCA solution follows the alternating least square (ALS) approach. It is iterative in nature.\nAs in PCA, MPCA works on centered data. Centering is a little more complicated for tensors, and it is problem dependent.\n\nMPCA features: Supervised MPCA feature selection is used in object recognition while unsupervised MPCA feature selection is employed in visualization task.\n\nVarious extensions of MPCA have been developed:\n- Uncorrelated MPCA (UMPCA) In contrast, the uncorrelated MPCA (UMPCA) generates uncorrelated multilinear features.\n- Boosting+MPCA\n- Non-negative MPCA (NMPCA)\n- Robust MPCA (RMPCA)\n- Multi-Tensor Factorization, that also finds the number of components automatically (MTF)\n\n- \"Matlab code\": MPCA.\n- \"Matlab code\": UMPCA (including data).\n- \"R code:\" MTF\n", "related": "NONE"}
{"id": "9732182", "url": "https://en.wikipedia.org/wiki?curid=9732182", "title": "Base rate", "text": "Base rate\n\nIn probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were \"medical professionals\", and 99% of the public were \"not\" \"medical professionals\", then the base rate of medical professionals is simply 1%.\nIn the sciences, including medicine, the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. \"1000 people... out of how many?\") is available. Note that controls may likewise offer further information for comparison; maybe the control groups, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' makes things worse, despite that initial proud claim about 1000 people.\n\nThe normative method for integrating base rates (prior probabilities) and featural evidence (likelihoods) is given by Bayes' rule.\n\nA Large number of psychological studies have examined a phenomenon called base-rate neglect\" or \"base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner. Mathematician Keith Devlin provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% reliable. He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a 'false positive' for 20% of people - who do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor, \"Out of the number of people who test positive (this is the base rate group that we care about), how many have the cancer?\" In assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor's coat and stethoscope, and prescribing medication, we have evidence which may allow us to conclude that the probability of this \"particular\" individual being a \"medical professional\" is considerably greater than the category base rate of 1%.\n", "related": "NONE"}
{"id": "33762888", "url": "https://en.wikipedia.org/wiki?curid=33762888", "title": "Inferential theory of learning", "text": "Inferential theory of learning\n\nInferential Theory of Learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been continuously developed by Ryszard S. Michalski, starting in the 1980s. The first known publication of ITL was in 1983. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored. Stored information will later be used by the learner for future inferences. Inferences are split into multiple categories including conclusive, deduction, and induction. In order for an inference to be considered complete it was required that all categories must be taken into account. This is how the ITL varies from other machine learning theories like Computational Learning Theory and Statistical Learning Theory; which both use singular forms of inference.\n\nThe most relevant published usage of ITL was in scientific journal published in 2012 and used ITL as a way to describe how agent-based learning works. According to the journal \"The Inferential Theory of Learning (ITL) provides an elegant way of describing learning processes by agents\".\n\n- Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1983), \"Machine Learning: An Artificial Intelligence Approach\", Tioga Publishing Company, .\n- Ryszard S. Michalski, Jaime G. Carbonell, Tom M. Mitchell (1986), \"Machine Learning: An Artificial Intelligence Approach, Volume II\", Morgan Kaufmann, .\n- Yves Kodratoff, Ryszard S. Michalski (1990), \"Machine Learning: An Artificial Intelligence Approach, Volume III\", Morgan Kaufmann, .\n- Ryszard S. Michalski, George Tecuci (1994), \"Machine Learning: A Multistrategy Approach\", Volume IV, Morgan Kaufmann, .\n- Naidenova, X. (Ed.),(2009), Machine Learning Methods for Commonsense Reasoning Processes: Interactive Models: Interactive Models, IGI Global.\n", "related": "NONE"}
{"id": "34042707", "url": "https://en.wikipedia.org/wiki?curid=34042707", "title": "Coupled pattern learner", "text": "Coupled pattern learner\n\nCoupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.\n\nSemi-supervised learning approaches using a small number of labeled examples with many unlabeled examples are usually unreliable as they produce an internally consistent, but incorrect set of extractions. CPL solves this problem by simultaneously learning classifiers for many different categories and relations in the presence of an ontology defining constraints that couple the training of these classifiers. It was introduced by Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr. and Tom M. Mitchell in 2009.\n\nCPL is an approach to semi-supervised learning that yields more accurate results by coupling the training of many information extractors. Basic idea behind CPL is that semi-supervised training of a single type of extractor such as ‘coach’ is much more difficult than simultaneously training many extractors that cover a variety of inter-related entity and relation types. Using prior knowledge about the relationships between these different entities and relations CPL makes unlabeled data as a useful constraint during training. For e.g., ‘coach(x)’ implies ‘person(x)’ and ‘not sport(x)’.\n\nCPL primarily relies on the notion of coupling the learning of multiple functions so as to constrain the semi-supervised learning problem. CPL constrains the learned function in two ways.\n1. Sharing among same-arity predicates according to logical relations\n2. Relation argument type-checking\n\nEach predicate P in the ontology has a list of other same-arity predicates with which P is mutually exclusive. If A is mutually exclusive with predicate B, A’s positive instances and patterns become negative instances and negative patterns for B. For example, if ‘city’, having an instance ‘Boston’ and a pattern ‘mayor of arg1’, is mutually exclusive with ‘scientist’, then ‘Boston’ and ‘mayor of arg1’ will become a negative instance and a negative pattern respectively for ‘scientist.’ Further, Some categories are declared to be a subset of another category. For e.g., ‘athlete’ is a subset of ‘person’.\n\nThis is a type checking information used to couple the learning of relations and categories. For example, the arguments of the ‘ceoOf’ relation are declared to be of the categories ‘person’ and ‘company’. CPL does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classified as belonging to the correct argument types.\n\nFollowing is a quick summary of the CPL algorithm. \n\nA large corpus of Part-Of-Speech tagged sentences and an initial ontology with predefined categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories.\n\nCPL finds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus. CPL extracts,\n- Category Instances\n- Category Patterns\n- Relation Instances\n- Relation Patterns\n\nCandidate instances and patterns are filtered to maintain high precision, and to avoid extremely specific patterns. An instance is only considered for assessment if it co-occurs with at least two promoted patterns in the text corpus, and if its co-occurrence count with all promoted patterns is at least three times greater than its co-occurrence count with negative patterns.\n\nCPL ranks candidate instances using the number of promoted patterns that they co-occur with so that candidates that occur with more patterns are ranked higher. Patterns are ranked using an estimate of the precision of each pattern.\n\nCPL ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate. Instances and patterns are only promoted if they co-occur with at least two promoted patterns or instances, respectively.\n\nMeta-Bootstrap Learner (MBL) was also proposed by the authors of CPL in. Meta-Bootstrap learner couples the training of multiple extraction techniques with a multi-view constraint, which requires the extractors to agree. It makes addition of coupling constraints on top of existing extraction algorithms, while treating them as black boxes, feasible. MBL assumes that the errors made by different extraction techniques are independent. Following is a quick summary of MBL.\n\nSubordinate algorithms used with MBL do not promote any instance on their own, they report the evidence about each candidate to MBL and MBL is responsible for promoting instances.\n\nIn their paper authors have presented results showing the potential of CPL to contribute new facts to existing repository of semantic knowledge, Freebase \n\n", "related": "\n- Co-training\n- Never-Ending Language Learning\n\n"}
{"id": "34061548", "url": "https://en.wikipedia.org/wiki?curid=34061548", "title": "Feature scaling", "text": "Feature scaling\n\nFeature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n\nSince the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n\nAnother reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.\n\nAlso known as min-max scaling or min-max normalization, is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as:\n\nformula_1\n\nwhere formula_2 is an original value, formula_3 is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).\n\nTo rescale a range between an arbitrary set of values [a, b], the formula becomes:\n\nformula_4\n\nwhere formula_5 are the min-max values.\n\nformula_6\n\nwhere formula_2 is an original value, formula_3 is the normalized value. There is another form of the means normalization which is when we divide by the standard deviation which is also called standardization.\n\nIn machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and artificial neural networks). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.\n\nformula_9\n\nWhere formula_2 is the original feature vector, formula_11 is the mean of that feature vector, and formula_12 is its standard deviation.\n\nAnother option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector: \n\nIn some applications (e.g., histogram features) it can be more practical to use the L norm (i.e., taxicab geometry) of the feature vector. This is especially important if in the following learning steps the scalar metric is used as a distance measure.\n\nIn stochastic gradient descent, feature scaling can sometimes improve the convergence speed of the algorithm . In support vector machines, it can reduce the time to find support vectors. Note that feature scaling changes the SVM result .\n\n", "related": "\n- fMLLR, Feature space Maximum Likelihood Linear Regression\n\n- Lecture by Andrew Ng on feature scaling\n"}
{"id": "34072838", "url": "https://en.wikipedia.org/wiki?curid=34072838", "title": "Preference learning", "text": "Preference learning\n\nPreference learning is a subfield in machine learning, which is a classification method based on observed preference information . In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.\n\nWhile the concept of preference learning has been emerged for some time in many fields such as economics, it's a relatively new topic in Artificial Intelligence research. Several workshops have been discussing preference learning and related topics in the past decade.\n\nThe main task in preference learning concerns problems in \"learning to rank\". According to different types of preference information observed, the tasks are categorized as three main problems in the book \"Preference Learning\":\n\nIn label ranking, the model has an instance space formula_1 and a finite set of labels formula_2. The preference information is given in the form formula_3 indicating instance formula_4 shows preference in formula_5 rather than formula_6. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.\n\nIt was observed some conventional classification problems can be generalized in the framework of label ranking problem: if a training instance formula_4 is labeled as class formula_5, it implies that formula_9. In the multi-label case, formula_4 is associated with a set of labels formula_11 and thus the model can extract a set of preference information formula_12. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.\n\nInstance ranking also has the instance space formula_13 and label set formula_14. In this task, labels are defined to have a fixed order formula_15 and each instance formula_16 is associated with a label formula_17. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.\n\nObject ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form formula_18 and the model should find out a ranking order among instances.\n\nThere are two practical representations of the preference information formula_19. One is assigning formula_20 and formula_21 with two real numbers formula_22 and formula_23 respectively such that formula_24. Another one is assigning a binary value formula_25 for all pairs formula_26 denoting whether formula_19 or formula_28. Corresponding to these two different representations, there are two different techniques applied to the learning process.\n\nIf we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called utility function. For label ranking the mapping is a function formula_29 such that formula_30. For instance ranking and object ranking, the mapping is a function formula_31.\n\nFinding the utility function is a regression learning problem which is well developed in machine learning.\n\nThe binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz and Hüllermeier proposed this approach in label ranking problem. For object ranking, there is an early approach by Cohen et al.\n\nUsing preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.\n\nPreference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.\n\nAnother application of preference learning is recommender systems. Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.\n\n", "related": "\n- Learning to rank\n\n- Preference Learning site\n"}
{"id": "21985449", "url": "https://en.wikipedia.org/wiki?curid=21985449", "title": "Proactive learning", "text": "Proactive learning\n\nProactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.\n\n\"Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain a learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same).\"\n\n\"In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an \"oracle\" (if we generalize the term to mean any source of expert information) may be incorrect (fallible) \nwith a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation.\nSuch an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors.\"\n\nProactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint.\n", "related": "NONE"}
{"id": "387537", "url": "https://en.wikipedia.org/wiki?curid=387537", "title": "Computational learning theory", "text": "Computational learning theory\n\nIn computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.\n\nTheoretical results in machine learning mainly deal with a type of inductive learning called supervised learning. In supervised learning, an algorithm is given samples that are labeled in some useful way. For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible. The algorithm takes these previously labeled samples and uses them to induce a classifier. This classifier is a function that assigns labels to samples, including samples that have not been seen previously by the algorithm. The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.\n\nIn addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. In\ncomputational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time\ncomplexity results:\n\n- Positive resultsShowing that a certain class of functions is learnable in polynomial time.\n- Negative resultsShowing that certain classes cannot be learned in polynomial time.\n\nNegative results often rely on commonly believed, but yet unproven assumptions, such as:\n\n- Computational complexity – P ≠ NP (the P versus NP problem);\n- Cryptographic – One-way functions exist.\n\nThere are several different approaches to computational learning theory based on making different assumptions about the\ninference principles used to generalize from limited data. This includes different definitions of probability (see frequency probability, Bayesian probability) and different assumptions on the generation of samples. The different approaches include:\n\n- Exact learning, proposed by Dana Angluin;\n- Probably approximately correct learning (PAC learning), proposed by Leslie Valiant;\n- VC theory, proposed by Vladimir Vapnik and Alexey Chervonenkis;\n- Bayesian inference;\n- Algorithmic learning theory, from the work of E. Mark Gold;\n- Online machine learning, from the work of Nick Littlestone.\n\nWhile its primary goal is to understand learning abstractly, computational learning theory has led to the development of practical algorithms. For example, PAC theory inspired boosting, VC theory led to support vector machines, and Bayesian inference led to belief networks.\n\n", "related": "\n- Grammar induction\n- Information theory\n- Stability (learning theory)\n- Error Tolerance (PAC learning)\n\n- Angluin, D. 1992. Computational learning theory: Survey and selected bibliography. In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing (May 1992), pages 351–369. http://portal.acm.org/citation.cfm?id=129712.129746\n- D. Haussler. Probably approximately correct learning. In AAAI-90 Proceedings of the Eight National Conference on Artificial Intelligence, Boston, MA, pages 1101–1108. American Association for Artificial Intelligence, 1990. http://citeseer.ist.psu.edu/haussler90probably.html\n\n- V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, 16(2):264–280, 1971.\n\n- A. Dhagat and L. Hellerstein, \"PAC learning with irrelevant attributes\", in 'Proceedings of the IEEE Symp. on Foundation of Computer Science', 1994. http://citeseer.ist.psu.edu/dhagat94pac.html\n\n- Oded Goldreich, Dana Ron. \"On universal learning algorithms\". http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.47.2224\n\n- M. Kearns and Leslie Valiant. 1989. Cryptographic limitations on learning boolean formulae and finite automata. In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, pages 433–444, New York. ACM. http://citeseer.ist.psu.edu/kearns89cryptographic.html\n\n- Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197–227, 1990 http://citeseer.ist.psu.edu/schapire90strength.html\n\n- Blumer, A.; Ehrenfeucht, A.; Haussler, D.; Warmuth, M. K. \"Occam's razor\" Inf.Proc.Lett. 24, 377–380, 1987.\n- A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–865, 1989.\n\n- L. Valiant. A Theory of the Learnable. Communications of the ACM, 27(11):1134–1142, 1984.\n\n- Michael Kearns and Ming Li. Learning in the presence of malicious errors. SIAM Journal on Computing, 22(4):807–837, August 1993. http://citeseer.ist.psu.edu/kearns93learning.html\n- Kearns, M. (1993). Efficient noise-tolerant learning from statistical queries. In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, pages 392–401. http://citeseer.ist.psu.edu/kearns93efficient.html\n\n- D.Haussler, M.Kearns, N.Littlestone and M. Warmuth, Equivalence of models for polynomial learnability, Proc. 1st ACM Workshop on Computational Learning Theory, (1988) 42-55.\n\nA description of some of these publications is given at important publications in machine learning.\n\n- Basics of Bayesian inference\n"}
{"id": "33998310", "url": "https://en.wikipedia.org/wiki?curid=33998310", "title": "Mountain car problem", "text": "Mountain car problem\n\nMountain Car, a standard testing domain in Reinforcement Learning, is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a test bed in various Reinforcement Learning papers.\n\nThe mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables: position and velocity. For any given state (position and velocity) of the car, the agent is given the possibility of driving left, driving right, or not using the engine at all. In the standard version of the problem, the agent receives a negative reward at every time step when the goal is not reached; the agent has no information about the goal until an initial success.\n\nThe mountain car problem appeared first in Andrew Moore's PhD Thesis (1990). It was later more strictly defined in Singh and Sutton's Reinforcement Leaning paper with eligibility traces. The problem became more widely studied when Sutton and Barto added it to their book Reinforcement Learning: An Introduction (1998). Throughout the years many versions of the problem have been used, such as those which modify the reward function, termination condition, and/or the start state.\n\nQ-learning and similar techniques for mapping discrete states to discrete actions need to be extended to be able to deal with the continuous state space of the problem. Approaches often fall into one of two categories, state space discretization or function approximation.\n\nIn this approach, two continuous state variables are pushed into discrete states by bucketing each continuous variable into multiple discrete states. This approach works with properly tuned parameters but a disadvantage is information gathered from one state is not used to evaluate another state. Tile coding can be used to improve discretization and involves continuous variables mapping into sets of buckets offset from one another. Each step of training has a wider impact on the value function approximation because when the offset grids are summed, the information is diffused.\n\nFunction approximation is another way to solve the mountain car. By choosing a set of basis functions beforehand, or by generating them as the car drives, the agent can approximate the value function at each state. Unlike the step-wise version of the value function created with discretization, function approximation can more cleanly estimate the true smooth function of the mountain car domain.\n\nAn interesting aspect of the problem involves the delay of actual reward. The agent isn't able to learn about the goal until a successful completion. Given a naive approach for each trial the car can only backup the reward of the goal slightly. This is a problem for naive discretization because each discrete state will only be backed up once, taking a larger number of episodes to learn the problem. This problem can be alleviated via the mechanism of eligibility traces, which will automatically backup the reward given to states before, dramatically increasing the speed of learning. Eligibility traces can be viewed as a bridge from temporal difference learning methods to Monte Carlo methods.\n\nThe mountain car problem has undergone many iterations. This section will focus on the standard well defined version from Sutton (2008).\n\nTwo-dimensional continuous state space.\n\nformula_1\n\nformula_2\n\nOne-dimensional discrete action space.\n\nformula_3\n\nFor every time step:\n\nformula_4\n\nFor every time step:\n\nformula_5\n\nformula_6\n\nformula_7\n\nOptionally, many implementations include randomness in both parameters to show better generalized learning.\n\nformula_8\n\nformula_9\n\nEnd the simulation when:\n\nformula_10\n\nThere are many versions of the mountain car which deviate in different ways from the standard model. Variables that vary include but are not limited to changing the constants (gravity and steepness) of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agent's ability to learn in a different manner. An example is changing the reward to be equal to the distance from the goal, or changing the reward to zero everywhere and one at the goal. Additionally we can use a 3D mountain car with a 4D continuous state space.\n\n- C++ Mountain Car Software. Richard s. Sutton.\n- Java Mountain Car with support for RL Glue\n- Python, with good discussion (blog post - down page)\n\n- Mountain Car with Replacing Eligibility Traces\n- Gaussian Processes with Mountain Car\n", "related": "NONE"}
{"id": "33890474", "url": "https://en.wikipedia.org/wiki?curid=33890474", "title": "Leave-one-out error", "text": "Leave-one-out error\n\n- Leave-one-out cross-validation (CVloo) Stability An algorithm f has CVloo stability β with respect to the loss function V if the following holds:\n\nformula_1\n- Expected-to-leave-one-out error (formula_2) Stability An algorithm f has formula_2 stability if for each n there exists aformula_4 and a formula_5 such that:\n\nformula_6, with formula_4and formula_5 going to zero for formula_9\n\nX and Y ⊂ R being respectively an input and an output space, we consider a training set \n\nformula_10\nof size m in formula_11 drawn i.i.d. from an unknown distribution D. A learning algorithm is a function formula_12 from formula_13 into formula_14which maps a learning set S onto a function formula_15 from X to Y. To avoid complex notation, we consider only deterministic algorithms. It is also assumed that the algorithm formula_16 is symmetric with respect to S, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable which does not limit the interest of the results presented here.\n\nThe loss of an hypothesis f with respect to an example formula_17 is then defined as formula_18.\nThe empirical error of f is formula_19.\n\nThe true error of f is formula_20\n\nGiven a training set S of size m, we will build, for all i = 1...,m, modified training sets as follows:\n- By removing the i-th element\nformula_21\n- By replacing the i-th element\nformula_22\n\n- S. Mukherjee, P. Niyogi, T. Poggio, and R. M. Rifkin. Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Adv. Comput. Math., 25(1-3):161–193, 2006\n", "related": "NONE"}
{"id": "35887507", "url": "https://en.wikipedia.org/wiki?curid=35887507", "title": "Representer theorem", "text": "Representer theorem\n\nIn statistical learning theory, a representer theorem is any of several related results stating that a minimizer formula_1 of a regularized empirical risk functional defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.\n\nThe following Representer Theorem and its proof are due to Schölkopf, Herbrich, and Smola:\n\nTheorem: Consider a positive-definite real-valued kernel formula_2 on a non-empty set formula_3 with a corresponding reproducing kernel Hilbert space formula_4. Let there be given\n- a training sample formula_5,\n- a strictly increasing real-valued function formula_6, and\n- an arbitrary error function formula_7,\nwhich together define the following regularized empirical risk functional on formula_4:\n\nThen, any minimizer of the empirical risk\n\nadmits a representation of the form:\n\nwhere formula_12 for all formula_13.\n\nProof:\nDefine a mapping\n\n(so that formula_15 is itself a map formula_16). Since formula_17 is a reproducing kernel, then\n\nwhere formula_19 is the inner product on formula_4.\n\nGiven any formula_21, one can use orthogonal projection to decompose any formula_22 into a sum of two functions, one lying in formula_23, and the other lying in the orthogonal complement:\n\nwhere formula_25 for all formula_26.\n\nThe above orthogonal decomposition and the reproducing property together show that applying formula_27 to any training point formula_28 produces\n\nwhich we observe is independent of formula_30. Consequently, the value of the error function formula_31 in (*) is likewise independent of formula_30. For the second term (the regularization term), since formula_30 is orthogonal to formula_34 and formula_35 is strictly monotonic, we have\n\nTherefore setting formula_37 does not affect the first term of (*), while it strictly decreasing the second term. Consequently, any minimizer formula_1 in (*) must have formula_37, i.e., it must be of the form\n\nwhich is the desired result.\n\nThe Theorem stated above is a particular example of a family of results that are collectively referred to as \"representer theorems\"; here we describe several such.\n\nThe first statement of a representer theorem was due to Kimeldorf and Wahba for the special case in which\n\nfor formula_42. Schölkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function formula_43 of the Hilbert space norm.\n\nIt is possible to generalize further by augmenting the regularized empirical risk functional through the addition of unpenalized offset terms. For example, Schölkopf, Herbrich, and Smola also consider the minimization\n\ni.e., we consider functions of the form formula_45, where formula_22 and formula_47 is an unpenalized function lying in the span of a finite set of real-valued functions formula_48. Under the assumption that the formula_49 matrix formula_50 has rank formula_51, they show that the minimizer formula_52 in formula_53\nadmits a representation of the form\n\nwhere formula_55 and the formula_56 are all uniquely determined.\n\nThe conditions under which a representer theorem exists were investigated by Argyriou, Micchelli, and Pontil, who proved the following:\n\nTheorem: Let formula_3 be a nonempty set, formula_17 a positive-definite real-valued kernel on formula_59 with corresponding reproducing kernel Hilbert space formula_4, and let formula_61 be a differentiable regularization function. Then given a training sample formula_62 and an arbitrary error function formula_63, a minimizer\n\nof the regularized empirical risk admits a representation of the form\n\nwhere formula_12 for all formula_13, if and only if there exists a nondecreasing function formula_68 for which\n\nEffectively, this result provides a necessary and sufficient condition on a differentiable regularizer formula_70 under which the corresponding regularized empirical risk minimization formula_71 will have a representer theorem. In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have representer theorems.\n\nRepresenter theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem formula_71. In most interesting applications, the search domain formula_4 for the minimization will be an infinite-dimensional subspace of formula_74, and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers. In contrast, the representation of formula_75 afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal formula_76-dimensional vector of coefficients formula_77; formula_78 can then be obtained by applying any standard function minimization algorithm. Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice.\n\nThe following provides an example of how to solve for the minimizer whose existence is guaranteed by the representer theorem. This method works for any positive definite kernel formula_79, and allows us to transform a complicated (possibly infinite dimensional) optimization problem into a simple linear system that can be solved numerically. \n\nAssume that we are using a least squares error function \n\nformula_80 \n\nand a regularization function formula_81\nfor some formula_42. By the representer theorem, the minimizer\n\nformula_83\n\nhas the form\n\nformula_84\n\nfor some formula_85. Noting that\n\nformula_86\n\nwe see that formula_87 has the form\n\nformula_88\nwhere formula_89 and formula_90. This can be factored out and simplified to\nformula_91\nSince formula_92 is positive definite, there is indeed a single global minima for this expression. Let formula_93 and note that formula_94 is convex. Then formula_87, the global minima, can be solved by setting formula_96. Recalling that all positive definite matricies are invertible, we see that\n\nformula_97\n\nso the minimizer may be found via a linear solve.\n\n", "related": "\n- Mercer's theorem\n- Kernel methods\n\n"}
{"id": "23864280", "url": "https://en.wikipedia.org/wiki?curid=23864280", "title": "Parity learning", "text": "Parity learning\n\nParity learning is a problem in machine learning. An algorithm that solves this problem must find a function \"ƒ\", given some samples (\"x\", \"ƒ\"(\"x\")) and the assurance that \"ƒ\" computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.\n\nIn Learning Parity with Noise (LPN), the samples may contain some error. Instead of samples (\"x\", \"ƒ\"(\"x\")), the algorithm is provided with (\"x\", \"y\"), where for random boolean formula_1\n\nformula_2\n\nThe noisy version of the parity learning problem is conjectured to be hard\n\n", "related": "\n- Learning with errors\n\n- Avrim Blum, Adam Kalai, and Hal Wasserman, “Noise-tolerant learning, the parity problem, and the statistical query model,” J. ACM 50, no. 4 (2003): 506–519.\n- Adam Tauman Kalai, Yishay Mansour, and Elad Verbin, “On agnostic boosting and parity learning,” in Proceedings of the 40th annual ACM symposium on Theory of computing (Victoria, British Columbia, Canada: ACM, 2008), 629–638, http://portal.acm.org/citation.cfm?id=1374466.\n- Oded Regev, “On lattices, learning with errors, random linear codes, and cryptography,” in Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (Baltimore, MD, USA: ACM, 2005), 84–93, http://portal.acm.org/citation.cfm?id=1060590.1060603.\n"}
{"id": "1299404", "url": "https://en.wikipedia.org/wiki?curid=1299404", "title": "Feature (machine learning)", "text": "Feature (machine learning)\n\nIn machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon being observed. Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition.\nThe concept of \"feature\" is related to that of explanatory variable used in statistical techniques such as linear regression.\n\nA set of numeric features can be conveniently described by a feature vector.\nOne of the way of achieving binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights,\ncomparing the result with a threshold, and deciding the class based on the comparison.\nAlgorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches.\n\nIn character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.\n\nIn speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others.\n\nIn spam detection algorithms, features may include the presence or absence of certain email headers, \nthe email structure, the language, the frequency of specific terms, the grammatical correctness of the text.\n\nIn computer vision, there are a large number of possible features, such as edges and objects.\n\nIn pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression. Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.\n\nThe vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.\n\nHigher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as \"Age = 'Year of death' minus 'Year of birth' \". This process is referred to as feature construction. Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C) that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems. Applications include studies of disease and emotion recognition from speech.\n\nThe initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability.\n\nExtracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself.\n\n", "related": "\n- Covariate\n- Dimensionality reduction\n- Feature engineering\n- Hashing trick\n- Statistical classification\n- Explainable Artificial Intelligence\n"}
{"id": "36126852", "url": "https://en.wikipedia.org/wiki?curid=36126852", "title": "Feature hashing", "text": "Feature hashing\n\nIn machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array. This trick is often attributed to Weinberger et al., but there exists a much earlier description of this method published by John Moody in 1989.\n\nIn a typical document classification task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a bag of words (BOW) representation is constructed: the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) of each of the documents in both the training and test sets.\n\nMachine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a term-document matrix where each row is a single document, and each column is a single feature/word; the entry in such a matrix captures the frequency (or weight) of the 'th term of the \"vocabulary\" in document . (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.)\nTypically, these vectors are extremely sparse—according to Zipf's law.\n\nThe common approach is to construct, at learning time or prior to that, a \"dictionary\" representation of the vocabulary of the training set, and use that to map words to indices. Hash tables and tries are common candidates for dictionary implementation. E.g., the three documents\n\n- \"John likes to watch movies. \"\n- \"Mary likes movies too.\"\n- \"John also likes football.\"\n\ncan be converted, using the dictionary\n\nto the term-document matrix\n\nThe problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows. On the contrary, if the vocabulary is kept fixed and not increased with a growing training set, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter. This difficulty is why feature hashing has been tried for spam filtering at Yahoo! Research.\n\nNote that the hashing trick isn't limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features.\n\nInstead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function to the features (e.g., words), then using the hash values directly as feature indices and updating the resulting vector at those indices. Here, we assume that feature actually means feature vector. \n\nThus, if our feature vector is [\"cat\",\"dog\",\"cat\"] and hash function is formula_2 if formula_3 is \"cat\" and formula_4 if formula_3 is \"dog\". Let us take the output feature vector dimension (N) to be 4. Then output x will be [0,2,1,0].\nIt has been suggested that a second, single-bit output hash function be used to determine the sign of the update value, to counter the effect of hash collisions. If such a hash function is used, the algorithm becomes\nThe above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of (,) pairs and let the learning and prediction algorithms consume such streams; a linear model can then be implemented as a single hash table representing the coefficient vector.\n\nWhen a second hash function \"ξ\" is used to determine the sign of a feature's value, the expected mean of each column in the output array becomes zero because \"ξ\" causes some collisions to cancel out. E.g., suppose an input contains two symbolic features \"f\"₁ and \"f\"₂ that collide with each other, but not with any other features in the same input; then there are four possibilities which, if we make no assumptions about \"ξ\", have equal probability, as listed in the table on the right.\n\nIn this example, there is a 50% probability that the hash collision cancels out. Multiple hash functions can be used to further reduce the risk of collisions.\n\nFurthermore, if \"φ\" is the transformation implemented by a hashing trick with a sign hash \"ξ\" (i.e. \"φ\"(\"x\") is the feature vector produced for a sample \"x\"), then inner products in the hashed space are unbiased:\n\nwhere the expectation is taken over the hashing function \"φ\". It can be verified thatformula_7 is a positive semi-definite kernel.\n\nRecent work extends the hashing trick to supervised mappings from words to indices,\nwhich are explicitly learned to avoid collisions of important terms.\n\nGanchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function.\nWeinberger et al. applied their variant of hashing to the problem of spam filtering, formulating this as a multi-task learning problem where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up.\n\nImplementations of the hashing trick are present in:\n\n- Apache Mahout\n- Gensim\n- scikit-learn\n- sofia-ml\n- Vowpal Wabbit\n- Apache Spark\n- R\n\n", "related": "\n- Bloom filter\n- Count–min sketch\n- Heaps' law\n- Locality-sensitive hashing\n- MinHash\n\n- Hashing Representations for Machine Learning on John Langford's website\n- What is the \"hashing trick\"? - MetaOptimize Q+A\n"}
{"id": "28037054", "url": "https://en.wikipedia.org/wiki?curid=28037054", "title": "Large margin nearest neighbor", "text": "Large margin nearest neighbor\n\nLarge margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.\n\nThe goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a \"training\" data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.\n\nThe main intuition behind LMNN is to learn a pseudometric under which all data instances in the training set are surrounded by at least k instances that share the same class label. If this is achieved, the leave-one-out error (a special case of cross validation) is minimized. Let the training data consist of a data set formula_1, where the set of possible class categories is formula_2.\n\nThe algorithm learns a pseudometric of the type \nFor formula_4 to be well defined, the matrix formula_5 needs to be positive semi-definite. The Euclidean metric is a special case, where formula_5 is the identity matrix. This generalization is often (falsely) referred to as Mahalanobis metric.\n\nFigure 1 illustrates the effect of the metric under varying formula_5. The two circles show the set of points with equal distance to the center formula_8. In the Euclidean case this set is a circle, whereas under the modified (Mahalanobis) metric it becomes an ellipsoid.\n\nThe algorithm distinguishes between two types of special data points: \"target neighbors\" and \"impostors\".\n\nTarget neighbors are selected before learning. Each instance formula_8 has exactly formula_10 different target neighbors within formula_11, which all share the same class label formula_12. The target neighbors are the data points that \"should become\" nearest neighbors \"under the learned metric\". Let us denote the set of target neighbors for a data point formula_8 as formula_14.\n\nAn impostor of a data point formula_8 is another data point formula_16 with a different class label (i.e. formula_17) which is one of the nearest neighbors of formula_8. During learning the algorithm tries to minimize the number of impostors for all data instances in the training set.\n\nLarge margin nearest neighbors optimizes the matrix formula_5 with the help of semidefinite programming. The objective is twofold: For every data point formula_8, the \"target neighbors\" should be \"close\" and the \"impostors\" should be \"far away\". Figure 1 shows the effect of such an optimization on an illustrative example. The learned metric causes the input vector formula_8 to be surrounded by training instances of the same class. If it was a test point, it would be classified correctly under the formula_22 nearest neighbor rule.\n\nThe first optimization goal is achieved by minimizing the average distance between instances and their target neighbors\n\nThe second goal is achieved by penalizing distances to impostors formula_24 that are less than one unit further away than target neighbors formula_16 (and therefore pushing them out of the local neighborhood of formula_8). The resulting value to be minimized can be stated as:\n\nWith a hinge loss function formula_28, which ensures that impostor proximity is not penalized when outside the margin. The margin of exactly one unit fixes the scale of the matrix formula_29. Any alternative choice formula_30 would result in a rescaling of formula_29 by a factor of formula_32.\n\nThe final optimization problem becomes:\n\nThe hyperparameter formula_38 is some positive constant (typically set through cross-validation). Here the variables formula_39 (together with two types of constraints) replace the term in the cost function. They play a role similar to slack variables to absorb the extent of violations of the impostor constraints. The last constraint ensures that formula_5 is positive semi-definite. The optimization problem is an instance of semidefinite programming (SDP). Although SDPs tend to suffer from high computational complexity, this particular SDP instance can be solved very efficiently due to the underlying geometric properties of the problem. In particular, most impostor constraints are naturally satisfied and do not need to be enforced during runtime (i.e. the set of variables formula_39is sparse). A particularly well suited solver technique is the working set method, which keeps a small set of constraints that are actively enforced and monitors the remaining (likely satisfied) constraints only occasionally to ensure correctness.\n\nLMNN was extended to multiple local metrics in the 2008 paper. \nThis extension significantly improves the classification error, but involves a more expensive optimization problem. In their 2009 publication in the Journal of Machine Learning Research, Weinberger and Saul derive an efficient solver for the semi-definite program. It can learn a metric for the MNIST handwritten digit data set in several hours, involving billions of pairwise constraints. An open source Matlab implementation is freely available at the authors web page.\n\nKumal et al. extended the algorithm to incorporate local invariances to multivariate polynomial transformations and improved regularization.\n\n", "related": "\n- Similarity learning\n- Linear discriminant analysis\n- Learning vector quantization\n- Pseudometric space\n- Nearest neighbor search\n- Cluster analysis\n- Data classification\n- Data mining\n- Machine learning\n- Pattern recognition\n- Predictive analytics\n- Dimension reduction\n- Neighbourhood components analysis\n- Matlab Implementation\n- ICML 2010 Tutorial on Metric Learning\n"}
{"id": "1053303", "url": "https://en.wikipedia.org/wiki?curid=1053303", "title": "Statistical learning theory", "text": "Statistical learning theory\n\nStatistical learning theory is a framework for machine learning\ndrawing from the fields of statistics and functional analysis. Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.\n\nThe goals of learning are understanding and prediction. Learning falls into many categories, including supervised learning, unsupervised learning, online learning, and reinforcement learning. From the perspective of statistical learning theory, supervised learning is best understood. Supervised learning involves learning from a training set of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output, such that the learned function can be used to predict the output from future input.\n\nDepending on the type of output, supervised learning problems are either problems of regression or problems of classification. If the output takes a continuous range of values, it is a regression problem. Using Ohm's Law as an example, a regression could be performed with voltage as input and current as an output. The regression would find the functional relationship between voltage and current to be , such that\nClassification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In facial recognition, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture.\n\nAfter learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.\n\nTake formula_2 to be the vector space of all possible inputs, and formula_3 to be\nthe vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space formula_4, i.e. there exists some unknown formula_5. The training set is made up of formula_6 samples from this probability distribution, and is notated \nEvery formula_8 is an input vector from the training data, and formula_9\nis the output that corresponds to it.\n\nIn this formalism, the inference problem consists of finding a function formula_10 such that formula_11. Let formula_12 be a space of functions formula_10 called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let formula_14 be the loss function, a metric for the difference between the predicted value formula_15 and the actual value formula_16. The expected risk is defined to be\nThe target function, the best possible function formula_18 that can be\nchosen, is given by the formula_18 that satisfies\n\nBecause the probability distribution formula_21 is unknown, a\nproxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the empirical risk\nA learning algorithm that chooses the function formula_23 that minimizes\nthe empirical risk is called empirical risk minimization.\n\nThe choice of loss function is a determining factor on the function formula_23 that will be chosen by the learning algorithm. The loss function\nalso affects the convergence rate for an algorithm. It is important for the loss function to be convex.\n\nDifferent loss functions are used depending on whether the problem is\none of regression or one of classification.\n\nThe most common loss function for regression is the square loss function (also known as the L2-norm). This familiar loss function is used in Ordinary Least Squares regression. The form is:\n\nThe absolute value loss (also known as the L1-norm) is also sometimes used:\n\nIn some sense the 0-1 indicator function is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with formula_27, this is:\nwhere formula_29 is the Heaviside step function.\n\nIn machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.\n\nOverfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well. Regularization can solve the overfitting problem and give\nthe problem stability.\n\nRegularization can be accomplished by restricting the hypothesis space formula_12. A common example would be restricting formula_12 to linear functions: this can be seen as a reduction to the standard problem of linear regression. formula_12 could also be restricted to polynomial of degree formula_33, exponentials, or bounded functions on L1. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.\n\nOne example of regularization is Tikhonov regularization. This consists of minimizing\nwhere formula_35 is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.\n\n", "related": "\n- Reproducing kernel Hilbert spaces are a useful choice for formula_12.\n- Proximal gradient methods for learning\n"}
{"id": "126706", "url": "https://en.wikipedia.org/wiki?curid=126706", "title": "Pattern recognition", "text": "Pattern recognition\n\nPattern recognition is the automated recognition of patterns and regularities in data. Pattern recognition is closely related to artificial intelligence and machine learning, together with applications such as data mining and knowledge discovery in databases (KDD), and is often used interchangeably with these terms. However, these are distinguished: machine learning is one approach to pattern recognition, while other approaches include hand-crafted (not learned) rules or heuristics; and pattern recognition is one approach to artificial intelligence, while other approaches include symbolic artificial intelligence. A modern definition of pattern recognition is:\nThis article focuses on machine learning approaches to pattern recognition. Pattern recognition systems are in many cases trained from labeled \"training\" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning). Machine learning is strongly related to pattern recognition and originates from artificial intelligence. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.\n\nIn machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of \"classes\" (for example, determine whether a given email is \"spam\" or \"non-spam\"). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.\n\nPattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform \"most likely\" matching of the inputs, taking into account their statistical variation. This is opposed to \"pattern matching\" algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is not generally a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.\n\nPattern recognition is generally categorized according to the type of learning procedure used to generate the output value. \"Supervised learning\" assumes that a set of \"training data\" (the \"training set\") has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a \"model\" that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of \"simple\", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words,and the data to be labeled \"is\" the training data.\n\nNote that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as \"clustering\", based on the common perception of the task as involving no training data to speak of, and of grouping the input data into \"clusters\" based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different: For example, in community ecology, the term \"classification\" is used to refer to what is commonly known as \"clustering\".\n\nThe piece of input data for which an output value is generated is formally termed an \"instance\". The instance is formally described by a vector of \"features\", which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of \"male\" or \"female\", or a blood type of \"A\", \"B\", \"AB\" or \"O\"), ordinal (consisting of one of a set of ordered items, e.g., \"large\", \"medium\" or \"small\"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be \"discretized\" into groups (e.g., less than 5, between 5 and 10, or greater than 10).\n\nMany common pattern recognition algorithms are \"probabilistic\" in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a \"best\" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the \"N\"-best labels with associated probabilities, for some value of \"N\", instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), \"N\" may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:\n- They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.)\n- Correspondingly, they can \"abstain\" when the confidence of choosing any particular output is too low.\n- Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of \"error propagation\".\n\nFeature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given. The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of formula_1 features the powerset consisting of all formula_2 subsets of features need to be explored. The Branch-and-Bound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features formula_1. For a large-scale comparison of feature-selection algorithms see \n\nTechniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.\n\nFormally, the problem of supervised pattern recognition can be stated as follows: Given an unknown function formula_4 (the \"ground truth\") that maps input instances formula_5 to output labels formula_6, along with training data formula_7 assumed to represent accurate examples of the mapping, produce a function formula_8 that approximates as closely as possible the correct mapping formula_9. (For example, if the problem is filtering spam, then formula_10 is some representation of an email and formula_11 is either \"spam\" or \"non-spam\"). In order for this to be a well-defined problem, \"approximates as closely as possible\" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function or cost function that assigns a specific value to \"loss\" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of formula_12. In practice, neither the distribution of formula_12 nor the ground truth function formula_4 are known exactly, but can be computed only empirically by collecting a large number of samples of formula_12 and hand-labeling them using the correct value of formula_16 (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function formula_8 labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a \"typical\" test set.\n\nFor a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form\nwhere the feature vector input is formula_19, and the function \"f\" is typically parameterized by some parameters formula_20. In a discriminative approach to the problem, \"f\" is estimated directly. In a generative approach, however, the inverse probability formula_21 is instead estimated and combined with the prior probability formula_22 using Bayes' rule, as follows:\n\nWhen the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation:\n\nThe value of formula_25 is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability formula_26 on different values of formula_25. Mathematically:\n\nwhere formula_29 is the value used for formula_25 in the subsequent evaluation procedure, and formula_31, the posterior probability of formula_25, is given by\n\nIn the Bayesian approach to this problem, instead of choosing a single parameter vector formula_34, the probability of a given label for a new instance formula_19 is computed by integrating over all possible values of formula_25, weighted according to the posterior probability:\n\nThe first pattern classifier – the linear discriminant presented by Fisher – was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix. Also the probability of each class formula_22 is estimated from the collected dataset. Note that the usage of 'Bayes rule' in a pattern classifier does not make the classification approach Bayesian.\n\nBayesian statistics has its origin in Greek philosophy where a distinction was already made between the 'a priori' and the 'a posteriori' knowledge. Later Kant defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities formula_22 can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.\n\nProbabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.\n\nWithin medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.\nOther typical applications of pattern recognition techniques are automatic speech recognition, classification of text into several categories (e.g., spam/non-spam email messages), the automatic recognition of handwriting on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms. The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.\n\nOptical character recognition is a classic example of the application of a pattern classifier, see\nOCR-example.\nThe method of signing one's name was captured with stylus and overlay starting in 1990. The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..\n\nArtificial neural networks (neural net classifiers) and deep learning have many real-world applications in image processing, a few examples:\n- identification and authentication: e.g., license plate recognition, fingerprint analysis, face detection/verification;, and voice-based authentication.\n- medical diagnosis: e.g., screening for cervical cancer (Papnet), breast tumors or heart sounds;\n- defence: various navigation and guidance systems, target recognition systems, shape recognition technology etc.\n- mobility: advanced driver assistance systems, autonomous vehicle technology, etc.\n\nFor a discussion of the aforementioned applications of neural networks in image processing, see e.g.\n\nIn psychology, pattern recognition (making sense of and identifying objects) is closely related to perception, which explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. \nA template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified.\nFeature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.\n\nAlgorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.\n\nParametric:\n- Linear discriminant analysis\n- Quadratic discriminant analysis\n- Maximum entropy classifier (aka logistic regression, multinomial logistic regression): Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.)\nNonparametric:\n- Decision trees, decision lists\n- Kernel estimation and K-nearest-neighbor algorithms\n- Naive Bayes classifier\n- Neural networks (multi-layer perceptrons)\n- Perceptrons\n- Support vector machines\n- Gene expression programming\n\n- Categorical mixture models\n- Hierarchical clustering (agglomerative or divisive)\n- K-means clustering\n- Correlation clustering\n- Kernel principal component analysis (Kernel PCA)\n\n- Boosting (meta-algorithm)\n- Bootstrap aggregating (\"bagging\")\n- Ensemble averaging\n- Mixture of experts, hierarchical mixture of experts\n\n- Bayesian networks\n- Markov random fields\n\nUnsupervised:\n- Multilinear principal component analysis (MPCA)\n\nSupervised (?):\n- Kalman filters\n- Particle filters\n\nSupervised:\n- Gaussian process regression (kriging)\n- Linear regression and extensions\n- Neural networks and Deep learning methods\n\nUnsupervised:\n- Independent component analysis (ICA)\n- Principal components analysis (PCA)\n\nSupervised:\n- Conditional random fields (CRFs)\n- Hidden Markov models (HMMs)\n- Maximum entropy Markov models (MEMMs)\n- Recurrent neural networks (RNNs)\n\nUnsupervised:\n- Hidden Markov models (HMMs)\n- Dynamic time warping (DTW)\n\n", "related": "\n- Adaptive resonance theory\n- Black box\n- Cache language model\n- Compound term processing\n- Computer-aided diagnosis\n- Data mining\n- Deep Learning\n- Information theory\n- List of numerical analysis software\n- List of numerical libraries\n- Machine learning\n- Multilinear subspace learning\n- Neocognitron\n- Perception\n- Perceptual learning\n- Predictive analytics\n- Prior knowledge for pattern recognition\n- Sequence mining\n- Template matching\n- Contextual image classification\n- List of datasets for machine learning research\n\n- An introductory tutorial to classifiers (introducing the basic terms, with numeric example)\n\n- The International Association for Pattern Recognition\n- List of Pattern Recognition web sites\n- Journal of Pattern Recognition Research\n- Pattern Recognition Info\n- Pattern Recognition (Journal of the Pattern Recognition Society)\n- International Journal of Pattern Recognition and Artificial Intelligence\n- International Journal of Applied Pattern Recognition\n- Open Pattern Recognition Project, intended to be an open source platform for sharing algorithms of pattern recognition\n- Improved Fast Pattern Matching Improved Fast Pattern Matching\n"}
{"id": "35272263", "url": "https://en.wikipedia.org/wiki?curid=35272263", "title": "Linear predictor function", "text": "Linear predictor function\n\nIn statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as \"weights\".\n\nThe basic form of a linear predictor function formula_1 for data point \"i\" (consisting of \"p\" explanatory variables), for \"i\" = 1, ..., \"n\", is\n\nwhere formula_3, for \"k\" = 1, ..., \"p\", is the value of the \"k\"-th explanatory variable for data point \"i\", and formula_4 are the \"coefficients\" (regression coefficients, weights, etc.) indicating the relative effect of a particular \"explanatory variable\" on the \"outcome\".\n\nIt is common to write the predictor function in a more compact form as follows:\n- The coefficients \"β\", \"β\", ..., \"β\" are grouped into a single vector β of size \"p\" + 1.\n- For each data point \"i\", an additional explanatory pseudo-variable \"x\" is added, with a fixed value of 1, corresponding to the intercept coefficient \"β\".\n- The resulting explanatory variables \"x\"(= 1), \"x\", ..., \"x\" are then grouped into a single vector x of size \"p\" + 1.\n\nThis makes it possible to write the linear predictor function as follows:\n\nusing the notation for a dot product between two vectors.\n\nAn equivalent form using matrix notation is as follows:\n\nwhere formula_7 and formula_8 are assumed to be a \"(p+1)\"-by-1 column vectors, formula_9 is the matrix transpose of formula_7 (so formula_9 is a 1-by-\"(p+1)\" row vector), and formula_12 indicates matrix multiplication between the 1-by-\"(p+1)\" row vector and the \"(p+1)\"-by-1 column vector, producing a 1-by-1 matrix that is taken to be a scalar.\n\nAn example of the usage of a linear predictor function is in linear regression, where each data point is associated with a continuous outcome \"y\", and the relationship written\n\nwhere formula_14 is a \"disturbance term\" or \"error variable\" — an \"unobserved\" random variable that adds noise to the linear relationship between the dependent variable and predictor function.\n\nIn some models (standard linear regression, in particular), the equations for each of the data points \"i\" = 1, ..., \"n\" are stacked together and written in vector form as\n\nwhere\n\nThe matrix \"X\" is known as the design matrix and encodes all known information about the independent variables. The variables formula_14 are random variables, which in standard linear regression are distributed according to a standard normal distribution; they express the influence of any unknown factors on the outcome.\n\nThis makes it possible to find optimal coefficients through the method of least squares using simple matrix operations. In particular, the optimal coefficients formula_18 as estimated by least squares can be written as follows:\n\nThe matrix formula_20 is known as the Moore-Penrose pseudoinverse of \"X\". The use of the matrix inverse in this formula requires that \"X\" is of full rank, i.e. there is not perfect multicollinearity among different explanatory variables (i.e. no explanatory variable can be perfectly predicted from the others). In such cases, the singular value decomposition can be used to compute the pseudoinverse.\n\nAlthough the outcomes (dependent variables) to be predicted are assumed to be random variables, the explanatory variables themselves are usually not assumed to be random. Instead, they are assumed to be fixed values, and any random variables (e.g. the outcomes) are assumed to be conditional on them. As a result, the data analyst is free to transform the explanatory variables in arbitrary ways, including creating multiple copies of a given explanatory variable, each transformed using a different function. Other common techniques are to create new explanatory variables in the form of interaction variables by taking products of two (or sometimes more) existing explanatory variables.\n\nWhen a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as basis functions. An example is polynomial regression, which uses a linear predictor function to fit an arbitrary degree polynomial relationship (up to a given order) between two sets of data points (i.e. a single real-valued explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable. Mathematically, the form looks like this:\n\nIn this case, for each data point \"i\", a set of explanatory variables is created as follows:\n\nand then standard linear regression is run. The basis functions in this example would be\n\nThis example shows that a linear predictor function can actually be much more powerful than it first appears: It only really needs to be linear in the \"coefficients\". All sorts of non-linear functions of the explanatory variables can be fit by the model.\n\nThere is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a \"K\"-dimensional output value is likely to be treated as \"K\" separate scalar-output basis functions). An example of this is radial basis functions (RBF's), which compute some transformed version of the distance to some fixed point:\n\nAn example is the Gaussian RBF, which has the same functional form as the normal distribution:\n\nwhich drops off rapidly as the distance from c increases.\n\nA possible usage of RBF's is to create one for every observed data point. This means that the result of an RBF applied to a new data point will be close to 0 unless the new point is near to the point around which the RBF was applied. That is, the application of the radial basis functions will pick out the nearest point, and its regression coefficient will dominate. The result will be a form of nearest neighbor interpolation, where predictions are made by simply using the prediction of the nearest observed data point, possibly interpolating between multiple nearby data points when they are all similar distances away. This type of nearest neighbor method for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression: But in fact, the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression.\n\nIt is even possible to fit some functions that appear non-linear in the coefficients by transforming the coefficients into new coefficients that do appear linear. For example, a function of the form formula_26 for coefficients formula_27 could be transformed into the appropriate linear function by applying the substitutions formula_28 leading to formula_29 which is linear. Linear regression and similar techniques could be applied and will often still find the optimal coefficients, but their error estimates and such will be wrong.\n\nThe explanatory variables may be of any type: real-valued, binary, categorical, etc. The main distinction is between continuous variables (e.g. income, age, blood pressure, etc.) and discrete variables (e.g. sex, race, political party, etc.). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), i.e. separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning \"variable does have the given value\" and a 0 meaning \"variable does not have the given value\". For example, a four-way discrete variable of blood type with the possible values \"A, B, AB, O\" would be converted to separate two-way dummy variables, \"is-A, is-B, is-AB, is-O\", where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable.\n\nNote that, for \"K\" categories, not all \"K\" dummy variables are independent of each other. For example, in the above blood type example, only three of the four dummy variables are independent, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, it's really only necessary to encode three of the four possibilities as dummy variables, and in fact if all four possibilities are encoded, the overall model becomes non-identifiable. This causes problems for a number of methods, such as the simple closed-form solution used in linear regression. The solution is either to avoid such cases by eliminating one of the dummy variables, and/or introduce a regularization constraint (which necessitates a more powerful, typically iterative, method for finding the optimal coefficients).\n\n", "related": "\n- Linear model\n"}
{"id": "37697003", "url": "https://en.wikipedia.org/wiki?curid=37697003", "title": "Random indexing", "text": "Random indexing\n\nRandom indexing is a dimensionality reduction method and computational framework for distributional semantics, based on the insight that very-high-dimensional vector space model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) are encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately.\n\nThis is the original point of the random projection approach to dimension reduction first formulated as the Johnson–Lindenstrauss lemma, and locality-sensitive hashing has some of the same starting points. Random indexing, as used in representation of language, originates from the work of Pentti Kanerva on sparse distributed memory, and can be described as an incremental formulation of a random projection.\n\nIt can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces—i.e. L2 normed vector spaces. In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma.\n\nThe TopSig technique extends the random indexing model to produce bit vectors for comparison with the Hamming distance similarity function. It is used for improving the performance of information retrieval and document clustering. In a similar line of research, Random Manhattan Integer Indexing (RMII) is proposed for improving the performance of the methods that employ the Manhattan distance between text units. Many random indexing methods primarily generate similarity from co-occurrence of items in a corpus. Reflexive Random Indexing (RRI) generates similarity from co-occurrence and from shared occurrence with other items.\n\n- Zadeh Behrang Qasemi, Handschuh Siegfried. (2015) Random indexing explained with high probability, TSD.\n", "related": "NONE"}
{"id": "14003441", "url": "https://en.wikipedia.org/wiki?curid=14003441", "title": "Bag-of-words model", "text": "Bag-of-words model\n\nThe bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.\n\nThe bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n\nAn early reference to \"bag of words\" in a linguistic context can be found in Zellig Harris's 1954 article on \"Distributional Structure\".\n\nThe following models a text document using bag-of-words. Here are two simple text documents:\n(1) John likes to watch movies. Mary likes movies too.\n\n(2) Mary also likes to watch football games.\nBased on these two text documents, a list is constructed as follows for each document:\n\n\"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"likes\",\"movies\",\"too\"\n\n\"Mary\",\"also\",\"likes\",\"to\",\"watch\",\"football\",\"games\"\nRepresenting each bag-of-words as a JSON object, and attributing to the respective JavaScript variable:\n\nBoW1 = {\"John\":1,\"likes\":2,\"to\":1,\"watch\":1,\"movies\":2,\"Mary\":1,\"too\":1};\nBoW2 = {\"Mary\":1,\"also\":1,\"likes\":1,\"to\":1,\"watch\":1,\"football\":1,\"games\":1};\n\nEach key is the word, and each value is the number of occurrences of that word in the given text document.\n\nThe order of elements is free, so, for example codice_1 is also equivalent to \"BoW1\". It is also what we expect from a strict \"JSON object\" representation.\n\nNote: if another document is like a union of these two, \n(3) John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.\nits JavaScript representation will be:\n\nBoW3 = {\"John\":1,\"likes\":3,\"to\":2,\"watch\":2,\"movies\":2,\"Mary\":2,\"too\":1,\"also\":1,\"football\":1,\"games\":1};\nSo, as we see in the bag algebra, the \"union\" of two documents in the bags-of-words representation is, formally, the disjoint union, summing the multiplicities of each element.\n\nIn practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text. For the example above, we can construct the following two lists to record the term frequencies of all the distinct words (BoW1 and BoW2 ordered as in BoW3):\n\n(1) [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]\n(2) [0, 1, 1, 1, 0, 1, 0, 1, 1, 1]\nEach entry of the lists refers to the count of the corresponding entry in the list (this is also the histogram representation). For example, in the first list (which represents document 1), the first two entries are \"1,2\":\n\n- The first entry corresponds to the word \"John\" which is the first word in the list, and its value is \"1\" because \"John\" appears in the first document 1 time.\n- The second entry corresponds to the word \"likes\", which is the second word in the list, and its value is \"2\" because \"likes\" appears in the first document 2 times\n\nThis list (or vector) representation does not preserve the order of the words in the original sentences. This is just the main feature of the Bag-of-words model. This kind of representation has several successful applications, such as email filtering.\nHowever, term frequencies are not necessarily the best representation for the text. Common words like \"the\", \"a\", \"to\" are almost always the terms with highest frequency in the text. Thus, having a high raw count does not necessarily mean that the corresponding word is more important. To address this problem, one of the most popular ways to \"normalize\" the term frequencies is to weight a term by the inverse of document frequency, or tf–idf. Additionally, for the specific purpose of classification, supervised alternatives have been developed to account for the class label of a document. Lastly, binary (presence/absence or 1/0) weighting is used in place of frequencies for some problems (e.g., this option is implemented in the WEKA machine learning software system).\n\nThe Bag-of-words model is an orderless document representation — only the counts of words matter. For instance, in the above example \"John likes to watch movies. Mary likes movies too\", the bag-of-words representation will not reveal that the verb \"likes\" always follows a person's name in this text. As an alternative, the \"n\"-gram model can store this spatial information. Applying to the same example above, a bigram model will parse the text into the following units and store the term frequency of each unit as before.\n[\n\nConceptually, we can view bag-of-word model as a special case of the n-gram model, with n=1. For n>1 the model is named w-shingling (where \"w\" is equivalent to \"n\" denoting the number of grouped words). See language model for a more detailed discussion.\n\nA common alternative to using dictionaries is the hashing trick, where words are mapped directly to indices with a hashing function. Thus, no memory is required to store a dictionary. Hash collisions are typically dealt via freed-up memory to increase the number of hash buckets. In practice, hashing simplifies the implementation of bag-of-words models and improves scalability.\n\nIn Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail (\"ham\"). \nImagine there are two literal bags full of words. One bag is filled with words found in spam messages, and the other with words found in legitimate e-mail. While any given word is likely to be somewhere in both bags, the \"spam\" bag will contain spam-related words such as \"stock\", \"Viagra\", and \"buy\" significantly more frequently, while the \"ham\" bag will contain more words related to the user's friends or workplace.\n\nTo classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be in.\n\n", "related": "\n- Additive smoothing\n- Bag-of-words model in computer vision\n- Document classification\n- Document-term matrix\n- Feature extraction\n- Hashing trick\n- Machine learning\n- MinHash\n- n-gram\n- Natural language processing\n- Vector space model\n- w-shingling\n- tf-idf\n\n- McTear, Michael (et al) (2016). \"The Conversational Interface\". Springer International Publishing.\n"}
{"id": "38059657", "url": "https://en.wikipedia.org/wiki?curid=38059657", "title": "Similarity learning", "text": "Similarity learning\n\nSimilarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, \nvisual identity tracking, face verification, and speaker verification.\n\nThere are four common setups for similarity and metric distance learning.\n\n- \"Regression similarity learning\"\n- \"Classification similarity learning\"\n- \"Ranking similarity learning\"\n- Locality sensitive hashing (LSH)\n\nA common approach for learning similarity, is to model the similarity function as a bilinear form. For example, in the case of ranking similarity learning, one aims to learn a matrix W that parametrizes the similarity function formula_17.\n\nSimilarity learning is closely related to \"distance metric learning\". Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, identity of indiscernibles, symmetry and subadditivity (or the triangle inequality). In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.\n\nWhen the objects formula_11 are vectors in formula_19, then any matrix formula_20 in the symmetric positive semi-definite cone formula_21 defines a distance pseudo-metric of the space of x through the form formula_22. When formula_20 is a symmetric positive definite matrix, formula_24 is a metric. Moreover, as any symmetric positive semi-definite matrix formula_25 can be decomposed as formula_26 where formula_27 and formula_28, the distance function formula_24 can be rewritten equivalently formula_30. The distance formula_31 corresponds to the Euclidean distance between the projected feature vectors formula_32 and formula_33. \nSome well-known approaches for metric learning include Large margin nearest neighbor, Information theoretic metric learning (ITML).\n\nIn statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.\n\nSimilarity learning is used in information retrieval for learning to rank, in face verification or face identification, and in recommendation systems. Also, many machine learning approaches rely on some metric. This includes unsupervised learning such as clustering, which groups together close or similar objects. It also includes supervised approaches like K-nearest neighbor algorithm which rely on labels of nearby objects to decide on the label of a new object. Metric learning has been proposed as a preprocessing step for many of these approaches.\n\nMetric and similarity learning naively scale quadratically with the dimension of the input space, as can easily see when the learned metric has a bilinear form formula_17. Scaling to higher dimensions can be achieved by enforcing a sparseness structure over the matrix model, as done with HDSL, and with COMET.\n\n", "related": "\n- Learning to rank\n- Latent semantic analysis\n\nFor further information on this topic, see the surveys on metric and similarity learning by Bellet et al. and Kulis.\n"}
{"id": "38870173", "url": "https://en.wikipedia.org/wiki?curid=38870173", "title": "Feature learning", "text": "Feature learning\n\nIn machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task.\n\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\nFeature learning can be either supervised or unsupervised.\n- In supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.\n- In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.\n\nSupervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include:\n\nDictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with \"L1\" regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).\n\nSupervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique applied dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an \"L1\" regularization on the representing weights for each data point (to enable sparse representation of data), and an \"L2\" regularization on the parameters of the classifier.\n\nNeural networks are a family of learning algorithms that use a \"network\" consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the network's input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).\n\nMultilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is Siamese networks.\n\nUnsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following.\n\n\"K\"-means clustering is an approach for vector quantization. In particular, given a set of \"n\" vectors, \"k\"-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, although suboptimal greedy algorithms have been developed.\n\nK-means clustering can be used to group an unlabeled set of inputs into \"k\" clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest is to add \"k\" binary features to each sample, where each feature \"j\" has value one iff the \"j\"th centroid learned by \"k\"-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has been used to train RBF networks). Coates and Ng note that certain variants of \"k\"-means behave similarly to sparse coding algorithms.\n\nIn a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that \"k\"-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. \"K\"-means also improves performance in the domain of NLP, specifically for named-entity recognition; there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings).\n\nPrincipal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of \"n\" input data vectors, PCA generates \"p\" (which is much smaller than the dimension of the input data) right singular vectors corresponding to the \"p\" largest singular values of the data matrix, where the \"k\"th row of the data matrix is the \"k\"th input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the \"p\" largest eigenvalues of the sample covariance matrix of the input vectors. These \"p\" singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.\n\nPCA is a linear feature learning approach since the \"p\" singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with \"p\" iterations. In the \"i\"th iteration, the projection of the data matrix on the \"(i-1)\"th eigenvector is subtracted, and the \"i\"th singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.\n\nPCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order moments of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).\n\nLocal linear embedding (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000). The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set.\n\nLLE consists of two major steps. The first step is for \"neighbor-preserving\", where each input data point \"Xi\" is reconstructed as a weighted sum of \"K\" nearest neighbor data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for \"dimension reduction,\" by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a least squares problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition.\n\nThe reconstruction weights obtained in the first step capture the \"intrinsic geometric properties\" of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the \"intrinsic geometric properties\" captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure.\n\nIndependent component analysis (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution.\n\nUnsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation.\n\nThe hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes. These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input at the bottom layer is raw data, and the output of the final layer is the final low-dimensional feature or representation.\n\nRestricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables. Such conditional independence facilitates computations.\n\nAn RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hinton's contrastive divergence (CD) algorithm.\n\nIn general training RBM by solving the maximization problem tends to result in non-sparse representations. Sparse RBM was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant formula_1.\n\nAn autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria are satisfied.\n\n", "related": "\n- Automated machine learning (AutoML)\n- Basis function\n- Deep learning\n- Feature detection (computer vision)\n- Feature extraction\n- Kernel trick\n- Vector quantization\n"}
{"id": "39182554", "url": "https://en.wikipedia.org/wiki?curid=39182554", "title": "Catastrophic interference", "text": "Catastrophic interference\n\nCatastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information. Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989), and Ratcliff (1990). It is a radical manifestation of the 'sensitivity-stability' dilemma or the 'stability-plasticity' dilemma. Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum. The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.\n\nThe term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).\n\nMcCloskey and Cohen (1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.\n\n- Experiment 1: \"Learning the ones and twos addition facts\"\nIn their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials. Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for a correct number. This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.\n\n- Experiment 2: \"Replication of Barnes and Underwood (1959) study\"\nIn their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.\n\nMcCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.\n\nOverall, McCloskey and Cohen (1989) concluded that: \n- at least some interference will occur whenever new learning alters the weights involved representing\n- the greater the amount of new learning, the greater the disruption in old knowledge\n- interference was catastrophic in the backpropagation networks when learning was sequential but not concurrent\n\nRatcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned. After inspecting the recognition performance models he found two major problems:\n- Well-learned information was catastrophically forgotten as new information was learned in both small and large backpropagation networks.\nEven one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989). Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference. \n- Discrimination between the studied items and previously unseen items decreased as the network learned more.\nThis finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.\n\nMany researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks. In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights where \"knowledge is stored\" are changed, it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input. Another way to conceptualize this is through visualizing learning as movement through a weight space. This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen. However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern. To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference. Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.\n\nMany of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another. Lewandowsky and Li (1995) noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0×0 + 0×1 + 1×0 + 0×0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1). Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors. Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference and other studies finding it does not.\n\nBelow are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:\n\nFrench (1991) proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using binary representation of input vectors, activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that neural networks which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer. French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropagation). Overall the guidelines for the process of 'activation sharpening' are as follows:\n\n1. Perform a forward activation pass by feeding an input from the input layer to the hidden layer and record the activations at the hidden layer\n2. \"Sharpen\" the activation of x number of most active nodes by a sharpening factor α:\n- \"A\" = \"A\" + \"α\"(1- \"A\") For nodes to be sharpened, i.e. more activated\n- \"A\" = \"A\" – \"αA\" For all other nodes\n- French suggested the number of nodes to be sharpened should be log \"n\" nodes, where n is the number of hidden layer nodes\n3. Use the difference between the old activation (A) and the sharpened activation (\"A\") as an error, backpropagate this error to the input layer, and modify the weights of input-to-output appropriately\n4. Do a full forward pass with the input through to the output layer\n5. Backpropagate as usual from the output to the input layer\n6. Repeat\n\nIn his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.\n\nAccording to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed. Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.\n\nKortge (1990) proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the delta rule). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value (sum of the inputs):\n\nWhen the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially. However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.\n\nMcRae and Hetherington (1993) argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.\n\nTo test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like \"JEP\" and \"ZEP\", was greater than for dissimilar CVCs, such as \"JEP\" and \"YUG\". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference. Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.\n\nFrench (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2). In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995). In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called \"pseudopatterns\". These pseudopatterns are approximations of previous inputs and they can be interleaved with the learning of new inputs. The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information. When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:\n\n- When a pattern is fed from the environment (a real input), the information travels both to the early processing area and the final storage area, however the teacher nodes will inhibit the output from the final storage area\n- The new pattern is learned by the early processing area by the standard backpropagation algorithm\n- At the same time random input is also fed into the network and causes pseudopatterns to be generated by the final storage area\n- Output from the final-storage area, in the form of pseudopatterns, will be used as a teacher for the early-processing area. In this way, the pseudopatterns are interleaved with the 'real inputs' from the environment\n- Once the new pattern and the pseudopattern are learned by the early processing area, its weights are copied to the corresponding weights in the final storage area.\n\nWhen tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.\n\nNot only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.\n\nFollowing the same basic idea contributed by Robins, Ans and Rousset (1997) have also proposed a two-network artificial neural architecture with \"memory self-refreshing\" that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a \"reverberating\" process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network \"attractors\", is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000) have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009) have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004) has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.\n\nSo far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004) who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.\n\nLatent Learning is a technique used by Gutstein & Stump (2015) both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.\n\nGiven a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC) (as opposed to 1 hot codes), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes \"without any exposure to the new classes\", they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of Latent Learning, as introduced by Tolman in 1930. In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.\n\nKirkpatrick et al. (2017) demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.\n\nPractopoietic theory proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of \"anapoiesis\" is applied. Anapoiesis stands for \"reconstruction of knowledge\"—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.\n", "related": "NONE"}
{"id": "1422176", "url": "https://en.wikipedia.org/wiki?curid=1422176", "title": "Developmental robotics", "text": "Developmental robotics\n\nDevelopmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence, developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.\n\nDevelopmental robotics is related to but differs from evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.\n\nDevRob is also related to work done in the domains of robotics and artificial life.\n\nCan a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is \"out of the factory\"? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950,\nbut it is only since the end of the 20th century that they began to be investigated systematically.\n\nBecause the concept of adaptive intelligent machines is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, cognitive robotics or computational neuroscience. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from traditional machine learning because it targets task-independent self-determined learning rather than task-specific inference over \"spoon-fed human-edited sensory data\" (Weng et al., 2001). It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features:\n1. It targets task-independent architectures and learning mechanisms, i.e. the machine/robot has to be able to learn new tasks that are unknown by the engineer;\n2. It emphasizes open-ended development and lifelong learning, i.e. the capacity of an organism to acquire continuously novel skills. This should not be understood as a capacity for learning \"anything\" or even “everything”, but just that the set of skills that is acquired can be infinitely extended at least in some (not all) directions;\n3. The complexity of acquired knowledge and skills shall increase (and the increase be controlled) progressively.\n\nDevelopmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self-organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development.\n\nDue to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is important being investigated is the acquisition of sensorimotor skills. These include the discovery of one's own body, including its structure and dynamics such as hand-eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills: the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind.\n\nThe sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development:\n1. Motivational systems, generating internal reward signals that drive exploration and learning, which can be of two main types:\n- extrinsic motivations push robots/organisms to maintain basic specific internal properties such as food and water level, physical integrity, or light (e.g. in phototropic systems);\n- intrinsic motivations push robot to search for novelty, challenge, compression or learning progress per se, thus generating what is sometimes called curiosity-driven learning and exploration, or alternatively active learning and exploration;\n2. Social guidance: as humans learn a lot by interacting with their peers, developmental robotics investigates mechanisms which can allow robots to participate to human-like social interaction. By perceiving and interpreting social cues, this may allow robots both to learn from humans (through diverse means such as imitation, emulation, stimulus enhancement, demonstration, etc. ...) and to trigger natural human pedagogy. Thus, social acceptance of developmental robots is also investigated;\n3. Statistical inference biases and cumulative knowledge/skill reuse: biases characterizing both representations/encodings and inference mechanisms can typically allow considerable improvement of the efficiency of learning and are thus studied. Related to this, mechanisms allowing to infer new knowledge and acquire new skills by reusing previously learnt structures is also an essential field of study;\n4. The properties of embodiment, including geometry, materials, or innate motor primitives/synergies often encoded as dynamical systems, can considerably simplify the acquisition of sensorimotor or social skills, and is sometimes referred as morphological computation. The interaction of these constraints with other constraints is an important axis of investigation;\n5. Maturational constraints: In human infants, both the body and the neural system grow progressively, rather than being full-fledged already at birth. This implies, for example, that new degrees of freedom, as well as increases of the volume and resolution of available sensorimotor signals, may appear as learning and development unfold. Transposing these mechanisms in developmental robots, and understanding how it may hinder or on the contrary ease the acquisition of novel complex skills is a central question in developmental robotics.\n\nWhile most developmental robotics projects interact closely with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in Neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields.\n\nAs developmental robotics is a relatively new research field and at the same time very ambitious, many fundamental open challenges remain to be solved.\n\nFirst of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open-ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces constitute a significant obstacle to be solved. Lifelong cumulative learning is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms.\n\nAmong the strategies to explore to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated.\n\nAnother important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far, mostly too limited to allow efficient general-purpose teaching from humans.\n\nA fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred to as the \"symbol grounding problem\" when it comes to language acquisition. Actually, the very existence and need for symbols in the brain is actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated.\n\nDuring biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics.\n\nAnother open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain mechanisms.\n\nSimilarly, in biology, developmental mechanisms (operating at the ontogenetic time scale) interact closely with evolutionary mechanisms (operating at the phylogenetic time scale) as shown in the flourishing \"evo-devo\" scientific literature.\nHowever, the interaction of those mechanisms in artificial organisms, developmental robots in particular, is still vastly understudied. The interaction of evolutionary mechanisms, unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics.\n\n- IEEE Transactions on Cognitive and Developmental Systems (previously known as IEEE Transactions on Autonomous Mental Development): https://cis.ieee.org/publications/t-cognitive-and-developmental-systems\n\n- International Conference on Development and Learning: http://www.cogsci.ucsd.edu/~triesch/icdl/\n- Epigenetic Robotics: https://www.lucs.lu.se/epirob/\n- ICDL-EpiRob: http://www.icdl-epirob.org/ (the two above joined since 2011)\n- Developmental Robotics: http://cs.brynmawr.edu/DevRob05/\nThe NSF/DARPA funded Workshop on Development and Learning was held April 5–7, 2000 at Michigan State University. It was the first international meeting devoted to computational understanding of mental development by robots and animals. The term \"by\" was used since the agents are active during development.\n\n", "related": "\n- Evolutionary developmental robotics\n- Robot learning\n\n- IEEE Technical Committee on Cognitive and Developmental Systems (CDSTC), previously known as IEEE Technical Committee on Autonomous Mental Development, https://cis.ieee.org/technical-committees/cognitive-and-developmental-systems-technical-committee\n- IEEE Technical Committee on Cognitive Robotics, https://www.ieee-ras.org/cognitive-robotics\n- IEEE Technical Committee on Robot Learning, https://www.ieee-ras.org/robot-learning/\n\n- Cognitive Development Lab, University of Indiana, US\n- Michigan State University – Embodied Intelligence Lab\n- Inria and Ensta ParisTech FLOWERS team, France: Exploration, interaction and learning in developmental robotics\n- University of Tokyo—Intelligent Systems and Informatics Lab\n- Cognitive Robotics Lab of Juergen Schmidhuber at IDSIA and Technical University of Munich\n- LIRA-Lab, University of Genova, Italy\n- CITEC at University of Bielefeld, Germany\n- Vision Lab, Psychology Department, Southern Illinois University Carbondale\n- FIAS (J. Triesch lab.)\n- LPP, CNRS (K. Oregan lab.)\n- AI Lab, SoftBank Robotics Europe, France\n- Departement of Computer Science, University of Aberdeen\n- Asada Laboratory, Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan\n- The University of Texas at Austin, UTCS Intelligent Robotics Lab\n- Bryn Mawr College's Developmental Robotics Project: research projects by faculty and students at Swarthmore and Bryn Mawr Colleges, Philadelphia, PA, USA\n- Jean Project: Information Sciences Institute of the University of Southern California\n- Cognitive Robotics (including Hide and Seek) at the Naval Research Laboratory\n- The Laboratory for Perceptual Robotics, University of Massachusetts Amherst Amherst, USA\n- Centre for Robotics and Neural Systems, Plymouth University Plymouth, United Kingdom\n- Laboratory of Computational Embodied Neuroscience, Institute of Cognitive Science and Technologies National Research Council, Rome, Italy\n- Neurocybernetic team, ETIS Lab., ENSEA – University of Cergy-Pontoise – CNRS, France\n- Machine Perception and Cognitive Robotics Lab, Florida Atlantic University, Boca Raton, Florida\n- Adaptive Systems Group, Department of Computer Science, Humboldt University of Berlin, Germany\n- Cognitive Developmental Robotics Lab (Nagai Lab), The University of Tokyo, Japan\n\n- RobotDoC Project (funded by European Commission)\n- Italk Project (funded by European Commission)\n- IM-CLeVeR Project (funded by European Commission)\n- ERC Grant EXPLORERS Project (funded by European Research Council)\n- RobotCub Project (funded by European Commission)\n- Feelix Growing Project (funded by European Commission)\n\nThe first undergraduate courses in DevRob were offered at Bryn Mawr College and Swarthmore College in the Spring of 2003 by Douglas Blank and Lisa Meeden, respectively.\nThe first graduate course in DevRob was offered at Iowa State University by Alexander Stoytchev in the Fall of 2005.\n"}
{"id": "847558", "url": "https://en.wikipedia.org/wiki?curid=847558", "title": "Confusion matrix", "text": "Confusion matrix\n\nIn the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).\n\nIt is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table).\n\nIf a classification system has been trained to distinguish between cats and dogs, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 13 animals — 8 cats and 5 dogs — the resulting confusion matrix could look like the table below:\n\nIn this confusion matrix, of the 8 actual cats, the system predicted that three were dogs, and of the five dogs, it predicted that two were cats. All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal.\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of \"false positives\", \"false negatives\", \"true positives\", and \"true negatives\". This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced; that is, when the numbers of observations in different classes vary greatly. For example, if there were 95 cats and only 5 dogs in the data, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate (sensitivity) for the cat class but a 0% recognition rate for the dog class. F1 score is even more unreliable in such cases, and here would yield over 97.4%, whereas informedness removes such bias and yields 0 as the probability of an informed decision for any form of guessing (here always guessing cat).\n\nAccording to Davide Chicco and Giuseppe Jurman, the most informative metric to evaluate a confusion matrix is the Matthews correlation coefficient (MCC).\n\nAssuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be:\n\nThe final table of confusion would contain the average values for all classes combined.\n\nLet us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 \"confusion matrix\", as follows:\n", "related": "NONE"}
{"id": "28801798", "url": "https://en.wikipedia.org/wiki?curid=28801798", "title": "Active learning (machine learning)", "text": "Active learning (machine learning)\n\nActive learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called \"teacher\" or \"oracle\".\n\nThere are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples. Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning.\n\nLet be the total set of all data under consideration. For example, in a protein engineering problem, would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.\n\nDuring each iteration, , is broken up into three subsets\n1. formula_1: Data points where the label is known.\n2. formula_2: Data points where the label is unknown.\n3. formula_3: A subset of that is chosen to be labeled.\n\nMost of the current research in active learning involves the best method to choose the data points for .\n\n- Membership Query Synthesis: This is where the learner generates its own instance from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small.\n- Pool-Based Sampling: In this scenario, instances are drawn from the entire data pool and assigned an informative score, a measurement of how well the learner “understands” the data. The system then selects the most informative instances and queries the teacher for the labels.\n- Stream-Based Selective Sampling: Here, each unlabeled data point is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint.\n\nAlgorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose:\n\n- Balance exploration and exploitation: the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label.\n- Expected model change: label those points that would most change the current model.\n- Expected error reduction: label those points that would most reduce the model's generalization error.\n- Exponentiated Gradient Exploration for Active Learning: In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration.\n- Uncertainty sampling: label those points for which the current model is least certain as to what the correct output should be.\n- Query by committee: a variety of models are trained on the current labeled data, and vote on the output for unlabeled data; label those points for which the \"committee\" disagrees the most\n- Querying from diverse subspaces or partitions: When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling.\n- Variance reduction: label those points that would minimize output variance, which is one of the components of error.\n- Conformal Predictors: This method predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction.\n- Mismatch-first farthest-traversal: The primary selection criteria is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criteria is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data.\n\nA wide variety of algorithms have been studied that fall into these categories.\n\nSome active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, , of each unlabeled datum in and treat as an -dimensional distance from that datum to the separating hyperplane.\n\nMinimum Marginal Hyperplane methods assume that the data with the smallest are those that the SVM is most uncertain about and therefore should be placed in to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest . Tradeoff methods choose a mix of the smallest and largest s.\n\n- 2016 \"Workshop Active Learning: Applications, Foundations and Emerging Trends\" at iKNOW, Graz, Austria\n- 2018 \"Interactive Adaptive Learning\" Workshop at ECML PKDD, Dublin, Ireland\n\n", "related": "\n- List of datasets for machine learning research\n\n- Active Learning Tutorial, S. Dasgupta and J. Langford.\n"}
{"id": "4375576", "url": "https://en.wikipedia.org/wiki?curid=4375576", "title": "Grammar induction", "text": "Grammar induction\n\nGrammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of \"re-write rules\" or \"productions\" or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n\nGrammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.\n\nSince the beginning of the century, these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.\nOther classes of grammars for which grammatical inference has been studied are combinatory categorial grammars, stochastic context-free grammars, contextual grammars and pattern languages.\n\nThe simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).\nHowever, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin.\n\nThere is a wide variety of methods for grammatical inference. Two of the classic sources are and . also devote a brief section to the problem, and cite a number of references. The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see \"Induction of regular languages\". A more recent textbook is de la Higuera (2010), which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni provide a survey that explores grammatical inference methods for natural languages.\n\nThe method proposed in Section 8.7 of suggests successively guessing grammar rules (productions) and testing them against positive and negative observations. The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded. This particular approach can be characterized as \"hypothesis testing\" and bears some similarity to Mitchel's version space algorithm. The text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.\n\nGrammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process. Formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators. Algorithms of this sort stem from the genetic programming paradigm pioneered by John Koza. Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the EBNF language made trees a more flexible approach.\n\nKoza represented Lisp programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the functions of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.\n\nIn the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.\n\nLike all greedy algorithms, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.\nThe decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.\nBecause there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.\n\nThese context-free grammar generating algorithms make the decision after every read symbol:\n- Lempel-Ziv-Welch algorithm creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.\n- Sequitur and its modifications.\n\nThese context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:\n- Byte pair encoding and its optimizations.\n\nA more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning context-free grammars and mildly context-sensitive languages and have been proven to be correct and efficient for large subclasses of these grammars.\n\nAngluin defines a \"pattern\" to be \"a string of constant symbols from Σ and variable symbols from a disjoint set\".\nThe language of such a pattern is the set of all its nonempty ground instances i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.\nA pattern is called descriptive for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.\n\nAngluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable \"x\".\nTo this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on \"x\" being the only variable, the state count can be drastically reduced.\n\nErlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.\n\nArimura et al. show that a language class obtained from limited unions of patterns can be learned in polynomial time.\n\nPattern theory, formulated by Ulf Grenander, is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.\n\nIn addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:\n- Identify the hidden variables of a data set using real world data rather than artificial stimuli, which was commonplace at the time.\n- Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.\n- Study the randomness and variability of these graphs.\n- Create the basic classes of stochastic models applied by listing the deformations of the patterns.\n- Synthesize (sample) from the models, not just analyze signals with it.\nBroad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.\n\nThe principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to semantic parsing, natural language understanding, example-based translation, morpheme analysis, and place name derivations. Grammar induction has also been used for lossless data compression and statistical inference via minimum message length (MML) and minimum description length (MDL) principles. Grammar induction has also been used in some probabilistic models of language acquisition.\n\n", "related": "\n- Artificial grammar learning#Artificial intelligence\n- Example-based machine translation\n- Inductive programming\n- Kolmogorov complexity\n- Language identification in the limit\n- Straight-line grammar\n- Syntactic pattern recognition\n\n"}
{"id": "40946774", "url": "https://en.wikipedia.org/wiki?curid=40946774", "title": "Pattern language (formal languages)", "text": "Pattern language (formal languages)\n\nIn theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning.\n\nGiven a finite set Σ of constant symbols and a countable set \"X\" of variable symbols disjoint from Σ, a pattern is a finite non-empty string of symbols from Σ∪\"X\".\nThe length of a pattern \"p\", denoted by |\"p\"|, is just the number of its symbols.\nThe set of all patterns containing exactly \"n\" distinct variables (each of which may occur several times) is denoted by \"P\", the set of all patterns at all by \"P\".\nA substitution is a mapping \"f\": \"P\" → \"P\" such that\n- \"f\" is a homomorphism with respect to string concatenation (⋅), formally: ∀\"p\",\"q\"∈\"P\". \"f\"(\"p\"⋅\"q\") = \"f\"(\"p\")⋅\"f\"(\"q\");\n- \"f\" is non-erasing, formally: ∀\"p\"∈\"P\". \"f\"(\"p\") ≠ ε, where ε denotes the empty string; and\n- \"f\" respects constants, formally: ∀\"s\"∈Σ. \"f\"(\"s\") = \"s\".\nIf \"p\" = \"f\"(\"q\") for some patterns \"p\", \"q\" ∈ \"P\" and some substitution \"f\", then \"p\" is said to be less general than \"q\", written \"p\"≤\"q\";\nin that case, necessarily |\"p\"| ≥ |\"q\"| holds.\nFor a pattern \"p\", its language is defined as the set of all less general patterns that are built from constants only, formally: \"L\"(\"p\") = { \"s\" ∈ Σ : \"s\" ≤ \"p\" }, where Σ denotes the set of all finite non-empty strings of symbols from Σ.\n\nFor example, using the constants Σ = { 0, 1 } and the variables \"X\" = { \"x\", \"y\", \"z\", ... }, the pattern 0\"x\"10\"xx\"1 ∈\"P\" and \"xxy\" ∈\"P\" has length 7 and 3, respectively.\nAn instance of the former pattern is 00\"z\"100\"z\"0\"z\"1 and 01\"z\"101\"z\"1\"z\"1, it is obtained by the substitution that maps \"x\" to 0\"z\" and to 1\"z\", respectively, and each other symbol to itself. Both 00\"z\"100\"z\"0\"z\"1 and 01\"z\"101\"z\"1\"z\"1 are also instances of \"xxy\". In fact, \"L\"(0\"x\"10\"xx\"1) is a subset of \"L\"(\"xxy\"). The language of the pattern \"x\"0 and \"x\"1 is the set of all bit strings which denote an even and odd binary number, respectively. The language of \"xx\" is the set of all strings obtainable by concatenating a bit string with itself, e.g. 00, 11, 0101, 1010, 11101110 ∈ \"L\"(\"xx\").\n\nThe problem of deciding whether \"s\" ∈ \"L\"(\"p\") for an arbitrary string \"s\" ∈ Σ and pattern \"p\" is NP-complete (see picture),\nand so is hence the problem of deciding \"p\" ≤ \"q\" for arbitrary patterns \"p\", \"q\".\n\nThe class of pattern languages is not closed under ...\n- union: e.g. for Σ = {0,1} as above, \"L\"(01)∪\"L\"(10) is not a pattern language;\n- complement: Σ \\ \"L\"(0) is not a pattern language;\n- intersection: \"L\"(\"x\"0\"y\")∩\"L\"(\"x\"1\"y\") is not a pattern language;\n- Kleene plus: \"L\"(0) is not a pattern language;\n- homomorphism: \"f\"(\"L\"(\"x\")) = \"L\"(0) is not a pattern language, assuming \"f\"(0) = 0 = \"f\"(1);\n- inverse homomorphism: \"f\"(111) = { 01, 10, 000 } is not a pattern language, assuming \"f\"(0) = 1 and \"f\"(1) = 11.\nThe class of pattern languages is closed under ...\n- concatenation: \"L\"(\"p\")⋅\"L\"(\"q\") = \"L\"(\"p\"⋅\"q\");\n- reversal: \"L\"(\"p\") = \"L\"(\"p\").\n\nIf \"p\", \"q\" ∈ \"P\" are patterns containing exactly one variable, then \"p\" ≤ \"q\" if and only if \"L\"(\"p\") ⊆ \"L\"(\"q\");\nthe same equivalence holds for patterns of equal length.\nFor patterns of different length, the above example \"p\" = 0\"x\"10\"xx\"1 and \"q\" = \"xxy\" shows that \"L\"(\"p\") ⊆ \"L\"(\"q\") may hold without implying \"p\" ≤ \"q\".\nHowever, any two patterns \"p\" and \"q\", of arbitrary lengths, generate the same language if and only if they are equal up to consistent variable renaming.\nEach pattern \"p\" is a common generalization of all strings in its generated language \"L\"(\"p\"), modulo associativity of (⋅).\n\nIn a refined Chomsky hierarchy, the class of pattern languages is a proper superclass and subclass of the singleton and the indexed languages, respectively, but incomparable to the language classes in between; due to the latter, the pattern language class is not explicitly shown in the table below.\n\nThe class of pattern languages is incomparable with the class of finite languages, with the class of regular languages, and with the class of context-free languages:\n- the pattern language \"L\"(\"xx\") is not context-free (hence neither regular nor finite) due to the pumping lemma;\n- the finite (hence also regular and context-free) language { 01, 10 } is not a pattern language.\nEach singleton language is trivially a pattern language, generated by a pattern without variables.\n\nEach pattern language can be produced by an indexed grammar:\nFor example, using Σ = { \"a\", \"b\", \"c\" } and \"X\" = { x, y },\nthe pattern \"a\" x \"b\" y \"c\" x \"a\" y \"b\" is generated by a grammar with nonterminal symbols \"N\" = { \"S\", \"S\", \"S\" } ∪ \"X\", terminal symbols \"T\" = Σ, index symbols \"F\" = { \"a\", \"b\", \"c\", \"a\", \"b\", \"c\" }, start symbol \"S\", and the following production rules:\n\nAn example derivation is:\n  ⇒   \n  ⇒   \n  ⇒   \n  ⇒   \n  ⇒   \n  ⇒ ... ⇒   \n  ⇒ ... ⇒   \n  ⇒ ... ⇒   \n\nIn a similar way, an index grammar can be constructed from any pattern.\n\nGiven a sample set \"S\" of strings, a pattern \"p\" is called descriptive of \"S\" if \"S\" ⊆ \"L\"(\"p\"), but not \"S\" ⊆ \"L\"(\"q\") ⊂ \"L\"(\"p\") for any other pattern \"q\".\n\nGiven any sample set \"S\", a descriptive pattern for \"S\" can be computed by \n- enumerating all patterns (up to variable renaming) not longer than the shortest string in \"S\",\n- selecting from them the patterns that generate a superset of \"S\",\n- selecting from them the patterns of maximal length, and\n- selecting from them a pattern that is minimal with respect to ≤.\nBased on this algorithm, the class of pattern languages can be identified in the limit from positive examples.\n", "related": "NONE"}
{"id": "40973765", "url": "https://en.wikipedia.org/wiki?curid=40973765", "title": "Bayesian optimization", "text": "Bayesian optimization\n\nBayesian optimization is a sequential design strategy for global optimization of black-box functions that doesn't require derivatives.\n\nThe term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.\n\nSince the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point.\n\nThere are several methods used to define the prior/posterior distribution over the objective function. The most common two methods use Gaussian Processes in a method called Kriging. Another less expensive method uses the Parzen-Tree Estimator to construct two distributions for 'high' and 'low' points, and then finds the location that maximizes the expected improvement.\n\nExamples of acquisition functions include probability of improvement, expected improvement, Bayesian expected losses, upper confidence bounds (UCB), Thompson sampling and hybrids of these. They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are expensive to evaluate.\n\nThe maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer. Acquisition functions are typically well-behaved and are often maximized with implementations of Newton's Method such as Broyden–Fletcher–Goldfarb–Shanno algorithm or the Nelder-Mead method.\n\nThe approach has been applied to solve a wide range of problems, including learning to rank, computer graphics and visual design, robotics, sensor networks, automatic algorithm configuration, automatic machine learning toolboxes, reinforcement learning, planning, visual attention, architecture configuration in deep learning, static program analysis and experimental particle physics.\n\n", "related": "\n- Multi-armed bandit\n- Thompson sampling\n- Global optimization\n- Bayesian experimental design\n\n- BoTorch, A modular and modern PyTorch-based open-source library for Bayesian optimization research with support for GPyTorch.\n- ParBayesianOptimization, A high performance, parallel implementation of Bayesian optimization with Gaussian processes in R.\n- GPyOpt, Python open-source library for Bayesian Optimization based on GPy.\n- Bayesopt, an efficient implementation in C/C++ with support for Python, Matlab and Octave.\n- Spearmint, a Python implementation focused on parallel and cluster computing.\n- Hyperopt, a Python implementation for hyperparameter optimization.\n- SMAC, a Java implementation of random-forest-based Bayesian optimization for general algorithm configuration.\n- pybo, a Python implementation of modular Bayesian optimization.\n- Bayesopt.m, a Matlab implementation of Bayesian optimization with or without constraints.\n- MOE MOE is a Python/C++/CUDA implementation of Bayesian Global Optimization using Gaussian Processes.\n- SigOpt SigOpt offers Bayesian Global Optimization as a SaaS service focused on enterprise use cases.\n- Metaopt, a Python implementation of hyperparameter optimization focused on parallel and cluster computing.\n- Mind Foundry OPTaaS offers Bayesian Global Optimization via web-services with flexible parameter constraints.\n"}
{"id": "213214", "url": "https://en.wikipedia.org/wiki?curid=213214", "title": "Early stopping", "text": "Early stopping\n\nIn machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.\n\nThis section presents some of the basic machine-learning concepts required for a description of early stopping methods.\n\nMachine learning algorithms train a model based on a finite set of training data. During this training, the model is evaluated based on how well it predicts the observations contained in the training set. In general, however, the goal of a machine learning scheme is to produce a model that generalizes, that is, that predicts previously unseen observations. Overfitting occurs when a model fits the data in the training set well, while incurring larger generalization error.\n\nRegularization, in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model.\nThis smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function as in Tikhonov regularization. Tikhonov regularization, along with principal component regression and many other regularization schemes, fall under the umbrella of spectral regularization, regularization characterized by the application of a filter. Early stopping also belongs to this class of methods.\n\nGradient descent methods are first-order, iterative, optimization methods. Each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function. By choosing the step-size appropriately, such a method can be made to converge to a local minimum of the objective function. Gradient descent is used in machine-learning by defining a \"loss function\" that reflects the error of the learner on the training set and then minimizing that function.\n\nEarly-stopping can be used to regularize non-parametric regression problems encountered in machine learning. For a given input space, formula_1, output space, formula_2, and samples drawn from an unknown probability measure, formula_3, on formula_4, the goal of such problems is to approximate a \"regression function\", formula_5, given by\n\nwhere formula_7 is the conditional distribution at formula_8 induced by formula_3.\nOne common choice for approximating the regression function is to use functions from a reproducing kernel Hilbert space. These spaces can be infinite dimensional, in which they can supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.\n\nThe early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number. They yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process.\n\nLet formula_10 and formula_11. Given a set of samples\n\ndrawn independently from formula_3, minimize the functional\n\nwhere, formula_15 is a member of the reproducing kernel Hilbert space formula_16. That is, minimize the expected risk for a Least-squares loss function. Since formula_17 depends on the unknown probability measure formula_3, it cannot be used for computation. Instead, consider the following empirical risk\n\nLet formula_20 and formula_21 be the \"t\"-th iterates of gradient descent applied to the expected and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size formula_22. The formula_20 form the \"population iteration\", which converges to formula_5, but cannot be used in computation, while the formula_21 form the \"sample iteration\" which usually converges to an overfitting solution.\n\nWe want to control the difference between the expected risk of the sample iteration and the minimum expected risk, that is, the expected risk of the regression function:\n\nThis difference can be rewritten as the sum of two terms: the difference in expected risk between the sample and population iterations and that between the population iteration and the regression function:\n\nThis equation presents a bias-variance tradeoff, which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution. That rule has associated probabilistic bounds on the generalization error. For the analysis leading to the early stopping rule and bounds, the reader is referred to the original article. In practice, data-driven methods, e.g. cross-validation can be used to obtain an adaptive stopping rule.\n\nBoosting refers to a family of algorithms in which a set of weak learners (learners that are only slightly correlated with the true process) are combined to produce a strong learner. It has been shown, for several boosting algorithms (including AdaBoost), that regularization via early stopping can provide guarantees of consistency, that is, that the result of the algorithm approaches the true solution as the number of samples goes to infinity.\n\nBoosting methods have close ties to the gradient descent methods described above can be regarded as a boosting method based on the formula_28 loss: \"LBoost\".\n\nThese early stopping rules work by splitting the original training set into a new training set and a validation set. The error on the validation set is used as a proxy for the generalization error in determining when overfitting has begun. These methods are most commonly employed in the training of neural networks. Prechelt gives the following summary of a naive implementation of holdout-based early stopping as follows:\n\nMore sophisticated forms use cross-validation – multiple partitions of the data into training set and validation set – instead of a single partition into a training set and validation set. Even this simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.\n\n", "related": "\n- Overfitting, early stopping is one of methods used to prevent overfitting\n- Generalization error\n- Regularization (mathematics)\n- Statistical learning theory\n- Boosting (machine learning)\n- Cross-validation, in particular using a \"validation set\"\n- Neural networks\n"}
{"id": "41644056", "url": "https://en.wikipedia.org/wiki?curid=41644056", "title": "Inductive programming", "text": "Inductive programming\n\nInductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.\n\nDepending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.\n\nInductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.\n\nOutput of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.\n\nIn many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to 'deductive' program synthesis, where the specification is usually complete.\n\nIn other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.\n\nThe diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.\n\nResearch on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann.\nThese approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.\n\nThe advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his \"relative least general generalization (rlgg)\", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.\n\nIn parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.\n\nThe early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community.\n\nIn the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).\n\nOther ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures; abstraction has also been explored as a more powerful approach to cumulative learning and function invention.\n\nOne powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).\n\nThe first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where \"learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations\".\n\nSince then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems.\n\nOther areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces.\n\n", "related": "\n- Automatic programming\n- Declarative programming\n- Evolutionary programming\n- Functional programming\n- Genetic programming\n- Grammar induction\n- Inductive reasoning\n- Inductive logic programming\n\n- Logic programming\n- Machine learning\n- Probabilistic programming language\n- Program synthesis\n- Programming by example\n- Programming by demonstration\n- Structure mining\n- Test-driven development\n\n- https://web.archive.org/web/20040906084947/http://www-ai.ijs.si/SasoDzeroski/ILPBook/\n\n- Inductive Programming community page, hosted by the University of Bamberg.\n"}
{"id": "41200806", "url": "https://en.wikipedia.org/wiki?curid=41200806", "title": "Proximal gradient methods for learning", "text": "Proximal gradient methods for learning\n\nProximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is formula_1 regularization (also known as Lasso) of the form\n\nProximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application. Such customized penalties can help to induce certain structure in problem solutions, such as \"sparsity\" (in the case of lasso) or \"group structure\" (in the case of group lasso).\n\nProximal gradient methods are applicable in a wide variety of scenarios for solving convex optimization problems of the form\nwhere formula_4 is convex and differentiable with Lipschitz continuous gradient, formula_5 is a convex, lower semicontinuous function which is possibly nondifferentiable, and formula_6 is some set, typically a Hilbert space. The usual criterion of formula_7 minimizes formula_8 if and only if formula_9 in the convex, differentiable setting is now replaced by\nwhere formula_11 denotes the subdifferential of a real-valued, convex function formula_12.\n\nGiven a convex function formula_13 an important operator to consider is its proximity operator formula_14 defined by\nwhich is well-defined because of the strict convexity of the formula_16 norm. The proximity operator can be seen as a generalization of a projection.\nWe see that the proximity operator is important because formula_17 is a minimizer to the problem formula_18 if and only if\n\nOne important technique related to proximal gradient methods is the Moreau decomposition, which decomposes the identity operator as the sum of two proximity operators. Namely, let formula_21 be a lower semicontinuous, convex function on a vector space formula_22. We define its Fenchel conjugate formula_23 to be the function\nThe general form of Moreau's decomposition states that for any formula_25 and any formula_20 that\nwhich for formula_28 implies that formula_29. The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.\n\nIn certain situations it may be easier to compute the proximity operator for the conjugate formula_30 instead of the function formula_31, and therefore the Moreau decomposition can be applied. This is the case for group lasso.\n\nConsider the regularized empirical risk minimization problem with square loss and with the formula_1 norm as the regularization penalty:\nwhere formula_34 The formula_1 regularization problem is sometimes referred to as \"lasso\" (least absolute shrinkage and selection operator). Such formula_1 regularization problems are interesting because they induce \" sparse\" solutions, that is, solutions formula_37 to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem\nwhere formula_39 denotes the formula_40 \"norm\", which is the number of nonzero entries of the vector formula_37. Sparse solutions are of particular interest in learning theory for interpretability of results: a sparse solution can identify a small number of important factors.\n\nFor simplicity we restrict our attention to the problem where formula_43. To solve the problem\nwe consider our objective function in two parts: a convex, differentiable term formula_45 and a convex function formula_46. Note that formula_47 is not strictly convex.\n\nLet us compute the proximity operator for formula_48. First we find an alternative characterization of the proximity operator formula_49 as follows:\n\nformula_50\n\nFor formula_46 it is easy to compute formula_52: the formula_53th entry of formula_52 is precisely\n\nUsing the recharacterization of the proximity operator given above, for the choice of formula_46 and formula_20 we have that formula_58 is defined entrywise by\n\nwhich is known as the soft thresholding operator formula_60.\n\nTo finally solve the lasso problem we consider the fixed point equation shown earlier:\n\nGiven that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial formula_62, and for formula_63 define\nNote here the effective trade-off between the empirical error term formula_65 and the regularization penalty formula_48. This fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step (formula_67) and a soft thresholding step (via formula_68).\n\nConvergence of this fixed point scheme is well-studied in the literature and is guaranteed under appropriate choice of step size formula_69 and loss function (such as the square loss taken here). Accelerated methods were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on formula_4. Such methods have been studied extensively in previous years.\nFor more general learning problems where the proximity operator cannot be computed explicitly for some regularization term formula_47, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.\n\nThere have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.\n\nIn the fixed point iteration scheme\none can allow variable step size formula_73 instead of a constant formula_69. Numerous adaptive step size schemes have been proposed throughout the literature. Applications of these schemes suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.\n\nElastic net regularization offers an alternative to pure formula_1 regularization. The problem of lasso (formula_1) regularization involves the penalty term formula_46, which is not strictly convex. Hence, solutions to formula_78 where formula_4 is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an formula_80 norm regularization penalty. For example, one can consider the problem\nwhere formula_34\nFor formula_83 the penalty term formula_84 is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small formula_85, the additional penalty term formula_86 acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.\n\nProximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory. Certain problems in learning can often involve data which has additional structure that is known \" a priori\". In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.\n\nGroup lasso is a generalization of the lasso method when features are grouped into disjoint blocks. Suppose the features are grouped into blocks formula_87. Here we take as a regularization penalty\n\nwhich is the sum of the formula_80 norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group formula_90 we have that proximity operator of formula_91 is given by\n\nwhere formula_90 is the formula_94th group.\n\nIn contrast to lasso, the derivation of the proximity operator for group lasso relies on the Moreau decomposition. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm.\n\nIn contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure. Such generalizations of group lasso have been considered in a variety of contexts. For overlapping groups one common approach is known as \"latent group lasso\" which introduces latent variables to account for overlap. Nested group structures are studied in \"hierarchical structure prediction\" and with directed acyclic graphs.\n\n", "related": "\n- Convex analysis\n- Proximal gradient method\n- Regularization\n- Statistical learning theory\n"}
{"id": "2090057", "url": "https://en.wikipedia.org/wiki?curid=2090057", "title": "Kernel density estimation", "text": "Kernel density estimation\n\nIn statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.\n\nLet (\"x\", \"x\", …, \"x\") be a univariate independent and identically distributed sample drawn from some distribution with an unknown density \"ƒ\". We are interested in estimating the shape of this function \"ƒ\". Its \"kernel density estimator\" is\nwhere \"K\" is the kernel — a non-negative function — and is a smoothing parameter called the \"bandwidth\". A kernel with subscript \"h\" is called the \"scaled kernel\" and defined as . Intuitively one wants to choose \"h\" as small as the data will allow; however, there is always a trade-off between the bias of the estimator and its variance. The choice of bandwidth is discussed in more detail below.\n\nA range of kernel functions are commonly used: uniform, triangular, biweight, triweight, Epanechnikov, normal, and others. The Epanechnikov kernel is optimal in a mean square error sense, though the loss of efficiency is small for the kernels listed previously. Due to its convenient mathematical properties, the normal kernel is often used, which means , where \"ϕ\" is the standard normal density function.\n\nThe construction of a kernel density estimate finds interpretations in fields outside of density estimation. For example, in thermodynamics, this is equivalent to the amount of heat generated when heat kernels (the fundamental solution to the heat equation) are placed at each data point locations \"x\". Similar methods are used to construct discrete Laplace operators on point clouds for manifold learning (e.g. diffusion map).\n\nKernel density estimates are closely related to histograms, but can be endowed with properties such as smoothness or continuity by using a suitable kernel. To see this, we compare the construction of histogram and kernel density estimators, using these 6 data points:\n\nFor the histogram, first the horizontal axis is divided into sub-intervals or bins which cover the range of the data. In this case, we have 6 bins each of width 2. Whenever a data point falls inside this interval, we place a box of height 1/12. If more than one data point falls inside the same bin, we stack the boxes on top of each other.\n\nFor the kernel density estimate, we place a normal kernel with standard deviation 2.25 (indicated by the red dashed lines) on each of the data points \"x\". The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate is evident compared to the discreteness of the histogram, as kernel density estimates converge faster to the true underlying density for continuous random variables.\n\nThe bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated random sample from the standard normal distribution (plotted at the blue spikes in the rug plot on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is \"undersmoothed\" since it contains too many spurious data artifacts arising from using a bandwidth \"h\" = 0.05, which is too small. The green curve is \"oversmoothed\" since using the bandwidth \"h\" = 2 obscures much of the underlying structure. The black curve with a bandwidth of \"h\" = 0.337 is considered to be optimally smoothed since its density estimate is close to the true density. An extreme situation is encountered in the limit formula_2 (no smoothing), where the estimate is a sum of \"n\" delta functions centered at the coordinates of analyzed samples. In the other extreme limit formula_3 the estimate retains the shape of the used kernel, centered on the mean of the samples (completely smooth).\n\nThe most common optimality criterion used to select this parameter is the expected \"L\" risk function, also termed the mean integrated squared error:\n\nUnder weak assumptions on \"ƒ\" and \"K\", -(\"ƒ\" is the, generally unknown, real density function),\nMISE (\"h\") = AMISE(\"h\") + \"o(1/(nh) + h)\" where \"o\" is the little o notation.\nThe AMISE is the Asymptotic MISE which consists of the two leading terms\n\nwhere formula_6 for a function \"g\", formula_7\nand \"ƒ\"\" is the second derivative of \"ƒ\". The minimum of this AMISE is the solution to this differential equation\n\nor\n\nNeither the AMISE nor the \"h\" formulas are able to be used directly since they involve the unknown density function \"ƒ\" or its second derivative \"ƒ\"\", so a variety of automatic, data-based methods have been developed for selecting the bandwidth. Many review studies have been carried out to compare their efficacies, with the general consensus that the plug-in selectors\n\nSubstituting any bandwidth \"h\" which has the same asymptotic order \"n\" as \"h\" into the AMISE\ngives that AMISE(\"h\") = \"O\"(\"n\"), where \"O\" is the big o notation. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator. Note that the \"n\" rate is slower than the typical \"n\" convergence rate of parametric methods.\n\nIf the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation.\n\nBandwidth selection for kernel density estimation of heavy-tailed distributions is said to be relatively difficult.\n\nIf Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for \"h\" (that is, the bandwidth that minimises the mean integrated squared error) is\n\nIn order to make the h value more robust to make the fitness well for both long-tailed and skew distribution and bimodal mixture distribution, it is better to substitute the value of formula_11 with another parameter A, which:\n\nAnother modification that will improve the model is to reduce the factor from 1.06 to 0.9. Then the final formula would be:\n\nwhere formula_11 is the standard deviation of the samples, n is the sample size. IQR is the interquartile range.\nThis approximation is termed the \"normal distribution approximation\", Gaussian approximation, or \"Silverman's (1986) rule of thumb\".\nWhile this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example, consider estimating the bimodal Gaussian mixture:\n\nfrom a sample of 200 points. The figure on the right below shows the true density and two kernel density estimates—one using the rule-of-thumb bandwidth, and the other using\na solve-the-equation bandwidth. The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed.\nThe Matlab script for this example uses \nkde.m and is given below.\n1. The same code with R language\n2. ` Data\nset.seed(1) #Used for reproducibility\ndata = c(rnorm(100,-10,1),rnorm(100,10,1)) #Two Normals mixed\n1. ` True\nphi = function(x) exp(-.5*x^2)/sqrt(2*pi) #Normal Density\ntpdf = function(x) phi(x+10)/2+phi(x-10)/2 #True Density\n1. ` Kernel\nh = sd(data)*(4/3/length(data))^(1/5) #Bandwidth estimated by Silverman's Rule of Thumb\nKernel2 = function(x) mean(phi((x-data)/h)/h) #Kernel Density\nkpdf = function(x) sapply(x,Kernel2) #Elementwise application\n1. ` Plot\nx=seq(-25,25,length=1000) #Linear Space\nplot(x,tpdf(x),type=\"l\",ylim=c(0,0.23),col=\"red\") #Plot True Density\npar(new=T)\nplot(x,kpdf(x),type=\"l\",ylim=c(0,0.23),xlab=\",ylab=\",axes=F) #Plot Kernel Density with Silverman's Rule of Thumb\nGiven the sample (\"x\", \"x\", …, \"x\"), it is natural to estimate the characteristic function as\nKnowing the characteristic function, it is possible to find the corresponding probability density function through the Fourier transform formula. One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate formula_16 is unreliable for large \"t\"’s. To circumvent this problem, the estimator formula_16 is multiplied by a damping function , which is equal to 1 at the origin and then falls to 0 at infinity. The “bandwidth parameter” \"h\" controls how fast we try to dampen the function formula_16. In particular when \"h\" is small, then \"ψ\"(\"t\") will be approximately one for a large range of \"t\"’s, which means that formula_16 remains practically unaltered in the most important region of \"t\"’s.\n\nThe most common choice for function \"ψ\" is either the uniform function }, which effectively means truncating the interval of integration in the inversion formula to , or the Gaussian function . Once the function \"ψ\" has been chosen, the inversion formula may be applied, and the density estimator will be\n\nwhere \"K\" is the Fourier transform of the damping function \"ψ\". Thus the kernel density estimator coincides with the characteristic function density estimator.\n\nWe can extend the definition of the (global) mode to a local sense and define the local modes:\n\nformula_21\n\nNamely, M is the collection of points for which the density function is locally maximized. A natural estimator of formula_22 is a plug-in from KDE, where formula_23 and formula_24 are KDE version of formula_23 and formula_26. Under mild assumptions, formula_27 is a consistent estimator of formula_22. Note that one can use the mean shift algorithm to compute the estimator formula_27 numerically.\n\nA non-exhaustive list of software implementations of kernel density estimators includes:\n- In Analytica release 4.4, the \"Smoothing\" option for PDF results uses KDE, and from expressions it is available via the built-in codice_1 function.\n- In C/C++, FIGTree is a library that can be used to compute kernel density estimates using normal kernels. MATLAB interface available.\n- In C++, libagf is a library for variable kernel density estimation.\n- In C++, mlpack is a library that can compute KDE using many different kernels. It allows to set an error tolerance for faster computation. Python and R interfaces available.\n- in C# and F#, Math.NET Numerics is an open source library for numerical computation which includes kernel density estimation\n- In CrimeStat, kernel density estimation is implemented using five different kernel functions – normal, uniform, quartic, negative exponential, and triangular. Both single- and dual-kernel density estimate routines are available. Kernel density estimation is also used in interpolating a Head Bang routine, in estimating a two-dimensional Journey-to-crime density function, and in estimating a three-dimensional Bayesian Journey-to-crime estimate.\n- In ELKI, kernel density functions can be found in the package codice_2\n- In ESRI products, kernel density mapping is managed out of the Spatial Analyst toolbox and uses the Quartic(biweight) kernel.\n- In Excel, the Royal Society of Chemistry has created an add-in to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.\n- In gnuplot, kernel density estimation is implemented by the codice_3 option, the datafile can contain a weight and bandwidth for each point, or the bandwidth can be set automatically according to \"Silverman's rule of thumb\" (see above).\n- In Haskell, kernel density is implemented in the statistics package.\n- In IGOR Pro, kernel density estimation is implemented by the codice_4 operation (added in Igor Pro 7.00). Bandwidth can be user specified or estimated by means of Silverman, Scott or Bowmann and Azzalini. Kernel types are: Epanechnikov, Bi-weight, Tri-weight, Triangular, Gaussian and Rectangular.\n- In Java, the Weka (machine learning) package provides weka.estimators.KernelEstimator, among others.\n- In JavaScript, the visualization package D3.js offers a KDE package in its science.stats package.\n- In JMP, The Distribution platform can be used to create univariate kernel density estimates, and the Fit Y by X platform can be used to create bivariate kernel density estimates. It uses the formula formula_30, but not the optimized formula metioned in book of sliverman.\n- In Julia, kernel density estimation is implemented in the KernelDensity.jl package.\n- In MATLAB, kernel density estimation is implemented through the codice_5 function (Statistics Toolbox). As of the 2018a release of MATLAB, both the bandwidth and kernel smoother can be specified, including other options such as specifying the range of the kernel density Alternatively, a free MATLAB software package which implements an automatic bandwidth selection method is available from the MATLAB Central File Exchange for\n- 1-dimensional data\n- 2-dimensional data\n- n-dimensional data A free MATLAB toolbox with implementation of kernel regression, kernel density estimation, kernel estimation of hazard function and many others is available on these pages (this toolbox is a part of the book ).\n- In Mathematica, numeric kernel density estimation is implemented by the function codice_6 here and symbolic estimation is implemented using the function codice_7 here both of which provide data-driven bandwidths.\n- In Minitab, the Royal Society of Chemistry has created a macro to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4.\n- In the NAG Library, kernel density estimation is implemented via the codice_8 routine (available in both the Fortran and the C versions of the Library).\n- In Nuklei, C++ kernel density methods focus on data from the Special Euclidean group formula_31.\n- In Octave, kernel density estimation is implemented by the codice_9 option (econometrics package).\n- In Origin, 2D kernel density plot can be made from its user interface, and two functions, Ksdensity for 1D and Ks2density for 2D can be used from its LabTalk, Python, or C code.\n- In Perl, an implementation can be found in the Statistics-KernelEstimation module\n- In PHP, an implementation can be found in the MathPHP library\n- In Python, many implementations exist: pyqt_fit.kde Module in the PyQt-Fit package, SciPy (codice_10 and codice_11), Statsmodels (codice_12 and codice_13), and Scikit-learn (codice_14) (see comparison). KDEpy supports weighted data and its FFT implementation is orders of magnitude faster than the other implementations. The commonly used pandas library offers support for kde plotting through the plot method (codice_15). The getdist package for weighted and correlated MCMC samples supports optimized bandwidth, boundary correction and higher-order methods for 1D and 2D distributions.\n- In R, it is implemented through codice_16 in the base distribution, and codice_17 function is used in stats package, this function uses the optimized formula in sliverman's book. codice_18 in the KernSmooth library, codice_19 in the AdaptGauss library (for pareto distribution density estimation), codice_20 in the ks library, codice_21 and codice_22 in the evmix library (latter for boundary corrected kernel density estimation for bounded support), codice_23 in the np library (numeric and categorical data), codice_24 in the sm library. For an implementation of the codice_25 function, which does not require installing any packages or libraries, see kde.R. The btb library, dedicated to urban analysis, implements kernel density estimation through codice_26.\n- In SAS, codice_27 can be used to estimate univariate and bivariate kernel densities.\n- In Apache Spark, you can use the codice_28 class (see official documentation for more details )\n- In Stata, it is implemented through codice_29; for example codice_30. Alternatively a free Stata module KDENS is available from here allowing a user to estimate 1D or 2D density functions.\n- In Swift, it is implemented through codice_31 in the open-source statistics library SwiftStats.\n\n", "related": "\n- Kernel (statistics)\n- Kernel smoothing\n- Kernel regression\n- Density estimation (with presentation of other examples)\n- Mean-shift\n- Scale space The triplets {(\"x\", \"h\", KDE with bandwidth \"h\" evaluated at \"x\": all \"x\", \"h\" > 0} form a scale space representation of the data.\n- Multivariate kernel density estimation\n- Variable kernel density estimation\n- Head/tail breaks\n\n- Introduction to kernel density estimation A short tutorial which motivates kernel density estimators as an improvement over histograms.\n- Kernel Bandwidth Optimization A free online tool that instantly generates an optimized kernel density estimate of your data.\n- Free Online Software (Calculator) computes the Kernel Density Estimation for any data series according to the following Kernels: Gaussian, Epanechnikov, Rectangular, Triangular, Biweight, Cosine, and Optcosine.\n- Kernel Density Estimation Applet An online interactive example of kernel density estimation. Requires .NET 3.0 or later.\n"}
{"id": "523173", "url": "https://en.wikipedia.org/wiki?curid=523173", "title": "Linear separability", "text": "Linear separability\n\nIn Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are \"linearly separable\" if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by hyperplane.\n\nThe problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.\n\nLet formula_1 and formula_2 be two sets of points in an \"n\"-dimensional Euclidean space. Then formula_1 and formula_2 are \"linearly separable\" if there exist \"n\" + 1 real numbers formula_5, such that every point formula_6 satisfies formula_7 and every point formula_8 satisfies formula_9, where formula_10 is the formula_11-th component of formula_12.\n\nEquivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap).\n\nThree non-collinear points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case):\n\nHowever, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need \"two\" straight lines and thus is not linearly separable:\n\nNotice that three points which are collinear and of the form \"+ ⋅⋅⋅ — ⋅⋅⋅ +\" are also not linearly separable.\n\nA Boolean function in \"n\" variables can be thought of as an assignment of \"0\" or \"1\" to each vertex of a Boolean hypercube in \"n\" dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be \"linearly separable\" provided these two sets of points are linearly separable. The number of distinct Boolean functions is formula_13where \"n\" is the number of variables passed into the function.\n\nClassifying data is a common task in machine learning.\nSuppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a \"new\" data point will be in. In the case of support vector machines, a data point is viewed as a \"p\"-dimensional vector (a list of \"p\" numbers), and we want to know whether we can separate such points with a (\"p\" − 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the \"maximum-margin hyperplane\" and the linear classifier it defines is known as a \"maximum margin classifier\".\n\nMore formally, given some training data formula_14, a set of \"n\" points of the form\n\nwhere the \"y\" is either 1 or −1, indicating the set to which the point formula_16 belongs. Each formula_17 is a \"p\"-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having formula_18 from those having formula_19. Any hyperplane can be written as the set of points formula_20 satisfying\n\nwhere formula_22 denotes the dot product and formula_23 the (not necessarily normalized) normal vector to the hyperplane. The parameter formula_24 determines the offset of the hyperplane from the origin along the normal vector formula_23.\n\nIf the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance.\n\n", "related": "\n- Perceptron\n- Vapnik–Chervonenkis dimension\n"}
{"id": "40678189", "url": "https://en.wikipedia.org/wiki?curid=40678189", "title": "Bias–variance tradeoff", "text": "Bias–variance tradeoff\n\nIn statistics and machine learning, the bias–variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:\n- The \"bias error\" is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).\n- The \"variance\" is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).\n\nThe bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the \"irreducible error\", resulting from noise in the problem itself.\n\nThis tradeoff applies to all forms of supervised learning: classification, regression (function fitting), and structured output learning, though, it does not apply in all learning algorithms. It has also been invoked to explain the effectiveness of heuristics in human learning.\n\nIt is important to note that the bias-variance tradeoff is not universal. For example, \"both\" bias \"and\" variance decrease when increasing the width of a neural network. This means that it is not necessary to control the size of a neural network to control variance. This does not contradict the bias-variance decomposition because the bias-variance decomposition does not imply a bias-variance tradeoff.\n\nThe bias-variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit but may \"underfit\" their training data, failing to capture important regularities.\n\nModels with high variance are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate – despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials) but may produce lower variance predictions when applied beyond the training set.\n\nSuppose that we have a training set consisting of a set of points formula_1 and real values formula_2 associated with each point formula_3. We assume that there is a function with noise formula_4, where the noise, formula_5, has zero mean and variance formula_6.\n\nWe want to find a function formula_7, that approximates the true function formula_8 as well as possible, by means of some learning algorithm based on a training dataset (sample) formula_9. We make \"as well as possible\" precise by measuring the mean squared error between formula_10 and formula_7: we want formula_12 to be minimal, both for formula_1 \"and for points outside of our sample\". Of course, we cannot hope to do so perfectly, since the formula_2 contain noise formula_5; this means we must be prepared to accept an \"irreducible error\" in any function we come up with.\n\nFinding an formula_16 that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function formula_16 we select, we can decompose its expected error on an unseen sample formula_18 as follows:\n\nwhere\n\nand\n\nThe expectation ranges over different choices of the training set formula_9, all sampled from the same joint distribution formula_23. The three terms represent:\n- the square of the \"bias\" of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function formula_8 using a learning method for linear models, there will be error in the estimates formula_25 due to this assumption;\n- the \"variance\" of the learning method, or, intuitively, how much the learning method formula_25 will move around its mean;\n- the irreducible error formula_6.\n\nSince all three terms are non-negative, this forms a lower bound on the expected error on unseen samples.\n\nThe more complex the model formula_25 is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model \"move\" more to capture the data points, and hence its variance will be larger.\n\nThe derivation of the bias–variance decomposition for squared error proceeds as follows. For notational convenience, we abbreviate formula_29, formula_30 and we drop the formula_31 subscript on our expectation operators. First, recall that, by definition, for any random variable formula_32, we have\n\nRearranging, we get:\n\nSince formula_35 is deterministic, i.e. independent of formula_31,\n\nThus, given formula_38 and formula_39 (because formula_5 is noise), implies formula_41\n\nAlso, since formula_42\n\nThus, since formula_5 and formula_16 are independent, we can write\n\nFinally, MSE loss function (or negative log-likelihood) is obtained by taking the expectation value over formula_47:\n\nThe bias–variance decomposition forms the conceptual basis for regression regularization methods such as Lasso and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance.\n\nThe bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0–1 loss (misclassification rate), it is possible to find a similar decomposition. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.\n\nEven though the bias–variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms: a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited.\n\nDimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance; for example,\n- linear and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias.\n- In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied.\n- In \"k\"-nearest neighbor models, a high value of leads to high bias and low variance (see below).\n- In instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars.\n- In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance.\n\nOne way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many \"weak\" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines \"strong\" learners in a way that reduces their variance. \n\nModel validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off. \n\nIn the case of -nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias–variance decomposition to the parameter :\n\nwhere formula_50 are the nearest neighbors of in the training set. The bias (first term) is a monotone rising function of , while the variance (second term) drops off as is increased. In fact, under \"reasonable assumptions\" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.\n\nWhile widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.\n\nGeman et al. argue that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of “hard wiring” that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance.\n\n", "related": "\n- Accuracy and precision\n- Bias of an estimator\n- Gauss–Markov theorem\n- Hyperparameter optimization\n- Minimum-variance unbiased estimator\n- Model selection\n- Regression model validation\n- Supervised learning\n"}
{"id": "405562", "url": "https://en.wikipedia.org/wiki?curid=405562", "title": "Solomonoff's theory of inductive inference", "text": "Solomonoff's theory of inductive inference\n\nSolomonoff's theory of inductive inference is Ray Solomonoff's mathematical formalization of Occam's razor. It explains observations of the world by the smallest computer program that outputs those observations. Solomonoff proved that this explanation is the most likely one, by assuming the world is generated by an unknown computer program. That is to say the probability distribution of all computer programs that output the observations favors the shortest one.\n\nPrediction is done using a completely Bayesian framework. The universal prior is calculated for all computable sequences—this is the universal a priori probability distribution;\nno computable hypothesis will have a zero probability. This means that Bayes rule of causation can be used in predicting the continuation of any particular computable sequence.\n\nThe theory is based in philosophical foundations, and was founded by Ray Solomonoff around 1960. It is a mathematically formalized combination of Occam's razor and the Principle of Multiple Explanations.\nAll computable theories which perfectly describe previous observations are used to calculate the probability of the next observation, with more weight put on the shorter computable theories. Marcus Hutter's universal artificial intelligence builds upon this to calculate the expected value of an action.\n\nThe proof of the \"razor\" is based on the known mathematical properties of a probability distribution over a countable set. These properties are relevant because the infinite set of all programs is a denumerable set. The sum S of the probabilities of all programs must be exactly equal to one (as per the definition of probability) thus the probabilities must roughly decrease as we enumerate the infinite set of all programs, otherwise S will be strictly greater than one. To be more precise, for every formula_1 > 0, there is some length \"l\" such that the probability of all programs longer than \"l\" is at most formula_1. This does not, however, preclude very long programs from having very high probability.\n\nFundamental ingredients of the theory are the concepts of algorithmic probability and Kolmogorov complexity. The universal prior probability of any prefix \"p\" of a computable sequence \"x\" is the sum of the probabilities of all programs (for a universal computer) that compute something starting with \"p\". Given some \"p\" and any computable but unknown probability distribution from which \"x\" is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of \"x\" in optimal fashion.\n\nThough Solomonoff's inductive inference is not computable, several AIXI-derived algorithms approximate it in order to make it run on a modern computer. The more computing power they are given, the closer their predictions are to the predictions of inductive inference (their mathematical limit is Solomonoff's inductive inference).\n\nAnother direction of inductive inference is based on E. Mark Gold's model of learning in the limit from 1967 and has developed since then more and more models of learning. The general scenario is the following: Given a class \"S\" of computable functions, is there a learner (that is, recursive functional) which for any input of the form (\"f\"(0),\"f\"(1)...,\"f\"(\"n\")) outputs a hypothesis (an index \"e\" with respect to a previously agreed on acceptable numbering of all computable functions; the indexed function may be required consistent with the given values of \"f\"). A learner \"M\" learns a function \"f\" if almost all its hypotheses are the same index \"e\", which generates the function \"f\"; \"M\" learns \"S\" if \"M\" learns every \"f\" in \"S\". Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable. \n\nMany related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold's pioneering paper in 1967 onwards. A far reaching extension of the Gold’s approach is developed by Schmidhuber's theory of generalized Kolmogorov complexities, which are kinds of super-recursive algorithms.\n\nThe third mathematically based direction of inductive inference makes use of the theory of automata and computation. In this context, the process of inductive inference is performed by an abstract automaton called an inductive Turing machine (Burgin, 2005).\n\"Inductive Turing machines\" represent the next step in the development of computer science providing better models for contemporary computers and computer networks (Burgin, 2001) and forming an important class of super-recursive algorithms as they satisfy all conditions in the definition of algorithm. Namely, each inductive Turing machines is a type of effective method in which a definite list of well-defined instructions for completing a task, when given an initial state, will proceed through a well-defined series of successive states, eventually terminating in an end-state. The difference between an inductive Turing machine and a Turing machine is that to produce the result a Turing machine has to stop, while in some cases an inductive Turing machine can do this without stopping. Stephen Kleene called procedures that could run forever without stopping by the name \"calculation procedure or algorithm\" (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit \"some object\" (Kleene 1952:137). This condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps, but inductive Turing machines do not always tell at which step the result has been obtained.\n\nSimple inductive Turing machines are equivalent to other models of computation. More advanced inductive Turing machines are much more powerful. It is proved (Burgin, 2005) that limiting partial recursive functions, trial and error predicates, general Turing machines, and simple inductive Turing machines are equivalent models of computation. However, simple inductive Turing machines and general Turing machines give direct constructions of computing automata, which are thoroughly grounded in physical machines. In contrast, trial and error predicates, limiting recursive functions and limiting partial recursive functions present syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial and error predicates as Turing machines are related to partial recursive functions and lambda-calculus.\n\nNote that only simple inductive Turing machines have the same structure (but different functioning semantics of the output mode) as Turing machines. Other types of inductive Turing machines have an essentially more advanced structure due to the structured memory and more powerful instructions. Their utilization for inference and learning allows achieving higher efficiency and better reflects learning of people (Burgin and Klinger, 2004).\n\nSome researchers confuse computations of inductive Turing machines with non-stopping computations or with infinite time computations. First, some of computations of inductive Turing machines halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not give. Second, some non-stopping computations of inductive Turing machines give results, while others do not give. Rules of inductive Turing machines determine when a computation (stopping or non-stopping) gives a result. Namely, an inductive Turing machine produces output from time to time and once this output stops changing, it is considered the result of the computation. It is necessary to know that descriptions of this rule in some papers are incorrect. For instance, Davis (2006: 128) formulates the rule when result is obtained without stopping as \"once the correct output has been produced any subsequent output will simply repeat this correct result.\" Third, in contrast to the widespread misconception, inductive Turing machines give results (when it happens) always after a finite number of steps (in finite time) in contrast to infinite and infinite-time computations.\nThere are two main distinctions between conventional Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine always informs (by halting or by coming to a final state) when the result is obtained, while a simple inductive Turing machine in some cases does inform about reaching the result, while in other cases (where the conventional Turing machine is helpless), it does not inform. People have an illusion that a computer always itself informs (by halting or by other means) when the result is obtained. In contrast to this, users themselves have to decide in many cases whether the computed result is what they need or it is necessary to continue computations. Indeed, everyday desktop computer applications like word processors and spreadsheets spend most of their time waiting in event loops, and do not terminate until directed to do so by users.\n\nEvolutionary approach to inductive inference is accomplished by another class of automata called evolutionary inductive Turing machines (Burgin and Eberbach, 2009; 2012). An evolutionary inductive Turing machine is a (possibly infinite) sequence \"E\" = {\"A\"[\"t\"]; \"t\" = 1, 2, 3, ... } of inductive Turing machines \"A\"[\"t\"] each working on generations X[t] which are coded as words in the alphabet of the machines \"A\"[\"t\"]. The goal is to build a “population” \"Z\" satisfying the inference condition. The automaton \"A\"[\"t\"] called a component, or a level automaton, of E represents (encodes) a one-level evolutionary algorithm that works with input generations \"X\"[\"i\"] of the population by applying the variation operators v and selection operator s. The first generation \"X\"[0] is given as input to \"E\" and is processed by the automaton \"A\"[1], which generates/produces the first generation \"X\"[1] as its transfer output, which goes to the automaton \"A\"[2]. For all \"t\" = 1, 2, 3, ..., the automaton \"A\"[\"t\"] receives the generation \"X\"[\"t\" − 1] as its input from \"A\"[\"t\" − 1] and then applies the variation operator v and selection operator \"s\", producing the generation \"X\"[\"i\" + 1] and sending it to \"A\"[\"t\" + 1] to continue evolution.\n\n", "related": "\n- Algorithmic information theory\n- Bayesian inference\n- Language identification in the limit\n- Inductive inference\n- Inductive probability\n- Mill's methods\n- Minimum description length\n- Minimum message length\n- For a philosophical viewpoint, see: Problem of induction and New riddle of induction\n\n- Burgin, M. (2005), \"Super-recursive Algorithms\", Monographs in computer science, Springer.\n- Burgin, M., \"How We Know What Technology Can Do\", \"Communications of the ACM\", v. 44, No. 11, 2001, pp. 82–88.\n- Burgin, M.; Eberbach, E., \"Universality for Turing Machines, Inductive Turing Machines and Evolutionary Algorithms\", \"Fundamenta Informaticae\", v. 91, No. 1, 2009, 53–77.\n- Burgin, M.; Eberbach, E., \"On Foundations of Evolutionary Computation: An Evolutionary Automata Approach\", in \"Handbook of Research on Artificial Immune Systems and Natural Computing: Applying Complex Adaptive Technologies\" (Hongwei Mo, Ed.), IGI Global, Hershey, Pennsylvania, 2009, 342–360.\n- Burgin, M.; Eberbach, E., \"Evolutionary Automata: Expressiveness and Convergence of Evolutionary Computation\", \"Computer Journal\", v. 55, No. 9, 2012, pp. 1023–1029.\n- Burgin, M.; Klinger, A. Experience, Generations, and Limits in Machine Learning, \"Theoretical Computer Science\", v. 317, No. 1/3, 2004, pp. 71–91\n- Davis, Martin (2006) \"The Church–Turing Thesis: Consensus and opposition]\". Proceedings, Computability in Europe 2006. Lecture Notes in Computer Science, 3988 pp. 125–132.\n- Gasarch, W.; Smith, C. H. (1997) \"A survey of inductive inference with an emphasis on queries\". \"Complexity, logic, and recursion theory\", Lecture Notes in Pure and Appl. Math., 187, Dekker, New York, pp. 225–260.\n- Hay, Nick. \"Universal Semimeasures: An Introduction,\" CDMTCS Research Report Series, University of Auckland, Feb. 2007.\n- Jain, Sanjay ; Osherson, Daniel ; Royer, James ; Sharma, Arun, \"Systems that Learn: An Introduction to Learning Theory\" (second edition), MIT Press, 1999.\n- Li Ming; Vitanyi, Paul, \"An Introduction to Kolmogorov Complexity and Its Applications\", 2nd Edition, Springer Verlag, 1997.\n- Osherson, Daniel ; Stob, Michael ; Weinstein, Scott, \"Systems That Learn, An Introduction to Learning Theory for Cognitive and Computer Scientists\", MIT Press, 1986.\n\n- Algorithmic probability – Scholarpedia\n"}
{"id": "42579971", "url": "https://en.wikipedia.org/wiki?curid=42579971", "title": "Inductive probability", "text": "Inductive probability\n\nInductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.\n\nThere are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Inference establishes new facts from data. Its basis is Bayes' theorem.\n\nInformation describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements.\n\nOccam's razor says the \"simplest theory, consistent with the data is most likely to be correct\". The \"simplest theory\" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.\n\nProbability and statistics was focused on probability distributions and tests of significance. Probability was formal, well defined, but limited in scope. In particular its application was limited to situations that could be defined as an experiment or trial, with a well defined population.\n\nBayes's theorem is named after Rev. Thomas Bayes 1701–1761. Bayesian inference broadened the application of probability to many situations where a population was not well defined. But Bayes' theorem always depended on prior probabilities, to generate new probabilities. It was unclear where these prior probabilities should come from.\n\nRay Solomonoff developed algorithmic probability which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs, that give shorter representations of the data circa 1964.\n\nChris Wallace and D. M. Boulton developed minimum message length circa 1968. Later Jorma Rissanen developed the minimum description length circa 1978. These methods allow information theory to be related to probability, in a way that can be compared to the application of Bayes' theorem, but which give a source and explanation for the role of prior probabilities.\n\nMarcus Hutter combined decision theory with the work of Ray Solomonoff and Andrey Kolmogorov to give a theory for the Pareto optimal behavior for an Intelligent agent, circa 1998.\n\nThe program with the shortest length that matches the data is the most likely to predict future data. This is the thesis behind the minimum message length and minimum description length methods.\n\nAt first sight Bayes' theorem appears different from the minimimum message/description length principle. At closer inspection it turns out to be the same. Bayes' theorem is about conditional probabilities, and states the probability that event \"B\" happens if firstly event \"A\" happens:\n\nbecomes in terms of message length \"L\",\n\nThis means that if all the information is given describing an event then the length of the information may be used to give the raw probability of the event. So if the information describing the occurrence of \"A\" is given, along with the information describing \"B\" given \"A\", then all the information describing \"A\" and \"B\" has been given.\n\nOverfitting occurs when the model matches the random noise and not the pattern in the data. For example, take the situation where a curve is fitted to a set of points. If a polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviations from the fitted curve will be smaller. Smaller information length means higher probability.\n\nHowever, the information needed to describe the curve must also be considered. The total information for a curve with many terms may be greater than for a curve with fewer terms, that has not as good a fit, but needs less information to describe the polynomial.\n\nSolomonoff's theory of inductive inference is also inductive inference. A bit string \"x\" is observed. Then consider all programs that generate strings starting with \"x\". Cast in the form of inductive inference, the programs are theories that imply the observation of the bit string \"x\".\n\nThe method used here to give probabilities for inductive inference is based on Solomonoff's theory of inductive inference.\n\nIf all the bits are 1, then people infer that there is a bias in the coin and that it is more likely also that the next bit is 1 also. This is described as learning from, or detecting a pattern in the data.\n\nSuch a pattern may be represented by a computer program. A short computer program may be written that produces a series of bits which are all 1. If the length of the program \"K\" is formula_3 bits then its prior probability is,\n\nThe length of the shortest program that represents the string of bits is called the Kolmogorov complexity.\n\nKolmogorov complexity is not computable. This is related to the halting problem. When searching for the shortest program some programs may go into an infinite loop.\n\nThe Greek philosopher Epicurus is quoted as saying \"If more than one theory is consistent with the observations, keep all theories\".\n\nAs in a crime novel all theories must be considered in determining the likely murderer, so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits.\n\nPrograms that are already longer than \"n\" have no predictive power. The raw (or prior) probability that the pattern of bits is random (has no pattern) is formula_5.\n\nEach program that produces the sequence of bits, but is shorter than the \"n\" is a theory/pattern about the bits with a probability of formula_6 where \"k\" is the length of the program.\n\nThe probability of receiving a sequence of bits \"y\" after receiving a series of bits \"x\" is then the conditional probability of receiving \"y\" given \"x\", which is the probability of \"x\" with \"y\" appended, divided by the probability of \"x\".\n\nThe programming language affects the predictions of the next bit in the string. The language acts as a prior probability. This is particularly a problem where the programming language codes for numbers and other data types. Intuitively we think that 0 and 1 are simple numbers, and that prime numbers are somehow more complex than numbers that may be composite.\n\nUsing the Kolmogorov complexity gives an unbiased estimate (a universal prior) of the prior probability of a number. As a thought experiment an intelligent agent may be fitted with a data input device giving a series of numbers, after applying some transformation function to the raw numbers. Another agent might have the same input device with a different transformation function. The agents do not see or know about these transformation functions. Then there appears no rational basis for preferring one function over another. A universal prior insures that although two agents may have different initial probability distributions for the data input, the difference will be bounded by a constant.\n\nSo universal priors do not eliminate an initial bias, but they reduce and limit it. Whenever we describe an event in a language, either using a natural language or other, the language has encoded in it our prior expectations. So some reliance on prior probabilities are inevitable.\n\nA problem arises where an intelligent agent's prior expectations interact with the environment to form a self reinforcing feed back loop. This is the problem of bias or prejudice. Universal priors reduce but do not eliminate this problem.\n\nThe theory of universal artificial intelligence applies decision theory to inductive probabilities. The theory shows how the best actions to optimize a reward function may be chosen. The result is a theoretical model of intelligence.\n\nIt is a fundamental theory of intelligence, which optimizes the agents behavior in,\n- Exploring the environment; performing actions to get responses that broaden the agents knowledge.\n- Competing or co-operating with another agent; games.\n- Balancing short and long term rewards.\n\nIn general no agent will always provide the best actions in all situations. A particular choice made by an agent may be wrong, and the environment may provide no way for the agent to recover from an initial bad choice. However the agent is Pareto optimal in the sense that no other agent will do better than this agent in this environment, without doing worse in another environment. No other agent may, in this sense, be said to be better.\n\nAt present the theory is limited by incomputability (the halting problem). Approximations may be used to avoid this. Processing speed and combinatorial explosion remain the primary limiting factors for artificial intelligence.\n\nProbability is the representation of uncertain or partial knowledge about the truth of statements. Probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data.\n\nThis description of probability may seem strange at first. In natural language we refer to \"the probability\" that the sun will rise tomorrow. We do not refer to \"your probability\" that the sun will rise. But in order for inference to be correctly modeled probability must be personal, and the act of inference generates new posterior probabilities from prior probabilities.\n\nProbabilities are personal because they are conditional on the knowledge of the individual. Probabilities are subjective because they always depend, to some extent, on prior probabilities assigned by the individual. Subjective should not be taken here to mean vague or undefined.\n\nThe term intelligent agent is used to refer to the holder of the probabilities. The intelligent agent may be a human or a machine. If the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event.\n\nIf however the agent uses the probability to interact with the environment there may be a feedback, so that two agents in the identical environment starting with only slightly different priors, end up with completely different probabilities. In this case optimal decision theory as in Marcus Hutter's Universal Artificial Intelligence will give Pareto optimal performance for the agent. This means that no other intelligent agent could do better in one environment without doing worse in another environment.\n\nIn deductive probability theories, probabilities are absolutes, independent of the individual making the assessment. But deductive probabilities are based on,\n- Shared knowledge.\n- Assumed facts, that should be inferred from the data.\n\nFor example, in a trial the participants are aware the outcome of all the previous history of trials. They also assume that each outcome is equally probable. Together this allows a single unconditional value of probability to be defined.\n\nBut in reality each individual does not have the same information. And in general the probability of each outcome is not equal. The dice may be loaded, and this loading needs to be inferred from the data.\n\nThe principle of indifference has played a key role in probability theory. It says that if N statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable.\n\nTaken seriously, in evaluating probability this principle leads to contradictions. Suppose there are 3 bags of gold in the distance and one is asked to select one. Then because of the distance one cannot see the bag sizes. You estimate using the principle of indifference that each bag has equal amounts of gold, and each bag has one third of the gold.\n\nNow, while one of us is not looking, the other takes one of the bags and divide it into 3 bags. Now there are 5 bags of gold. The principle of indifference now says each bag has one fifth of the gold. A bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold.\n\nTaken as a value associated with the bag the values are different therefore contradictory. But taken as an estimate given under a particular scenario, both values are separate estimates given under different circumstances and there is no reason to believe they are equal.\n\nEstimates of prior probabilities are particularly suspect. Estimates will be constructed that do not follow any consistent frequency distribution. For this reason prior probabilities are considered as estimates of probabilities rather than probabilities.\n\nA full theoretical treatment would associate with each probability,\n- The statement\n- Prior knowledge\n- Prior probabilities\n- The estimation procedure used to give the probability.\n\nInductive probability combines two different approaches to probability.\n- Probability and information\n- Probability and frequency\n\nEach approach gives a slightly different viewpoint. Information theory is used in relating probabilities to quantities of information. This approach is often used in giving estimates of prior probabilities.\n\nFrequentist probability defines probabilities as objective statements about how often an event occurs. This approach may be stretched by defining the trials to be over possible worlds. Statements about possible worlds define events.\n\nWhereas logic represents only two values; true and false as the values of statement, probability associates a number in [0,1] to each statement. If the probability of a statement is 0, the statement is false. If the probability of a statement is 1 the statement is true.\n\nIn considering some data as a string of bits the prior probabilities for a sequence of 1s and 0s, the probability of 1 and 0 is equal. Therefore, each extra bit halves the probability of a sequence of bits.\nThis leads to the conclusion that,\nWhere formula_8 is the probability of the string of bits formula_9 and formula_10 is its length.\n\nThe prior probability of any statement is calculated from the number of bits needed to state it. See also information theory.\n\nTwo statements formula_11 and formula_12 may be represented by two separate encodings. Then the length of the encoding is,\n\nor in terms of probability,\n\nBut this law is not always true because there may be a shorter method of encoding formula_12 if we assume formula_11. So the above probability law applies only if formula_11 and formula_12 are \"independent\".\n\nThe primary use of the information approach to probability is to provide estimates of the complexity of statements. Recall that Occam's razor states that \"All things being equal, the simplest theory is the most likely to be correct\". In order to apply this rule, first there needs to be a definition of what \"simplest\" means. Information theory defines simplest to mean having the shortest encoding.\n\nKnowledge is represented as statements. Each statement is a Boolean expression. Expressions are encoded by a function that takes a description (as against the value) of the expression and encodes it as a bit string.\n\nThe length of the encoding of a statement gives an estimate of the probability of a statement. This probability estimate will often be used as the prior probability of a statement.\n\nTechnically this estimate is not a probability because it is not constructed from a frequency distribution. The probability estimates given by it do not always obey the law of total of probability. Applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement.\n\nAn expression is constructed from sub expressions,\n- Constants (including function identifier).\n- Application of functions.\n- quantifiers.\n\nA Huffman code must distinguish the 3 cases. The length of each code is based on the frequency of each type of sub expressions.\n\nInitially constants are all assigned the same length/probability. Later constants may be assigned a probability using the Huffman code based on the number of uses of the function id in all expressions recorded so far. In using a Huffman code the goal is to estimate probabilities, not to compress the data.\n\nThe length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter.\n\nThe length of a quantifier is the length of the expression being quantified over.\n\nNo explicit representation of natural numbers is given. However natural numbers may be constructed by applying the successor function to 0, and then applying other arithmetic functions. A distribution of natural numbers is implied by this, based on the complexity of constructing each number.\n\nRational numbers are constructed by the division of natural numbers. The simplest representation has no common factors between the numerator and the denominator. This allows the probability distribution of natural numbers may be extended to rational numbers.\n\nThe probability of an event may be interpreted as the frequencies of outcomes where the statement is true divided by the total number of outcomes. If the outcomes form a continuum the frequency may need to be replaced with a measure.\n\nEvents are sets of outcomes. Statements may be related to events. A Boolean statement B about outcomes defines a set of outcomes b,\n\nEach probability is always associated with the state of knowledge at a particular point in the argument. Probabilities before an inference are known as prior probabilities, and probabilities after are known as posterior probabilities.\n\nProbability depends on the facts known. The truth of a fact limits the domain of outcomes to the outcomes consistent with the fact. Prior probabilities are the probabilities before a fact is known. Posterior probabilities are after a fact is known. The posterior probabilities are said to be conditional on the fact. the probability that formula_12 is true given that formula_11 is true is written as: formula_22\n\nAll probabilities are in some sense conditional. The prior probability of formula_12 is,\n\nIn the frequentist approach, probabilities are defined as the ratio of the number of outcomes within an event to the total number of outcomes. In the possible world model each possible world is an outcome, and statements about possible worlds define events. The probability of a statement being true is the number of possible worlds where the statement is true divided by the total number of possible worlds. The probability of a statement formula_11 being true about possible worlds is then,\n\nFor a conditional probability.\n\nthen\n\nUsing symmetry this equation may be written out as Bayes' law.\n\nThis law describes the relationship between prior and posterior probabilities when new facts are learnt.\n\nWritten as quantities of information Bayes' Theorem becomes,\n\nTwo statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is,\n\nthen Bayes' Theorem reduces to,\n\nFor a set of mutually exclusive possibilities formula_33, the sum of the posterior probabilities must be 1.\n\nSubstituting using Bayes' theorem gives the law of total probability\n\nThis result is used to give the extended form of Bayes' theorem,\n\nThis is the usual form of Bayes' theorem used in practice, because it guarantees the sum of all the posterior probabilities for formula_33 is 1.\n\nFor mutually exclusive possibilities, the probabilities add.\n\nUsing\nThen the alternatives\nare all mutually exclusive. Also,\n\nso, putting it all together,\n\nAs,\nthen\n\nImplication is related to conditional probability by the following equation,\n\nDerivation,\n\nBayes' theorem may be used to estimate the probability of a hypothesis or theory H, given some facts F. The posterior probability of H is then\n\nor in terms of information,\n\nBy assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is formula_52\n\nformula_53 represents the amount of information needed to represent the facts F, if H is true. formula_54 is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true. This is the evidence that the hypothesis H is true.\n\nIf formula_54 is estimated from encoding length then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory.\n\nIf a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability formula_56.\n\nProbabilities may be calculated from the extended form of Bayes' theorem. Given all mutually exclusive hypothesis formula_57 which give evidence, such that,\n\nand also the hypothesis R, that none of the hypothesis is true, then,\n\nIn terms of information,\n\nIn most situations it is a good approximation to assume that formula_61 is independent of formula_62, which means formula_63 giving,\n\nAbductive inference starts with a set of facts \"F\" which is a statement (Boolean expression). Abductive reasoning is of the form,\n\nThe theory \"T\", also called an explanation of the condition \"F\", is an answer to the ubiquitous factual \"why\" question. For example, for the condition \"F\" is \"Why do apples fall?\". The answer is a theory \"T\" that implies that apples fall;\n\nInductive inference is of the form,\n\nIn terms of abductive inference, \"all objects in a class C or set have a property P\" is a theory that implies the observed condition, \"All observed objects in a class C have a property P\".\n\nSo inductive inference is a special case of abductive inference. In common usage the term inductive inference is often used to refer to both abductive and inductive inference.\n\nInductive inference is related to generalization. Generalizations may be formed from statements by replacing a specific value with membership of a category, or by replacing membership of a category with membership of a broader category. In deductive logic, generalization is a powerful method of generating new theories that may be true. In inductive inference generalization generates theories that have a probability of being true.\n\nThe opposite of generalization is specialization. Specialization is used in applying a general rule to a specific case. Specializations are created from generalizations by replacing membership of a category by a specific value, or by replacing a category with a sub category.\n\nThe Linnaen classification of living things and objects forms the basis for generalization and specification. The ability to identify, recognize and classify is the basis for generalization. Perceiving the world as a collection of objects appears to be a key aspect of human intelligence. It is the object oriented model, in the non computer science sense.\n\nThe object oriented model is constructed from our perception. In particularly vision is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another. Computer vision uses this mapping to construct 3D images from stereo image pairs.\n\nInductive logic programming is a means of constructing theory that implies a condition. Plotkin's \"relative least general generalization (rlgg)\" approach constructs the simplest generalization consistent with the condition.\n\nIsaac Newton used inductive arguments in constructing his law of universal gravitation. Starting with the statement,\n- The center of an apple falls towards the center of the earth.\n\nGeneralizing by replacing apple for object, and earth for object gives, in a two body system,\n- The center of an object falls towards the center of another object.\n\nThe theory explains all objects falling, so there is strong evidence for it. The second observation,\n- The planets appear to follow an elliptical path.\n\nAfter some complicated mathematical calculus, it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse. So induction gives evidence for the inverse square law.\n\nUsing Galileo's observation that all objects drop with the same speed,\n\nwhere formula_68 and formula_69 vectors towards the center of the other object. Then using Newton's third law formula_70\n\nImplication determines condition probability as,\n\nSo,\n\nThis result may be used in the probabilities given for Bayesian hypothesis testing. For a single theory, H = T and,\n\nor in terms of information, the relative probability is,\n\nNote that this estimate for P(T|F) is not a true probability. If formula_77 then the theory has evidence to support it. Then for a set of theories formula_78, such that formula_77,\n\ngiving,\n\nMake a list of all the shortest programs formula_84 that each produce a distinct infinite string of bits, and satisfy the relation,\n\nwhere formula_86 is the result of running the program formula_84 and formula_88 truncates the string after \"n\" bits.\n\nThe problem is to calculate the probability that the source is produced by program formula_89 given that the truncated source after n bits is \"x\". This is represented by the conditional probability,\n\nUsing the extended form of Bayes' theorem\n\nThe extended form relies on the law of total probability. This means that the formula_92 must be distinct possibilities, which is given by the condition that each formula_84 produce a different infinite string. Also one of the conditions formula_92 must be true. This must be true, as in the limit as formula_95 there is always at least one program that produces formula_96.\n\nAs formula_84 are chosen so that formula_98 then,\n\nThe apriori probability of the string being produced from the program, given no information about the string, is based on the size of the program,\n\ngiving,\n\nPrograms that are the same or longer than the length of \"x\" provide no predictive power. Separate them out giving,\n\nThen identify the two probabilities as,\n\nBut the prior probability that \"x\" is a random set of bits is formula_5. So,\n\nThe probability that the source is random, or unpredictable is,\n\nA model of how worlds are constructed is used in determining the probabilities of theories,\n- A random bit string is selected.\n- A condition is constructed from the bit string.\n- A world is constructed that is consistent with the condition.\n\nIf \"w\" is the bit string then the world is created such that formula_108 is true. An intelligent agent has some facts about the word, represented by the bit string \"c\", which gives the condition,\n\nThe set of bit strings identical with any condition \"x\" is formula_110.\n\nA theory is a simpler condition that explains (or implies) \"C\". The set of all such theories is called \"T\",\n\nextended form of Bayes' theorem may be applied\nwhere,\n\nTo apply Bayes' theorem the following must hold: formula_33 is a partition of the event space.\n\nFor formula_117 to be a partition, no bit string \"n\" may belong to two theories. To prove this assume they can and derive a contradiction,\n\nSecondly prove that \"T\" includes all outcomes consistent with the condition. As all theories consistent with \"C\" are included then formula_108 must be in this set.\n\nSo Bayes theorem may be applied as specified giving,\n\nUsing the implication and condition probability law, the definition of formula_117 implies,\n\nThe probability of each theory in \"T\" is given by,\n\nso,\n\nFinally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy,\n\ngiving\n\nThis is the probability of the theory \"t\" after observing that the condition \"C\" holds.\n\nTheories that are less probable than the condition \"C\" have no predictive power. Separate them out giving,\n\nThe probability of the theories without predictive power on \"C\" is the same as the probability of \"C\". So,\n\nSo the probability \n\nand the probability of no prediction for C, written as formula_132,\n\nThe probability of a condition was given as,\n\nBit strings for theories that are more complex than the bit string given to the agent as input have no predictive power. There probabilities are better included in the \"random\" case. To implement this a new definition is given as \"F\" in,\n\nUsing \"F\", an improved version of the abductive probabilities is,\n\n- William of Ockham\n- Thomas Bayes\n- Ray Solomonoff\n- Andrey Kolmogorov\n- Chris Wallace\n- D. M. Boulton\n- Jorma Rissanen\n- Marcus Hutter\n\n", "related": "\n- Abductive reasoning\n- Algorithmic probability\n- Algorithmic information theory\n- Bayesian inference\n- Information theory\n- Inductive inference\n- Inductive logic programming\n- Inductive reasoning\n- Learning\n- Minimum message length\n- Minimum description length\n- Occam's razor\n- Solomonoff's theory of inductive inference\n- Universal artificial intelligence\n\n- Rathmanner, S and Hutter, M., \"A Philosophical Treatise of Universal Induction\" in Entropy 2011, 13, 1076–1136: A very clear philosophical and mathematical analysis of Solomonoff's Theory of Inductive Inference.\n- C.S. Wallace, Statistical and Inductive Inference by Minimum Message Length, Springer-Verlag (Information Science and Statistics), , May 2005 – chapter headings, table of contents and sample pages.\n"}
{"id": "37787103", "url": "https://en.wikipedia.org/wiki?curid=37787103", "title": "Universal portfolio algorithm", "text": "Universal portfolio algorithm\n\nThe universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late Stanford University information theorist Thomas M. Cover.\nThe algorithm rebalances the portfolio at the beginning of each trading period. At the beginning of the first trading period it starts with a naive diversification. In the following trading periods the portfolio composition depends on the historical total return of all possible constant-rebalanced portfolios.\n", "related": "NONE"}
{"id": "41370976", "url": "https://en.wikipedia.org/wiki?curid=41370976", "title": "Kernel embedding of distributions", "text": "Kernel embedding of distributions\n\nIn machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. This learning framework is very general and can be applied to distributions over any space formula_1 on which a sensible kernel function (measuring similarity between elements of formula_1) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in formula_3, discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf. A review of recent works on kernel embedding of distributions can be found in.\n\nThe analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.\n\nMethods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages: \n1. Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables\n2. Intermediate density estimation is not needed\n3. Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel)\n4. If a \"characteristic\" kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick, computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations\n5. Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution) to the kernel embedding of the true underlying distribution can be proven.\n6. Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods\nThus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.\n\nLet formula_4 denote a random variable with domain formula_5 and distribution formula_6 Given a kernel formula_7 on formula_8 the Moore–Aronszajn theorem asserts the existence of a RKHS formula_9 (a Hilbert space of functions formula_10 equipped with inner products formula_11 and norms formula_12) in which the element formula_13 satisfies the reproducing property \n\nOne may alternatively consider formula_13 an implicit feature mapping formula_16 from formula_5 to formula_18 (which is therefore also called the feature space), so that formula_19 can be viewed as a measure of similarity between points formula_20 While the similarity measure is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel.\n\nThe kernel embedding of the distribution formula_21 in formula_18 (also called the kernel mean or mean map) is given by:\n\nIf formula_21 allows a square integrable density formula_25, then formula_26, where formula_27 is the Hilbert–Schmidt integral operator. A kernel is \"characteristic\" if the mean embedding formula_28 is injective. Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used.\n\nGiven formula_29 training examples formula_30 drawn independently and identically distributed (i.i.d.) from formula_31 the kernel embedding of formula_21 can be empirically estimated as\n\nIf formula_34 denotes another random variable (for simplicity, assume the co-domain of formula_34 is also formula_5 with the same kernel formula_7 which satisfies formula_38), then the joint distribution formula_39 can be mapped into a tensor product feature space formula_40 via \n\nBy the equivalence between a tensor and a linear map, this joint embedding may be interpreted as an uncentered cross-covariance operator formula_42 from which the cross-covariance of mean-zero functions formula_43 can be computed as \n\nGiven formula_29 pairs of training examples formula_46 drawn i.i.d. from formula_21, we can also empirically estimate the joint distribution kernel embedding via\n\nGiven a conditional distribution formula_49 one can define the corresponding RKHS embedding as \n\nNote that the embedding of formula_51 thus defines a family of points in the RKHS indexed by the values formula_52 taken by conditioning variable formula_4. By fixing formula_4 to a particular value, we obtain a single element in formula_9, and thus it is natural to define the operator\n\nwhich given the feature mapping of formula_52 outputs the conditional embedding of formula_34 given formula_59 Assuming that for all formula_60 it can be shown that \n\nThis assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains. Nevertheless, even in cases where the assumption fails, formula_62 may still be used to approximate the conditional kernel embedding formula_63 and in practice, the inversion operator is replaced with a regularized version of itself formula_64 (where formula_65 denotes the identity matrix).\n\nGiven training examples formula_66 the empirical kernel conditional embedding operator may be estimated as \n\nwhere formula_68 are implicitly formed feature matrices, formula_69 is the Gram matrix for samples of formula_4, and formula_71 is a regularization parameter needed to avoid overfitting.\n\nThus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of formula_34 in the feature space:\n\nwhere formula_74 and formula_75\n\n- The expectation of any function formula_76 in the RKHS can be computed as an inner product with the kernel embedding:\n\n- In the presence of large sample sizes, manipulations of the formula_78 Gram matrix may be computationally demanding. Through use of a low-rank approximation of the Gram matrix (such as the incomplete Cholesky factorization), running time and memory requirements of kernel-embedding-based learning algorithms can be drastically reduced without suffering much loss in approximation accuracy.\n\n- If formula_7 is defined such that formula_80 takes values in formula_81 for all formula_82 with formula_83 (as is the case for the widely used radial basis function kernels), then with probability at least formula_84:\n\n- The rate of convergence (in RKHS norm) of the empirical kernel embedding to its distribution counterpart is formula_90 and does \"not\" depend on the dimension of formula_4.\n\n- Statistics based on kernel embeddings thus avoid the curse of dimensionality, and though the true underlying distribution is unknown in practice, one can (with high probability) obtain an approximation within formula_90 of the true kernel embedding based on a finite sample of size formula_29.\n\n- For the embedding of conditional distributions, the empirical estimate can be seen as a \"weighted\" average of feature mappings (where the weights formula_94 depend on the value of the conditioning variable and capture the effect of the conditioning on the kernel embedding). In this case, the empirical estimate converges to the conditional distribution RKHS embedding with rate formula_95 if the regularization parameter formula_71 is decreased as formula_97 though faster rates of convergence may be achieved by placing additional assumptions on the joint distribution.\n\n- Letting formula_98 denote the space of continuous bounded functions on compact domain formula_99, we call a kernel formula_7 \"universal\" if formula_13 is continuous for all formula_52 and the RKHS induced by formula_7 is dense in formula_98.\n\n- If formula_7 induces a strictly positive definite kernel matrix for any set of distinct points, then it is a universal kernel. For example, the widely used Gaussian RBF kernel\n\n- If formula_7 is shift-invariant formula_109 and its representation in Fourier domain is\n\n- If formula_113 is universal, then it is \"characteristic\", i.e. the kernel embedding is one-to-one.\n\n- The empirical kernel conditional distribution embedding operator formula_114 can alternatively be viewed as the solution of the following regularized least squares (function-valued) regression problem\n\n- One can thus select the regularization parameter formula_71 by performing cross-validation based on the squared loss function of the regression problem.\n\nThis section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al. The following notation is adopted: \n\n- formula_118 joint distribution over random variables formula_119\n\n- formula_120 marginal distribution of formula_4; formula_122 marginal distribution of formula_123\n\n- formula_124 conditional distribution of formula_125 given formula_126 with corresponding conditional embedding operator formula_127\n\n- formula_128 prior distribution over formula_125\n\n- formula_130 is used to distinguish distributions which incorporate the prior from distributions formula_131 which do not rely on the prior\n\nIn practice, all embeddings are empirically estimated from data formula_132 and it assumed that a set of samples formula_133 may be used to estimate the kernel embedding of the prior distribution formula_134.\n\nIn probability theory, the marginal distribution of formula_4 can be computed by integrating out formula_125 from the joint density (including the prior distribution on formula_34)\n\nThe analog of this rule in the kernel embedding framework states that formula_139 the RKHS embedding of formula_140, can be computed via\n\nwhere formula_142 is the kernel embedding of formula_143 In practical implementations, the kernel sum rule takes the following form\n\nwhere \n\nis the empirical kernel embedding of the prior distribution, formula_146 formula_147, and formula_148 are Gram matrices with entries formula_149 respectively.\n\nIn probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions \n\nThe analog of this rule in the kernel embedding framework states that formula_151 the joint embedding of formula_152 can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with formula_153\n\nwhere \n\nIn practical implementations, the kernel chain rule takes the following form\n\nIn probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as \n\nThe analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution\n\nwhere from the chain rule: \n\nIn practical implementations, the kernel Bayes' rule takes the following form\n\nwhere \n\nTwo regularization parameters are used in this framework: formula_164 for the estimation of formula_165 and formula_166 for the estimation of the final conditional embedding operator \n\nThe latter regularization is done on square of formula_168 because formula_169 may not be positive definite.\n\nThe maximum mean discrepancy (MMD) is a distance-measure between distributions formula_170 and formula_171 which is defined as the squared distance between their embeddings in the RKHS \n\nWhile most distance-measures between distributions such as the widely used Kullback–Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies, the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the \"maximum mean discrepancy\" refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions \n\nGiven \"n\" training examples from formula_170 and \"m\" samples from formula_171, one can formulate a test statistic based on the empirical estimate of the MMD\n\nto obtain a two-sample test of the null hypothesis that both samples stem from the same distribution (i.e. formula_177) against the broad alternative formula_178.\n\nAlthough learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on \"n\" samples drawn from an underlying distribution formula_179. This can be done by solving the following optimization problem \n\nwhere the maximization is done over the entire space of distributions on formula_182 Here, formula_183 is the kernel embedding of the proposed density formula_184 and formula_185 is an entropy-like quantity (e.g. Entropy, KL divergence, Bregman divergence). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of \"M\" candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families.\n\nA measure of the statistical dependence between random variables formula_4 and formula_34 (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert–Schmidt Independence Criterion \n\nand can be used as a principled replacement for mutual information, Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent), and can be used to measure dependence between different types of data (e.g. images and text captions). Given \"n\" i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in formula_189 time, where the Gram matrices of the two datasets are approximated using formula_190 with formula_191. The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: feature selection (BAHSIC ), clustering (CLUHSIC ), and dimensionality reduction (MUHSIC ).\n\nHSIC can be extended to measure the dependence of multiple random variables. The question of when HSIC captures independence in this case has recently been studied: for \nmore than two variables\n- on formula_192: the characteristic property of the individual kernels remains an equivalent condition.\n- on general domains: the characteristic property of the kernel components is necessary but \"not sufficient\".\n\nBelief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given \"n\" samples of random variables represented by nodes in a Markov random field, the incoming message to node \"t\" from node \"u\" can be expressed as \n\nif it assumed to lie in the RKHS. The kernel belief propagation update message from \"t\" to node \"s\" is then given by \n\nwhere formula_195 denotes the element-wise vector product, formula_196 is the set of nodes connected to \"t\" excluding node \"s\", formula_197, formula_198 are the Gram matrices of the samples from variables formula_199, respectively, and formula_200 is the feature matrix for the samples from formula_201.\n\nThus, if the incoming messages to node \"t\" are linear combinations of feature mapped samples from formula_202, then the outgoing message from this node is also a linear combination of feature mapped samples from formula_203. This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled.\n\nIn the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states formula_204 and the emission probabilities formula_205 for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible.\n\nOne common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state formula_206 at time step \"t\" given a history of previous observations formula_207 from the system. In filtering, a belief state formula_208 is recursively maintained via a prediction step (where updates formula_209 are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates formula_210 are computed by applying Bayes' rule to condition on a new observation). The RKHS embedding of the belief state at time \"t+1\" can be recursively expressed as \n\nby computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule. Assuming a training sample formula_212 is given, one can in practice estimate \n\nand filtering with kernel embeddings is thus implemented recursively using the following updates for the weights formula_214 \n\nwhere formula_217 denote the Gram matrices of formula_218 and formula_219 respectively, formula_220 is a transfer Gram matrix defined as formula_221 and formula_222\n\nThe support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels formula_223. SMMs solve the standard SVM dual optimization problem using the following expected kernel\n\nwhich is computable in closed form for many common specific distributions formula_225 (such as the Gaussian distribution) combined with popular embedding kernels formula_7 (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples formula_227 via\n\nUnder certain choices of the embedding kernel formula_7, the SMM applied to training examples formula_230 is equivalent to a SVM trained on samples formula_231, and thus the SMM can be viewed as a \"flexible\" SVM in which a different data-dependent kernel (specified by the assumed form of the distribution formula_225) may be placed on each training point.\n\nThe goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples formula_233 and a test set formula_234 where the formula_235 are unknown, three types of differences are commonly assumed between the distribution of the training examples formula_236 and the test distribution formula_237:\n1. Covariate shift in which the marginal distribution of the covariates changes across domains: formula_238\n2. Target shift in which the marginal distribution of the outputs changes across domains: formula_239\n3. Conditional shift in which formula_240 remains the same across domains, but the conditional distributions differ: formula_241. In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that formula_242 changes only under location-scale (LS) transformations on formula_126 is commonly imposed to make the problem tractable.\n\nBy utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio formula_244 obtained directly from the kernel embeddings of the marginal distributions of formula_4 in each domain without any need for explicit estimation of the distributions. Target shift, which cannot be similarly dealt with since no samples from formula_34 are available in the test domain, is accounted for by weighting training examples using the vector formula_247 which solves the following optimization problem (where in practice, empirical approximations must be used) \n\nTo deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data formula_250 (where formula_195 denotes the element-wise vector product). To ensure similar distributions between the new transformed training samples and the test data, formula_252 are estimated by minimizing the following empirical kernel embedding distance \n\nIn general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes.\n\nGiven \"N\" sets of training examples sampled i.i.d. from distributions formula_254, the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain formula_255 where no data from the test domain is available at training time. If conditional distributions formula_256 are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals formula_170. Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains. DICA thus extracts \"invariants\", features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis, transfer component analysis, and covariance operator inverse regression. \n\nDefining a probability distribution formula_258 on the RKHS formula_9 with \n\nDICA measures dissimilarity between domains via distributional variance which is computed as \n\nwhere \n\nso formula_263 is a formula_264 Gram matrix over the distributions from which the training data are sampled. Finding an orthogonal transform onto a low-dimensional subspace \"B\" (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that \"B\" aligns with the bases of a central subspace \"C\" for which formula_34 becomes independent of formula_4 given formula_267 across all domains. In the absence of target values formula_34, an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of formula_4 (in the feature space) across all domains (rather than preserving a central subspace).\n\nIn distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between \"sets of points\". Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images.\n\nGiven formula_270 training data, where the formula_271 bag contains samples from a probability distribution formula_272 and the formula_273 output label is formula_274, one can tackle the distribution regression task by taking the embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel ridge regression problem formula_275\n\nwhere \n\nwith a formula_7 kernel on the domain of formula_272-s formula_280, formula_281 is a kernel on the embedded distributions, and formula_282 is the RKHS determined by formula_281. Examples for formula_281 include the linear kernel formula_285, the Gaussian kernel formula_286, the exponential kernel formula_287, the Cauchy kernel formula_288, the generalized t-student kernel formula_289, or the inverse multiquadrics kernel formula_290.\n\nThe prediction on a new distribution formula_291 takes the simple, analytical form\nwhere formula_293, formula_294, formula_295, formula_296. Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true formula_272-s) minimax optimal rate. In the formula_298 objective function formula_299-s are real numbers; the results can also be extended to the case when formula_299-s are formula_301-dimensional vectors, or more generally elements of a separable Hilbert space using operator-valued formula_281 kernels.\n\nIn this simple example, which is taken from Song et al., formula_303 are assumed to be discrete random variables which take values in the set formula_304 and the kernel is chosen to be the Kronecker delta function, so formula_305. The feature map corresponding to this kernel is the standard basis vector formula_306. The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are formula_307 matrices specifying joint probability tables, and the explicit form of these embeddings is\n\nThe conditional distribution embedding operator, \n\nis in this setting a conditional probability table\n\nand \n\nThus, the embeddings of the conditional distribution under a fixed value of formula_4 may be computed as\n\nIn this discrete-valued setting with the Kronecker delta kernel, the kernel sum rule becomes\n\nThe kernel chain rule in this case is given by\n\n- Information Theoretical Estimators toolbox (distribution regression demonstration).\n", "related": "NONE"}
{"id": "33886025", "url": "https://en.wikipedia.org/wiki?curid=33886025", "title": "Stability (learning theory)", "text": "Stability (learning theory)\n\nStability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (\"A\" to \"Z\") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.\n\nStability can be studied for many types of learning problems, from language learning to inverse problems in physics and engineering, as it is a property of the learning process rather than the type of information being learned. The study of stability gained importance in computational learning theory in the 2000s when it was shown to have a connection with generalization. It was shown that for large classes of learning algorithms, notably empirical risk minimization algorithms, certain types of stability ensure good generalization.\n\nA central goal in designing a machine learning system is to guarantee that the learning algorithm will generalize, or perform accurately on new examples after being trained on a finite number of them. In the 1990s, milestones were reached in obtaining generalization bounds for supervised learning algorithms. The technique historically used to prove generalization was to show that an algorithm was consistent, using the uniform convergence properties of empirical quantities to their means. This technique was used to obtain generalization bounds for the large class of empirical risk minimization (ERM) algorithms. An ERM algorithm is one that selects a solution from a hypothesis space formula_1 in such a way to minimize the empirical error on a training set formula_2.\n\nA general result, proved by Vladimir Vapnik for an ERM binary classification algorithms, is that for any target function and input distribution, any hypothesis space formula_1 with VC-dimension formula_4, and formula_5 training examples, the algorithm is consistent and will produce a training error that is at most formula_6 (plus logarithmic factors) from the true error. The result was later extended to almost-ERM algorithms with function classes that do not have unique minimizers.\n\nVapnik's work, using what became known as VC theory, established a relationship between generalization of a learning algorithm and properties of the hypothesis space formula_1 of functions being learned. However, these results could not be applied to algorithms with hypothesis spaces of unbounded VC-dimension. Put another way, these results could not be applied when the information being learned had a complexity that was too large to measure. Some of the simplest machine learning algorithms—for instance, for regression—have hypothesis spaces with unbounded VC-dimension. Another example is language learning algorithms that can produce sentences of arbitrary length.\n\nStability analysis was developed in the 2000s for computational learning theory and is an alternative method for obtaining generalization bounds. The stability of an algorithm is a property of the learning process, rather than a direct property of the hypothesis space formula_1, and it can be assessed in algorithms that have hypothesis spaces with unbounded or undefined VC-dimension such as nearest neighbor. A stable learning algorithm is one for which the learned function does not change much when the training set is slightly modified, for instance by leaving out an example. A measure of Leave one out error is used in a Cross Validation Leave One Out (CVloo) algorithm to evaluate a learning algorithm's stability with respect to the loss function. As such, stability analysis is the application of sensitivity analysis to machine learning.\n\n- Early 1900s - Stability in learning theory was earliest described in terms of continuity of the learning map formula_9, traced to Andrey Nikolayevich Tikhonov.\n- 1979 - Devroye and Wagner observed that the leave-one-out behavior of an algorithm is related to its sensitivity to small changes in the sample.\n- 1999 - Kearns and Ron discovered a connection between finite VC-dimension and stability.\n- 2002 - In a landmark paper, Bousquet and Elisseeff proposed the notion of \"uniform hypothesis stability\" of a learning algorithm and showed that it implies low generalization error. Uniform hypothesis stability, however, is a strong condition that does not apply to large classes of algorithms, including ERM algorithms with a hypothesis space of only two functions.\n- 2002 - Kutin and Niyogi extended Bousquet and Elisseeff's results by providing generalization bounds for several weaker forms of stability which they called \"almost-everywhere stability\". Furthermore, they took an initial step in establishing the relationship between stability and consistency in ERM algorithms in the Probably Approximately Correct (PAC) setting.\n- 2004 - In an unusual publication (on a theorem!) for the journal Nature, Poggio et al. proved the relationship between stability and ERM consistency in the general case. They proposed a statistical form of leave-one-out-stability which they called \"CVEEEloo stability\", and showed that it is a) sufficient for generalization in bounded loss classes, and b) necessary and sufficient for consistency (and thus generalization) of ERM algorithms for certain loss functions (such as the square loss, the absolute value and the binary classification loss).\n- 2010 - Shalev Shwartz noticed problems with the original results of Vapnik due to the complex relations between hypothesis space and loss class. They discuss stability notions that capture different loss classes and different types of learning, supervised and unsupervised.\n\nWe define several terms related to learning algorithms training sets, so that we can then define stability in multiple ways and present theorems from the field.\n\nA machine learning algorithm, also known as a learning map formula_9, maps a training data set, which is a set of labeled examples formula_11, onto a function formula_12 from formula_13 to formula_14, where formula_13 and formula_14 are in the same space of the training examples. The functions formula_12 are selected from a hypothesis space of functions called formula_1.\n\nThe training set from which an algorithm learns is defined as\n\nformula_19\n\nand is of size formula_20 in formula_21\n\ndrawn i.i.d. from an unknown distribution D.\n\nThus, the learning map formula_9 is defined as a mapping from formula_23 into formula_1, mapping a training set formula_2 onto a function formula_26 from formula_13 to formula_14. Here, we consider only deterministic algorithms where formula_9 is symmetric with respect to formula_2, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable.\n\nThe loss formula_31 of a hypothesis formula_12 with respect to an example formula_33 is then defined as formula_34.\n\nThe empirical error of formula_12 is formula_36.\n\nThe true error of formula_12 is formula_38\n\nGiven a training set S of size m, we will build, for all i = 1...,m, modified training sets as follows:\n- By removing the i-th element\nformula_39\n- By replacing the i-th element\nformula_40\n\nAn algorithm formula_9 has hypothesis stability β with respect to the loss function V if the following holds:\n\nformula_42\n\nAn algorithm formula_9 has point-wise hypothesis stability β with respect to the loss function V if the following holds:\n\nformula_44\n\nAn algorithm formula_9 has error stability β with respect to the loss function V if the following holds:\n\nformula_46\n\nAn algorithm formula_9 has uniform stability β with respect to the loss function V if the following holds:\n\nformula_48\n\nA probabilistic version of uniform stability β is:\n\nformula_49\n\nAn algorithm is said to be stable, when the value of formula_50 decreases as formula_51.\n\nAn algorithm formula_9 has CVloo stability β with respect to the loss function V if the following holds:\n\nformula_53\n\nThe definition of (CVloo) Stability is equivalent to Pointwise-hypothesis stability seen earlier.\n\nAn algorithm formula_9 has formula_54 stability if for each n there exists a formula_57 and a formula_58 such that:\n\nformula_59, with formula_57 and formula_58 going to zero for formula_62\n\nFrom Bousquet and Elisseeff (02):\n\nFor symmetric learning algorithms with bounded loss, if the algorithm has Uniform Stability with the probabilistic definition above, then the algorithm generalizes.\n\nUniform Stability is a strong condition which is not met by all algorithms but is, surprisingly, met by the large and important class of Regularization algorithms.\nThe generalization bound is given in the article.\n\nFrom Mukherjee et al. (06):\n\n- For symmetric learning algorithms with bounded loss, if the algorithm has \"both\" Leave-one-out cross-validation (CVloo) Stability and Expected-leave-one-out error (formula_54) Stability as defined above, then the algorithm generalizes.\n- Neither condition alone is sufficient for generalization. However, both together ensure generalization (while the converse is not true).\n- For ERM algorithms specifically (say for the square loss), Leave-one-out cross-validation (CVloo) Stability is both necessary and sufficient for consistency and generalization.\n\nThis is an important result for the foundations of learning theory, because it shows that two previously unrelated properties of an algorithm, stability and consistency, are equivalent for ERM (and certain loss functions).\nThe generalization bound is given in the article.\n\nThis is a list of algorithms that have been shown to be stable, and the article where the associated generalization bounds are provided.\n\n- Linear regression\n- k-NN classifier with a {0-1} loss function.\n- Support Vector Machine (SVM) classification with a bounded kernel and where the regularizer is a norm in a Reproducing Kernel Hilbert Space. A large regularization constant formula_64 leads to good stability.\n- Soft margin SVM classification.\n- Regularized Least Squares regression.\n- The minimum relative entropy algorithm for classification.\n- A version of bagging regularizers with the number formula_65 of regressors increasing with formula_5.\n- Multi-class SVM classification.\n- All learning algorithms with Tikhonov regularization satisfies Uniform Stability criteria and are, thus, generalizable.\n\n- S.Kutin and P.Niyogi.Almost-everywhere algorithmic stability and generalization error. In Proc. of UAI 18, 2002\n- S. Rakhlin, S. Mukherjee, and T. Poggio. Stability results in learning theory. Analysis and Applications, 3(4):397–419, 2005\n- V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995\n- Vapnik, V., Statistical Learning Theory. Wiley, New York, 1998\n- Poggio, T., Rifkin, R., Mukherjee, S. and Niyogi, P., \"Learning Theory: general conditions for predictivity\", Nature, Vol. 428, 419-422, 2004\n- Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, Stability of Randomized Learning Algorithms, Journal of Machine Learning Research 6, 55–79, 2010\n- Elisseeff, A. Pontil, M., Leave-one-out Error and Stability of Learning Algorithms with Applications, NATO SCIENCE SERIES SUB SERIES III COMPUTER AND SYSTEMS SCIENCES, 2003, VOL 190, pages 111-130\n- Shalev Shwartz, S., Shamir, O., Srebro, N., Sridharan, K., Learnability, Stability and Uniform Convergence, Journal of Machine Learning Research, 11(Oct):2635-2670, 2010\n", "related": "NONE"}
{"id": "35867897", "url": "https://en.wikipedia.org/wiki?curid=35867897", "title": "Bayesian interpretation of kernel regularization", "text": "Bayesian interpretation of kernel regularization\n\nIn machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective. Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces. In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function. Kernel methods have traditionally been used in supervised learning problems where the \"input space\" is usually a \"space of vectors\" while the \"output space\" is a \"space of scalars\". More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning.\n\nA mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is \"finite-dimensional\". The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together.\n\nThe classical supervised learning problem requires estimating the output for some new input point formula_1 by learning a scalar-valued estimator formula_2 on the basis of a training set formula_3 consisting of formula_4 input-output pairs, formula_5. Given a symmetric and positive bivariate function formula_6 called a \"kernel\", one of the most popular estimators in machine learning is given by\n\nwhere formula_7 is the kernel matrix with entries formula_8, formula_9, and formula_10. We will see how this estimator can be derived both from a regularization and a Bayesian perspective.\n\nThe main assumption in the regularization perspective is that the set of functions formula_11 is assumed to belong to a reproducing kernel Hilbert space formula_12.\n\nA reproducing kernel Hilbert space (RKHS) formula_12 is a Hilbert space of functions defined by a symmetric, positive-definite function formula_14 called the \"reproducing kernel\" such that the function formula_15 belongs to formula_12 for all formula_17. There are three main properties make an RKHS appealing:\n\n1. The \"reproducing property\", which gives name to the space,\n\nwhere formula_19 is the inner product in formula_12.\n\n2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points,\n\nThis allows the construction in a unified framework of both linear and generalized linear models.\n\n3. The squared norm in an RKHS can be written as\n\nand could be viewed as measuring the \"complexity\" of the function.\n\nThe estimator is derived as the minimizer of the regularized functional\n\nwhere formula_23 and formula_24 is the norm in formula_12. The first term in this functional, which measures the average of the squares of the errors between the formula_26 and the formula_27, is called the \"empirical risk\" and represents the cost we pay by predicting formula_26 for the true value formula_27. The second term in the functional is the squared norm in a RKHS multiplied by a weight formula_30 and serves the purpose of stabilizing the problem as well as of adding a trade-off between fitting and complexity of the estimator. The weight formula_30, called the \"regularizer\", determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of formula_30).\n\nThe explicit form of the estimator in equation () is derived in two steps. First, the representer theorem states that the minimizer of the functional () can always be written as a linear combination of the kernels centered at the training-set points,\n\nfor some formula_33. The explicit form of the coefficients formula_34 can be found by substituting for formula_35 in the functional (). For a function of the form in equation (), we have that\n\nWe can rewrite the functional () as\n\nThis functional is convex in formula_38 and therefore we can find its minimum by setting the gradient with respect to formula_38 to zero,\n\nSubstituting this expression for the coefficients in equation (), we obtain the estimator stated previously in equation (),\n\nThe notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the \"Gaussian process\".\n\nAs part of the Bayesian framework, the Gaussian process specifies the \"prior distribution\" that describes the prior beliefs about the properties of the function being modeled. These beliefs are updated after taking into account observational data by means of a \"likelihood function\" that relates the prior beliefs to the observations. Taken together, the prior and likelihood lead to an updated distribution called the \"posterior distribution\" that is customarily used for predicting test cases.\n\nA Gaussian process (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint Normal distribution. The mean vector and covariance matrix of the Gaussian distribution completely specify the GP. GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the \"kernel\" of the GP. Let a function formula_42 follow a Gaussian process with mean function formula_43 and kernel function formula_44,\n\nIn terms of the underlying Gaussian distribution, we have that for any finite set formula_46 if we let formula_47 then\n\nwhere formula_49 is the mean vector and formula_50 is the covariance matrix of the multivariate Gaussian distribution.\n\nIn a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid),\n\nThis assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance formula_52. The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs formula_53 and the variance of the noise formula_52, and thus the posterior distribution can be computed analytically. For a test input vector formula_1, given the training data formula_56, the posterior distribution is given by\n\nwhere formula_58 denotes the set of parameters which include the variance of the noise formula_52 and any parameters from the covariance function formula_44 and where\n\nA connection between regularization theory and Bayesian theory can only be achieved in the case of \"finite dimensional RKHS\". Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction.\n\nIn the finite dimensional case, every RKHS can be described in terms of a feature map formula_62 such that\n\nFunctions in the RKHS with kernel formula_64 can be then be written as\n\nand we also have that\n\nWe can now build a Gaussian process by assuming formula_67 to be distributed according to a multivariate Gaussian distribution with zero mean and identity covariance matrix,\n\nIf we assume a Gaussian likelihood we have\n\nwhere formula_70. The resulting posterior distribution is the given by\n\nWe can see that a \"maximum posterior (MAP)\" estimate is equivalent to the minimization problem defining Tikhonov regularization, where in the Bayesian case the regularization parameter is related to the noise variance.\n\nFrom a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting formula_72 in place of formula_73, the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions formula_42 that approximate the labels formula_73 as much as possible.\n\n", "related": "\n- Regularized least squares\n- Bayesian linear regression\n- Bayesian interpretation of Tikhonov regularization\n"}
{"id": "5721403", "url": "https://en.wikipedia.org/wiki?curid=5721403", "title": "Machine Learning (journal)", "text": "Machine Learning (journal)\n\nMachine Learning is a peer-reviewed scientific journal, published since 1986.\nIt should be distinguished from the journal \"Machine intelligence\" which was established in the mid-1960s. \n\nIn 2001, forty editors and members of the editorial board of \"Machine Learning\" resigned in order to support the \"Journal of Machine Learning Research\" (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of \"JMLR\", in which authors retained copyright over their papers and archives were freely available on the internet.\n\nFollowing the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.\n\n", "related": "NONE"}
{"id": "5721283", "url": "https://en.wikipedia.org/wiki?curid=5721283", "title": "Journal of Machine Learning Research", "text": "Journal of Machine Learning Research\n\nThe Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria), David Blei (Columbia University) and Bernhard Schölkopf (Max Planck Institute for Intelligent Systems).\n\nThe journal was established as an open-access alternative to the journal \"Machine Learning\". In 2001, forty editorial board members of \"Machine Learning\" resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the \"Journal of Machine Learning Research\" allows authors to publish articles for free and retain copyright, while archives are freely available online.\n\nPrint editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing.\n\nIn response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007 and now publishes proceedings for several leading machine learning conferences including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems.\n", "related": "NONE"}
{"id": "30511763", "url": "https://en.wikipedia.org/wiki?curid=30511763", "title": "AIXI", "text": "AIXI\n\nAIXI is a theoretical mathematical formalism for artificial general intelligence.\nIt combines Solomonoff induction with sequential decision theory.\nAIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutter's 2005 book \"Universal Artificial Intelligence\".\n\nAIXI is a reinforcement learning agent. It maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis (or environment). In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.\n\nAIXI is a reinforcement learning agent that interacts with some stochastic and unknown but computable environment formula_1. The interaction proceeds in time steps, from formula_2 to formula_3, where formula_4 is the lifespan of the AIXI agent. At time step \"t\", the agent chooses an action formula_5 (e.g. a limb movement) and executes it in the environment, and the environment responds with a \"percept\" formula_6, which consists of an \"observation\" formula_7 (e.g., a camera image) and a reward formula_8, distributed according to the conditional probability formula_9, where formula_10 is the \"history\" of actions, observations and rewards. The environment formula_1 is thus mathematically represented as a probability distribution over \"percepts\" (observations and rewards) which depend on the \"full\" history, so there is no Markov assumption (as opposed to other RL algorithms). Note again that this probability distribution is \"unknown\" to the AIXI agent. Furthermore, note again that formula_1 is computable, that is, the observations and rewards received by the agent from the environment formula_1 can be computed by some program (which runs on a Turing machine), given the past actions of the AIXI agent .\n\nThe \"only\" goal of the AIXI agent is to maximise formula_14, that is, the sum of rewards from time step 1 to m.\n\nThe AIXI agent is associated with a stochastic policy formula_15, which is the function it uses to choose actions at every time step, where formula_16 is the space of all possible actions that AIXI can take and formula_17 is the space of all possible \"percepts\" that can be produced by the environment. The environment (or probability distribution) formula_1 can also be thought of as a stochastic policy (which is a function): formula_19, where the formula_20 is the Kleene star operation.\n\nIn general, at time step formula_21 (which ranges from 1 to m), AIXI, having previously executed actions formula_22 (which is often abbreviated in the literature as formula_23) and having observed the history of percepts formula_24 (which can be abbreviated as formula_25), chooses and executes in the environment the action, formula_26, defined as follows \n\nor, using parentheses, to disambiguate the precedences\n\nIntuitively, in the definition above, AIXI considers the sum of the total reward over all possible \"futures\" up to formula_29 time steps ahead (that is, from formula_21 to formula_31), weighs each of them by the complexity of programs formula_32 (that is, by formula_33) consistent with the agent's past (that is, the previously executed actions, formula_23, and received percepts, formula_25) that can generate that future, and then picks the action that maximises expected future rewards .\n\nLet us break this definition down in order to attempt to fully understand it.\n\nformula_36 is the \"percept\" (which consists of the observation formula_37 and reward formula_38) received by the AIXI agent at time step formula_21 from the environment (which is unknown and stochastic). Similarly, formula_40 is the percept received by AIXI at time step formula_31 (the last time step where AIXI is active).\n\nformula_42 is the sum of rewards from time step formula_21 to time step formula_31, so AIXI needs to look into the future to choose its action at time step formula_21.\n\nformula_46 denotes a monotone universal Turing machine, and formula_32 ranges over all (deterministic) programs on the universal machine formula_46, which receives as input the program formula_32 and the sequence of actions formula_50 (that is, all actions), and produces the sequence of percepts formula_51. The universal Turing machine formula_46 is thus used to \"simulate\" or compute the environment responses or percepts, given the program formula_32 (which \"models\" the environment) and all actions of the AIXI agent: in this sense, the environment is \"computable\" (as stated above). Note that, in general, the program which \"models\" the \"current\" and actual environment (where AIXI needs to act) is unknown because the current environment is also unknown. \n\nformula_54 is the length of the program formula_32 (which is encoded as a string of bits). Note that formula_56. Hence, in the definition above, formula_57 should be interpreted as a mixture (in this case, a sum) over all computable environments (which are consistent with the agent's past), each weighted by its complexity formula_33. Note that formula_59 can also be written as formula_60, and formula_61 is the sequence of actions already executed in the environment by the AIXI agent. Similarly, formula_62, and formula_63 is the sequence of percepts produced by the environment so far.\n\nLet us now put all these components together in order to understand this equation or definition.\n\nAt time step t, AIXI chooses the action formula_26 where the function formula_65 attains its maximum. \n\nThe parameters to AIXI are the universal Turing machine \"U\" and the agent's lifetime \"m\", which need to be chosen. The latter parameter can be removed by the use of discounting.\n\nAccording to Hutter, the word \"AIXI\" can have several interpretations. AIXI can stand for AI based on Solomonoff's distribution, denoted by formula_66 (which is the Greek letter xi), or e.g. it can stand for AI \"crossed\" (X) with induction (I). There are other interpretations.\n\nAIXI's performance is measured by the expected total number of rewards it receives.\nAIXI has been proven to be optimal in the following ways.\n\n- Pareto optimality: there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment.\n- Balanced Pareto optimality: Like Pareto optimality, but considering a weighted sum of environments.\n- Self-optimizing: a policy \"p\" is called self-optimizing for an environment formula_1 if the performance of \"p\" approaches the theoretical maximum for formula_1 when the length of the agent's lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing.\n\nIt was later shown by Hutter and Jan Leike that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which they describe as undermining all previous optimality claims for AIXI.\n\nHowever, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesn't consider itself to be contained by the environment it interacts with. It also assumes the environment is computable. Since AIXI is incomputable (see below), it assigns zero probability to its own existence.\n\nLike Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXI\"tl\", which performs at least as well as the provably best time \"t\" and space \"l\" limited agent. Another approximation to AIXI with a restricted environment class is MC-AIXI (FAC-CTW) (which stands for Monte Carlo AIXI FAC-Context-Tree Weighting), which has had some success playing simple games such as partially observable Pac-Man.\n\n", "related": "\n- Gödel machine\n\n- \"Universal Algorithmic Intelligence: A mathematical top->down approach\", Marcus Hutter, ; also in \"Artificial General Intelligence\", eds. B. Goertzel and C. Pennachin, Springer, 2007, , pp. 227–290, .\n"}
{"id": "43269516", "url": "https://en.wikipedia.org/wiki?curid=43269516", "title": "Sample complexity", "text": "Sample complexity\n\nThe sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.\n\nMore precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1.\n\nThere are two variants of sample complexity:\n- The weak variant fixes a particular input-output distribution;\n- The strong variant takes the worst-case sample complexity over all input-output distributions.\n\nThe No Free Lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite, i.e. that there is no algorithm that can learn the globally-optimal target function using a finite number of training samples.\n\nHowever, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the VC dimension on the class of target functions.\n\nLet formula_1 be a space which we call the input space, and formula_2 be a space which we call the output space, and let formula_3 denote the product formula_4. For example, in the setting of binary classification, formula_1 is typically a finite-dimensional vector space and formula_2 is the set formula_7.\n\nFix a hypothesis space formula_8 of functions formula_9. A learning algorithm over formula_8 is a computable map from formula_11 to formula_8. In other words, it is an algorithm that takes as input a finite sequence of training samples and outputs a function from formula_1 to formula_2. Typical learning algorithms include empirical risk minimization, without or with Tikhonov regularization.\n\nFix a loss function formula_15, for example, the square loss formula_16, where formula_17. For a given distribution formula_18 on formula_4, the expected risk of a hypothesis (a function) formula_20 is\n\nIn our setting, we have formula_22, where formula_23 is a learning algorithm and formula_24 is a sequence of vectors which are all drawn independently from formula_18. Define the optimal riskformula_26Set formula_27, for each formula_28. Note that formula_29 is a random variable and depends on the random variable formula_30, which is drawn from the distribution formula_31. The algorithm formula_23 is called consistent if formula_33 probabilistically converges to formula_34. In other words, for all formula_35, there exists a positive integer formula_36, such that, for all formula_37, we have\n\nformula_38\nThe sample complexity of formula_23 is then the minimum formula_36 for which this holds, as a function of formula_41, and formula_42. We write the sample complexity as formula_43 to emphasize that this value of formula_36 depends on formula_41, and formula_42. If formula_23 is not consistent, then we set formula_48. If there exists an algorithm for which formula_49 is finite, then we say that the hypothesis space formula_50 is learnable.\n\nIn others words, the sample complexity formula_49 defines the rate of consistency of the algorithm: given a desired accuracy formula_52 and confidence formula_42, one needs to sample formula_49 data points to guarantee that the risk of the output function is within formula_52 of the best possible, with probability at least formula_56 .\n\nIn probably approximately correct (PAC) learning, one is concerned with whether the sample complexity is \"polynomial\", that is, whether formula_49 is bounded by a polynomial in formula_58 and formula_59. If formula_49 is polynomial for some learning algorithm, then one says that the hypothesis space formula_61 is PAC-learnable. Note that this is a stronger notion than being learnable.\n\nOne can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense, that is, there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input-output space with a specified target error. More formally, one asks whether there exists a learning algorithm formula_23, such that, for all formula_35, there exists a positive integer formula_36 such that for all formula_37, we have\n\nformula_66\nwhere formula_27, with formula_24 as above. The No Free Lunch Theorem says that without restrictions on the hypothesis space formula_8, this is not the case, i.e., there always exist \"bad\" distributions for which the sample complexity is arbitrarily large.\n\nThus, in order to make statements about the rate of convergence of the quantity\nformula_70\none must either\n- constrain the space of probability distributions formula_18, e.g. via a parametric approach, or\n- constrain the space of hypotheses formula_8, as in distribution-free approaches.\n\nThe latter approach leads to concepts such as VC dimension and Rademacher complexity which control the complexity of the space formula_8. A smaller hypothesis space introduces more bias into the inference process, meaning that formula_74 may be greater than the best possible risk in a larger space. However, by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions. This trade-off leads to the concept of regularization.\n\nIt is a theorem from VC theory that the following three statements are equivalent for a hypothesis space formula_8:\n1. formula_8 is PAC-learnable.\n2. The VC dimension of formula_8 is finite.\n3. formula_8 is a uniform Glivenko-Cantelli class.\nThis gives a way to prove that certain hypothesis spaces are PAC learnable, and by extension, learnable.\n\nformula_79, and let formula_8 be the space of affine functions on formula_1, that is, functions of the form formula_82 for some formula_83. This is the linear classification with offset learning problem. Now, note that four coplanar points in a square cannot be shattered by any affine function, since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two. Thus, the VC dimension of formula_8 is formula_85, so it is finite. It follows by the above characterization of PAC-learnable classes that formula_8 is PAC-learnable, and by extension, learnable.\n\nSuppose formula_8 is a class of binary functions (functions to formula_88). Then, formula_8 is formula_90-PAC-learnable with a sample of size:\n\nformula_91\nwhere formula_92 is the VC dimension of formula_8.\nMoreover, any formula_90-PAC-learning algorithm for formula_8 must have sample-complexity:\nformula_96\nThus, the sample-complexity is a linear function of the VC dimension of the hypothesis space.\n\nSuppose formula_8 is a class of real-valued functions with range in formula_98. Then, formula_8 is formula_90-PAC-learnable with a sample of size:\n\nformula_101\nwhere formula_102 is Pollard's pseudo-dimension of formula_8.\n\nIn addition to the supervised learning setting, sample complexity is relevant to semi-supervised learning problems including active learning, where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels. The concept of sample complexity also shows up in reinforcement learning, online learning, and unsupervised algorithms, e.g. for dictionary learning.\n", "related": "NONE"}
{"id": "43218024", "url": "https://en.wikipedia.org/wiki?curid=43218024", "title": "Evaluation of binary classifiers", "text": "Evaluation of binary classifiers\n\nThe evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.\n\nGiven a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification – ideally a perfect classification, but in practice the output of another gold standard test – and cross tabulates the data into a 2×2 contingency table, comparing the two classifications. One then evaluates the classifier \"relative\" to the gold standard by computing summary statistics of these 4 numbers. Generally these statistics will be scale invariant (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of homogeneous functions, most simply homogeneous linear or homogeneous quadratic functions.\n\nSay we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called \"true positives\" (TP). Some have the disease, but the test incorrectly claims they don't. They are called \"false negatives\" (FN). Some don't have the disease, and the test says they don't – \"true negatives\" (TN). Finally, there might be healthy people who have a positive test result – \"false positives\" (FP). These can be arranged into a 2×2 contingency table (confusion matrix), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis.\n\nThese numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions.\n\nThe contingency table and the most common derived ratios are summarized below; see sequel for details.\n\nNote that the columns correspond to the \"condition actually\" being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the rows correspond to the \"test\" being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above.\n\nThe fundamental prevalence-independent statistics are sensitivity and specificity.\n\nSensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as \"the probability that the test is positive given that the patient is sick\". With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market).\n\nSpecificity (SPC) or True Negative Rate (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as \"the probability that the test result is negative given that the patient is not sick\". With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarded).\n\nThe relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the Receiver Operating Characteristic (ROC) curve.\n\nIn theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator.\n\nModern pregnancy tests \"do not\" use the pregnancy itself to determine pregnancy status; rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a \"surrogate marker to indicate\" that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100% (because false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100% (because false negatives are possible).\n\nIn addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question \"If the test result is \"positive\", how well does that \"predict\" an actual presence of disease?\". It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally.\n\nPrevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result.\n\nHowever, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result.\n\nThere are various relationships between these ratios.\n\nIf the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity:\n\nIf the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity:\n\nIn addition to the paired metrics, there are also single metrics that give a single number to evaluate the test.\n\nPerhaps the simplest statistic is accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/Total Population = (TP + TN)/(TP + TN + FP + FN). This is often not very useful, compared to the marginal ratios, as it does not yield useful marginal interpretations, due to mixing true positives (test positive, condition positive) and true negatives (test negative, condition negative) – in terms of the condition table, it sums the diagonal; further, it is prevalence-dependent. The complement is the Fraction Incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the antidiagonal, divided by the total population.\n\nThe diagnostic odds ratio (DOR) is a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of True Rates or Prediction Values). This has a useful interpretation – as an odds ratio – and is prevalence-independent.\n\nAn F-score is a combination of the precision and the recall, providing a single score. There is a one-parameter family of statistics, with parameter \"β,\" which determines the relative weights of precision and recall. The traditional or balanced F-score (F1 score) is the harmonic mean of precision and recall:\n\nNote, however, that the F-scores do not take the true negative rate into account, and are more suited to information retrieval and information extraction evaluation where the true negatives are innumerable. Instead, measures such as the Phi coefficient, Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (Youden's J statistic or deltap').\n\n", "related": "\n- Population Impact Measures\n- Attributable risk\n- Attributable risk percent\n- Scoring rule (for probability predictions)\n"}
{"id": "43502368", "url": "https://en.wikipedia.org/wiki?curid=43502368", "title": "Vanishing gradient problem", "text": "Vanishing gradient problem\n\nIn machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receive an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range , and backpropagation computes gradients by the chain rule. This has the effect of multiplying of these small numbers to compute gradients of the \"front\" layers in an -layer network, meaning that the gradient (error signal) decreases exponentially with while the front layers train very slowly.\n\nBack-propagation allowed researchers to train supervised deep artificial neural networks from scratch, initially with little success. Hochreiter's diploma thesis of 1991 formally identified the reason for this failure in the \"vanishing gradient problem\", which not only affects many-layered feedforward networks, but also recurrent networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network. (The combination of unfolding and backpropagation is termed backpropagation through time.)\n\nWhen activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.\n\nTo overcome this problem, several methods were proposed. One is Jürgen Schmidhuber's multi-level hierarchy of networks (1992) pre-trained one level at a time through unsupervised learning, fine-tuned through backpropagation. Here each level learns a compressed representation of the observations that is fed to the next level.\n\nSimilar ideas have been used in feed-forward neural networks for unsupervised pre-training to structure a neural network, making it first learn generally useful feature detectors. Then the network is trained further by supervised backpropagation to classify labeled data. The deep belief network model by Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued latent variables. It uses a restricted Boltzmann machine to model each new layer of higher level features. Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an \"ancestral pass\") from the top level feature activations. Hinton reports that his models are effective feature extractors over high-dimensional, structured data.\n\nAnother technique particularly used for recurrent neural networks is the long short-term memory (LSTM) network of 1997 by Hochreiter & Schmidhuber. In 2009, deep multidimensional LSTM networks demonstrated the power of deep learning with many nonlinear layers, by winning three ICDAR 2009 competitions in connected handwriting recognition, without any prior knowledge about the three different languages to be learned.\n\nHardware advances have meant that from 1991 to 2015, computer power (especially as delivered by GPUs) has increased around a million-fold, making standard backpropagation feasible for networks several layers deeper than when the vanishing gradient problem was recognized. Schmidhuber notes that this \"is basically what is winning many of the image recognition competitions now\", but that it \"does not really overcome the problem in a fundamental way\" since the original models tackling the vanishing gradient problem by Hinton et al. (2006) were trained in a Xeon processor, not GPUs.\n\nOne of the newest and most effective ways to resolve the vanishing gradient problem is with residual neural networks, or ResNets (not to be confused with recurrent neural networks). It was noted prior to ResNets that a deeper network would actually have higher \"training\" error than the shallow network. This intuitively can be understood as data disappearing through too many layers of the network, meaning output from a shallow layer was diminished through the greater number of layers in the deeper network, yielding a worse result. Going with this intuitive hypothesis, Microsoft research found that splitting a deep network into three layer chunks and passing the input into each chunk straight through to the next chunk, along with the residual-output of the chunk minus the input to the chunk that is reintroduced, helped eliminate much of this disappearing signal problem. No extra parameters or changes to the learning algorithm were needed. ResNets yielded lower training error (and test error) than their shallower counterparts simply by reintroducing outputs from shallower layers in the network to compensate for the vanishing data.\n\nNote that ResNets are an ensemble of relatively shallow nets and do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network – rather, they avoid the problem simply by constructing ensembles of many short networks together. (Ensemble by Construction)\n\nRectifiers such as ReLU suffer less from the vanishing gradient problem, because they only saturate in one direction.\n\nBehnke relied only on the sign of the gradient (Rprop) when training his Neural Abstraction Pyramid to solve problems like image reconstruction and face localization.\n\nNeural networks can also be optimized by using a universal search algorithm on the space of neural network's weights, e.g., random guess or more systematically genetic algorithm. This approach is not based on gradient and avoids the vanishing gradient problem.\n\n", "related": "\n- Spectral radius\n"}
{"id": "41929726", "url": "https://en.wikipedia.org/wiki?curid=41929726", "title": "Query-level feature", "text": "Query-level feature\n\nA query-level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.\n\nExample QLFs:\n- How many times has this query been run in the last month?\n- How many words are in the query?\n- What is the sum/average/min/max/median of the BM25F values for the query?\n", "related": "NONE"}
{"id": "43932548", "url": "https://en.wikipedia.org/wiki?curid=43932548", "title": "Random projection", "text": "Random projection\n\nIn mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are known for their power, simplicity, and low error rates when compared to other methods. According to experimental results, random projection preserves distances well, but empirical results are sparse.\nThey have been applied to many natural language tasks under the name random indexing.\n\nDimensionality reduction, as the name suggests, is reducing the number of random variables using various mathematical methods from statistics and machine learning. Dimensionality reduction is often used to reduce the problem of managing and manipulating large data sets. Dimensionality reduction techniques generally use linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions. For this purpose there are various related techniques, including: principal component analysis, linear discriminant analysis, canonical correlation analysis, discrete cosine transform, random projection, etc.\n\nRandom projection is a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of error for faster processing times and smaller model sizes. The dimensions and distribution of random projection matrices are controlled so as to approximately preserve the pairwise distances between any two samples of the dataset.\n\nThe core idea behind random projection is given in the Johnson-Lindenstrauss lemma, which states that if points in a vector space are of sufficiently high dimension, then they may be projected into a suitable lower-dimensional space in a way which approximately preserves the distances between the points.\n\nIn random projection, the original d-dimensional data is projected to a k-dimensional (k « d) subspace, using a random formula_1 - dimensional matrix R whose columns have unit lengths. Using matrix notation: If formula_2 is the original set of N d-dimensional observations, then formula_3 is the projection of the data onto a lower k-dimensional subspace. Random projection is computationally simple: form the random matrix \"R\" and project the formula_4 data matrix X onto K dimensions of order formula_5. If the data matrix X is sparse with about c nonzero entries per column, then the complexity of this operation is of order formula_6.\n\nThe random matrix R can be generated using a Gaussian distribution. The first row is a random unit vector uniformly chosen from formula_7. The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. In this way of choosing R, R is an orthogonal matrix (the inverse of its transpose), and the following properties are satisfied:\n- Spherical symmetry: For any orthogonal matrix formula_8, RA and R have the same distribution.\n- Orthogonality: The rows of R are orthogonal to each other.\n- Normality: The rows of R are unit-length vectors.\n\nAchlioptas has shown that the Gaussian distribution can be replaced by a much simpler distribution such as\nThis is efficient for database applications because the computations can be performed using integer arithmetic.\n\nIt was later shown how to use integer arithmetic while making the distribution even sparser, having very few nonzeroes per column, in work on the Sparse JL Transform. This is advantageous since a sparse embedding matrix means being able to project the data to lower dimension even faster.\n\nThe Johnson-Lindenstrauss lemma states that large sets of vectors in a high-dimensional space can be linearly mapped in a space of much lower (but still high) dimension \"n\" with approximate preservation of distances. One of the explanations of this effect is the exponentially high quasiorthogonal dimension of \"n\"-dimensional Euclidean space. There are exponentially large (in dimension \"n\") sets of almost orthogonal vectors (with small value of inner products) in \"n\"–dimensional Euclidean space. This observation is useful in indexing of high-dimensional data.\n\nQuasiorthogonality of large random sets is important for methods of random approximation in machine learning. In high dimensions, exponentially large numbers of randomly and independently chosen vectors from equidistribution on a sphere (and from many other distributions) are almost orthogonal with probability close to one. This implies that in order to represent an element of such a high-dimensional space by linear combinations of randomly and independently chosen vectors, it may often be necessary to generate samples of exponentially large length if we use bounded coefficients in linear combinations. On the other hand, if coefficients with arbitrarily large values are allowed, the number of randomly generated elements that are sufficient for approximation is even less than dimension of the data space.\n\n- RandPro - An R package for random projection\n- sklearn.random_projection - Python module for random projection\n- Weka implementation\n\n", "related": "\n- Locality-sensitive hashing\n- Random mapping\n\n- Fodor, I. (2002) \"A survey of dimension reduction techniques\". Center for Applied Scientific Computing, Lawrence Livermore National, Technical Report UCRL-ID-148494\n- ADITYA KRISHNA MENON (2007) \"Random projections and applications to dimensionality reduction\". School of Information Technologies, The University of Sydney, Australia\n- ADITYA Ramdas \"A Random Introduction To Random Projections\". Carnegie Mellon University\n"}
{"id": "43808044", "url": "https://en.wikipedia.org/wiki?curid=43808044", "title": "Action model learning", "text": "Action model learning\n\nAction model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about \"effects\" and \"preconditions\" of the \"actions\" that can be executed within its \"environment\". This knowledge is usually represented in logic-based action description language and used as the input for automated planners.\n\nLearning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from reinforcement learning. It enables reasoning about actions instead of expensive trials in the world. Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's \"observations\". It differs from standard supervised learning in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected.\n\nUsual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments).\n\nGiven a training set formula_1 consisting of examples formula_2, where formula_3 are observations of a world state from two consecutive time steps formula_4 and formula_5 is an \"action instance\" observed in time step formula_6, the goal of action model learning in general is to construct an \"action model\" formula_7, where formula_8 is a description of domain dynamics in action description formalism like STRIPS, ADL or PDDL and formula_9 is a probability function defined over the elements of formula_8.\n\nHowever, many state of the art \"action learning methods\" assume determinism and do not induce formula_9. In addition to determinism, individual methods differ in how they deal with other attributes of domain (e.g. partial observability or sensoric noise).\n\nRecent action learning methods take various approaches and employ a wide variety of tools from different areas of artificial intelligence and computational logic. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm, which uses agent's observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability (SAT) solver. Another technique, in which learning is converted into a satisfiability problem (weighted MAX-SAT in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System).\nTwo mutually similar, fully declarative approaches to action learning were based on logic programming paradigm Answer Set Programming (ASP) and its extension, Reactive ASP. In another example, bottom-up inductive logic programming approach was employed. Several different solutions are not directly logic-based. For example, the action model learning using a perceptron algorithm or the multi level greedy search over the space of\npossible action models. In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning.\n\nMost action learning research papers are published in journals and conferences focused on artificial intelligence in general (e.g. Journal of Artificial Intelligence Research (JAIR), Artificial Intelligence, Applied Artificial Intelligence (AAI) or AAAI conferences). Despite mutual relevance of the topics, action model learning is usually not addressed on planning conferences like ICAPS.\n\n", "related": "\n- Machine learning\n- Automated planning and scheduling\n- Action language\n- PDDL\n- Architecture description language\n- Inductive reasoning\n- Computational logic\n- Knowledge representation\n"}
{"id": "44439173", "url": "https://en.wikipedia.org/wiki?curid=44439173", "title": "Bradley–Terry model", "text": "Bradley–Terry model\n\nThe Bradley–Terry model is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals and drawn from some population, it estimates the probability that the pairwise comparison turns out true, as\n\nwhere is a positive real-valued score assigned to individual . The comparison can be read as \" is preferred to \", \" ranks higher than \", or \" beats \", depending on the application.\n\nFor example, may represent the skill of a team in a sports tournament, estimated from the number of times has won a match. formula_2 then represents the probability that will win a match against . Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.\n\nThe model is named after R. A. Bradley and M. E. Terry, who presented it in 1952, although it had already been studied by Zermelo in the 1920s.\n\nReal-world applications of the model include estimation of the influence of statistical journals, or ranking documents by relevance in machine-learned search engines.\nIn the latter application, formula_3 may reflect that document is more relevant to the user's query than document , so it should be displayed earlier in the results list. The individual then express the relevance of the document, and can be estimated from the frequency with which users click particular \"hits\" when presented with a result list.\n\nThe Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of parameters .\nAnother variant, in fact the version considered by Bradley and Terry, uses exponential score functions formula_4 so that\n\nor, using the logit (and disallowing ties),\n\nreducing the model to logistic regression on pairs of individuals.\n\nThe following algorithm computes the parameters of the basic version of the model from a sample of observations. Formally, it computes a maximum likelihood estimate, i.e., it maximizes the likelihood of the observed data. The algorithm dates back to the work of Zermelo.\n\nThe observations required are the outcomes of previous comparisons, for example, pairs where beats . Summarizing these outcomes as , the number of times has beaten , we obtain the log-likelihood of the parameter vector as\n\nDenote the number of comparisons \"won\" by as . Starting from an arbitrary vector , the algorithm iteratively performs the update\n\nfor all . After computing all of the new parameters, they should be renormalized,\n\nThis estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique maximum.\n\n", "related": "\n- Ordinal regression\n- Rasch model\n- Scale (social sciences)\n- Elo rating system\n- Thurstonian model\n"}
{"id": "44108758", "url": "https://en.wikipedia.org/wiki?curid=44108758", "title": "Quantum machine learning", "text": "Quantum machine learning\n\nQuantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. \"quantum-enhanced machine learning\". While machine learning algorithms are used to compute immense quantities of data, quantum machine learning increases such capabilities intelligently, by creating opportunities to conduct analysis on quantum states and systems. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster with the assistance of quantum devices. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term \"quantum machine learning\" is often associated with classical machine learning methods applied to data generated from quantum experiments (i.e. \"machine learning of quantum systems\"), such as learning quantum phase transitions or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Finally, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\n\nA number of quantum algorithms for machine learning are based on the idea of \"amplitude encoding\", that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of formula_1 qubits is described by formula_2 complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits formula_1, which amounts to a logarithmic growth in the number of amplitudes and thereby the dimension of the input.\n\nMany quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the paper's authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entrywise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows at least quadratically in the dimension of the matrix.\n\nQuantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes.\n\nA crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task.\n\nAnother approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Another application is a quadratic speedup in the training of perceptron.\n\nAmplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework.\n\nReinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols in superconducting circuits and in systems of trapped ions have been proposed.\n\nQuantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schrödinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system.\n\nSampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.\n\nA computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.\n\nSome research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.\n\nThe D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.\n\nInspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.\n\nQuantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.\n\nQuantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutsch's model of a quantum computational network. Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set. Such gates make certain phases unable to be observed and generate specific oscillations. Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing. Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size. A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time. The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10±20 nm) electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of formula_4 and formula_5 in Dirac notation.\n\nHidden Quantum Markov Models (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well. Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: density matrices. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived. Additionally, since classical HMMs are a particular kind of Bayes net, an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous Bayesian inference, which should allow for the general construction of the quantum versions of probabilistic graphical models.\n\nIn the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.\n\nOne class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way.\n\nGoing beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning.\n\nThe term \"quantum machine learning\" sometimes refers to \"classical\" machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians and automatically generating quantum experiments.\n\nQuantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider \"specific problems\" and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.\n\nThe starting point in learning theory is typically a \"concept class\", a set of possible concepts. Usually a concept is a function on some domain, such as formula_6. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on \"n\" bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown \"target concept\" from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.\n\nIn active learning, a learner can make \"membership queries\" to the target concept \"c\", asking for its value \"c(x)\" on inputs \"x\" chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of \"quantum exact learning\", the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of \"time\" the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions).\n\nA natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples \"(x,c(x))\", where \"x\" is distributed according to some unknown distribution \"D\". The learner's goal is to output a hypothesis function \"h\" such that \"h(x)=c(x)\" with high probability when \"x\" is drawn according to \"D\". The learner has to be able to produce such an 'approximately correct' \"h\" for every \"D\" and every target concept \"c\" in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples formula_7. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and quantum sample complexity are the same up to constant factors. However, for learning under some fixed distribution \"D\", quantum examples can be very helpful, for example for learning DNF under the uniform distribution. When considering \"time\" complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).\n\nThis passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis \"h\" is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.\n\nThe earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.\n\nUsing a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.\n\nPhotonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.\n\nRecently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.\n\nSince 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qbits, utilizing both trapped-ion and superconductive quantum computing methods.\n\n", "related": "\n- Quantum computing\n- Quantum algorithm for linear systems of equations\n- Quantum annealing\n- Quantum neural network\n- Quantum image\n"}
{"id": "44632031", "url": "https://en.wikipedia.org/wiki?curid=44632031", "title": "M-Theory (learning framework)", "text": "M-Theory (learning framework)\n\nIn Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.\n\nThe core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex.\n\nA great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions. It can be seen from different distances, different viewpoints, under different lighting, partially occluded, etc. In addition, for particular classes objects, such as faces, highly complex specific transformations may be relevant, such as changing facial expressions. For learning to recognize images, it is greatly beneficial to factor out these variations. It results in much simpler classification problem and, consequently, in great reduction of sample complexity of the model.\n\nA simple computational experiment illustrates this idea. Two instances of a classifier were trained to distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One can see that the second classifier performed quite well even after receiving a single example from each category, while performance of the first classifier was close to random guess even after seeing 20 examples.\n\nInvariant representations has been incorporated into several learning architectures, such as neocognitrons. Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps to take into account some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-Theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities.\n\nAnother core idea of M-Theory is close in spirit to ideas from the field of compressed sensing. An implication from Johnson–Lindenstrauss lemma says that a particular number of images can be embedded into a low-dimensional feature space with the same distances between images by using random projections. This result suggests that dot product between the observed image and some other image stored in memory, called template, can be used as a feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly.\n\nThe two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations. The key observation is how dot product between image formula_1 and a template formula_2 behaves when image is transformed (by such transformations as translations, rotations, scales, etc.). If transformation formula_3 is a member of a unitary group of transformations, then the following holds:\n\nformula_4\n\nIn other words, the dot product of transformed image and a template is equal to the dot product of original image and inversely transformed template. For instance, for image rotated by 90 degrees, the inversely transformed template would be rotated by -90 degrees.\n\nConsider the set of dot products of an image formula_1 to all possible transformations of template: formula_6. If one applies a transformation formula_3 to formula_1, the set would become formula_9. But because of the property (1), this is equal to formula_10. The set formula_11 is equal to just the set of all elements in formula_12. To see this, note that every formula_13 is in formula_12 due to the closure property of groups, and for every formula_15 in G there exist its prototype formula_16 such as formula_17 (namely, formula_18). Thus, formula_19. One can see that the set of dot products remains the same despite that a transformation was applied to the image! This set by itself may serve as a (very cumbersome) invariant representation of an image. More practical representations can be derived from it.\n\nIn the introductory section, it was claimed that M-Theory allows to learn invariant representations. This is because templates and their transformed versions can be learned from visual experience - by exposing the system to sequences of transformations of objects. It is plausible that similar visual experiences occur in early period of human life, for instance when infants twiddle toys in their hands. Because templates may be totally unrelated to images that the system later will try to classify, memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life. However, as it is shown later, for some kinds of transformations, specific templates are needed.\n\nTo implement the ideas described in previous sections, one need to know how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.\n\nOrbit formula_20 is a set of images formula_21 generated from a single image formula_1 under the action of the group formula_23.\n\nIn other words, images of an object and of its transformations correspond to an orbit formula_20. If two orbits have a point in common they are identical everywhere, i.e. an orbit is an invariant and unique representation of an image. So, two images are called equivalent when they belong to the same orbit: formula_25 if formula_26 such that formula_27. Conversely, two orbits are different if none of the images in one orbit coincide with any image in the other.\n\nA natural question arises: how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution formula_28 induced by the group's action on images formula_1 (formula_21 can be seen as a realization of a random variable).\nThis probability distribution formula_28 can be almost uniquely characterized by formula_32 one-dimensional probability distributions formula_33 induced by the (one-dimensional) results of projections formula_34, where formula_35 are a set of templates (randomly chosen images) (based on the Cramer-Wold theorem and concentration of measures).\nConsider formula_36 images formula_37. Let formula_38 , where formula_39 is a universal constant. Then\n\nformula_40\n\nwith probability formula_41, for all formula_42 formula_43 formula_44.\nThis result (informally) says that an approximately invariant and unique representation of an image formula_1 can be obtained from the estimates of formula_32 1-D probability distributions formula_47 for formula_48. The number formula_32 of projections needed to discriminate formula_36 orbits, induced by formula_36 images, up to precision formula_52 (and with confidence formula_41) is formula_38, where formula_39 is a universal constant.\nTo classify an image, the following \"recipe\" can be used:\n1. Memorize a set of images/objects called templates;\n2. Memorize observed transformations for each template;\n3. Compute dot products of its transformations with image;\n4. Compute histogram of the resulting values, called \"signature\" of the image;\n5. Compare the obtained histogram with signatures stored in memory.\n\nEstimates of such one-dimensional probability density functions (PDFs) formula_33 can be written in terms of histograms as formula_57, where formula_58 is a set of nonlinear functions. These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation.\n\nIn the \"recipe\" for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is compact.\n\nSuch groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are locally compact. For locally compact groups, invariance is achievable within certain range of transformations.\n\nAssume that formula_59 is a subset of transformations from formula_12 for which the transformed patterns exist in memory. For an image formula_1 and template formula_62, assume that formula_63 is equal to zero everywhere except some subset of formula_59. This subset is called support of formula_63 and denoted as formula_66. It can be proven that if for a transformation formula_16, support set will also lie within formula_68, then signature of formula_1 is invariant with respect to formula_16. This theorem determines the range of transformations for which invariance is guaranteed to hold.\n\nOne can see that the smaller is formula_66, the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small formula_72 for a generic image. This property is called localization: templates are sensitive only to images within a small range of transformations. Note that although minimizing formula_72 is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates: Gabor functions.\n\nThe desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex. The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon.\n\nMany interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized.\n\nAs it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects. More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition. Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture.\n\nThe previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well.\n\nFirstly, hierarchical architectures best accomplish the goal of ‘parsing’ a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchical architectures, representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy.\n\nSecondly, hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts. This facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts. As a result, sample complexity of learning compositional concepts may be greatly reduced.\n\nFinally, hierarchical architectures have better tolerance to clutter. Clutter problem arises when the target object is in front of a non-uniform background, which functions as a distractor for the visual task. Hierarchical architecture provides signatures for parts of target objects, which do not include parts of background and are not affected by background variations.\n\nIn hierarchical architectures, one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, as in the case of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide \"covariant\" rather than invariant, signatures. The property of covariance can be written as formula_74, where formula_75 is a layer, formula_76 is the signature of image on that layer, and formula_77 stands for \"distribution of values of the expression for all formula_78\".\n\nM-theory is based on a quantitative theory of the ventral stream of visual cortex. Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art machine vision systems that usually require a lot of data in order to recognize objects. Prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters. No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in V1 and V2. Although some of the systems inspired - to various degrees - by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases.\n\nM-theory learning framework employs a novel hypothesis about the main computational function of the ventral stream: the representation of new objects/images in terms of a signature, which is invariant to transformations learned during visual experience. This allows recognition from very few labeled examples - in the limit, just one.\n\nNeuroscience suggests that natural functionals for a neuron to compute is a high-dimensional dot product between an \"image patch\" and another image patch (called template) \nwhich is stored in terms of synaptic weights (synapses per neuron). The standard computational model of a neuron is based on a dot product and a threshold. Another important feature of the visual cortex is that it consists of simple and complex cells. This idea was originally proposed by Hubel and Wiesel. M-theory employs this idea. Simple cells compute dot products of an image and transformations of templates formula_79 for formula_80 (formula_81 is a number of simple cells). Complex cells are responsible for pooling and computing empirical histograms or statistical moments of it. The following formula for constructing histogram can be computed by neurons:\n\nformula_82\n\nwhere formula_83 is a smooth version of step function, formula_84 is the width of a histogram bin, and formula_36 is the number of the bin.\n\nIn authors applied M-theory to unconstrained face recognition in natural photographs. Unlike the DAR (detection, alignment, and recognition) method, which handles clutter by detecting objects and cropping closely around them so that very little background remains, this approach accomplishes detection and alignment implicitly by storing transformations of training images (templates) rather than explicitly detecting and aligning or cropping faces at test time. This system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems. \nThe resulting end-to-end system achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult: significantly jittered (misaligned) version of LFW and SUFR-W (for example, the model's accuracy in the LFW \"unaligned & no outside data used\" category is 87.55±1.41% compared to state-of-the-art APEM (adaptive probabilistic elastic matching): 81.70±1.78%).\n\nThe theory was also applied to a range of recognition tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) scene understanding tasks that requires the recognition of both shape-based as well as texture-based objects (on StreetScenes data set). The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems constellation models, the hierarchical SVM-based face-detection system. A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.\n\nThis theory can also be extended for the speech recognition domain.\nAs an example, in an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.\n", "related": "NONE"}
{"id": "470314", "url": "https://en.wikipedia.org/wiki?curid=470314", "title": "Instantaneously trained neural networks", "text": "Instantaneously trained neural networks\n\nInstantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization. This separation is done using the nearest hyperplane that can be written down instantaneously. In the two most important implementations the neighborhood of generalization either varies with the training sample (CC1 network) or remains constant (CC4 network). These networks use unary coding for an effective representation of the data sets.\n\nThis type of network was first proposed in a 1993 paper of Subhash Kak. Since then, instantaneously trained neural networks have been proposed as models of short term learning and used in web search, and financial time series prediction applications. They have also been used in instant classification of documents and for deep learning and data mining.\n\nAs in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs and by optical implementation.\n\nIn the CC4 network, which is a three-stage network, the number of input nodes is one more than the size of the training vector, with the extra node serving as the biasing node whose input is always 1. For binary input vectors, the weights from the input nodes to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula:\n\nwhere formula_2 is the radius of generalization and formula_3 is the Hamming weight (the number of 1s) of the binary sequence. From the hidden layer to the output layer the weights are 1 or -1 depending on whether the vector belongs to a given output class or not. The neurons in the hidden and output layers output 1 if the weighted sum to the input is 0 or positive and 0, if the weighted sum to the input is negative:\n\nThe CC4 network has also been modified to include non-binary input with varying radii of generalization so that it effectively provides a CC1 implementation.\n\nIn feedback networks the Willshaw network as well as the Hopfield network are able to learn instantaneously.\n", "related": "NONE"}
{"id": "45049676", "url": "https://en.wikipedia.org/wiki?curid=45049676", "title": "Adversarial machine learning", "text": "Adversarial machine learning\n\nAdversarial machine learning is a technique employed in the field of machine learning which attempts to fool models through malicious input. This technique can be applied for a variety of reasons, the most common being to attack or cause a malfunction in standard machine learning models.\n\nMachine learning techniques were originally designed for stationary and benign environments in which the training and test data are assumed to be generated from the same statistical distribution. However, when those models are implemented in the real world, the presence of intelligent and adaptive adversaries may violate that statistical assumption to some degree, depending on the adversary. This technique shows how a malicious adversary can surreptitiously manipulate the input data so as to exploit specific vulnerabilities of learning algorithms and compromise the security of the machine learning system.\n\nAs early as \"Snow Crash\" (1992), science fiction writers have posited scenarios of technology being vulnerable to specially-constructed data. In \"Zero History\" (2010), a character dons a t-shirt decorated in a way that renders him invisible to electronic surveillance.\n\nIn 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters were being defeated by simple \"evasion attacks\" as spammers inserted \"good words\" into their spam emails. (Around 2007, some spammers would also add random noise to fuzz words within \"image spam\" in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published \"Can Machine Learning Be Secure?\", an influential paper suggesting a broad taxonomy of attacks against machine learning. As late as 2013 many researchers continued to hope that non-linear classifiers (such as SVMs and neural networks) might be naturally robust to adversarial examples. In 2012, deep neural networks were unexpectedly crowned as the dominant path for advanced computer vision; starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled with tiny adjustments of the input.\n\nExamples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of “bad” words or the insertion of “good” words; attacks in computer security, such as obfuscating malware code within network packets or to mislead signature detection; attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user; or to compromise users' template galleries that adapt to updated traits over time.\n\nIn 2017, researchers at Kyushu University showed that by changing only one-pixel it was possible to fool current deep learning algorithms. Moreover, at the same year, researchers at the Massachusetts Institute of Technology 3-D printed a toy turtle with a texture engineered to make Google's object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology. In 2018, Google Brain published a machine-tweaked image of a dog that looked like a cat both to computers and to humans. A 2019 study from Johns Hopkins University showed that, when asked, humans can guess how machines will misclassify adversarial images. Researchers have also discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle will classify it as a merge or speed limit sign. In 2020, cyber security firm McAfee demonstrated an obsolete version of Tesla's Mobileye can be fooled into accelerating 50 miles per hour over the speed limit, by adding a two-inch strip of black tape to a speed limit sign. McAfee stated that \"Even to a trained eye, (the black tape) hardly looks suspicious or malicious, and many who saw it didn't realize the sign had been altered at all\", while Mobileye stated that the tape could have fooled human drivers as well.\n\nTo understand the security properties of learning algorithms in adversarial settings, the following main issues should be addressed:\n- identifying potential vulnerabilities of machine learning algorithms during learning and classification\n- devising appropriate attacks that correspond to the identified threats and evaluating their impact on the targeted system\n- proposing countermeasures to improve the security of machine learning algorithms against the considered attacks\n\nThis process amounts to simulating a proactive arms race (instead of a reactive one, as depicted in Figures 1 and 2), where system designers try to anticipate the adversary in order to understand whether there are potential vulnerabilities that should be fixed in advance; for instance, by means of specific countermeasures such as additional features or different learning algorithms. However, proactive approaches are not necessarily superior to reactive ones: under some circumstances, reactive approaches are more suitable for improving system security.\nThe first step of the arms race described above is identifying potential attacks against machine learning algorithms. A substantial amount of work has been done in this direction.\n\nAttacks against (supervised) machine learning algorithms have been categorized along three primary axes: their influence on the classifier, the security violation they cause, and their specificity.\n\n- \"Attack influence\": An attack can have a causative influence if it aims to introduce vulnerabilities to be exploited at the classification phase by manipulating training data, or an exploratory influence if the attack aims to find and subsequently exploit vulnerabilities at classification phase. The attacker's capabilities might also be influenced by the presence of data manipulation constraints.\n- \"Security violation\": An attack can cause an integrity violation if it aims to get malicious samples misclassified as legitimate, or it may cause an availability violation if the goal is to increase the wrong classification rate of legitimate samples, making the classifier unusable (e.g., a denial of service).\n- \"Attack specificity\": An attack can be targeted if specific samples are considered (e.g. the adversary aims to allow a specific intrusion or wants a given spam email to get past the filter), or indiscriminate.\n\nThis taxonomy has been extended into a more comprehensive threat model that allows one to make explicit assumptions on the adversary's goal, knowledge of the attacked system, capability of manipulating the input data and/or the system components, and on the corresponding (potentially, formally-defined) attack strategy. Two of the main attack scenarios identified according to this threat model are described below.\n\nEvasion attacks are the most prevalent type of attack that may be encountered in adversarial settings during system operation. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware code. In the evasion setting, malicious samples are modified at test time to evade detection; that is, to be misclassified as legitimate. No attacker influence over the training data is assumed. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade the textual analysis performed by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.\n\nMachine learning algorithms are often re-trained on data collected during operation to adapt to changes in the underlying data distribution. For instance, intrusion detection systems (IDSs) are often re-trained on a set of samples collected during network operation. Within this scenario, an attacker may poison the training data by injecting carefully designed samples to eventually compromise the whole learning process. Poisoning may thus be regarded as an adversarial contamination of the training data. Examples of poisoning attacks against machine learning algorithms including learning in the presence of worst-case adversarial label flips in the training data can be found in the following reference links. Adversarial stop signs (stop signs that look normal to the human eye but are classified as non-stop signs by neural networks) are primary examples of poisoning attacks.\n\nClustering algorithms have been increasingly adopted in security applications to find dangerous or illicit activities. For instance, clustering of malware and computer viruses aims to identify and categorize different existing malware families, and to generate specific signatures for their detection by anti-viruses or signature-based intrusion detection systems like Snort.\n\nHowever, clustering algorithms were not originally devised to deal with deliberate attack attempts that are designed to subvert the clustering process itself. If clustering can be safely adopted in such settings, this remains questionable.\n\nA number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed in the field of adversarial machine learning, including:\n\n- The definition of secure learning algorithms\n- The use of multiple classifier systems\n- The study of privacy-preserving learning\n- Ladder algorithm for Kaggle-style competitions\n- Game theoretic models for adversarial machine learning and data mining\n- Sanitizing training data from adversarial poisoning attacks\n\nSome software libraries are available, mainly for testing purposes and research.\n- AdversariaLib - includes implementation of evasion attacks\n- AdLib - Python library with a scikit-style interface which includes implementations of a number of published evasion attacks and defenses\n- AlfaSVMLib - Adversarial Label Flip Attacks against Support Vector Machines\n- Poisoning Attacks against Support Vector Machines, and Attacks against Clustering Algorithms\n- deep-pwning - Metasploit for deep learning which currently has attacks on deep neural networks using Tensorflow. This framework currently updates to maintain compatibility with the latest versions of Python.\n- Cleverhans - A Tensorflow Library to test existing deep learning models versus known attacks\n- foolbox - Python Library to create adversarial examples, implements multiple attacks\n- SecML - Python Library for secure and explainable machine learning - includes implementation of a wide range of ML and attack algorithms, support for dense and sparse data, multiprocessing, visualization tools.\n- NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security\n- Special Issue on \"Machine Learning in Adversarial Environments\" in the journal of Machine Learning\n- Dagstuhl Perspectives Workshop on \"Machine Learning Methods for Computer Security\"\n- Workshop on Artificial Intelligence and Security, (AISec) Series\n\n", "related": "\n- Pattern recognition\n"}
{"id": "45390860", "url": "https://en.wikipedia.org/wiki?curid=45390860", "title": "Domain adaptation", "text": "Domain adaptation\n\nDomain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources.\nNote that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.\n\nLet formula_1 be the input space (or description space) and let formula_2 be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) formula_3 able to attach a label from formula_2 to an example from formula_1. This model is learned from a learning sample formula_6.\n\nUsually in supervised learning (without domain adaptation), we suppose that the examples formula_7 are drawn i.i.d. from a distribution formula_8 of support formula_9 (unknown and fixed). The objective is then to learn formula_10 (from formula_11) such that it commits the least error possible for labelling new examples coming from the distribution formula_8.\n\nThe main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions formula_8 and formula_14 on formula_9. The domain adaptation task then consists of the transfer of knowledge from the source domain formula_8 to the target one formula_14. The goal is then to learn formula_10 (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain formula_14.\n\nThe major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?\n\nThere are several contexts of domain adaptation. They differ in the information considered for the target task.\n1. The unsupervised domain adaptation: the learning sample contains a set of labeled source examples, a set of unlabeled source examples and a set of unlabeled target examples.\n2. The semi-supervised domain adaptation: in this situation, we also consider a \"small\" set of labeled target examples.\n3. The supervised domain adaptation: all the examples considered are supposed to be labeled.\n\nThe objective is to reweight the source labeled sample such that it \"looks like\" the target sample (in term of the error measure considered).\n\nA method for adapting consists in iteratively \"auto-labeling\" the target examples. The principle is simple:\n1. a model formula_10 is learned from the labeled examples;\n2. formula_10 automatically labels some target examples;\n3. a new model is learned from the new labeled examples.\nNote that there exist other iterative approaches, but they usually need target labeled examples.\n\nThe goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.\nThis can be achieved through the use of Adversarial machine learning techniques where feature representations from samples in different domains are encouraged to be indistinguishable.\n\nThe goal is to construct a Bayesian hierarchical model formula_22, which is essentially a factorization model for counts formula_23, to derive domain-dependent latent representations allowing both domain-specific and globally shared latent factors.\n", "related": "NONE"}
{"id": "45378845", "url": "https://en.wikipedia.org/wiki?curid=45378845", "title": "Logic learning machine", "text": "Logic learning machine\n\nLogic Learning Machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm, developed by Marco Muselli, Senior Researcher at the Italian National Research Council CNR-IEIIT in Genoa.\nLogic Learning Machine is implemented in the Rulex suite.\n\nLLM has been employed in different fields, including orthopaedic patient classification, DNA microarray analysis and Clinical Decision Support System.\n\nThe Switching Neural Network approach was developed in the 1990s to overcome the drawbacks of the most commonly used machine learning methods. In particular, black box methods, such as multilayer perceptron and support vector machine, had good accuracy but could not provide deep insight into the studied phenomenon. On the other hand, decision trees were able to describe the phenomenon but often lacked accuracy. Switching Neural Networks made use of Boolean algebra to build sets of intelligible rules able to obtain very good performance. In 2014, an efficient version of Switching Neural Network was developed and implemented in the Rulex suite with the name Logic Learning Machine. Also a LLM version devoted to regression problems was developed.\n\nLike other machine learning methods, LLM uses data to build a model able to perform a good forecast about future behaviors. LLM starts from a table including a target variable (output) and some inputs and generates a set of rules that return the output value formula_1 corresponding to a given configuration of inputs. A rule is written in the form:\n\nwhere \"consequence\" contains the output value whereas \"premise\" includes one or more conditions on the inputs. According to the input type, conditions can have different forms:\n- for categorical variables the input value must be in a given subset :formula_3.\n- for ordered variables the condition is written as an inequality or an interval: formula_4 or formula_5\n\nA possible rule is therefore in the form\n\nAccording to the output type, different versions of Logic Learning Machine have been developed:\n- Logic Learning Machine for classification, when the output is a categorical variable, which can assume values in a finite set\n- Logic Learning Machine for regression, when the output is an integer or real number.\n\n- Rulex official site\n", "related": "NONE"}
{"id": "45627703", "url": "https://en.wikipedia.org/wiki?curid=45627703", "title": "Native-language identification", "text": "Native-language identification\n\nNative-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2). NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.\n\nNLI works under the assumption that an author's L1 will dispose them towards particular language production patterns in their L2, as influenced by their native language. This relates to cross-linguistic influence (CLI), a key topic in the field of second-language acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages.\n\nUsing large-scale English data, NLI methods achieve over 80% accuracy in predicting the native language of texts written by authors from 11 different L1 backgrounds. This can be compared to a baseline of 9% for choosing randomly.\n\nThis identification of L1-specific features has been used to study language transfer effects in second-language acquisition. This is useful for developing pedagogical material, teaching methods, L1-specific instructions and generating learner feedback that is tailored to their native language.\n\nNLI methods can also be applied in forensic linguistics as a method of performing authorship profiling in order to infer the attributes of an author, including their linguistic background.\nThis is particularly useful in situations where a text, e.g. an anonymous letter, is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source.\nThis has already attracted interest and funding from intelligence agencies.\n\nNatural language processing methods are used to extract and identify language usage patterns common to speakers of an L1-group. This is done using language learner data, usually from a learner corpus. Next, machine learning is applied to train classifiers, like support vector machines, for predicting the L1 of unseen texts.\nA range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems.\n\nVarious linguistic feature types have been applied for this task. These include syntactic features such as constituent parses, grammatical dependencies and part-of-speech tags.\nSurface level lexical features such as character, word and lemma n-grams have also been found to be quite useful for this task. However, it seems that character n-grams are the single best feature for the task.\n\nThe Building Educational Applications (BEA) workshop at NAACL 2013 hosted the inaugural NLI shared task. The competition resulted in 29 entries from teams across the globe, 24 of which also published a paper describing their systems and approaches.\n\n", "related": "\n- Crosslinguistic influence\n- Foreign language writing aid\n- Computer-assisted language learning\n- Language education\n- Natural language processing\n- Language transfer\n"}
{"id": "28255458", "url": "https://en.wikipedia.org/wiki?curid=28255458", "title": "Constrained conditional model", "text": "Constrained conditional model\n\nA constrained conditional model (CCM) is a machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference.\n\nModels of this kind have recently attracted much attention within the natural language processing (NLP) community.\nFormulating problems as constrained optimization problems over the output of learned models has several advantages. It allows one to focus on the modeling of problems by providing the opportunity to incorporate domain-specific knowledge as global constraints using a first order language. Using this declarative framework frees the developer from low level feature engineering while capturing the problem's domain-specific properties and guarantying exact inference. From a machine learning perspective it allows decoupling the stage of model generation (learning) from that of the constrained inference stage, thus helping to simplify the learning stage while improving the quality of the solutions. For example, in the case of generating compressed sentences, rather than simply relying on a language model to retain the most commonly used n-grams in the sentence, constraints can be used to ensure that if a modifier is kept in the compressed sentence, its subject will also be kept.\n\nMaking decisions in many domains (such as natural language processing and computer vision problems) often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. These settings are applicable not only to Structured Learning problems such as semantic role labeling, but also for cases that require making use of multiple pre-learned components, such as summarization, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain- or problem-specific constraints.\n\nConstrained conditional models form a learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints (written, for example, using a first-order representation) as a way to support decisions in an expressive output space while maintaining modularity and tractability of training and inference. These constraints can express either hard restrictions, completely prohibiting some assignments, or soft restrictions, penalizing unlikely assignments. In most applications of this framework in NLP, following, Integer Linear Programming (ILP) was used as the inference framework, although other algorithms can be used for that purpose.\n\nGiven a set of feature functions formula_1 and a set of constraints formula_2, defined over an input structure formula_3 and an output structure formula_4, a constraint conditional model is characterized by two weight vectors, w and formula_5, and is defined as the solution to the following optimization problem:\nEach constraint formula_7 is a boolean mapping indicating if the joint assignment formula_8 violates a constraint, and formula_5 is the penalty incurred for violating the constraints. Constraints assigned an infinite penalty are known as hard constraints, and represent unfeasible assignments to the optimization problem.\n\nThe objective function used by CCMs can be decomposed and learned in several ways, ranging from a complete joint training of the model along with the constraints to completely decoupling the learning and the inference stage. In the latter case, several local models are learned independently and the dependency between these models is considered only at decision time via a global decision process. The advantages of each approach are discussed in which studies the two training paradigms: (1) local models: L+I (learning + inference) and (2) global model: IBT (Inference based training), and shows both theoretically and experimentally that while IBT (joint training) is best in the limit, under some conditions (basically, ”good” components) L+I can generalize better.\n\nThe ability of CCM to combine local models is especially beneficial in cases where joint learning is computationally intractable or when training data are not available for joint learning. This flexibility distinguishes CCM from the other learning frameworks that also combine statistical information with declarative constraints, such as Markov logic network, that emphasize joint training.\n\nCCM can help reduce supervision by using domain knowledge (expressed as constraints) to drive learning. These settings were studied in \n(CODL) and show that by incorporating domain knowledge the performance of the learned model improves significantly.\n\nCCMs have also been applied to latent learning frameworks, where the learning problem is defined over a latent representation layer. Since the notion of a \"correct representation\" is inherently ill-defined, no gold-standard labeled data regarding the representation decision is available to the learner. Identifying the correct (or optimal) learning representation is viewed as a structured prediction process and therefore modeled as a CCM. \nThis problem was covered in several papers, in both supervised and unsupervised settings. In all cases research showed that explicitly modeling the interdependencies between representation decisions via constraints results in an improved performance.\n\nThe advantages of the CCM declarative formulation and the availability of off-the-shelf solvers have led to a large variety of natural language processing tasks being formulated within the framework, including semantic role labeling, syntactic parsing, coreference resolution, summarization, transliteration, natural language generation and joint information extraction.\n\nMost of these works use an integer linear programming (ILP) solver to solve the decision problem. Although theoretically solving an Integer Linear Program is exponential in the size of the decision problem, in practice using state-of-the-art solvers and approximate inference techniques large scale problems can be solved efficiently.\n\nThe key advantage of using an ILP solver for solving the optimization problem defined by a constrained conditional model is the declarative formulation used as input for the ILP solver, consisting of a linear objective function and a set of linear constraints.\n\n- CCM Tutorial Predicting Structures in NLP: Constrained Conditional Models and Integer Linear Programming in NLP\n\n- University of Illinois Cognitive Computation Group\n- Workshop on Integer Linear Programming for Natural Language Processing, NAACL-2009\n", "related": "NONE"}
{"id": "46207323", "url": "https://en.wikipedia.org/wiki?curid=46207323", "title": "Feature engineering", "text": "Feature engineering\n\nFeature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself .\n\nA feature is an attribute or property shared by all of the independent units on which analysis or prediction is to be done. Any attribute could be a feature, as long as it is useful to the model.\n\nThe purpose of a feature, other than being an attribute, would be much easier to understand in the context of a problem. A feature is a characteristic that might help when solving the problem.\n\nFeatures are important to predictive models and influence results . \n\nIt is asserted that feature engineering plays an important part of Kaggle competitions and machine learning project's success or failure .\n\nThe feature engineering process is:\n\n- Brainstorming or testing features;\n- Deciding what features to create;\n- Creating features;\n- Checking how the features work with your model;\n- Improving your features if needed;\n- Go back to brainstorming/creating more features until the work is done.\n\nA feature could be strongly relevant (i.e., the feature has information that doesn't exist in any other feature), relevant, weakly relevant (some information that other features include) or irrelevant. Even if some features are irrelevant, having too many is better than missing those that are important. Feature selection can be used to prevent overfitting.\n\nFeature explosion can be caused by feature combination or feature templates, both leading to a quick growth in the total number of features.\n\n- Feature templates - implementing feature templates instead of coding new features\n- Feature combinations - combinations that cannot be represented by the linear system\n\nFeature explosion can be stopped via techniques such as: regularization, kernel method, feature selection.\n\nAutomation of feature engineering is a research topic that dates back to at least the late 1990s. The academic literature on the topic can be roughly separated into two strings: First, Multi-relational decision tree learning (MRDTL), which uses a supervised algorithm that is similar to a decision tree. Second, more recent approaches, like Deep Feature Synthesis, which use simpler methods.\n\nMulti-relational decision tree learning (MRDTL) generates features in the form of SQL queries by successively adding new clauses to the queries. For instance, the algorithm might start out with \nSELECT COUNT(*) FROM ATOM t1 LEFT JOIN MOLECULE t2 ON t1.mol_id = t2.mol_id GROUP BY t1.mol_id\nThe query can then successively be refined by adding conditions, such as \"WHERE t1.charge <= -0.392\".\n\nHowever, most of the academic studies on MRDTL use implementations based on existing relational databases, which results in many redundant operations. These redundancies can be reduced by using tricks such as tuple id propagation. More recently, it has been demonstrated that the efficiency can be increased further by using incremental updates, which completely eliminates redundancies.\n\nIn 2015, researchers at MIT presented the Deep Feature Synthesis algorithm and demonstrated its effectiveness in online data science competitions where it beat 615 of 906 human teams. Deep Feature Synthesis is available as an open source library called Featuretools. That work was followed by other researchers including IBM's OneBM and Berkeley's ExploreKit. The researchers at IBM stated that feature engineering automation \"helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost.\"\n\n", "related": "\n- Covariate\n- Data transformation\n- Feature learning\n- Hashing trick\n- Kernel method\n- List of datasets for machine learning research\n- Space mapping\n\n"}
{"id": "313845", "url": "https://en.wikipedia.org/wiki?curid=313845", "title": "Formal concept analysis", "text": "Formal concept analysis\n\nFormal concept analysis (FCA) is a principled way of deriving a \"concept hierarchy\" or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s.\n\nFormal concept analysis finds practical application in fields including data mining, text mining, machine learning, knowledge management, semantic web, software development, chemistry and biology.\n\nThe original motivation of formal concept analysis was the search for real-world meaning of mathematical order theory. One such possibility of very general nature is that data tables can be transformed into algebraic structures called \"complete lattices\", and that these can be utilized for data visualization and interpretation. A data table that represents a heterogeneous relation between objects and attributes, tabulating pairs of the form \"object \"g\" has attribute \"m\"\", is considered as a basic data type. It is referred to as a \"formal context\". In this theory, a \"formal concept\" is defined to be a pair (\"A\", \"B\"), where \"A\" is a set of objects (called the \"extent\") and \"B\" is a set of attributes (the \"intent\") such that\n\n- the extent \"A\" consists of all objects that share the attributes in \"B\", and dually\n- the intent \"B\" consists of all attributes shared by the objects in \"A\".\n\nIn this way, formal concept analysis formalizes the semantic notions of extension and intension.\n\nThe formal concepts of any formal context can—as explained below—be ordered in a hierarchy called more formally the context's \"concept lattice.\" The concept lattice can be graphically visualized as a \"line diagram\", which then may be helpful for understanding the data. Often however these lattices get too large for visualization. Then the mathematical theory of formal concept analysis may be helpful, e.g., for decomposing the lattice into smaller pieces without information loss, or for embedding it into another structure which is easier to interpret.\n\nThe theory in its present form goes back to the early 1980s and a research group led by Rudolf Wille, and Peter Burmeister at the Technische Universität Darmstadt. Its basic mathematical definitions, however, were already introduced in the 1930s by Garrett Birkhoff as part of general lattice theory. Other previous approaches to the same idea arose from various French research groups, but the Darmstadt group normalised the field and systematically worked out both its mathematical theory and its philosophical foundations. The latter refer in particular to Charles S. Peirce, but also to the \"Port-Royal Logic\".\n\nIn his article \"Restructuring Lattice Theory\" (1982), initiating formal concept analysis as a mathematical discipline, Wille starts from a discontent with the current lattice theory and pure mathematics in general: The production of theoretical results—often achieved by \"elaborate mental gymnastics\"—were impressive, but the connections between neighboring domains, even parts of a theory were getting weaker.\n\nThis aim traces back to the educationalist Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) critiqueable. Hence, by its origins formal concept analysis aims at interdisciplinarity and democratic control of research.\n\nIt corrects the starting point of lattice theory during the development of formal logic in the 19th century. Then—and later in model theory—a concept as unary predicate had been reduced to its extent. Now again, the philosophy of concepts should become less abstract by considering the intent. Hence, formal concept analysis is oriented towards the categories extension and intension of linguistics and classical conceptual logic.\n\nFormal concept analysis aims at the clarity of concepts according to Charles S. Peirce's pragmatic maxim by unfolding observable, elementary properties of the subsumed objects. In his late philosophy, Peirce assumed that logical thinking aims at perceiving reality, by the triade concept, judgement and conclusion. Mathematics is an abstraction of logic, develops patterns of possible realities and therefore may support rational communication. On this background, Wille defines:\n\nThe data in the example is taken from a semantic field study, where different kinds of bodies of water were systematically categorized by their attributes. For the purpose here it has been simplified.\n\nThe data table represents a \"formal context\", the \"line diagram\" next to it shows its \"concept lattice\". Formal definitions follow below.\n  \n\nThe above line diagram consists of circles, connecting line segments, and labels. Circles represent \"formal concepts\". The lines allow to read off the subconcept-superconcept hierarchy. Each object and attribute name is used as a label exactly once in the diagram, with objects below and attributes above concept circles. This is done in a way that an attribute can be reached from an object via an ascending path if and only if the object has the attribute.\n\nIn the diagram shown, e.g. the object \"reservoir\" has the attributes \"stagnant\" and \"constant\", but not the attributes \"temporary, running, natural, maritime\". Accordingly, \"puddle\" has exactly the characteristics \"temporary, stagnant\" and \"natural\".\n\nThe original formal context can be reconstructed from the labelled diagram, as well as the formal concepts. The extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept. The intent consists of those attributes to which there is an ascending path from that concept circle (in the diagram). In this diagram the concept immediately to the left of the label \"reservoir\" has the intent \"stagnant\" and \"natural\" and the extent \"puddle, maar, lake, pond, tarn, pool, lagoon,\" and \"sea\".\n\nA formal context is a triple \"K\" = (\"G\", \"M\", \"I\"), where \"G\" is a set of \"objects\", \"M\" is a set of \"attributes\", and \"I\" ⊆ \"G\" × \"M\" is a binary relation called \"incidence\" that expresses which objects \"have\" which attributes. For subsets \"A\" ⊆ \"G\" of objects and subsets \"B\" ⊆ \"M\" of attributes, one defines two \"derivation operators\" as follows:\n\n\"A\"' = {\"m\" ∈ \"M\" | \"(g,m)\" ∈ \"I\" for all \"g\" ∈ \"A\"}, and dually\n\n\"B\"' = {\"g\" ∈ \"G\" | \"(g,m)\" ∈ \"I\" for all \"m\" ∈ \"B\"}.\n\nApplying either derivation operator and then the other constitutes two closure operators:\n\n\"A\"   ↦  \"A\"\" = (\"A\"')'   for \"A\" ⊆ G   (extent closure), and\n\n\"B\"   ↦  \"B\"\" = (\"B\"')'   for \"B\" ⊆ M   (intent closure).\n\nThe derivation operators define a Galois connection between sets of objects and of attributes. This is why in French a concept lattice is sometimes called a \"treillis de Galois\" (Galois lattice).\n\nWith these derivation operators, Wille gave an elegant definition of a formal concept:\n\na pair (\"A\",\"B\") is a \"formal concept\" of a context (\"G\", \"M\", \"I\") provided that:\n\n\"A\" ⊆ \"G\",   \"B\" ⊆ \"M\",   \"A\"′ = \"B\",   and  \"B\"′ = \"A\".\n\nEquivalently and more intuitively, (\"A\",\"B\") is a formal concept precisely when:\n- every object in \"A\" has every attribute in \"B\",\n- for every object in \"G\" that is not in \"A\", there is some attribute in \"B\" that the object does not have,\n- for every attribute in \"M\" that is not in \"B\", there is some object in \"A\" that does not have that attribute.\n\nFor computing purposes, a formal context may be naturally represented as a (0,1)-matrix \"K\" in which the rows correspond to the objects, the columns correspond to the attributes, and each entry \"k\" equals to 1 if \"object \"i\" has attribute \"j\".\" In this matrix representation, each formal concept corresponds to a maximal submatrix (not necessarily contiguous) all of whose elements equal 1. It is however misleading to consider a formal context as \"boolean\", because the negated incidence (\"object \"g\" does not have attribute \"m\"\") is not concept forming in the same way as defined above. For this reason, the values 1 and 0 or TRUE and FALSE are usually avoided when representing formal contexts, and a symbol like formula_1 is used to express incidence.\n\nThe concepts (\"A\", \"B\") of a context \"K\" can be (partially) ordered by the inclusion of extents, or, equivalently, by the dual inclusion of intents. An order ≤ on the concepts is defined as follows: for any two concepts (\"A\", \"B\") and (\"A\", \"B\") of \"K\", we say that (\"A\", \"B\") ≤ (\"A\", \"B\") precisely when \"A\" ⊆ \"A\". Equivalently, (\"A\", \"B\") ≤ (\"A\", \"B\") whenever \"B\" ⊇ \"B\".\n\nIn this order, every set of formal concepts has a greatest common subconcept, or meet. Its extent consists of those objects that are common to all extents of the set. Dually, every set of formal concepts has a \"least common superconcept\", the intent of which comprises all attributes which all objects of that set of concepts have.\n\nThese meet and join operations satisfy the axioms defining a lattice, in fact a complete lattice. Conversely, it can be shown that every complete lattice is the concept lattice of some formal context (up to isomorphism).\n\nReal-world data is often given in the form of an object-attribute table, where the attributes have \"values\". Formal concept analysis handles such data by transforming them into the basic type of a (\"one-valued\") formal context. The method is called \"conceptual scaling\".\n\nThe negation of an attribute \"m\" is an attribute ¬\"m\", the extent of which is just the complement of the extent of \"m\", i.e., with (¬\"m\")' = G \\ m'. It is in general \"not\" assumed that negated attributes are available for concept formation. But pairs of attributes which are negations of each other often naturally occur, for example in contexts derived from conceptual scaling.\n\nFor possible negations of formal concepts see the section concept algebras below.\n\nAn \"implication\" \"A → B\" relates two sets \"A\" and \"B\" of attributes and expresses that every object possessing each attribute from \"A\" also has each attribute from \"B\". When (\"G\",\"M\",\"I\") is a formal context and \"A\", \"B\" are subsets of the set \"M\" of attributes (i.e., \"A,B ⊆ M\"), then the implication \"A → B\" \"is valid\" if \"A′ ⊆ B′\". For each finite formal context, the set of all valid implications has a \"canonical basis\", an irredundant set of implications from which all valid implications can be derived by the natural inference (Armstrong rules). This is used in \"attribute exploration\", a knowledge acquisition method based on implications.\n\nFormal concept analysis has elaborate mathematical foundations, making the field versatile. As a basic example we mention the \"arrow relations\", which are simple and easy to compute, but very useful. They are defined as follows: For \"g\" ∈ \"G\" and \"m\" ∈ \"M\" let\n\n\"g\" ↗ \"m\"  ⇔  \"(g, m)\" ∉ \"I\" and if \"m'⊆n' \" and \"m' ≠ n' \", then \"(g, n)\" ∈ \"I\",\n\nand dually\n\n\"g\" ↙ \"m\"  ⇔  \"(g, m)\" ∉ \"I\" and if \"g'⊆h' \" and \"g' ≠ h' \", then \"(h, m)\" ∈ \"I\".\n\nSince only non-incident object-attribute pairs can be related, these relations can conveniently be recorded in the table representing a formal context. Many lattice properties can be read off from the arrow relations, including distributivity and several of its generalizations. They also reveal structural information and can be used for determining, e.g., the congruence relations of the lattice.\n\n- Triadic concept analysis replaces the binary incidence relation between objects and attributes by a ternary relation between objects, attributes, and conditions. An incidence \"(g,m,c)\" then expresses that \"the object g has the attribute m under the condition c\". Although \"triadic concepts\" can be defined in analogy to the formal concepts above, the theory of the \"trilattices\" formed by them is much less developed than that of concept lattices, and seems to be difficult. Voutsadakis has studied the \"n\"-ary case.\n- Fuzzy concept analysis: Extensive work has been done on a fuzzy version of formal concept analysis.\n- Concept algebras: Modelling negation of formal concepts is somewhat problematic because the complement (\"G\" \\ \"A\", \"M\" \\ \"B\") of a formal concept (\"A\", \"B\") is in general not a concept. However, since the concept lattice is complete one can consider the join (\"A\", \"B\") of all concepts (\"C\", \"D\") that satisfy \"C\" ⊆ \"G\" \\ \"A\"; or dually the meet (\"A\", \"B\") of all concepts satisfying \"D\" ⊆ \"M\" \\ \"B\". These two operations are known as \"weak negation\" and \"weak opposition\", respectively. This can be expressed in terms of the \"derivation operators\". Weak negation can be written as (\"A\", \"B\") = ((\"G\" \\ \"A\")<nowiki>\"</nowiki>, (\"G\" \\ \"A\")'), and weak opposition can be written as (\"A\", \"B\") = ((\"M\" \\ \"B\")', (\"M\" \\ \"B\")<nowiki>\"</nowiki>). The concept lattice equipped with the two additional operations Δ and 𝛁 is known as the \"concept algebra\" of a context. Concept algebras generalize power sets. Weak negation on a concept lattice \"L\" is a \"weak complementation\", i.e. an order-reversing map Δ: \"L\" → \"L\" which satisfies the axioms \"x\" ≤ \"x\" and (\"x\"⋀\"y\") ⋁ (\"x\"⋀\"y\") = \"x\". Weak composition is a dual weak complementation. A (bounded) lattice such as a concept algebra, which is equipped with a weak complementation and a dual weak complementation, is called a \"weakly dicomplemented lattice\". Weakly dicomplemented lattices generalize distributive orthocomplemented lattices, i.e. Boolean algebras.\n\nTemporal concept analysis (TCA) is an extension of Formal Concept Analysis (FCA) aiming at a conceptual description of temporal phenomena. It provides animations in concept lattices obtained from data about changing objects. It offers a general way of understanding change of concrete or abstract objects in continuous, discrete or hybrid space and time. TCA applies conceptual scaling to temporal data bases.\n\nIn the simplest case TCA considers objects that change in time like a particle in physics, which, at each time, is at exactly one place. That happens in those temporal data where the attributes 'temporal object' and 'time' together form a key of the data base. Then the state (of a temporal object at a time in a view) is formalized as a certain object concept of the formal context describing the chosen view. In this simple case, a typical visualization of a temporal system is a line diagram of the concept lattice of the view into which trajectories of temporal objects are embedded.\nTCA generalizes the above mentioned case by considering temporal data bases with an arbitrary key. That leads to the notion of distributed objects which are at any given time at possibly many places, as for example, a high pressure zone on a weather map. The notions of 'temporal objects', 'time' and 'place' are represented as formal concepts in scales. A state is formalized as a set of object concepts.\nThat leads to a conceptual interpretation of the ideas of particles and waves in physics.\n\nThere is a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices. For a survey, see Kuznetsov and Obiedkov or the book by Ganter and Obiedkov, where also some pseudo-code can be found. Since the number of formal concepts may be exponential in the size of the formal context, the complexity of the algorithms usually is given with respect to the output size. Concept lattices with a few million elements can be handled without problems.\n\nMany FCA software applications are available today. The main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules. Most of these tools are academic open-source applications, such as:\n- ConExp\n- ToscanaJ\n- Lattice Miner\n- Coron\n- FcaBedrock\n\nA formal context can naturally be interpreted as a bipartite graph. The formal concepts then correspond to the maximal bicliques in that graph. The mathematical and algorithmic results of formal concept analysis may thus be used for the theory of maximal bicliques. The notion of bipartite dimension (of the complemented bipartite graph) translates to that of \"Ferrers dimension\" (of the formal context) and of order dimension (of the concept lattice) and has applications e.g. for Boolean matrix factorization.\n\nGiven an object-attribute numerical data-table, the goal of biclustering is to group together some objects having similar values of some attributes. For example, in gene expression data, it is known that genes (objects) may share a common behavior for a subset of biological situations (attributes) only: one should accordingly produce local patterns to characterize biological processes, the latter should possibly overlap, since a gene may be involved in several processes. The same remark applies for recommender systems where one is interested in local patterns characterizing groups of users that strongly share almost the same tastes for a subset of items.\n\nA bicluster in a binary object-attribute data-table is a pair \"(A,B)\" consisting of an inclusion-maximal set of objects \"A\" and an inclusion-maximal set of attributes \"B\" such that almost all objects from \"A\" have almost all attributes from \"B\" and vice versa.\n\nOf course, formal concepts can be considered as \"rigid\" biclusters where all objects have all attributes and vice versa. Hence, it is not surprising that some bicluster definitions coming from practice are just definitions of a formal concept.\n\nA bicluster of similar values in a numerical object-attribute data-table is usually defined as a pair consisting of an inclusion-maximal set of objects and an inclusion-maximal set of attributes having similar values for the objects. Such a pair can be represented as an inclusion-maximal rectangle in the numerical table, modulo rows and columns permutations. In it was shown that biclusters of similar values correspond to triconcepts of a triadic context where the third dimension is given by a scale that represents numerical attribute values by binary attributes.\n\nThis fact can be generalized to \"n\"-dimensional case, where \"n\"-dimensional clusters of similar values in \"n\"-dimensional data are represented by \"n+1\"-dimensional concepts. This reduction allows one to use standard definitions and algorithms from multidimensional concept analysis for computing multidimensional clusters.\n\nIn the theory of knowledge spaces it is assumed that in any knowledge space the family of \"knowledge states\" is union-closed. The complements of knowledge states therefore form a closure system and may be represented as the extents of some formal context.\n\nThe formal concept analysis can be used as a qualitative method for data analysis. Since the early beginnings of FBA in the early 1980s, the FBA research group at TU Darmstadt has gained experience from more than 200 projects using the FBA (as of 2005). Including the fields of: medicine and cell biology, genetics, ecology, software engineering, ontology, information and library sciences, office administration, law, linguistics, political science.\n\nMany more examples are e.g. described in: \"Formal Concept Analysis. Foundations and Applications\", conference papers at regular conferences such as: \"International Conference on Formal Concept Analysis\" (ICFCA), \"Concept Lattices and their Applications\" (CLA), or \"International Conference on Conceptual Structures\" (ICCS).\n\n", "related": "\n- Association rule learning\n- Cluster analysis\n- Commonsense reasoning\n- Conceptual analysis\n- Conceptual clustering\n- Concept learning\n- Correspondence analysis\n- Description logic\n- Factor analysis\n- Graphical model\n- Grounded theory\n- Inductive logic programming\n- Pattern theory\n- Statistical relational learning\n- Schema (genetic algorithms)\n\n- A Formal Concept Analysis Homepage\n- Demo\n- 11th International Conference on Formal Concept Analysis. ICFCA 2013 – Dresden, Germany – May 21–24, 2013\n"}
{"id": "43169442", "url": "https://en.wikipedia.org/wiki?curid=43169442", "title": "Deeplearning4j", "text": "Deeplearning4j\n\nEclipse Deeplearning4j is a deep learning programming library written for Java and the Java virtual machine (JVM) and a computing framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.\n\nDeeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group headquartered in San Francisco. It is supported commercially by the startup Skymind, which bundles DL4J, Tensorflow, Keras and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer. Deeplearning4j was contributed to the Eclipse Foundation in October 2017.\n\nDeeplearning4j relies on the widely used programming language Java, though it is compatible with Clojure and includes a Scala application programming interface (API). It is powered by its own open-source numerical computing library, ND4J, and works with both central processing units (CPUs) and graphics processing units (GPUs).\n\nDeeplearning4j has been used in several commercial and academic applications. The code is hosted on GitHub. A support forum is maintained on Gitter.\n\nThe framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders, and recurrent nets can be added to one another to create deep nets of varying types. It also has extensive visualization tools, and a computation graph.\n\nTraining with Deeplearning4j occurs in a cluster. Neural nets are trained in parallel via iterative reduce, which works on Hadoop-YARN and on Spark. Deeplearning4j also integrates with CUDA kernels to conduct pure GPU operations, and works with distributed GPUs.\n\nDeeplearning4j includes an n-dimensional array class using ND4J that allows scientific computing in Java and Scala, similar to the functions that NumPy provides to Python. It's effectively based on a library for linear algebra and matrix manipulation in a production environment.\n\nDataVec vectorizes various file formats and data types using an input/output format system similar to Hadoop's use of MapReduce; that is, it turns various data types into columns of scalars termed vectors. DataVec is designed to vectorize CSVs, images, sound, text, video, and time series.\n\nDeeplearning4j includes a vector space modeling and topic modeling toolkit, implemented in Java and integrating with parallel GPUs for performance. It is designed to handle large text sets.\n\nDeeplearning4j includes implementations of term frequency–inverse document frequency (tf–idf), deep learning, and Mikolov's word2vec algorithm, doc2vec, and GloVe, reimplemented and optimized in Java. It relies on t-distributed stochastic neighbor embedding (t-SNE) for word-cloud visualizations.\n\nReal-world use cases for Deeplearning4j include network intrusion detection and cybersecurity, fraud detection for the financial sector, anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising, and image recognition. Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner, Prediction.io, and Weka.\n\nDeeplearning4j serves machine-learning models for inference in production using the free developer edition of SKIL, the Skymind Intelligence Layer. A model server serves the parametric machine-learning models that makes decisions about data. It is used for the inference stage of a machine-learning workflow, after data pipelines and model training. A model server is the tool that allows data science research to be deployed in a real-world production environment.\n\nWhat a Web server is to the Internet, a model server is to AI. Where a Web server receives an HTTP request and returns data about a Web site, a model server receives data, and returns a decision or prediction about that data: e.g. sent an image, a model server might return a label for that image, identifying faces or animals in photographs.\n\nThe SKIL model server is able to import models from Python frameworks such as Tensorflow, Keras, Theano and CNTK, overcoming a major barrier in deploying deep learning models.\n\nDeeplearning4j is as fast as Caffe for non-trivial image recognition tasks using multiple GPUs. For programmers unfamiliar with HPC on the JVM, there are several parameters that must be adjusted to optimize neural network training time. These include setting the heap space, the garbage collection algorithm, employing off-heap memory and pre-saving data (pickling) for faster ETL. Together, these optimizations can lead to a 10x acceleration in performance with Deeplearning4j.\n\nDeeplearning4j can be used via multiple API languages including Java, Scala, Python, Clojure and Kotlin. Its Scala API is called ScalNet. Keras serves as its Python API. And its Clojure wrapper is known as DL4CLJ. The core languages performing the large-scale mathematical operations necessary for deep learning are C, C++ and CUDA C.\n\nTensorflow, Keras and Deeplearning4j work together. Deeplearning4j can import models from Tensorflow and other Python frameworks if they have been created with Keras.\n\n", "related": "\n- Comparison of deep learning software\n- Artificial intelligence\n- Machine learning\n- Deep learning\n"}
{"id": "46963137", "url": "https://en.wikipedia.org/wiki?curid=46963137", "title": "Local case-control sampling", "text": "Local case-control sampling\n\nIn machine learning, local case-control sampling is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most \"surprising\" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling.\n\nIn classification, a dataset is a set of \"N\" data points formula_1, where formula_2 is a feature vector, formula_3 is a label. Intuitively, a dataset is imbalanced when certain important statistical patterns are rare. The lack of observations of certain patterns does not always imply their irrelevance. For example, in medical studies of rare diseases, the small number of infected patients (cases) conveys the most valuable information for diagnosis and treatments.\n\nFormally, an imbalanced dataset exhibits one or more of the following properties:\n- \"Marginal Imbalance\". A dataset is marginally imbalanced if one class is rare compared to the other class. In other words, formula_4.\n- \"Conditional Imbalance\". A dataset is conditionally imbalanced when it is easy to predict the correct labels in most cases. For example, if formula_5, the dataset is conditionally imbalanced if formula_6 and formula_7.\n\nIn logistic regression, given the model formula_8, the prediction is made according to formula_9. The local-case control sampling algorithm assumes the availability of a pilot model formula_10. Given the pilot model, the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model. For a sample formula_11, define the acceptance probability as formula_12. The algorithm proceeds as follows:\n\n1. Generate independent formula_13 for formula_14.\n2. Fit a logistic regression model to the subsample formula_15, obtaining the unadjusted estimates formula_16.\n3. The output model is formula_17, where formula_18 and formula_19.\n\nThe algorithm can be understood as selecting samples that surprises the pilot model. Intuitively these samples are closer to the decision boundary of the classifier and is thus more informative.\n\nIn practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size formula_20, first estimate a model formula_21 using formula_22 samples from weighted case control sampling, then collect another formula_22 samples using local case-control sampling.\n\nIt is possible to control the sample size by multiplying the acceptance probability with a constant formula_24. For a larger sample size, pick formula_25 and adjust the acceptance probability to formula_26. For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling.\n\nThe algorithm has the following properties. When the pilot is consistent, the estimates using the samples from local case-control sampling is consistent even under model misspecification. If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with formula_25, the factor 2 is improved to formula_28.\n", "related": "NONE"}
{"id": "233488", "url": "https://en.wikipedia.org/wiki?curid=233488", "title": "Machine learning", "text": "Machine learning\n\nMachine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.\n\nMachine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.\n\nThe name \"machine learning\" was coined in 1959 by Arthur Samuel. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience \"E\" with respect to some class of tasks \"T\" and performance measure \"P\" if its performance at tasks in \"T\", as measured by \"P\", improves with experience \"E\".\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". In Turing's proposal the various characteristics that could be possessed by a \"thinking machine\" and the various implications in constructing one are exposed.\n\nMachine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs. For example, if the task were determining whether an image contained a certain object, the training data for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object. In special cases, the input may be only partially available, or restricted to special feedback. Semi-supervised learning algorithms develop mathematical models from incomplete training data, where a portion of the sample input doesn't have labels.\n\nClassification algorithms and regression algorithms are types of supervised learning. Classification algorithms are used when the outputs are restricted to a limited set of values. For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. For an algorithm that identifies spam emails, the output would be the prediction of either \"spam\" or \"not spam\", represented by the Boolean values true and false. Regression algorithms are named for their continuous outputs, meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price of an object.\n\nIn unsupervised learning, the algorithm builds a mathematical model from a set of data that contains only inputs and no desired output labels. Unsupervised learning algorithms are used to find structure in the data, like grouping or clustering of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in feature learning. Dimensionality reduction is the process of reducing the number of \"features\", or inputs, in a set of data.\n\nActive learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget and optimize the choice of inputs for which it will acquire training labels. When used interactively, these can be presented to a human user for labeling. Reinforcement learning algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment and are used in autonomous vehicles or in learning to play a game against a human opponent. Other specialized algorithms in machine learning include topic modeling, where the computer program is given a set of natural language documents and finds other documents that cover similar topics. Machine learning algorithms can be used to find the unobservable probability density function in density estimation problems. Meta learning algorithms learn their own inductive bias based on previous experience. In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.\n\nArthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term \"Machine Learning\" in 1959 while at IBM. A representative book of the machine learning research during the 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. The interest of machine learning related to pattern recognition continued during the 1970s, as described in the book of Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. \nAs a scientific endeavor, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\n\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\n\nMachine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory. It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.\n\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on \"known\" properties learned from the training data, data mining focuses on the discovery of (previously) \"unknown\" properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to \"reproduce known\" knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously \"unknown\" knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.\n\nMachine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\n\nLeo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest.\n\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call \"statistical learning\".\n\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\n\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.\n\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nThe types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\n\nSupervised learning algorithms include classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\nIn the case of semi-supervised learning algorithms, some of the training examples are missing training labels, but they can nevertheless be used to improve the quality of a model. In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\nUnsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses other domains involving summarizing and explaining data features.\n\nCluster analysis is the assignment of a set of observations into subsets (called \"clusters\") so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some \"similarity metric\" and evaluated, for example, by \"internal compactness\", or the similarity between members of the same cluster, and \"separation\", the difference between clusters. Other methods are based on \"estimated density\" and \"graph connectivity\".\n\nSemi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\nSelf-learning as machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning named Crossbar Adaptive Array (CAA). It is a learning with no external rewards and no external teacher advices. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. \nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n\nIt is a system with only one input, situation s, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal seeking behavior, in an environment that contains both desirable and undesirable situations. \n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.\n\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\n\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\n\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\n\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\n\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule formula_1 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\n\nInductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\n\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term \"inductive\" here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\nPerforming machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.\n\nSupport vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is oftentimes extended by regularization (mathematics) methods to mitigate overfitting and high bias, as can be seen in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (e.g. used for trendline fitting in Microsoft Excel ), Logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher dimensional space. \n\nA Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\nUsually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model.\n\nFederated learning is a new approach to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.\n\nThere are many applications for machine learning, including:\n- Agriculture\n- Anatomy\n- Adaptive websites\n- Affective computing\n- Banking\n- Bioinformatics\n- Brain–machine interfaces\n- Cheminformatics\n- Citizen science\n- Computer networks\n- Computer vision\n- Credit-card fraud detection\n- Data quality\n- DNA sequence classification\n- Economics\n- Financial market analysis\n- General game playing\n- Handwriting recognition\n- Information retrieval\n- Insurance\n- Internet fraud detection\n- Linguistics\n- Machine learning control\n- Machine perception\n- Machine translation\n- Marketing\n- Medical diagnosis\n- Natural language processing\n- Natural language understanding\n- Online advertising\n- Optimization\n- Recommender systems\n- Robot locomotion\n- Search engines\n- Sentiment analysis\n- Sequence mining\n- Software engineering\n- Speech recognition\n- Structural health monitoring\n- Syntactic pattern recognition\n- Telecommunication\n- Theorem proving\n- Time series forecasting\n- User behavior analytics\nIn 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning.\n\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\n\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment.\n\nMachine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.”\n\nClassification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\n\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC's associated Area Under the Curve (AUC).\n\nMachine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\n\nBecause human languages contain biases, machines trained on language \"corpora\" will necessarily also learn these biases.\n\nOther forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these \"greed\" biases are addressed.\n\nSoftware suites containing a variety of machine learning algorithms include the following:\n\n- CNTK\n- Deeplearning4j\n- ELKI\n- Keras\n- Caffe\n- ML.NET\n- Mahout\n- Mallet\n- mlpack\n- MXNet\n- Neural Lab\n- GNU Octave\n- OpenNN\n- Orange\n- Perl Data Language\n- scikit-learn\n- Shogun\n- Spark MLlib\n- Apache SystemML\n- TensorFlow\n- ROOT (TMVA with ROOT)\n- Torch / PyTorch\n- Weka / MOA\n- Yooreeka\n- R\n\n- KNIME\n- RapidMiner\n\n- Amazon Machine Learning\n- Angoss KnowledgeSTUDIO\n- Azure Machine Learning\n- Ayasdi\n- IBM Data Science Experience\n- Google Prediction API\n- IBM SPSS Modeler\n- KXEN Modeler\n- LIONsolver\n- Mathematica\n- MATLAB\n- Microsoft Azure\n- Neural Designer\n- NeuroSolutions\n- Oracle Data Mining\n- Oracle AI Platform Cloud Service\n- RCASE\n- SAS Enterprise Miner\n- SequenceL\n- Splunk\n- STATISTICA Data Miner\n- \"Journal of Machine Learning Research\"\n- \"Machine Learning\"\n- \"Nature Machine Intelligence\"\n- \"Neural Computation\"\n\n- Conference on Neural Information Processing Systems\n- International Conference on Machine Learning\n\n- Nils J. Nilsson, \"Introduction to Machine Learning\".\n- Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). \"The Elements of Statistical Learning\", Springer. .\n- Pedro Domingos (September 2015), \"The Master Algorithm\", Basic Books,\n- Ian H. Witten and Eibe Frank (2011). \"Data Mining: Practical machine learning tools and techniques\" Morgan Kaufmann, 664pp., .\n- Ethem Alpaydin (2004). \"Introduction to Machine Learning\", MIT Press, .\n- David J. C. MacKay. \"Information Theory, Inference, and Learning Algorithms\" Cambridge: Cambridge University Press, 2003.\n- Richard O. Duda, Peter E. Hart, David G. Stork (2001) \"Pattern classification\" (2nd edition), Wiley, New York, .\n- Christopher Bishop (1995). \"Neural Networks for Pattern Recognition\", Oxford University Press. .\n- Stuart Russell & Peter Norvig, (2009). \"Artificial Intelligence – A Modern Approach\". Pearson, .\n- Ray Solomonoff, \"An Inductive Inference Machine\", IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.\n- Ray Solomonoff, \"An Inductive Inference Machine\" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.\n\n- International Machine Learning Society\n- mloss is an academic database of open-source machine learning software.\n- Machine Learning Crash Course by Google. This is a free course on machine learning through the use of TensorFlow.\n", "related": "NONE"}
{"id": "42129549", "url": "https://en.wikipedia.org/wiki?curid=42129549", "title": "OpenNN", "text": "OpenNN\n\nOpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open-source, licensed under the GNU Lesser General Public License.\n\nThe software implements any number of layers of non-linear processing units for supervised learning. This deep architecture allows the design of neural networks with universal approximation properties. Additionally, it allows multiprocessing programming by means of OpenMP, in order to increase computer performance.\n\nOpenNN contains data mining algorithms as a bundle of functions. These can be embedded in other software tools, using an application programming interface, for the integration of the predictive analytics tasks. In this regard, a graphical user interface is missing but some functions can be supported by specific visualization tools.\n\nThe development started in 2003 at the International Center for Numerical Methods in Engineering, within the research project funded by the European Union called RAMFLOOD (Risk Assessment and Management of FLOODs). Then it continued as part of similar projects.\nAt present, OpenNN is being developed by the startup company Artelnics.\n\nOpenNN is a general purpose artificial intelligence software package. It uses machine learning techniques for solving data mining and predictive analytics tasks in different fields. For instance, the library has been applied in the engineering, energy, or chemistry sectors.\n\n", "related": "\n- Comparison of deep learning software\n- Neural Designer, also developed by Artelnics\n- Artificial intelligence\n- Machine learning\n- Deep learning\n- Artificial neural network\n"}
{"id": "31877832", "url": "https://en.wikipedia.org/wiki?curid=31877832", "title": "Ball tree", "text": "Ball tree\n\nIn computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as \"balls\". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.\n\nA ball tree is a binary tree in which every node defines a D-dimensional hypersphere, or ball, containing a subset of the points to be searched. Each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls. While the balls themselves may intersect, each point is assigned to one or the other ball in the partition according to its distance from the ball's center. Each leaf node in the tree defines a ball and enumerates all data points inside that ball.\n\nEach node in the tree defines the smallest ball that contains all data points in its subtree. This gives rise to the useful property that, for a given test point , the distance to any point in a ball in the tree is greater than or equal to the distance from to the ball. Formally:\n\nWhere formula_2 is the minimum possible distance from any point in the ball to some point .\n\nBall-trees are related to the M-tree, but only support binary splits, whereas in the M-tree each level splits formula_3 to formula_4 fold, thus leading to a shallower tree structure, therefore need fewer distance computations, which usually yields faster queries. Furthermore, M-trees can better be stored on disk, which is organized in pages. The M-tree also keeps the distances from the parent node precomputed to speed up queries.\n\nVantage-point trees are also similar, but they binary split into one ball, and the remaining data, instead of using two balls.\n\nA number of ball tree construction algorithms are available. The goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type (e.g. nearest-neighbor) efficiently in the average case. The specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data. However, a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes. Given the varied distributions of real-world data sets, this is a difficult task, but there are several heuristics that partition the data well in practice. In general, there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric. \nThis section briefly describes the simplest of these algorithms. A more in-depth discussion of five algorithms was given by Stephen Omohundro.\n\nThe simplest such procedure is termed the \"k-d Construction Algorithm\", by analogy with the process used to construct k-d trees. This is an off-line algorithm, that is, an algorithm that operates on the entire data set at once. The tree is built top-down by recursively splitting the data points into two sets. Splits are chosen along the single dimension with the greatest spread of points, with the sets partitioned by the median value of all points along that dimension. Finding the split for each internal node requires linear time in the number of samples contained in that node, yielding an algorithm with time complexity formula_5, where \"n\" is the number of data points.\n\n function construct_balltree is\n\nAn important application of ball trees is expediting nearest neighbor search queries, in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric (e.g. Euclidean distance). A simple search algorithm, sometimes called KNS1, exploits the distance property of the ball tree. In particular, if the algorithm is searching the data structure with a test point \"t\", and has already seen some point \"p\" that is closest to \"t\" among the points encountered so far, then any subtree whose ball is further from \"t\" than \"p\" can be ignored for the rest of the search.\n\nThe ball tree nearest-neighbor algorithm examines nodes in depth-first order, starting at the root. During the search, the algorithm\nmaintains a max-first priority queue (often implemented with a heap), denoted \"Q\" here, of the k nearest points encountered so far. At each node \"B\", it may perform one of three operations, before finally returning an updated version of the priority queue:\n\n1. If the distance from the test point \"t\" to the current node \"B\" is greater than the furthest point in \"Q\", ignore \"B\" and return \"Q\".\n2. If \"B\" is a leaf node, scan through every point enumerated in \"B\" and update the nearest-neighbor queue appropriately. Return the updated queue.\n3. If \"B\" is an internal node, call the algorithm recursively on \"B\"'s two children, searching the child whose center is closer to \"t\" first. Return the queue after each of these calls has updated it in turn.\n\nPerforming the recursive search in the order described in point 3 above increases likelihood that the further child will be pruned \nentirely during the search.\n\n function knn_search is\n\nIn comparison with several other data structures, ball trees have been shown to perform fairly well on \nthe nearest-neighbor search problem, particularly as their number of dimensions grows.\nHowever, the best nearest-neighbor data structure for a given application will depend on the dimensionality, number of data points, and underlying structure of the data.\n", "related": "NONE"}
{"id": "47228422", "url": "https://en.wikipedia.org/wiki?curid=47228422", "title": "User behavior analytics", "text": "User behavior analytics\n\nUser behavior analytics (UBA) as defined by Gartner is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats. Instead of tracking devices or security events, UBA tracks a system's users. Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.\n\nThe problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that \"Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too.\" \n\nDevelopments in UBA technology led Gartner to evolve the category to user and entity behavior analytics (\"UEBA\"). In September 2015, Gartner published the Market Guide for User and Entity Analytics by Vice President and Distinguished Analyst, Avivah Litan, that provided a thorough definition and explanation. UEBA was referred to in earlier Gartner reports but not in much depth. Expanding the definition from UBA includes devices, applications, servers, data, or anything with an IP address. It moves beyond the fraud-oriented UBA focus to a broader one encompassing \"malicious and abusive behavior that otherwise went unnoticed by existing security monitoring systems, such as SIEM and DLP.\" The addition of \"entity\" reflects that devices may play a role in a network attack and may also be valuable in uncovering attack activity. \"When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.\"\n\nParticularly in the computer security market, there are many vendors for UEBA applications. They can be \"differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).\" \nAccording to the 2015 market guide released by Gartner, \"the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.\" The report further projected, \"Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.\"\n\n", "related": "\n- Behavioral analytics\n- Network behavior anomaly detection\n"}
