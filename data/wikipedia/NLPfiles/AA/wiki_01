{"id": "47527969", "url": "https://en.wikipedia.org/wiki?curid=47527969", "title": "Word2vec", "text": "Word2vec\n\nWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located close to one another in the space.\n\nWord2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google and patented. The algorithm has been subsequently analysed and explained by other researchers. Embedding vectors created using the Word2vec algorithm have many advantages compared to earlier algorithms such as latent semantic analysis.\n\nWord2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. In the continuous bag-of-words architecture, the model predicts the current word from a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). In the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weighs nearby context words more heavily than more distant context words. According to the authors' note, CBOW is faster while skip-gram is slower but does a better job for infrequent words.\n\nResults of word2vec training can be sensitive to parametrization. The following are some important parameters in word2vec training.\n\nA Word2vec model can be trained with hierarchical softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors. As training epochs increase, hierarchical softmax stops being useful.\n\nHigh frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed.\n\nQuality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish. Typically, the dimensionality of the vectors is set to be between 100 and 1,000.\n\nThe size of the context window determines how many words before and after a given word would be included as context words of the given word. According to the authors' note, the recommended value is 10 for skip-gram and 5 for CBOW.\n\nAn extension of word2vec to construct embeddings from entire documents (rather than the individual words) has been proposed. This extension is called paragraph2vec or doc2vec and has been implemented in the C, Python and Java/Scala tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.\n\nAn extension of word vectors for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns. A similar variant, dna2vec, has shown that there is correlation between Needleman-Wunsch similarity score and cosine similarity of dna2vec word vectors.\n\nAn extension of word vectors for creating a dense vector representation of unstructured radiology reports has been proposed by Banerjee et al. One of the biggest challenges with Word2Vec is how to handle unknown or out-of-vocabulary (OOV) words and morphologically similar words. This can particularly be an issue in domains like medicine where synonyms and related words can be used depending on the preferred style of radiologist, and words may have been used infrequently in a large corpus. If the word2vec model has not encountered a particular word before, it will be forced to use a random vector, which is generally far from its ideal representation.\n\nIWE combines Word2vec with a semantic dictionary mapping technique to tackle the major challenges of information extraction from clinical texts, which include ambiguity of free text narrative style, lexical variations, use of ungrammatical and telegraphic phases, arbitrary ordering of words, and frequent appearance of abbreviations and acronyms. Of particular interest, the IWE model (trained on the one institutional dataset) successfully translated to a different institutional dataset which demonstrates good generalizability of the approach across institutions.\n\nThe reasons for successful word embedding learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth's distributional hypothesis. However, they note that this explanation is \"very hand-wavy\" and argue that a more formal explanation would be preferable.\n\nLevy et al. (2015) show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks. Arora et al. (2016) explain word2vec and related algorithms as performing inference for a simple generative model for text, which involves a random walk generation process based upon loglinear topic model. They use this to explain some properties of word embeddings, including their use to solve analogies.\n\nThe word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al. (2013) found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated for a range of semantic relations (such as Country–Capital) as well as syntactic relations (e.g. present tense–past tense)\n\nMikolov et al. (2013) develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They developed a set of 8,869 semantic relations and 10,675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec, or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.\n\nThe use of different model parameters and different corpus sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.\n\nIn models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.\n\nAccuracy increases overall as the number of words used increases, and as the number of dimensions increases. Mikolov et al. report that doubling the amount of training data results in an increase in computational complexity equivalent to doubling the number of vector dimensions.\n\nAltszyler and coauthors (2017) studied Word2vec performance in two semantic tests for different corpus size. They found that Word2vec has a steep learning curve, outperforming another word-embedding technique (LSA) when it is trained with medium to large corpus size (more than 10 million words). However, with a small training corpus LSA showed better performance. Additionally they show that the best parameter setting depends on the task and the training corpus. Nevertheless, for skip-gram models trained in medium size corpora, with 50 dimensions, a window size of 15 and 10 negative samples seems to be a good parameter setting.\n\n- C\n- C#\n- Python (TensorFlow)\n- Python (Gensim)\n- Java/Scala\n- Wikipedia2Vec (introduction)\n\n", "related": "\n- Autoencoder\n- Document-term matrix\n- Feature extraction\n- Feature learning\n- Neural network language models\n- Vector space model\n- Thought vector\n- fastText\n- GloVe\n- Normalized compression distance\n"}
{"id": "47577902", "url": "https://en.wikipedia.org/wiki?curid=47577902", "title": "Trax Retail", "text": "Trax Retail\n\nTrax is a technology company headquartered in Singapore, with offices throughout the Asia-Pacific, Europe, the Middle East, North America, and South America. Founded in 2010 by Joel Bar-El and Dror Feldheim, Trax has more than 150 customers in the retail and FMCG industries, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Customers use the company’s computer vision technology to collect, measure, and analyze what’s happening on physical store shelves. Trax’s services are available in 45 markets. \n\nTrax's development centre, which opened in July 2012, is in Tel Aviv. In 2015, the company opened its first two regional offices: London in January and São Paulo, Brazil, in April. In March 2016, Trax established its US headquarters in Atlanta, Georgia. The company opened two more regional offices in Shanghai and Mexico City in June and September 2016, respectively. \n\nTrax closed its first round of funding for US$1.1 million in June 2011, its second round of funding for US$6.4 million in December 2012, and its third round of funding for US$15.7 million in February 2014. In December 2014, the company announced its fourth round of investment for US$15 million and a fifth round of funding for US$40 million on June 8, 2016.On February 8, 2017, Trax closed its sixth round of funding for US$19.5 million. On June 30, 2017, Trax announced its seventh round of funding for US$64 million led by global private equity giant Warburg Pincus.\n\nIn 2018, Trax raised $125 million in an investment round led by Chinese private equity firm Boyu Capital, and Singapore’s GIC Pte later came on board as a shareholder.\n\nIn July 2019, Trax announced that it had closed a US$100 million Series D investment. HOPU Investment Management, an Asian alternative asset manager, led the transaction. Trax will use the investment from HOPU to develop its presence both in China and globally and to help deploy its retailer solutions at scale.\n\nOn July 12, 2017, Trax announced that it had acquired Nielsen Store Observations (NSO) project-based services in the US from Nielsen Corporation.\n\nOn January 18, 2018, Trax announced that it had acquired US-based Quri, a provider of crowdsourced data on in-store conditions for the consumer packaged goods (CPG) industry.\n\nIn June 2019, Trax announced its acquisition of LenzTech, a Chinese-based company providing artificial intelligence (AI), crowdsourcing services, and Big Data analysis to the retail market.\n\nIn July 2019, Trax announced that it had acquired Shopkick, a US-based company whose shopping app for smartphones and tablets allows users to earn rewards for their online and in-store shopping activities. Later that month, Trax announced the acquisition of its main global competitor, Planorama, a European provider of image recognition for retail execution.\n\nIn January 2019, Trax announced a partnership with Google Cloud Platform. Using Google Cloud Platform, Trax aims to deliver its Retail Watch image recognition product and machine learning capability to retailers to provide real-time shelf insights, such as out-of-stock products.\n\nIn April 2019, Trax and IRI formed an alliance to provide CPG manufacturers with greater insights for better and faster in-store execution and analytics. \n\nIn July 2019, Trax announced a strategic partnership agreement with Kantar Group to help CPG manufacturers and retailers optimise category management and product assortment. \n\nTrax offers Computer Vision-led products and services to CPG manufacturers and retailers. To CPGs, Trax delivers in-store execution, market measurement and shelf strategy solutions, while retailers can leverage store management offerings. These products and services draw on store digitization, and data discovery and visualization. \nTrax’s computer vision technology uses artificial intelligence, fine-grained image recognition, and machine learning engines to convert store images into shelf insights. The technology can recognise products that are similar or identical, such as branded drinks or shampoo bottles but can also differentiate between them based on variety and size. \n\nTrax piloted its machine learning algorithms with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better the technology gets at recognizing the same products in different shapes and sizes.\n\nTrax has received a number of awards and recognition, such as the Deloitte 2016 Technology Fast 50 Award and Frost & Sullivan's 2017 Product Innovation Award.\n\n", "related": "NONE"}
{"id": "47845063", "url": "https://en.wikipedia.org/wiki?curid=47845063", "title": "Stochastic block model", "text": "Stochastic block model\n\nThe stochastic block model is a generative model for random graphs. This model tends to produce graphs containing \"communities\", subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data.\n\nThe stochastic block model takes the following parameters:\n- The number formula_1 of vertices;\n- a partition of the vertex set formula_2 into disjoint subsets formula_3, called \"communities\";\n- a symmetric formula_4 matrix formula_5 of edge probabilities.\n\nThe edge set is then sampled at random as follows: any two vertices formula_6 and formula_7 are connected by an edge with probability formula_8. An example problem is: given a graph with formula_1 vertices, where the edges are sampled as described, recover the groups formula_3.\n\nIf the probability matrix is a constant, in the sense that formula_11 for all formula_12, then the result is the Erdős–Rényi model formula_13. This case is degenerate—the partition into communities becomes irrelevant—but it illustrates a close relationship to the Erdős–Rényi model.\n\nThe \"planted partition model\" is the special case that the values of the probability matrix formula_5 are a constant formula_15 on the diagonal and another constant formula_16 off the diagonal. Thus two vertices within the same community share an edge with probability formula_15, while two vertices in different communities share an edge with probability formula_16. Sometimes it is this restricted model that is called the stochastic block model. The case where formula_19 is called an \"assortative\" model, while the case formula_20 is called \"disassortative\".\n\nReturning to the general stochastic block model, a model is called \"strongly assortative\" if formula_21 whenever formula_22: all diagonal entries dominate all off-diagonal entries. A model is called \"weakly assortative\" if formula_23 whenever formula_24: each diagonal entry is only required to dominate the rest of its own row and column. \"Disassortative\" forms of this terminology exist, by reversing all inequalities. Algorithmic recovery is often easier against block models with assortative or disassortative conditions of this form.\n\nMuch of the literature on algorithmic community detection addresses three statistical tasks: detection, partial recovery, and exact recovery.\n\nThe goal of detection algorithms is simply to determine, given a sampled graph, whether the graph has latent community structure. More precisely, a graph might be generated, with some known prior probability, from a known stochastic block model, and otherwise from a similar Erdos-Renyi model. The algorithmic task is to correctly identify which of these two underlying models generated the graph.\n\nIn partial recovery, the goal is to approximately determine the latent partition into communities, in the sense of finding a partition that is correlated with the true partition significantly better than a random guess.\n\nIn exact recovery, the goal is to recover the latent partition into communities exactly. The community sizes and probability matrix may be known or unknown.\n\nStochastic block models exhibit a sharp threshold effect reminiscent of percolation thresholds. Suppose that we allow the size formula_1 of the graph to grow, keeping the community sizes in fixed proportions. If the probability matrix remains fixed, tasks such as partial and exact recovery become feasible for all non-degenerate parameter settings. However, if we scale down the probability matrix at a suitable rate as formula_1 increases, we observe a sharp phase transition: for certain settings of the parameters, it will become possible to achieve recovery with probability tending to 1, whereas on the opposite side of the parameter threshold, the probability of recovery tends to 0 no matter what algorithm is used.\n\nFor partial recovery, the appropriate scaling is to take formula_27 for fixed formula_28, resulting in graphs of constant average degree. In the case of two equal-sized communities, in the assortative planted partition model with probability matrix\nformula_29\npartial recovery is feasible with probability formula_30 whenever formula_31, whereas any estimator fails partial recovery with probability formula_32 whenever formula_33.\n\nFor exact recovery, the appropriate scaling is to take formula_34, resulting in graphs of logarithmic average degree. Here a similar threshold exists: for the assortative planted partition model with formula_35 equal-sized communities, the threshold lies at formula_36. In fact, the exact recovery threshold is known for the fully general stochastic block model.\nIn principle, exact recovery can be solved in its feasible range using maximum likelihood, but this amounts to solving a constrained or regularized cut problem such as minimum bisection that is typically NP-complete. Hence, no known efficient algorithms will correctly compute the maximum-likelihood estimate in the worst case.\n\nHowever, a wide variety of algorithms perform well in the average case, and many high-probability performance guarantees have been proven for algorithms in both the partial and exact recovery settings. Successful algorithms include spectral clustering of the vertices, semidefinite programming, forms of belief propagation, and community detection among others.\n\nSeveral variants of the model exist. One minor tweak allocates vertices to communities randomly, according to a categorical distribution, rather than in a fixed partition. More significant variants include the censored block model and the mixed-membership block model.\n", "related": "NONE"}
{"id": "28650287", "url": "https://en.wikipedia.org/wiki?curid=28650287", "title": "Cleverbot", "text": "Cleverbot\n\nCleverbot is a chatterbot web application that uses an artificial intelligence (AI) algorithm to have conversations with humans. It was created by British AI scientist Rollo Carpenter. It was preceded by Jabberwacky, a chatbot project that began in 1986 and went online in 1997. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web, the number of conversations held has exceeded 150 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app.\n\nUnlike some other chatterbots, Cleverbot's responses are not pre-programmed. Instead, it learns from human input: Humans type into the box below the Cleverbot logo and the system finds all keywords or an exact phrase matching the input. After searching through its saved conversations, it responds to the input by finding how a human responded to that input when it was asked, in part or in full, by Cleverbot.\n\nCleverbot participated in a formal Turing test at the 2011 Techniche festival at the Indian Institute of Technology Guwahati on 3 September 2011. Out of the 1334 votes cast, Cleverbot was judged to be 59.3% human, compared to the rating of 63.3% human achieved by human participants. A score of 50.05% or higher is often considered to be a passing grade. The software running for the event had to handle just 1 or 2 simultaneous requests, whereas online Cleverbot is usually talking to around 80,000 people at once.\n\nCleverbot is constantly being controversial growing in data size at a rate of 400 to 7 million interactions per second. Updates to the software have been mostly behind the scenes. In 2014, Cleverbot was upgraded to use GPU serving techniques. Unlike Eliza, the program does not respond in a fixed way, instead choosing its responses heuristically using fuzzy logic, the whole of the conversation being compared to the millions that have taken place before. Cleverbot now uses over 279 million interactions, about 3-4% of the data it has already accumulated. The developers of Cleverbot are attempting to build a new version using machine learning techniques.\n\nA significant part of the engine behind Cleverbot and an API for accessing it has been made available to developers in the form of Cleverscript. A service for directly accessing Cleverbot has been made available to developers in the form of Cleverbot.io.\n\nAn app that uses the Cleverscript engine to play a game of 20 Questions, has been launched under the name \"Clevernator\". Unlike other such games, the player asks the questions and it is the role of the AI to understand, and answer factually. An app that allows owners to create and talk to their own small Cleverbot-like AI has been launched, called \"Cleverme!\" for Apple products.\n\nIn early 2017, a Twitch stream of two Google Home devices modified to talk to each other using Cleverbot.io garnered over 700,000 visitors and over 30,000 peak concurrent viewers.\n\n", "related": "\n- Omegle\n\n- Cleverscript website\n- Cleverbot.io website\n- Livestream of 2 cleverbots chatting with each other on Twitch\n"}
{"id": "926722", "url": "https://en.wikipedia.org/wiki?curid=926722", "title": "Relational data mining", "text": "Relational data mining\n\nRelational data mining is the data mining technique for relational\ndatabases. Unlike traditional data mining algorithms, which look for\npatterns in a single table (propositional patterns), \nrelational data mining algorithms look for patterns among multiple tables\n(relational patterns). For most types of propositional\npatterns, there are corresponding relational patterns. For example,\nthere are relational classification rules (relational classification), relational regression tree, and relational association rules.\n\nThere are several approaches to relational data mining:\n1. Inductive Logic Programming (ILP)\n2. Statistical Relational Learning (SRL)\n3. Graph Mining\n4. Propositionalization\n5. Multi-view learning\n\nMulti-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations \"live in\", \"nearby\" and \"humid\": “Those who \"live in\" a place which is \"near by\" a city with \"humid\" climate type and also are \"younger\" than 20 -> their \"health condition\" is good”. Such association rules are extractable from RDBMS data or semantic web data.\n\n- Safarii: a Data Mining environment for analysing large relational databases based on a multi-relational data mining engine.\n- Dataconda: a software, free for research and teaching purposes, that helps mining relational databases without the use of SQL.\n\n- Relational dataset repository: a collection of publicly available relational datasets.\n\n", "related": "\n- Data mining\n- Structure mining\n- Database mining\n\n- Web page for a text book on relational data mining\n"}
{"id": "47937215", "url": "https://en.wikipedia.org/wiki?curid=47937215", "title": "The Master Algorithm", "text": "The Master Algorithm\n\nThe Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.\n\nThe book outlines five tribes of machine learning: inductive reasoning, connectionism, evolutionary computation, Bayes' theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgements. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying \"master algorithm\".\n\nTowards the end of the book the author pictures a \"master algorithm\" in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesn't yet exist, he briefly reviews his own invention of the Markov logic network.\n\nIn 2016 Bill Gates recommended the book, alongside Nick Bostrom's \"Superintelligence\", as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese President Xi Jinping's bookshelf.\n\nA computer science educator stated in \"Times Higher Education\" that the examples are clear and accessible. In contrast, \"The Economist\" agreed Domingos \"does a good job\" but complained that he \"constantly invents metaphors that grate or confuse\". \"Kirkus Reviews\" praised the book, stating \"Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights.\"\n\nA \"New Scientist\" review called it \"compelling but rather unquestioning\".\n\n- https://www.wsj.com/articles/the-sum-of-human-knowledge-1442610803\n- http://www.kdnuggets.com/2015/09/book-master-algorithm-pedro-domingos.html\n- http://www.kdnuggets.com/2014/08/interview-pedro-domingos-master-algorithm-new-deep-learning.html (interview)\n", "related": "NONE"}
{"id": "9583985", "url": "https://en.wikipedia.org/wiki?curid=9583985", "title": "Committee machine", "text": "Committee machine\n\nA committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers.\n\nIn this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes the following methods:\n- Ensemble averaging\nIn ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.\n- Boosting\nIn boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.\n\nIn this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:\n- Mixture of experts\nIn mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.\n- Hierarchical mixture of experts\nIn hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion.\n", "related": "NONE"}
{"id": "44628821", "url": "https://en.wikipedia.org/wiki?curid=44628821", "title": "Matrix regularization", "text": "Matrix regularization\n\nIn the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over\n\nto find a vector formula_2 that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem can be written as\n\nwhere the vector norm enforcing a regularization penalty on formula_2 has been extended to a matrix norm on formula_5.\n\nMatrix regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning.\n\nConsider a matrix formula_6 to be learned from a set of examples, formula_7, where formula_8 goes from formula_9 to formula_10, and formula_11 goes from formula_9 to formula_13. Let each input matrix formula_14 be formula_15, and let formula_6 be of size formula_17. A general model for the output formula_18 can be posed as\n\nwhere the inner product is the Frobenius inner product. For different applications the matrices formula_14 will have different forms, but for each of these the optimization problem to infer formula_6 can be written as\n\nwhere formula_23 defines the empirical error for a given formula_6, and formula_25 is a matrix regularization penalty. The function formula_25 is typically chosen to be convex and is often selected to enforce sparsity (using formula_27-norms) and/or smoothness (using formula_28-norms). Finally, formula_6 is in the space of matrices formula_30 with Frobenius inner product formula_31.\n\nIn the problem of matrix completion, the matrix formula_32 takes the form\n\nwhere formula_34 and formula_35 are the canonical basis in formula_36 and formula_37. In this case the role of the Frobenius inner product is to select individual elements formula_38 from the matrix formula_6. Thus, the output formula_18 is a sampling of entries from the matrix formula_6.\n\nThe problem of reconstructing formula_6 from a small set of sampled entries is possible only under certain restrictions on the matrix, and these restrictions can be enforced by a regularization function. For example, it might be assumed that formula_6 is low-rank, in which case the regularization penalty can take the form of a nuclear norm.\n\nwhere formula_45, with formula_8 from formula_9 to formula_48, are the singular values of formula_6.\n\nModels used in multivariate regression are parameterized by a matrix of coefficients. In the Frobenius inner product above, each matrix formula_5 is\n\nsuch that the output of the inner product is the dot product of one row of the input with one column of the coefficient matrix. The familiar form of such models is\n\nMany of the vector norms used in single variable regression can be extended to the multivariate case. One example is the squared Frobenius norm, which can be viewed as an formula_28-norm acting either entrywise, or on the singular values of the matrix:\n\nIn the multivariate case the effect of regularizing with the Frobenius norm is the same as the vector case; very complex models will have larger norms, and, thus, will be penalized more.\n\nThe setup for multi-task learning is almost the same as the setup for multivariate regression. The primary difference is that the input variables are also indexed by task (columns of formula_55). The representation with the Frobenius inner product is then\n\nThe role of matrix regularization in this setting can be the same as in multivariate regression, but matrix norms can also be used to couple learning problems across tasks. In particular, note that for the optimization problem\n\nthe solutions corresponding to each column of formula_55 are decoupled. That is, the same solution can be found by solving the joint problem, or by solving an isolated regression problem for each column. The problems can be coupled by adding an additional regulatization penalty on the covariance of solutions\n\nwhere formula_60 models the relationship between tasks. This scheme can be used to both enforce similarity of solutions across tasks, and to learn the specific structure of task similarity by alternating between optimizations of formula_6 and formula_60. When the relationship between tasks is known to lie on a graph, the Laplacian matrix of the graph can be used to couple the learning problems.\n\nRegularization by spectral filtering has been used to find stable solutions to problems such as those discussed above by addressing ill-posed matrix inversions (see for example Filter function for Tikhonov regularization). In many cases the regularization function acts on the input (or kernel) to ensure a bounded inverse by eliminating small singular values, but it can also be useful to have spectral norms that act on the matrix that is to be learned.\n\nThere are a number of matrix norms that act on the singular values of the matrix. Frequently used examples include the Schatten p-norms, with \"p\" = 1 or 2. For example, matrix regularization with a Schatten 1-norm, also called the nuclear norm, can be used to enforce sparsity in the spectrum of a matrix. This has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank. In this case the optimization problem becomes:\n\nSpectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression. In this setting, a reduced rank coefficient matrix can be found by keeping just the top formula_10 singular values, but this can be extended to keep any reduced set of singular values and vectors.\n\nSparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the Lasso method). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise formula_66-norm of the matrix, but the formula_66-norm is not convex. In practice this can be implemented by convex relaxation to the formula_27-norm. While entry-wise regularization with an formula_27-norm will find solutions with a small number of nonzero elements, applying an formula_27-norm to different groups of variables can enforce structure in the sparsity of solutions.\n\nThe most straightforward example of structured sparsity uses the formula_71 norm with formula_72 and formula_73:\n\nFor example, the formula_75 norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group. The grouping effect is achieved by taking the formula_28-norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the formula_28-norms of each column.\n\nMore generally, the formula_75 norm can be applied to arbitrary groups of variables:\n\nwhere the index formula_80 is across groups of variables, and formula_81 indicates the cardinality of group formula_80.\n\nAlgorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via matching pursuit: and proximal gradient methods. By writing the proximal gradient with respect to a given coefficient, formula_83, it can be seen that this norm enforces a group-wise soft threshold\n\nwhere formula_85 is the indicator function for group norms formula_86.\n\nThus, using formula_75 norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix formula_55) will depend on the same sparse set of input variables.\n\nThe ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning. This can be useful when there are multiple types of input data (color and texture, for example) with different appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps formula_89 and formula_90 that lie in corresponding reproducing kernel Hilbert spaces formula_91, then a larger space, formula_92, can be created as the sum of two spaces:\n\nassuming linear independence in formula_89 and formula_90. In this case the formula_75-norm is again the sum of norms:\n\nThus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width.\n\n", "related": "\n- Regularization (mathematics)\n"}
{"id": "48777199", "url": "https://en.wikipedia.org/wiki?curid=48777199", "title": "Manifold regularization", "text": "Manifold regularization\n\nIn machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is \"smooth\": data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.\n\nManifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function formula_1 from among a hypothesis space of functions formula_2. The hypothesis space is an RKHS, meaning that it is associated with a kernel formula_3, and so every candidate function formula_1 has a norm formula_5, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.\n\nFormally, given a set of labeled training data formula_6 with formula_7 and a loss function formula_8, a learning algorithm using Tikhonov regularization will attempt to solve the expression\n\nwhere formula_10 is a hyperparameter that controls how much the algorithm will prefer simpler functions to functions that fit the data better.\n\nManifold regularization adds a second regularization term, the \"intrinsic regularizer\", to the \"ambient regularizer\" used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space formula_11, but instead from a nonlinear manifold formula_12. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.\n\nThere are many possible choices for formula_13. Many natural choices involve the gradient on the manifold formula_14, which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient formula_15 should be small where the \"marginal probability density\" formula_16, the probability density of a randomly drawn data point appearing at formula_17, is large. This gives one appropriate choice for the intrinsic regularizer:\n\nIn practice, this norm cannot be computed directly because the marginal distribution formula_19 is unknown, but it can be estimated from the provided data. In particular, if the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include formula_20 labeled examples (pairs of an input formula_17 and a label formula_22) and formula_23 unlabeled examples (inputs without associated labels). Define formula_24 to be a matrix of edge weights for a graph, where formula_25 is a distance measure between the data points formula_26 and formula_27. Define formula_28 to be a diagonal matrix with formula_29 and formula_30 to be the Laplacian matrix formula_31. Then, as the number of data points formula_32 increases, formula_30 converges to the Laplace–Beltrami operator formula_34, which is the divergence of the gradient formula_35. Then, if formula_36 is a vector of the values of formula_1 at the data, formula_38, the intrinsic norm can be estimated:\n\nAs the number of data points formula_32 increases, this empirical definition of formula_41 converges to the definition when formula_19 is known.\n\nUsing the weights formula_43 and formula_44 for the ambient and intrinsic regularizers, the final expression to be solved becomes:\n\nAs with other kernel methods, formula_2 may be an infinite-dimensional space, so if the regularization expression cannot be solved explicitly, it is impossible to search the entire space for a solution. Instead, a representer theorem shows that under certain conditions on the choice of the norm formula_13, the optimal solution formula_48 must be a linear combination of the kernel centered at each of the input points: for some weights formula_49,\n\nUsing this result, it is possible to search for the optimal solution formula_48 by searching the finite-dimensional space defined by the possible choices of formula_49.\n\nManifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function formula_8 and hypothesis space formula_2. Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.\n\nRegularized least squares (RLS) is a family of regression algorithms: algorithms that predict a value formula_55 for its inputs formula_17, with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the kernel method. The problem statement for RLS results from choosing the loss function formula_8 in Tikhonov regularization to be the mean squared error:\n\nThanks to the representer theorem, the solution can be written as a weighted sum of the kernel evaluated at the data points:\n\nand solving for formula_60 gives:\n\nwhere formula_3 is defined to be the kernel matrix, with formula_63, and formula_64 is the vector of data labels.\n\nAdding a Laplacian term for manifold regularization gives the Laplacian RLS statement:\n\nThe representer theorem for manifold regularization again gives\n\nand this yields an expression for the vector formula_60. Letting formula_3 be the kernel matrix as above, formula_64 be the vector of data labels, and formula_70 be the formula_71 block matrix formula_72:\n\nwith a solution of\n\nLapRLS has been applied to problems including sensor networks,\nmedical imaging,\nobject detection,\nspectroscopy,\ndocument classification,\ndrug-protein interactions,\nand compressing images and videos.\n\nSupport vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or \"classes\". Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, formula_75:\n\nAdding the intrinsic regularization term to this expression gives the LapSVM problem statement:\n\nAgain, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:\n\nformula_79 can be found by writing the problem as a linear program and solving the dual problem. Again letting formula_3 be the kernel matrix and formula_70 be the block matrix formula_72, the solution can be shown to be\n\nwhere formula_84 is the solution to the dual problem\n\nand formula_86 is defined by\n\nLapSVM has been applied to problems including geographical imaging,\nmedical imaging,\nface recognition,\nmachine maintenance,\nand brain-computer interfaces.\n\n- Manifold regularization assumes that data with different labels are not likely to be close together. This assumption is what allows the technique to draw information from unlabeled data, but it only applies to some problem domains. Depending on the structure of the data, it may be necessary to use a different semi-supervised or transductive learning algorithm.\n- In some datasets, the intrinsic norm of a function formula_13 can be very close to the ambient norm formula_5: for example, if the data consist of two classes that lie on perpendicular lines, the intrinsic norm will be equal to the ambient norm. In this case, unlabeled data have no effect on the solution learned by manifold regularization, even if the data fit the algorithm's assumption that the separator should be smooth. Approaches related to co-training have been proposed to address this limitation.\n- If there are a very large number of unlabeled examples, the kernel matrix formula_3 becomes very large, and a manifold regularization algorithm may become prohibitively slow to compute. Online algorithms and sparse approximations of the manifold may help in this case.\n\n- The ManifoldLearn library and the Primal LapSVM library implement LapRLS and LapSVM in MATLAB.\n- The Dlib library for C++ includes a linear manifold regularization function.\n\n", "related": "\n- Manifold learning\n- Semi-supervised learning\n- Transduction (machine learning)\n- Spectral graph theory\n- Reproducing kernel Hilbert space\n- Tikhonov regularization\n- Differential geometry\n"}
{"id": "48833041", "url": "https://en.wikipedia.org/wiki?curid=48833041", "title": "Error tolerance (PAC learning)", "text": "Error tolerance (PAC learning)\n\nIn PAC learning, error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated.\n\nIn the following, let formula_1 be our formula_2-dimensional input space. Let formula_3 be a class of functions that we wish to use in order to learn a formula_4-valued target function formula_5 defined over formula_1. Let formula_7 be the distribution of the inputs over formula_1. The goal of a learning algorithm formula_9 is to choose the best function formula_10 such that it minimizes formula_11. Let us suppose we have a function formula_12 that can measure the complexity of formula_5. Let formula_14 be an oracle that, whenever called, returns an example formula_15 and its correct label formula_16.\n\nWhen no noise corrupts the data, we can define learning in the Valiant setting:\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the Valiant setting if there exists a learning algorithm formula_9 that has access to formula_14 and a polynomial formula_21 such that for any formula_22 and formula_23 it outputs, in a number of calls to the oracle bounded by formula_24 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_27.\n\nIn the following we will define learnability of formula_5 when data have suffered some modification.\n\nIn the classification noise model a noise rate formula_29 is introduced. Then, instead of formula_30 that returns always the correct label of example formula_15, algorithm formula_32 can only call a faulty oracle formula_33 that will flip the label of formula_15 with probability formula_35. As in the Valiant case, the goal of a learning algorithm formula_9 is to choose the best function formula_10 such that it minimizes formula_11. In applications it is difficult to have access to the real value of formula_35, but we assume we have access to its upperbound formula_40. Note that if we allow the noise rate to be formula_41, then learning becomes impossible in any amount of computation time, because every label conveys no information about the target function.\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the classification noise model if there exists a learning algorithm formula_9 that has access to formula_33 and a polynomial formula_21 such that for any formula_47, formula_48 and formula_49 it outputs, in a number of calls to the oracle bounded by formula_50 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_53.\n\nStatistical Query Learning is a kind of active learning problem in which the learning algorithm formula_9 can decide if to request information about the likelihood formula_55 that a function formula_5 correctly labels example formula_15, and receives an answer accurate within a tolerance formula_58. Formally, whenever the learning algorithm formula_9 calls the oracle formula_60, it receives as feedback probability formula_61, such that formula_62.\n\nDefinition:\nWe say that formula_5 is efficiently learnable using formula_3 in the statistical query learning model if there exists a learning algorithm formula_9 that has access to formula_60 and polynomials formula_67, formula_68, and formula_69 such that for any formula_22 the following hold:\n1. formula_60 can evaluate formula_55 in time formula_73;\n2. formula_74 is bounded by formula_75\n3. formula_9 outputs a model formula_77 such that formula_78, in a number of calls to the oracle bounded by formula_79.\n\nNote that the confidence parameter formula_80 does not appear in the definition of learning. This is because the main purpose of formula_80 is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now formula_60 always guarantees to meet the approximation criterion formula_62, the failure probability is no longer needed.\n\nThe statistical query model is strictly weaker than the PAC model: any efficiently SQ-learnable class is efficiently PAC learnable in the presence of classification noise, but there exist efficient PAC-learnable problems such as parity that are not efficiently SQ-learnable.\n\nIn the malicious classification model an adversary generates errors to foil the learning algorithm. This setting describes situations of error burst, which may occur when for a limited time transmission equipment malfunctions repeatedly. Formally, algorithm formula_9 calls an oracle formula_85 that returns a correctly labeled example formula_15 drawn, as usual, from distribution formula_7 over the input space with probability formula_88, but it returns with probability formula_89 an example drawn from a distribution that is not related to formula_7. \nMoreover, this maliciously chosen example may strategically selected by an adversary who has knowledge of formula_5, formula_89, formula_7, or the current progress of the learning algorithm.\n\nDefinition:\nGiven a bound formula_94 for formula_95, we say that formula_5 is efficiently learnable using formula_3 in the malicious classification model, if there exist a learning algorithm formula_9 that has access to formula_85 and a polynomial formula_100 such that for any formula_22, formula_23 it outputs, in a number of calls to the oracle bounded by formula_103 , a function formula_10 that satisfies with probability at least formula_26 the condition formula_53.\n\nIn the nonuniform random attribute noise model the algorithm is learning a Boolean function, a malicious oracle formula_107 may flip each formula_108-th bit of example formula_109 independently with probability formula_110.\n\nThis type of error can irreparably foil the algorithm, in fact the following theorem holds:\n\nIn the nonuniform random attribute noise setting, an algorithm formula_9 can output a function formula_10 such that formula_113 only if formula_114.\n", "related": "NONE"}
{"id": "48841414", "url": "https://en.wikipedia.org/wiki?curid=48841414", "title": "Multiple instance learning", "text": "Multiple instance learning\n\nIn machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled \"bags\", each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.\n\nBabenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn't.\n\nDepending on the type and variation in training data, machine learning can be roughly categorized into three frameworks: supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags.\n\nKeeler et al., in his work in the early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn't say exactly which of its low-energy shapes are responsible for that.\nOne of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn't really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning. \nSolution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but APR was designed with Musk data in mind.\n\nProblem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework. Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction.\n\nTake image classification for example. Given an image, we want to know its target class based on its visual content. For instance, the target class might be \"beach\", where the image contains both \"sand\" and \"water\". In MIL terms, the image is described as a \"bag\" formula_1, where each formula_2 is the feature vector (called \"instance\") extracted from the corresponding formula_3-th region in the image and formula_4 is the total regions (instances) partitioning the image. The bag is labeled \"positive\" (\"beach\") if it contains both \"sand\" region instances and \"water\" region instances.\n\nExamples of where MIL is applied are:\n- Molecule activity\n- Predicting binding sites of Calmodulin binding proteins\n- Predicting function for alternatively spliced isoforms ,\n- Image classification\n- Text or document categorization\n- Predicting functional binding sites of MicroRNA targets\n- Medical image classification ,\nNumerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning.\n\nIf the space of instances is formula_5, then the set of bags is the set of functions formula_6, which is isomorphic to the set of multi-subsets of formula_5. For each bag formula_8 and each instance formula_9, formula_10 is viewed as the number of times formula_11 occurs in formula_12. Let formula_13 be the space of labels, then a \"multiple instance concept\" is a map formula_14. The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where formula_15.\n\nMost of the work on multiple instance learning, including Dietterich et al. (1997) and Maron & Lozano-P´erez (1997) early papers, make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption.\n\nThe standard assumption takes each instance formula_16 to have an associated label formula_17 which is hidden to the learner. The pair formula_18 is called an \"instance-level concept\". A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let formula_19 be a bag. The label of formula_12 is then formula_21. Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one.\n\nStandard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions. Reason for this is the belief that standard MI assumption is appropriate for the Musk dataset, but since MIL can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, standard formula_22 presence-based formula_22 threshold-based formula_22 count-based, with the count-based assumption being the most general and the standard assumption being the least general. One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions.\n\nThe presence-based assumption is a generalization of the standard assumption, wherein a bag must contain one or more instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let formula_25 be the set of required instance-level concepts, and let formula_26 denote the number of times the instance-level concept formula_27 occurs in the bag formula_12. Then formula_29 for all formula_30. Note that, by taking formula_31 to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption.\n\nA further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept formula_30 is associated a threshold formula_33. For a bag formula_12, formula_35 for all formula_30.\n\nThe count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept formula_30 has a lower threshold formula_33 and upper threshold formula_39 with formula_40. A bag formula_12 is labeled according to formula_42 for all formula_30.\n\nScott, Zhang, and Brown (2005) describe another generalization of the standard model, which they call \"generalized multiple instance learning\" (GMIL). The GMIL assumption specifies a set of required instances formula_44. A bag formula_45 is labeled positive if it contains instances which are sufficiently close to at least formula_46 of the required instances formula_47. Under only this condition, the GMIL assumption is equivalent to the presence-based assumption. However, Scott et al. describe a further generalization in which there is a set of attraction points formula_44 and a set of repulsion points formula_49. A bag is labeled positive if and only if it contains instances which are sufficiently close to at least formula_46 of the attraction points and are sufficiently close to at most formula_51 of the repulsion points. This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy.\n\nIn contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag formula_12 as a distribution formula_53 over instances formula_5, and similarly view labels as a distribution formula_55 over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution formula_56.\n\nSince formula_53 is typically considered fixed but unknown, algorithms instead focus on computing the empirical version: formula_58, where formula_59 is the number of instances in bag formula_12. Since formula_55 is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version.\n\nWhile the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that formula_62, where formula_63 is a weight function over instances and formula_64.\n\n There are two major flavors of algorithms for Multiple Instance Learning: instance-based and metadata-based, or embedding-based algorithms. The term \"instance-based\" denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept. For a survey of some of the modern MI algorithms see Foulds and Frank \nThe earliest proposed MI algorithms were a set of \"iterated-discrimination\" algorithms developed by Dietterich et al., and Diverse Density developed by Maron and Lozano-Pérez. Both of these algorithms operated under the standard assumption.\n\nBroadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an axis parallel rectangle (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively: starting from a random instance formula_65 in a positive bag, the APR is expanded to the smallest APR covering any instance formula_66 in a new positive bag formula_67. This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance formula_68 contained in the APR is given a \"relevance\", corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives.\n\nAfter the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows: a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability. Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions.\n\nIn its simplest form, Diverse Density (DD) assumes a single representative instance formula_69 as the concept. This representative instance must be \"dense\" in that it is much closer to instances from positive bags than from negative bags, as well as \"diverse\" in that it is close to at least one instance from each positive bag.\n\nLet formula_70 be the set of positively labeled bags and let formula_71 be the set of negatively labeled bags, then the best candidate for the representative instance is given by formula_72, where the diverse density formula_73 under the assumption that bags are independently distributed given the concept formula_69. Letting formula_75 denote the jth instance of bag i, the noisy-or model gives:\nformula_78 is taken to be the scaled distance formula_79 where formula_80 is the scaling vector. This way, if every positive bag has an instance close to formula_81, then formula_82 will be high for each formula_3, but if any negative bag formula_84 has an instance close to formula_81, formula_86 will be low. Hence, formula_87 is high only if every positive bag has an instance close to formula_81 and no negative bags have an instance close to formula_81. The candidate concept formula_90 can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to formula_90. Though Diverse Density was originally proposed by Maron et al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 and DD-SVM in 2004, and MILES in 2006 \n\nA number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including\n- Support vector machines\n- Artificial neural networks\n- Decision trees\n- Boosting\n\nPost 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above.\n\n- Weidmann proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept\n- Scott et al. proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles formula_92 in the original space of instances, and defines a new feature space of Boolean vectors. A bag formula_12 is mapped to a vector formula_94 in this new feature space, where formula_95 if APR formula_96 covers formula_12, and formula_98 otherwise. A single-instance algorithm can then be applied to learn the concept in this new feature space.\n\nBecause of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements.\n\n- Xu (2003) proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption.\n\nBy mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based.\n\n- One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity.\n- Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags.\n- A modification of k-nearest neighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) suggest the (maximum and minimum, respectively) Hausdorff metrics for bags formula_99 and formula_12:\nThey define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting.\n\nSo far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case.\n\n- One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if formula_5 is the space of features and formula_13 is the space of labels, an MIML concept is a map formula_105. Zhou and Zhang (2006) propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem.\n- Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the \"prime instance\", which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem.\n\n", "related": "\n- Supervised learning\n- Multi-label classification\n\nRecent reviews of the MIL literature include:\n- , which provides an extensive review and comparative study of the different paradigms,\n- , which provides a thorough review of the different assumptions used by different paradigms in the literature.\n"}
{"id": "48827727", "url": "https://en.wikipedia.org/wiki?curid=48827727", "title": "Learnable function class", "text": "Learnable function class\n\nIn statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.\n\nLet formula_1 be the sample space, where formula_2 are the labels and formula_3 are the covariates (predictors). formula_4 is a collection of mappings (functions) under consideration to link formula_3 to formula_2. formula_7 is a pre-given loss function (usually non-negative). Given a probability distribution formula_8 on formula_9, define the expected risk formula_10 to be:\nThe general goal in statistical learning is to find the function in formula_12 that minimizes the expected risk. That is, to find solutions to the following problem:\nBut in practice the distribution formula_14 is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions formula_15 that satisfies\nOne usual algorithm to find such a sequence is through empirical risk minimization.\n\nWe can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is:\n\nThe intuition behind the more strict requirement is as such: the rate at which sequence formula_17 converges to the minimizer of the expected risk can be very different for different formula_8. Because in real world the true distribution formula_14 is always unknown, we would want to select a sequence that performs well under all cases.\n\nHowever, by the no free lunch theorem, such a sequence that satisfies () does not exist if formula_12 is too complex. This means we need to be careful and not allow too \"many\" functions in formula_12 if we want () to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence formula_17 that satisfies () are known as learnable classes.\n\nIt is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies (). Thus in these settings not only do we know that the problem posed by () is solvable, we also immediately have an algorithm that gives the solution.\n\nIf the true relationship between formula_2 and formula_3 is formula_25, then by selecting the appropriate loss function, formula_26 can always be expressed as the minimizer of the expected loss across all possible functions. That is,\n\nHere we let formula_28 be the collection of all possible functions mapping formula_29 onto formula_30. formula_26 can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over formula_28. Thus we often consider a subset of formula_28, formula_12, to carry out searches on. By doing so, we risk that formula_26 might not be an element of formula_12. This tradeoff can be mathematically expressed as\n\nIn the above decomposition, part formula_37 does not depend on the data and is non-stochastic. It describes how far away our assumptions (formula_12) are from the truth (formula_28). formula_37 will be strictly greater than 0 if we make assumptions that are too strong (formula_12 too small). On the other hand, failing to put enough restrictions on formula_12 will cause it to be not learnable, and part formula_43 will not stochastically converge to 0. This is the well-known overfitting problem in statistics and machine learning literature.\n\nA good example where learnable classes are used is the so-called Tikhonov regularization in reproducing kernel Hilbert space (RKHS). Specifically, let formula_44 be an RKHS, and formula_45 be the norm on formula_44 given by its inner product. It is shown in that formula_47 is a learnable class for any finite, positive formula_48. The empirical minimization algorithm to the dual form of this problem is\n\nThis was first introduced by Tikhonov to solve ill-posed problems. Many statistical learning algorithms can be expressed in such a form (for example, the well-known ridge regression).\n\nThe tradeoff between formula_43 and formula_37 in () is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of formula_52, which are essentially balls in formula_44 with centers at 0. As formula_48 gets larger, formula_55 gets closer to the entire space, and formula_37 is likely to become smaller. However we will also suffer smaller convergence rates in formula_43. The way to choose an optimal formula_48 in finite sample settings is usually through cross-validation.\n\nPart formula_43 in () is closely linked to empirical process theory in statistics, where the empirical risk formula_60 are known as empirical processes. In this field, the function class formula_12 that satisfies the stochastic convergence\n\nare known as uniform Glivenko–Cantelli classes. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent. Interplay between formula_43 and formula_37 in statistics literature is often known as the bias-variance tradeoff.\n\nHowever, note that in the authors gave an example of stochastic convex optimization for General Setting of Learning where learnability is not equivalent with uniform convergence.\n", "related": "NONE"}
{"id": "48987892", "url": "https://en.wikipedia.org/wiki?curid=48987892", "title": "Isotropic position", "text": "Isotropic position\n\nIn the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix. \n\nLet formula_1 be a distribution over vectors in the vector space formula_2.\nThen formula_1 is in isotropic position if, for vector formula_4 sampled from the distribution,\n\nA \"set\" of vectors is said to be in isotropic position if the uniform distribution over that set is in isotropic position. In particular, every orthonormal set of vectors is isotropic.\n\nAs a related definition, a convex body formula_6 in formula_2 is called isotropic if it has volume formula_8, center of mass at the origin, and there is a constant formula_9 such that\n\nfor all vectors formula_11 in formula_2; here formula_13 stands \nfor the standard Euclidean norm.\n\n", "related": "\n- Whitening transformation\n"}
{"id": "48844125", "url": "https://en.wikipedia.org/wiki?curid=48844125", "title": "Structured sparsity regularization", "text": "Structured sparsity regularization\n\nStructured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods. Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable formula_1 (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space formula_2 (i.e., the domain, space of features or explanatory variables). \"Sparsity regularization methods\" focus on selecting the input variables that best describe the output. \"Structured sparsity regularization methods\" generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in formula_2.\n\nCommon motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of formula_2 may be higher than the number of observations formula_5), and reduction of computational complexity. Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups, non-overlapping groups, and acyclic graphs. Examples of uses of structured sparsity methods include face recognition, magnetic resonance image (MRI) processing, socio-linguistic analysis in natural language processing, and analysis of genetic expression in breast cancer.\n\nConsider the linear kernel regularized empirical risk minimization problem with a loss function formula_6 and the formula_7 \"norm\" as the regularization penalty:\nwhere formula_9, and formula_10 denotes the formula_7 \"norm\", defined as the number of nonzero entries of the vector formula_12. formula_13 is said to be sparse if formula_14. Which means that the output formula_15 can be described by a small subset of input variables.\n\nMore generally, assume a dictionary formula_16 with formula_17 is given, such that the target function formula_18 of a learning problem can be written as:\nThe formula_7 norm formula_22 as the number of non-zero components of formula_12 is defined as \nformula_27 is said to be sparse if formula_28.\n\nHowever, while using the formula_7 norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the formula_30 norm; this has been shown to still favor sparser solutions and is additionally convex.\n\nStructured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization. Consider the above regularized empirical risk minimization problem with a general kernel and associated feature map formula_16 with formula_17.\nThe regularization term formula_34 penalizes each formula_35 component independently, which means that the algorithm will suppress input variables independently from each other.\n\nIn several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. Structured sparsity regularization methods allow to impose such structure by adding structure to the norms defining the regularization term.\n\nThe non-overlapping group case is the most basic instance of structured sparsity. In it, an \"a priori\" partition of the coefficient vector formula_12 in formula_37 non-overlapping groups is assumed. Let formula_38 be the vector of coefficients in group formula_39, we can define a regularization term and its group norm as\nwhere formula_41 is the group formula_42 norm formula_43 , formula_44 is group formula_39, and formula_46 is the \"j-th\" component of group formula_44.\n\nThe above norm is also referred to as group Lasso. This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.\n\nOverlapping groups is the structure sparsity case where a variable can belong to more than one group formula_39. This case is often of interest as it can represent a more general class of relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.\n\nThere are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:\n\nThe \"intersection of complements\" approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to. Consider again the group Lasso for a regularized empirical risk minimization problem:\nwhere formula_41 is the group formula_42 norm, formula_44 is group formula_39, and formula_46 is the \"j-th\" component of group formula_44.\n\nAs in the non-overlapping groups case, the \"group Lasso\" regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients formula_56. However, as in this case groups may overlap, we take the intersection of the complements of those groups that are not set to zero.\n\nThis \"intersection of complements\" selection criteria implies the modeling choice that we allow some coefficients within a particular group formula_39 to be set to zero, while others within the same group formula_39 may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.\n\nA different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.\n\nThe formulation of the union of groups approach is also referred to as latent group Lasso, and requires to modify the group formula_42 norm considered above and introduce the following regularizer \nwhere formula_61, formula_62 is the vector of coefficients of group g, and formula_63 is a vector with coefficients formula_46 for all variables formula_65 in group formula_39 , and formula_67 in all others, i.e., formula_68 if formula_65 in group formula_39 and formula_71 otherwise.\n\nThis regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring formula_72 produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.\n\nThe objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group formula_30 regularization term. An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.\n\nAn example of a way to fix this is to introduce the squared formula_42 norm of the weight vector as an additional regularization term while keeping the formula_30 regularization term from the group lasso approach. If the coefficient of the squared formula_42 norm term is greater than formula_67, then because the squared formula_42 norm term is strongly convex, the resulting objective function will also be strongly convex. Provided that the formula_42 coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group formula_42 regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach. Thus this approach allows for simpler optimization while maintaining sparsity.\n\n\"See: Submodular set function\"\n\nBesides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a directed acyclic graph over the variables while in the context of grid-based norms, the structure can be represented using a grid.\n\n\"See:\" Unsupervised learning\n\nUnsupervised learning methods are often used to learn the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, \"hierarchies\" are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.\n\nHierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents. Hierarchical models using Bayesian non-parametric methods have been used to learn topic models, which are statistical models for discovering the abstract \"topics\" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods. Hierarchical norms have been applied to bioinformatics, computer vision and topic models.\n\nIf the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes. Such methods have applications in computer vision\n\nThe problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:\nWhere formula_10 denotes the formula_7 \"norm\", defined as the number of nonzero entries of the vector formula_12.\n\nAlthough this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.\n\nTwo main approaches for solving the optimization problem are: 1) greedy methods, such as step-wise regression in statistics, or matching pursuit in signal processing; and 2) convex relaxation formulation approaches and proximal gradient optimization methods.\n\nA natural approximation for the best subset selection problem is the formula_30 norm regularization:\nSuch as scheme is called basis pursuit or the Lasso, which substitutes the formula_7 \"norm\" for the convex, non-differentiable formula_30 norm.\n\nProximal gradient methods, also called forward-backward splitting, are optimization methods useful for minimizing functions with a convex and differentiable component, and a convex potentially non-differentiable component.\n\nAs such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems of the following form: \nWhere formula_90 is a convex and differentiable loss function like the quadratic loss, and formula_91 is a convex potentially non-differentiable regularizer such as the formula_30 norm.\n\nStructured Sparsity regularization can be applied in the context of multiple kernel learning. Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.\n\nIn the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown. Assume for this example that rather than only one dictionary, several finite dictionaries are considered.\n\nFor simplicity, the case in which there are only two dictionaries formula_93 and formula_94 where formula_95 and formula_96 are integers, will be considered. The atoms in formula_26 as well as the atoms in formula_98 are assumed to be linearly independent. Let formula_99 be the union of the two dictionaries. Consider the linear space of functions formula_100 given by linear combinations of the form\n\nformula_101\n\nfor some coefficient vectors formula_102, where formula_103. Assume the atoms in formula_104 to still be linearly independent, or equivalently, that the map formula_105 is one to one. The functions in the space formula_100 can be seen as the sums of two components, one in the space formula_107, the linear combinations of atoms in formula_26 and one in formula_109, the linear combinations of the atoms in formula_98.\n\nOne choice of norm on this space is formula_111. Note that we can now view formula_100 as a function space in which formula_107, formula_109 are subspaces. In view of the linear independence assumption, formula_100 can be identified with formula_116 and formula_117 with formula_118 respectively. The norm mentioned above can be seen as the group norm in formula_100associated to the subspaces formula_107, formula_109, providing a connection to structured sparsity regularization.\n\nHere, formula_107, formula_109 and formula_100 can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps formula_125, given by formula_126, formula_127, given by formula_128, and formula_129, given by the concatenation of formula_130, respectively.\n\nIn the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces formula_107 and formula_109. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.\n\nThe above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis\n\nspaces.\n\nConsidering sparse multiple kernel learning is useful in several situations including the following:\n- Data fusion: When each kernel corresponds to a different kind of modality/feature.\n- Nonlinear variable selection: Consider kernels formula_133 depending only one dimension of the input.\n\nGenerally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.\n\nStructured sparsity regularization methods have been used in a number of settings where it is desired to impose an \"a priori\" input variable structure to the regularization process. Some such applications are:\n- Compressive sensing in magnetic resonance imaging (MRI), reconstructing MR images from a small number of measurements, potentially yielding significant reductions in MR scanning time\n- Robust face recognition in the presence of misalignment, occlusion and illumination variation\n- Uncovering socio-linguistic associations between lexical frequencies used by Twitter authors, and the socio-demographic variables of their geographic communities\n- Gene selection analysis of breast cancer data using priors of overlapping groups, e.g., biologically meaningful gene sets\n\n", "related": "\n- Statistical learning theory\n- Regularization\n- Sparse approximation\n- Proximal gradient methods\n- Convex analysis\n- Feature selection\n"}
{"id": "48813654", "url": "https://en.wikipedia.org/wiki?curid=48813654", "title": "Sparse dictionary learning", "text": "Sparse dictionary learning\n\nSparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as \"sparse coding\") in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called \"atoms\" and they compose a \"dictionary\". Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation. \n\nOne of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP or fast non-iterative algorithms can be used to recover the signal. \n\nOne of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting. \nGiven the input dataset formula_1 we wish to find a dictionary formula_2 and a representation formula_3 such that both formula_4 is minimized and the representations formula_5 are sparse enough. This can be formulated as the following optimization problem:\n\nformula_6, where formula_7, formula_8\n\nformula_9 is required to constrain formula_10 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of formula_5.formula_12 controls the trade off between the sparsity and the minimization error.\n\nThe minimization problem above is not convex because of the ℓ-\"norm\" and solving this problem is NP-hard. In some cases \"L\"-norm is known to ensure sparsity and so the above becomes a convex optimization problem with respect to each of the variables formula_10 and formula_14 when the other one is fixed, but it is not jointly convex in formula_15.\nThe dictionary formula_10 defined above can be \"undercomplete\" if formula_17 or \"overcomplete\" in case formula_18 with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.\n\nUndercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms formula_19 to be orthogonal. The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial. And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.\n\nOvercomplete dictionaries, however, do not require the atoms to be orthogonal (they will never be a basis anyway) thus allowing for more flexible dictionaries and richer data representations. \n\nAn overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents the given signal in a best way. Learned dictionaries are capable of giving sparser solutions as compared to predefined transform matrices.\n\nAs the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.\n\nThe problem of finding an optimal sparse coding formula_20 with a given dictionary formula_10 is known as sparse approximation (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as matching pursuit and LASSO) which are incorporated into the algorithms described below.\n\nThe method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem. The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:\n\nformula_22\n\nHere, formula_23 denotes the Frobenius norm. MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by formula_24 where formula_25 is a Moore-Penrose pseudoinverse. After this update formula_26 is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).\n\nMOD has proved to be a very efficient method for low-dimensional input data formula_27 requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.\n\nK-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means. It enforces that each element of the input data formula_28 is encoded by a linear combination of not more than formula_29 elements in a way identical to the MOD approach:\n\nformula_30\n\nThis algorithm's essence is to first fix the dictionary, find the best possible formula_31 under the above constraint (using Orthogonal Matching Pursuit) and then iteratively update the atoms of dictionary formula_10 in the following manner:\n\nformula_33\n\nThe next steps of the algorithm include rank-1 approximation of the residual matrix formula_34, updating formula_35 and enforcing the sparsity of formula_36 after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.\n\nOne can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem. The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set formula_9. The step that occurs at i-th iteration is described by this expression:\n\nformula_38, where formula_39 is a random subset of formula_40 and formula_41 is a gradient step.\n\nAn algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function. Consider the following Lagrangian:\n\nformula_42, where formula_43 is a constraint on the norm of the atoms and formula_44 are the so-called dual variables forming the diagonal matrix formula_45.\n\nWe can then provide an analytical expression for the Lagrange dual after minimization over formula_10:\n\nformula_47.\n\nAfter applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient) we get the value of formula_10:\n\nformula_49\n\nSolving this problem is less computational hard because the amount of dual variables formula_50 is a lot of times much less than the amount of variables in the primal problem.\n\nIn this approach, the optimization problem is formulated as:\n\nformula_51, where formula_52is the permitted error in the reconstruction LASSO.\n\nIt finds an estimate of formula_53by minimizing the least square error subject to a \"L\"-norm constraint in the solution vector, formulated as:\n\nformula_54, where formula_55controls the trade-off between sparsity and the reconstruction error. This gives the global optimal solution. See also Online dictionary learning for Sparse coding\n\nParametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones. This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include: \n- Translation-invariant dictionaries. These dictionaries are composed by the translations of the atoms originating from the dictionary constructed for a finite-size signal patch. This allows the resulting dictionary to provide a representation for the arbitrary-sized signal.\n- Multiscale dictionaries. This method focuses on constructing a dictionary that is composed of differently scaled dictionaries to improve sparsity.\n- Sparse dictionaries. This method focuses on not only providing a sparse representation but also constructing a sparse dictionary which is enforced by the expression formula_56 where formula_57 is some pre-defined analytical dictionary with desirable properties such as fast computation and formula_58 is a sparse matrix. Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches.\n\nMany common approaches to sparse dictionary learning rely on the fact that the whole input data formula_59 (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points formula_60 becoming available.\n\nA dictionary can be learned in an online manner the following way:\n1. For formula_61\n2. Draw a new sample formula_62\n3. Find a sparse coding using LARS: formula_63\n4. Update dictionary using block-coordinate approach: formula_64\n\nThis method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).\n\nThe dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.\n\nIt also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.\n\nSparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis and unsupervised clustering. In evaluations with the Bag-of-Words model, sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.\n\nDictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), functional MRI (fMRI), and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.\n\n", "related": "\n- Sparse approximation\n- Sparse PCA\n- Matrix factorization\n- K-SVD\n- Neural sparse coding\n"}
{"id": "49316492", "url": "https://en.wikipedia.org/wiki?curid=49316492", "title": "Darkforest", "text": "Darkforest\n\nDarkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as Darkfmcts3.\n\nDarkforest is of similar strength to programs like CrazyStone and Zen. It has been tested against a professional human player at the 2016 UEC cup. Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques.\n\nDarkforest is named after Liu Cixin's science fiction novel \"The Dark Forest\".\n\nCompeting with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep Convolutional Neural Network designed for long-term predictions, Darkforest has been able to substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.\n\nAgainst human players, Darkfores2 achieves a stable \"3d ranking\" on KGS Go Server, which roughly corresponds to an advanced amateur human player. However, after adding Monte Carlo Tree Search to Darkfores2 to create a much stronger player named darkfmcts3, it can achieve a \"5d ranking\" on the KGS Go Server.\n\ndarkfmcts3 is on par with state-of-the-art Go AIs such as Zen, DolBaram and Crazy Stone but lags behind AlphaGo. It won 3rd place in January 2016 KGS Bot Tournament against other Go AIs.\n\nAfter Google's AlphaGo won against Fan Hui in 2015, Facebook made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.\n\nDarkforest uses a neural network to sort through the 10 board positions, and find the most powerful next move. However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so Darkfores2 combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of Darkforest, with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games. This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in Darkfores2 by running the processes in parallel with frequent communication between the two.\n\nGo is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do. In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that \"felt human\".\n\nIt has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays tenuki (\"move elsewhere\") pointlessly when local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, it plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.\n\nThe family of Darkforest computer go programs is based on convolution neural networks. The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search. Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a Monte Carlo tree search.\n\nDarkfmcts3 relies on a convolution neural networks that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players liberties are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. Ko (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. formula_1, where formula_2 is a real number at a position), and each player's territory (binary, based on which player a location is closer to).\n\nDarkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a rectified linear unit, a popular activation function for deep neural networks. A key innovation of Darkfmct3 compared to previous approaches is that it uses only one softmax function to predict the next move, which enables the approach to reduce the overall number of parameters. Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla stochastic gradient descent.\n\nDarkfmct3 synchronously couples a convolutional neural network with a Monte Carlo tree search. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.\n\nDarkfores2 beats Darkforest, its neural network-only predecessor, around 90% of the time, and Pachi, one of the best search-based engines, around 95% of the time. On the Kyu rating system, Darkforest holds a 1-2d level. Darkfores2 achieves a stable 3d level on KGS Go Server as a ranked bot. With the added Monte Carlo tree search, Darkfmcts3 with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n\n", "related": "\n- Go and mathematics\n\n- Source code on Github\n"}
{"id": "5008963", "url": "https://en.wikipedia.org/wiki?curid=5008963", "title": "Inauthentic text", "text": "Inauthentic text\n\nAn inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.\n\nSometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low.\n\nWith the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase \"Colorless green ideas sleep furiously\" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.\n\nThe first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score.\n\n", "related": "\n- Scraper site\n- Spamdexing\n\n- An Inauthentic Paper Detector from Indiana University School of Informatics\n"}
{"id": "50211107", "url": "https://en.wikipedia.org/wiki?curid=50211107", "title": "Bayesian structural time series", "text": "Bayesian structural time series\n\nBayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data.\n\nThe model has also promising application in the field of analytical marketing. In particular, it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators. Difference-in-differences models and interrupted time series designs are alternatives to this approach. \"In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls.\"\n\nThe model consists of three main components:\n1. Kalman filter. The technique for time series decomposition. In this step, a researcher can add different state variables: trend, seasonality, regression, and others.\n2. Spike-and-slab method. In this step, the most important regression predictors are selected.\n3. Bayesian model averaging. Combining the results and prediction calculation.\nThe model could be used to discover the causations with its counterfactual prediction and the observed data.\n\nA possible drawback of the model can be its relatively complicated mathematical underpinning and difficult implementation as a computer program. However, the programming language R has ready-to-use packages for calculating the BSTS model, which do not require strong mathematical background from a researcher.\n\n", "related": "\n- Bayesian inference using Gibbs sampling\n- Correlation does not imply causation\n\n- Scott, S. L., & Varian, H. R. 2014a. Bayesian variable selection for nowcasting economic time series. \"Economic Analysis of the Digital Economy.\"\n- Scott, S. L., & Varian, H. R. 2014b. Predicting the present with bayesian structural time series. \"International Journal of Mathematical Modelling and Numerical Optimisation.\"\n- Varian, H. R. 2014. Big Data: New Tricks for Econometrics. \"Journal of Economic Perspectives\"\n- Brodersen, K. H., Gallusser, F., Koehler, J., Remy, N., & Scott, S. L. 2015. Inferring causal impact using Bayesian structural time-series models. \"The Annals of Applied Statistics.\"\n- R package \"bsts\".\n- R package \"CausalImpact\".\n- O’Hara, R. B., & Sillanpää, M. J. 2009. A review of Bayesian variable selection methods: what, how and which. \"Bayesian analysis.\"\n- Hoeting, J. A., Madigan, D., Raftery, A. E., & Volinsky, C. T. 1999. Bayesian model averaging: a tutorial. \"Statistical science.\"\n"}
{"id": "50222574", "url": "https://en.wikipedia.org/wiki?curid=50222574", "title": "Semantic folding", "text": "Semantic folding\n\nSemantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.\n\nSemantic folding theory draws inspiration from Douglas R. Hofstadter's \"Analogy as the Core of Cognition\" which suggests that the brain makes sense of the world by identifying and applying analogies. The theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a similarity measure and offers, as a solution, the sparse binary vector employing a two-dimensional topographic semantic space as a distributional reference frame. The theory builds on the computational theory of the human cortex known as hierarchical temporal memory (HTM), and positions itself as a complementary theory for the representation of language semantics.\n\nA particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level.\n\nAnalogous to the structure of the neocortex, Semantic Folding theory posits the implementation of a semantic space as a two-dimensional grid. This grid is populated by context-vectors in such a way as to place similar context-vectors closer to each other, for instance, by using competitive learning principles. This vector space model is presented in the theory as an equivalence to the well known word space model described in the Information Retrieval literature.\n\nGiven a semantic space (implemented as described above) a word-vector can be obtained for any given word Y by employing the following algorithm:\n\nFor each position X in the semantic map (where X represents cartesian coordinates)\n\nThe result of this process will be a word-vector containing all the contexts in which the word Y appears and will therefore be representative of the semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse distributed representation (SDR) format [Schütze, 1993] & [Sahlgreen, 2006]. Some properties of word-SDRs that are of particular interest with respect to computational semantics are:\n- high noise resistance: As a result of similar contexts being placed closer together in the underlying map, word-SDRs are highly tolerant of false or shifted \"bits\".\n- boolean logic: It is possible to manipulate word-SDRs in a meaningful way using boolean (OR, AND, exclusive-OR) and/or arithmetical (SUBtract) functions .\n- sub-sampling: Word-SDRs can be sub-sampled to a high degree without any appreciable loss of semantic information.\n- topological two-dimensional representation: The SDR representation maintains the topological distribution of the underlying map therefore words with similar meanings will have similar word-vectors. This suggests that a variety of measures can be applied to the calculation of semantic similarity, from a simple overlap of vector elements, to a range of distance measures such as: Euclidean distance, Hamming distance, Jaccard distance, cosine similarity, Levenshtein distance, Sørensen-Dice index, etc.\n\nSemantic spaces in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: Vocabulary mismatch (the fact that the same meaning can be expressed in many ways) and ambiguity of natural language (the fact that the same term can have several meanings).\n\nThe application of semantic spaces in natural language processing (NLP) aims at overcoming limitations of rule-based or model-based approaches operating on the keyword level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning. Rule-based and machine learning-based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.\n\nResearch in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: latent semantic analysis from Microsoft and Hyperspace Analogue to Language from the University of California. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the accuracy of modelling associative relations between words (e.g. \"spider-web\", \"lighter-cigarette\", as opposed to synonymous relations such as \"whale-dolphin\", \"astronaut-driver\") was achieved by explicit semantic analysis (ESA) in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 dimensions (where each dimension represents an Article in Wikipedia). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.\n\nMore recently, advances in neural networking techniques in combination with other new approaches (tensors) led to a host of new recent developments: Word2vec from Google and GloVe from Stanford University.\n\nSemantic folding represents a novel, biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with 16,000 dimensions (a semantic fingerprint) in a 2D semantic map (the semantic universe). Sparse binary representation are advantageous in terms of computational efficiency, and allow for the storage of very large numbers of possible patterns.\n\nThe topological distribution over a two-dimensional grid (outlined above) lends itself to a bitmap type visualization of the semantics of any word or text, where each active semantic feature can be displayed as e.g. a pixel. As can be seen in the images shown here, this representation allows for a direct visual comparison of the semantics of two (or more) linguistic items.\n\nImage 1 clearly demonstrates that the two disparate terms \"dog\" and \"car\" have, as expected, very obviously different semantics.\n\nImage 2 shows that only one of the meaning contexts of \"jaguar\", that of \"Jaguar\" the car, overlaps with the meaning of Porsche (indicating partial similarity). Other meaning contexts of \"jaguar\" e.g. \"jaguar\" the animal clearly have different non-overlapping contexts.\nThe visualization of semantic similarity using Semantic Folding bears a strong resemblance to the fMRI images produced in a research study conducted by A.G. Huth et al., where it is claimed that words are grouped in the brain by meaning.\n", "related": "NONE"}
{"id": "50227596", "url": "https://en.wikipedia.org/wiki?curid=50227596", "title": "Spike-and-slab regression", "text": "Spike-and-slab regression\n\nIn statistics, spike-and-slab regression is a Bayesian variable selection technique that is particularly useful when the number of possible predictors is larger than the number of observations.\n\nInitially, the idea of the spike-and-slab model was proposed by Mitchell & Beauchamp (1988). The approach was further significantly developed by Madigan & Raftery (1994) and George & McCulloch (1997). The final adjustments to the model were done by Ishwaran & Rao (2005).\n\nSuppose we have \"P\" possible predictors in some model. Vector \"γ\" has a length equal to \"P\" and consists of zeros and ones. This vector indicates whether a particular variable is included in the regression or not. If no specific prior information on initial inclusion probabilities of particular variables is available, a Bernoulli prior distribution is a common default choice. Conditional on a predictor being in the regression, we identify a prior distribution for the model coefficient, which corresponds to that variable (\"β\"). A common choice on that step is to use a Normal prior with mean equal to zero and a large variance calculated based on formula_1 (where formula_2 is a design matrix of explanatory variables of the model).\n\nA draw of \"γ\" from its prior distribution is a list of the variables included in the regression. Conditional on this set of selected variables, we take a draw from the prior distribution of the regression coefficients (if \"γ\" = 1 then \"β\" ≠ 0 and if \"γ\" = 0 then \"β\" = 0). \"βγ\" denotes the subset of \"β\" for which \"γ\" = 1. In the next step, we calculate a posterior probability distribution for both inclusion and coefficients by applying a standard statistical procedure. All steps of the described algorithm are repeated thousands of times using Markov chain Monte Carlo (MCMC) technique. As a result, we obtain a posterior distribution of \"γ\" (variable inclusion in the model), \"β\" (regression coefficient values) and the corresponding prediction of \"y\".\n\nThe model got its name (spike-and-slab) due to the shape of the two prior distributions. The \"spike\" is the probability of a particular coefficient in the model to be zero. The \"slab\" is the prior distribution for the regression coefficient values.\n\nAn advantage of Bayesian variable selection techniques is that they are able to make use of prior knowledge about the model. In the absence of such knowledge, some reasonable default values can be used; to quote Scott and Varian (2013): \"For the analyst who prefers simplicity at the cost of some reasonable assumptions, useful prior information can be reduced to an expected model size, an expected \"R\", and a sample size \"ν\" determining the weight given to the guess at \"R\".\" Some researchers suggest the following default values: \"R\" = 0.5, \"ν\" = 0.01, and = 0.5 (parameter of a prior Bernoulli distribution).\n\nA possible drawback of the Spike-and-Slab model can be its mathematical complexity (in comparison to linear regression). A deep understanding of this model requires sound knowledge in stochastic processes. On the other hand, some modern statistical software (e.g. R) have ready-to-use solutions for calculating various Bayesian variable selection models. In this case, it would be enough for a researcher to know the idea of the method, required model parameters and input variables. The analysis of the model outcomes (distribution of \"γ\", \"β\", and corresponding predictions of \"y\") can be more challenging in comparison to linear regression case. The spike-and-slab model produces inclusion probabilities for each of possible predictors. This can cause difficulties when comparing results to the studies with simple regression (usually only regression coefficients with corresponding statistics are available).\n\n", "related": "\n- Bayesian inference using Gibbs sampling\n- Bayesian structural time series\n"}
{"id": "50336055", "url": "https://en.wikipedia.org/wiki?curid=50336055", "title": "Glossary of artificial intelligence", "text": "Glossary of artificial intelligence\n\nThis glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence, its sub-disciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision.\n\n \n\n \n\n", "related": "\n- Artificial intelligence\n"}
{"id": "30909817", "url": "https://en.wikipedia.org/wiki?curid=30909817", "title": "Multilinear subspace learning", "text": "Multilinear subspace learning\n\nMultilinear subspace learning is an approach to dimensionality reduction. \nDimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor. Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).\n\nThe mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as the sensor provides them; as matrices or higher order tensors, their representations are computed by performing N multiple linear projections.\n\nMultilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).\n\nWith the advances in data acquisition and storage technology, big data (or massive data sets) are being generated on a daily basis in a wide range of emerging applications. Most of these big data are multidimensional. Moreover, they are usually very-high-dimensional, with a large amount of redundancy, and only occupying a part of the input space. Therefore, dimensionality reduction is frequently employed to map high-dimensional data to a low-dimensional space while retaining as much information as possible.\n\nLinear subspace learning algorithms are traditional dimensionality reduction techniques that represent input data as vectors and solve for an optimal linear mapping to a lower-dimensional space. Unfortunately, they often become inadequate when dealing with massive multidimensional data. They result in very-high-dimensional vectors, lead to the estimation of a large number of parameters.\n\nMultilinear Subspace Learning employ different types of data tensor analysis tools for dimensionality reduction. Multilinear Subspace learning can be applied to observations whose measurements were vectorized and organized into a data tensor, or whose measurements are treated as a matrix and concatenated into a tensor.\n\nHistorically, multilinear principal component analysis has been referred to as \"M-mode PCA\", a terminology which was coined by Peter Kroonenberg. In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA terminology as a way to better differentiate between multilinear tensor decompositions that computed 2nd order statistics associated with each data tensor mode(axis)s, and subsequent work on Multilinear Independent Component Analysis that computed higher order statistics associated with each tensor mode/axis. MPCA is an extension of PCA.\n\nMultilinear independent component analysis is an extension of ICA.\n\n- Multilinear extension of LDA\n- TTP-based: Discriminant Analysis with Tensor Representation (DATER)\n- TTP-based: General tensor discriminant analysis (GTDA)\n- TVP-based: Uncorrelated Multilinear Discriminant Analysis (UMLDA)\n\n- Multilinear extension of CCA\n- TTP-based: Tensor Canonical Correlation Analysis (TCCA)\n- TVP-based: Multilinear Canonical Correlation Analysis (MCCA)\n- TVP-based: Bayesian Multilinear Canonical Correlation Analysis (BMTF)\n\n- A TTP is a direct projection of a high-dimensional tensor to a low-dimensional tensor of the same order, using \"N\" projection matrices for an \"N\"th-order tensor. It can be performed in \"N\" steps with each step performing a tensor-matrix multiplication (product). The \"N\" steps are exchangeable. This projection is an extension of the higher-order singular value decomposition (HOSVD) to subspace learning. Hence, its origin is traced back to the Tucker decomposition in 1960s.\n\n- A TVP is a direct projection of a high-dimensional tensor to a low-dimensional vector, which is also referred to as the rank-one projections. As TVP projects a tensor to a vector, it can be viewed as multiple projections from a tensor to a scalar. Thus, the TVP of a tensor to a \"P\"-dimensional vector consists of \"P\" projections from the tensor to a scalar. The projection from a tensor to a scalar is an elementary multilinear projection (EMP). In EMP, a tensor is projected to a point through \"N\" unit projection vectors. It is the projection of a tensor on a single line (resulting a scalar), with one projection vector in each mode. Thus, the TVP of a tensor object to a vector in a \"P\"-dimensional vector space consists of \"P\" EMPs. This projection is an extension of the canonical decomposition, also known as the parallel factors (PARAFAC) decomposition.\n\nThere are \"N\" sets of parameters to be solved, one in each mode. The solution to one set often depends on the other sets (except when \"N=1\", the linear case). Therefore, the suboptimal iterative procedure in is followed.\n\n1. Initialization of the projections in each mode\n2. For each mode, fixing the projection in all the other mode, and solve for the projection in the current mode.\n3. Do the mode-wise optimization for a few iterations or until convergence.\n\nThis is originated from the alternating least square method for multi-way data analysis.\n\nThe advantages of MSL over traditional linear subspace modeling, in common domains where the representation is naturally somewhat tensorial, are:\n\n- MSL preserves the structure and correlation that the original data had before projection, by operating on a natural tensorial representation of the multidimensional data.\n- MSL can learn more compact representations than its linear counterpart; in other words, it needs to estimate a much smaller number of parameters. Thus, MSL can handle big tensor data more efficiently, by performing computations on a representation with many fewer dimensions. This leads to lower demand on computational resources.\n\nHowever, MSL algorithms are iterative and are not guaranteed to converge; where an MSL algorithm does converge, it may do so at a local optimum. (In contrast, traditional linear subspace modeling techniques often produce an exact closed-form solution.) MSL convergence problems can often be mitigated by choosing an appropriate subspace dimensionality, and by appropriate strategies for initialization, for termination, and for choosing the order in which projections are solved.\n\n- Survey: A survey of multilinear subspace learning for tensor data (open access version).\n- Lecture: Video lecture on UMPCA at the 25th International Conference on Machine Learning (ICML 2008).\n\n- MATLAB Tensor Toolbox by Sandia National Laboratories.\n- The MPCA algorithm written in Matlab (MPCA+LDA included).\n- The UMPCA algorithm written in Matlab (data included).\n- The UMLDA algorithm written in Matlab (data included).\n\n- 3D gait data (third-order tensors): 128x88x20(21.2M); 64x44x20(9.9M); 32x22x10(3.2M);\n\n", "related": "\n- CP decomposition\n- Dimension reduction\n- Multilinear algebra\n- Multilinear Principal Component Analysis\n- Tensor\n- Tensor decomposition\n- Tensor software\n- Tucker decomposition\n"}
{"id": "49786340", "url": "https://en.wikipedia.org/wiki?curid=49786340", "title": "Movidius", "text": "Movidius\n\nMovidius is a company based in San Mateo, California that designs specialised low-power processor chips for computer vision. The company was acquired by Intel in September 2016.\n\nMovidius was co-founded in Dublin in 2005 by Sean Mitchell and David Moloney. Between 2006 and 2016, it raised nearly $90 million in capital funding. In May, 2013 the company appointed Remi El-Ouazzane as CEO. In January, 2016 the company announced a partnership with Google. Movidius has been active in the Google Project Tango project. Movidius announced a planned acquisition by Intel in September 2016.\n\nThe company's Myriad 2 chip is an always-on manycore vision processing unit that can function on power-constrained devices. The \"Fathom\" is a USB stick containing a Myriad 2 processor, allowing a vision accelerator to be added to devices using ARM processors including PCs, drones, robots, IoT devices and video surveillance for tasks such as identifying people or objects. It can run at between 80 and 150 GFLOPS on little more than 1W of power.\n\nIntel's Myriad X VPU (vision processing unit) is the third generation and most advanced VPU from Movidius, an Intel company. Intel's Myriad X VPU is the first of its class to feature the Neural Compute Engine—a dedicated hardware accelerator for deep neural network deep-learning inferences. The Neural Compute Engine in conjunction with the 16 SHAVE cores and an ultra-high throughput intelligent memory fabric makes Myriad X a strong option for on-device deep neural networks and computer vision applications. Intel's Myriad X VPU has received additional upgrades to imaging and vision engines including additional programmable SHAVE cores, upgraded and expanded vision accelerators, and a new native 4K image processor pipeline with support for up to 8 HD sensors connecting directly to the VPU. As with Myriad 2, the Myriad X VPU is programmable via the Myriad Development Kit (MDK) which includes all necessary development tools, frameworks and APIs to implement custom vision, imaging and deep neural network workloads on the chip.\n\nThe Intel Movidius Neural Compute Stick (NCS) is a tiny fanless deep-learning device that can be used to learn AI programming at the edge. NCS is powered by the same low-power, high-performance Intel Movidius Vision Processing Unit that can be found in millions of smart security cameras, gesture-controlled drones, industrial machine vision equipment, and more. Supported frameworks are TensorFlow and Caffe. \n\nOn 14 November 2018, the company announced the latest version of NCS, marketed as \"Neural Compute Stick 2\" at the AI DevCon event in Beijing.\n\nGoogle Clips camera uses Myriad 2 VPU.\nThe Intel RealSense Tracking Camera T265 is another product that uses the Myriad 2.\n\n", "related": "\n- Vision processing unit\n- MPSoC\n- Coprocessor\n- Convolutional neural network\n"}
{"id": "50828755", "url": "https://en.wikipedia.org/wiki?curid=50828755", "title": "Timeline of machine learning", "text": "Timeline of machine learning\n\nThis page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.\n\n", "related": "\n- History of artificial intelligence\n- Machine learning\n- Timeline of artificial intelligence\n- Timeline of machine translation\n"}
{"id": "995455", "url": "https://en.wikipedia.org/wiki?curid=995455", "title": "Savi Technology", "text": "Savi Technology\n\nSavi Technology was founded in 1989 and is based in Alexandria, Virginia.\n\nSavi delivers live streaming facts and insights about the location, condition, and security of in-transit goods. Using big data analytics, Savi equips shippers, carriers, 3PLs, and governments to optimize supply chain logistics before, during and after transit, reducing costs and inventory while improving service. Savi is trusted to run the world’s largest and most complex asset tracking and monitoring network serving the US DoD, Allied military and more than 1000 commercial companies around the globe.\n\nSavi Technology, an innovator in big data/machine learning analytic solutions, supply-chain-management software and sensor technology, offers sensor analytics solutions for logistics and supply chain operations for shippers, governments, third-party logistics providers and technology providers. Savi solutions track shipment locations in real time and apply analytics to accurately predict the arrival of goods. The company's in-transit visibility platform, Savi Visibility™ provides 100% visibility on all multimodal in-transit shipments. The company's analytics solution, Savi Insight™ ingests, cleans, normalizes and analyzes massive types of real-time data streams, IoT data, sensor data, telematics and carrier feeds, social and weather data as well as milestone based data, such as (EDI) enterprise data information, (ERP), and historical information, applies proprietary algorithms and machine learning to transform supply chains by delivering logistics teams with actionable operational insights. The company offers a variety of hardware solutions including tags (also called sensors) that enable governments and organizations to access real-time information on the location, condition, and security status of assets and shipments; mobile IoT sensors, fixed and mobile readers; active radio-frequency identification devices and sensors; and portable deployment kits (PDKs). In addition, Savi provides professional services, including program management, systems integration, system and network design, support, and hosting. It serves the U.S. Department of Defense, the U.S. and allied militaries, civilian governmental organizations, and commercial companies, as well as transportation, third-party logistics (3PL), pharmaceuticals, retail, life sciences, and manufacturing industries worldwide.\n\n- Bloomberg\n- The Washington Post\n", "related": "NONE"}
{"id": "2934910", "url": "https://en.wikipedia.org/wiki?curid=2934910", "title": "Cognitive robotics", "text": "Cognitive robotics\n\nCognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.\n\nWhile traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics.\n\nCognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.\n\nA preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to \"expect\" a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.\n\nOnce a robot can coordinate its motors to produce a desired result, the technique of \"learning by imitation\" may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.\n\nA more complex learning approach is \"autonomous knowledge acquisition\": the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.\n\nA somewhat more directed mode of exploration can be achieved by \"curiosity\" algorithms, such as Intelligent Adaptive Curiosity or Category-Based Intrinsic Motivation. These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.\n\nSome researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships.\n\nSome of the fundamental questions to still be answered in cognitive robotics are:\n- How much human programming should or can be involved to support the learning processes?\n- How can one quantify progress? Some of the adopted ways is the reward and punishment. But what kind of reward and what kind of punishment? In humans, when teaching a child for example, the reward would be candy or some encouragement, and the punishment can take many forms. But what is an effective way with robots?\n\nCognitive Robotics book by Hooman Samani, takes a multidisciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects.\n\n", "related": "\n- Artificial intelligence\n- Intelligent agent\n- Cognitive science\n- Cybernetics\n- Developmental robotics\n- Embodied cognitive science\n- Epigenetic robotics\n- Evolutionary robotics\n- Hybrid intelligent system\n- Intelligent control\n\n- The Symbolic and Subsymbolic Robotic Intelligence Control System (SS-RICS)\n- Intelligent Systems Group - University of Utrecht\n- The Cognitive Robotics Group - University of Toronto\n- The IDSIA Robotics Lab and Cognitive Robotics Lab of Juergen Schmidhuber\n- What Does the Future Hold for Cognitive Robots? - Idaho National Laboratory\n- Cognitive Robotics at the Naval Research Laboratory\n- Cognitive robotics at ENSTA autonomous embodied systems, evolving in complex and non-constraint environments, using mainly vision as sensor.\n- The Center for Intelligent Systems - Vanderbilt University\n- Institute for Cognition and Robotics (CoR-Lab) at Bielefeld University\n- SocioCognitive Robotics at Delft University of Technology\n- Autonomous Systems Laboratory at Universidad Politecnica de Madrid\n- Knowledge Technology at Universität Hamburg\n- The Cognitive Robotics Association, founded in 1998, directed by Gerhard Lakemeyer, University of Aachen, organizes every two year the Cognitive Robotics Workshop and it is generously supported by the AI journal\n\n- iCub\n- RoboBusiness: Robots that Dream of Being Better\n- www.Conscious-Robots.com\n- The cognitive Robotics Association\n"}
{"id": "51112472", "url": "https://en.wikipedia.org/wiki?curid=51112472", "title": "Dataiku", "text": "Dataiku\n\nDataiku is a computer software company headquartered in New York City. The company develops collaborative data science software marketed for big data.\n\nThe company was founded in Paris in 2013 by 4 co-founders. Two of them met while working at French search engine company Exalead, including chief executive Florian Douetteau, and Clément Sténac.\n\nDataiku opened an office in New York City in 2015 which became the company headquarters. They opened an office in London in the summer of 2016, and announced an office in Sydney in February 2019.\n\nIn 2017, Dataiku entered the Gartner Magic Quadrant for Data Science Platforms as a \"visionary\", moved up to the \"challenger\" quadrant in 2019 and then to \"leader\" in the 2020 edition.\n\nThe software Dataiku Data Science Studio (DSS) was announced in 2014, supporting predictive modelling to build business applications. Later versions of DSS added other features.\n\nDataiku offers a free edition and enterprise versions with additional features, such as multi-user collaboration or real-time scoring.\n\nFor its first two years, the company relied on its own capital. In January 2015, Dataiku raised $3.6 million from Serena Capital and Alven Capital, two French technology venture capital funds. This was followed by $14 million raised with FirstMark Capital, a New York City-based venture capital firm in October 2016. In September 2017 the company raised a $28 million Series B investment from Battery Ventures, as well as historic investors. In December 2018 Dataiku raised $101 million for data science platform. Among investors were Iconiq Capital, Alven Capital, Battery Ventures, Dawn Capital and FirstMark Capital. In December 2019, CapitalG purchased some of the shares previously owned by Serena Capital in a secondary round that valued Dataiku at $1.4 billion.\n", "related": "NONE"}
{"id": "50773876", "url": "https://en.wikipedia.org/wiki?curid=50773876", "title": "Algorithm selection", "text": "Algorithm selection\n\nAlgorithm selection (sometimes also called per-instance algorithm selection or offline algorithm selection) is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, algorithms have different performances. That is, while one algorithm performs well on some instances, it performs poorly on others and vice versa for another algorithm. If we can identify when to use which algorithm, we can get the best of both worlds and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms.\n\nGiven a portfolio formula_1 of algorithms formula_2, a set of instances formula_3 and a cost metric formula_4, the algorithm selection problem consists of finding a mapping formula_5 from instances formula_6 to algorithms formula_1 such that the cost formula_8 across all instances is optimized.\n\nA well-known application of algorithm selection is the Boolean satisfiability problem. Here, the portfolio of algorithms is a set of (complementary) SAT solvers, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other formula_9-hard problems (such as mixed integer programming, CSP, AI planning, TSP, MAXSAT, QBF and answer set programming). Competition-winning systems in SAT are SATzilla, 3S and CSHC\n\nIn machine learning, algorithm selection is better known as meta-learning. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set.\n\nThe algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features formula_10, algorithm selection can be seen as a multi-class classification problem by learning a mapping formula_11 for a given instance formula_12.\n\nInstance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas, or number of samples, features, class balance for ML data sets to get an impression about their characteristics.\n\nWe distinguish between two kinds of features: \n1. Static features are in most cases some counts and statistics (e.g., clauses-to-variables ratio in SAT). These features ranges from very cheap features (e.g. number of variables) to very complex features (e.g., statistics about variable-clause graphs).\n2. Probing features (sometimes also called landmarking features) are computed by running some analysis of algorithm behavior on an instance (e.g., accuracy of a cheap decision tree algorithm on an ML data set, or running for a short time a stochastic local search solver on a Boolean formula). These feature often cost more than simple static features.\n\nDepending on the used performance metric formula_13, feature computation can be associated with costs.\nFor example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system.\nSAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for CNF formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds).\n\nIt is important to take the overhead of feature computation into account in practice in such scenarios; otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with prefect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted.\n\nOne of the first successful algorithm selection approaches predicted the performance of each algorithm formula_14 and selecting the algorithm with the best predicted performance formula_15 for a new instance formula_12.\n\nA common assumption is that the given set of instances formula_6 can be clustered into homogeneous subsets \nand for each of these subsets, there is one well-performing algorithm for all instances in there.\nSo, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster.\nA new instance is assigned to a cluster and the associated algorithm selected.\n\nA more modern approach is cost-sensitive hierarchical clustering using supervised learning to identify the homogeneous instance subsets.\n\nA common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms) \nand choose the class that was predicted most often by the pairwise models.\nWe can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms.\nThis is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference.\nTherefore, each instance formula_12 for training a classification model formula_19 vs formula_20 is associated with a cost formula_21.\n\nThe algorithm selection problem can be effectively applied under the following assumptions:\n- The portfolio formula_1 of algorithms is complementary with respect to the instance set formula_6, i.e., there is no single algorithm formula_2 that dominates the performance of all other algorithms on formula_6 (see figures to the right for examples on complementary analysis).\n- In some application, the computation of instance features is associated with a cost. For example, if the cost metric is running time, we have also to consider the time to compute the instance features. In such cases, the cost to compute features should not be larger than the performance gain through algorithm selection.\n\nAlgorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied.\nApplication domains include:\n\n- hard combinatorial problems: SAT, Mixed Integer Programming, CSP, AI Planning, TSP, MAXSAT, QBF and Answer Set Programming\n- combinatorial auctions\n- in machine learning, the problem is known as meta-learning\n- software design\n- black-box optimization\n- multi-agent systems\n- numerical optimization\n- linear algebra, differential equations\n- evolutionary algorithms\n- vehicle routing problem\n- power systems\n\nFor an extensive list of literature about algorithm selection, we refer to a literature overview.\n\nOnline algorithm selection in Hyper-heuristic refers to switching between different algorithms during the solving process. In contrast, (offline) algorithm selection is an one-shot game where we select an algorithm for a given instance only once.\n\nAn extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely.\n\nGiven the increasing importance of parallel computation,\nan extension of algorithm selection for parallel computation is parallel portfolio selection,\nin which we select a subset of the algorithms to simultaneously run in a parallel portfolio.\n\n- Algorithm Selection Library (ASlib)\n- Algorithm selection literature\n", "related": "NONE"}
{"id": "3920550", "url": "https://en.wikipedia.org/wiki?curid=3920550", "title": "Transfer learning", "text": "Transfer learning\n\nTransfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.\n\nAndrew Ng said in his NIPS 2016 tutorial that TL will be next driver of ML commercial success after supervised learning to highlight the importance of TL.\n\nIn 1993, Lorien Pratt published a paper on transfer in machine learning, formulating the discriminability-based transfer (DBT) algorithm.\n\nIn 1997, the journal \"Machine Learning\" published a special issue devoted to transfer learning, and by 1998, the field had advanced to include multi-task learning, along with a more formal analysis of its theoretical foundations. \"Learning to Learn\", edited by Pratt and Sebastian Thrun, is a 1998 review of the subject.\n\nTransfer learning has also been applied in cognitive science, with the journal \"Connection Science\"\npublishing a special issue on reuse of neural networks through transfer in 1996.\n\nThe definition of transfer learning is given in terms of domain and task. The domain formula_1 consists of: a feature space formula_2 and a marginal probability distribution formula_3, where formula_4. Given a specific domain, formula_5, a task consists of two components: a label space formula_6 and an objective predictive function formula_7(denoted by formula_8), which is learned from the training data consisting of pairs, which consist of pairs formula_9, where formula_10 and formula_11. The function formula_7 can be used to predict the corresponding label,formula_13, of a new instance formula_14.\n\nGiven a source domain formula_15 and learning task formula_16, a target domain formula_17and learning task formula_18, transfer learning aims to help improve the learning of the target predictive function formula_19 in formula_17 using the knowledge in formula_15 and formula_16, where formula_23, or formula_24.\n\nAlgorithms are available for transfer learning in Markov logic networks and Bayesian networks. Transfer learning has also been applied to cancer subtype\ndiscovery, building utilization, general game playing, text classification, digit recognition and spam filtering.\n\n", "related": "\n- Crossover (genetic algorithm)\n- Domain adaptation\n- General game playing\n- Multi-task learning\n- Multitask optimization\n"}
{"id": "460689", "url": "https://en.wikipedia.org/wiki?curid=460689", "title": "Evolutionary programming", "text": "Evolutionary programming\n\nEvolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\n\nIt was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite-state machines as predictors and evolved them.\nCurrently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.\n\nIts main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.\n\n", "related": "\n- Artificial intelligence\n- Genetic algorithm\n- Genetic operator\n\n- Fogel, L.J., Owens, A.J., Walsh, M.J. (1966), \"Artificial Intelligence through Simulated Evolution\", John Wiley.\n- Fogel, L.J. (1999), \"Intelligence through Simulated Evolution : Forty Years of Evolutionary Programming\", John Wiley.\n- Eiben, A.E., Smith, J.E. (2003), \"Introduction to Evolutionary Computing\", Springer.\n\n- The Hitch-Hiker's Guide to Evolutionary Computation: What's Evolutionary Programming (EP)?\n- Evolutionary Programming by Jason Brownlee (PhD)\n"}
{"id": "40254", "url": "https://en.wikipedia.org/wiki?curid=40254", "title": "Genetic algorithm", "text": "Genetic algorithm\n\nIn computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. John Holland introduced genetic algorithms in 1960 based on the concept of Darwin’s theory of evolution; his student David E. Goldberg further extended GA in 1989.\n\nIn a genetic algorithm, a population of candidate solutions (called individuals, creatures, or phenotypes) to an optimization problem is evolved toward better solutions. Each candidate solution has a set of properties (its chromosomes or genotype) which can be mutated and altered; traditionally, solutions are represented in binary as strings of 0s and 1s, but other encodings are also possible.\n\nThe evolution usually starts from a population of randomly generated individuals, and is an iterative process, with the population in each iteration called a \"generation\". In each generation, the fitness of every individual in the population is evaluated; the fitness is usually the value of the objective function in the optimization problem being solved. The more fit individuals are stochastically selected from the current population, and each individual's genome is modified (recombined and possibly randomly mutated) to form a new generation. The new generation of candidate solutions is then used in the next iteration of the algorithm. Commonly, the algorithm terminates when either a maximum number of generations has been produced, or a satisfactory fitness level has been reached for the population.\n\nA typical genetic algorithm requires:\n\n1. a genetic representation of the solution domain,\n2. a fitness function to evaluate the solution domain.\n\nA standard representation of each candidate solution is as an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size, which facilitates simple crossover operations. Variable length representations may also be used, but crossover implementation is more complex in this case. Tree-like representations are explored in genetic programming and graph-form representations are explored in evolutionary programming; a mix of both linear chromosomes and trees is explored in gene expression programming.\n\nOnce the genetic representation and the fitness function are defined, a GA proceeds to initialize a population of solutions and then to improve it through repetitive application of the mutation, crossover, inversion and selection operators.\n\nThe population size depends on the nature of the problem, but typically contains several hundreds or thousands of possible solutions. Often, the initial population is generated randomly, allowing the entire range of possible solutions (the search space). Occasionally, the solutions may be \"seeded\" in areas where optimal solutions are likely to be found.\n\nDuring each successive generation, a portion of the existing population is selected to breed a new generation. Individual solutions are selected through a \"fitness-based\" process, where fitter solutions (as measured by a fitness function) are typically more likely to be selected. Certain selection methods rate the fitness of each solution and preferentially select the best solutions. Other methods rate only a random sample of the population, as the former process may be very time-consuming.\n\nThe fitness function is defined over the genetic representation and measures the \"quality\" of the represented solution. The fitness function is always problem dependent. For instance, in the knapsack problem one wants to maximize the total value of objects that can be put in a knapsack of some fixed capacity. A representation of a solution might be an array of bits, where each bit represents a different object, and the value of the bit (0 or 1) represents whether or not the object is in the knapsack. Not every such representation is valid, as the size of objects may exceed the capacity of the knapsack. The \"fitness\" of the solution is the sum of values of all objects in the knapsack if the representation is valid, or 0 otherwise.\n\nIn some problems, it is hard or even impossible to define the fitness expression; in these cases, a simulation may be used to determine the fitness function value of a phenotype (e.g. computational fluid dynamics is used to determine the air resistance of a vehicle whose shape is encoded as the phenotype), or even interactive genetic algorithms are used.\n\nThe next step is to generate a second generation population of solutions from those selected through a combination of genetic operators: crossover (also called recombination), and mutation.\n\nFor each new solution to be produced, a pair of \"parent\" solutions is selected for breeding from the pool selected previously. By producing a \"child\" solution using the above methods of crossover and mutation, a new solution is created which typically shares many of the characteristics of its \"parents\". New parents are selected for each new child, and the process continues until a new population of solutions of appropriate size is generated.\nAlthough reproduction methods that are based on the use of two parents are more \"biology inspired\", some research suggests that more than two \"parents\" generate higher quality chromosomes.\n\nThese processes ultimately result in the next generation population of chromosomes that is different from the initial generation. Generally the average fitness will have increased by this procedure for the population, since only the best organisms from the first generation are selected for breeding, along with a small proportion of less fit solutions. These less fit solutions ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children.\n\nOpinion is divided over the importance of crossover versus mutation. There are many references in Fogel (2006) that support the importance of mutation-based search.\n\nAlthough crossover and mutation are known as the main genetic operators, it is possible to use other operators such as regrouping, colonization-extinction, or migration in genetic algorithms.\n\nIt is worth tuning parameters such as the mutation probability, crossover probability and population size to find reasonable settings for the problem class being worked on. A very small mutation rate may lead to genetic drift (which is non-ergodic in nature). A recombination rate that is too high may lead to premature convergence of the genetic algorithm. A mutation rate that is too high may lead to loss of good solutions, unless elitist selection is employed.\n\nIn addition to the main operators above, other heuristics may be employed to make the calculation faster or more robust. The \"speciation\" heuristic penalizes crossover between candidate solutions that are too similar; this encourages population diversity and helps prevent premature convergence to a less optimal solution.\n\nThis generational process is repeated until a termination condition has been reached. Common terminating conditions are:\n\n- A solution is found that satisfies minimum criteria\n- Fixed number of generations reached\n- Allocated budget (computation time/money) reached\n- The highest ranking solution's fitness is reaching or has reached a plateau such that successive iterations no longer produce better results\n- Manual inspection\n- Combinations of the above\n\nGenetic algorithms are simple to implement, but their behavior is difficult to understand. In particular it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:\n\n1. A description of a heuristic that performs adaptation by identifying and recombining \"building blocks\", i.e. low order, low defining-length schemata with above average fitness.\n2. A hypothesis that a genetic algorithm performs adaptation by implicitly and efficiently implementing this heuristic.\n\nGoldberg describes the heuristic as follows:\n\nDespite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.\n\nThere are limitations of the use of a genetic algorithm compared to alternative optimization algorithms:\n\n- Repeated fitness function evaluation for complex problems is often the most prohibitive and limiting segment of artificial evolutionary algorithms. Finding the optimal solution to complex high-dimensional, multimodal problems often requires very expensive fitness function evaluations. In real world problems such as structural optimization problems, a single function evaluation may require several hours to several days of complete simulation. Typical optimization methods cannot deal with such types of problem. In this case, it may be necessary to forgo an exact evaluation and use an approximated fitness that is computationally efficient. It is apparent that amalgamation of approximate models may be one of the most promising approaches to convincingly use GA to solve complex real life problems.\n- Genetic algorithms do not scale well with complexity. That is, where the number of elements which are exposed to mutation is large there is often an exponential increase in search space size. This makes it extremely difficult to use the technique on problems such as designing an engine, a house or a plane. In order to make such problems tractable to evolutionary search, they must be broken down into the simplest representation possible. Hence we typically see evolutionary algorithms encoding designs for fan blades instead of engines, building shapes instead of detailed construction plans, and airfoils instead of whole aircraft designs. The second problem of complexity is the issue of how to protect parts that have evolved to represent good solutions from further destructive mutation, particularly when their fitness assessment requires them to combine well with other parts.\n- The \"better\" solution is only in comparison to other solutions. As a result, the stop criterion is not clear in every problem.\n- In many problems, GAs have a tendency to converge towards local optima or even arbitrary points rather than the global optimum of the problem. This means that it does not \"know how\" to sacrifice short-term fitness to gain longer-term fitness. The likelihood of this occurring depends on the shape of the fitness landscape: certain problems may provide an easy ascent towards a global optimum, others may make it easier for the function to find the local optima. This problem may be alleviated by using a different fitness function, increasing the rate of mutation, or by using selection techniques that maintain a diverse population of solutions, although the No Free Lunch theorem proves that there is no general solution to this problem. A common technique to maintain diversity is to impose a \"niche penalty\", wherein, any group of individuals of sufficient similarity (niche radius) have a penalty added, which will reduce the representation of that group in subsequent generations, permitting other (less similar) individuals to be maintained in the population. This trick, however, may not be effective, depending on the landscape of the problem. Another possible technique would be to simply replace part of the population with randomly generated individuals, when most of the population is too similar to each other. Diversity is important in genetic algorithms (and genetic programming) because crossing over a homogeneous population does not yield new solutions. In evolution strategies and evolutionary programming, diversity is not essential because of a greater reliance on mutation.\n- Operating on dynamic data sets is difficult, as genomes begin to converge early on towards solutions which may no longer be valid for later data. Several methods have been proposed to remedy this by increasing genetic diversity somehow and preventing early convergence, either by increasing the probability of mutation when the solution quality drops (called \"triggered hypermutation\"), or by occasionally introducing entirely new, randomly generated elements into the gene pool (called \"random immigrants\"). Again, evolution strategies and evolutionary programming can be implemented with a so-called \"comma strategy\" in which parents are not maintained and new parents are selected only from offspring. This can be more effective on dynamic problems.\n- GAs cannot effectively solve problems in which the only fitness measure is a single right/wrong measure (like decision problems), as there is no way to converge on the solution (no hill to climb). In these cases, a random search may find a solution as quickly as a GA. However, if the situation allows the success/failure trial to be repeated giving (possibly) different results, then the ratio of successes to failures provides a suitable fitness measure.\n- For specific optimization problems and problem instances, other optimization algorithms may be more efficient than genetic algorithms in terms of speed of convergence. Alternative and complementary algorithms include evolution strategies, evolutionary programming, simulated annealing, Gaussian adaptation, hill climbing, and swarm intelligence (e.g.: ant colony optimization, particle swarm optimization) and methods based on integer linear programming. The suitability of genetic algorithms is dependent on the amount of knowledge of the problem; well known problems often have better, more specialized approaches.\n\nThe simplest algorithm represents each chromosome as a bit string. Typically, numeric parameters can be represented by integers, though it is possible to use floating point representations. The floating point representation is natural to evolution strategies and evolutionary programming. The notion of real-valued genetic algorithms has been offered but is really a misnomer because it does not really represent the building block theory that was proposed by John Henry Holland in the 1970s. This theory is not without support though, based on theoretical and experimental results (see below). The basic algorithm performs crossover and mutation at the bit level. Other variants treat the chromosome as a list of numbers which are indexes into an instruction table, nodes in a linked list, hashes, objects, or any other imaginable data structure. Crossover and mutation are performed so as to respect data element boundaries. For most data types, specific variation operators can be designed. Different chromosomal data types seem to work better or worse for different specific problem domains.\n\nWhen bit-string representations of integers are used, Gray coding is often employed. In this way, small changes in the integer can be readily affected through mutations or crossovers. This has been found to help prevent premature convergence at so-called \"Hamming walls\", in which too many simultaneous mutations (or crossover events) must occur in order to change the chromosome to a better solution.\n\nOther approaches involve using arrays of real-valued numbers instead of bit strings to represent chromosomes. Results from the theory of schemata suggest that in general the smaller the alphabet, the better the performance, but it was initially surprising to researchers that good results were obtained from using real-valued chromosomes. This was explained as the set of real values in a finite population of chromosomes as forming a \"virtual alphabet\" (when selection and recombination are dominant) with a much lower cardinality than would be expected from a floating point representation.\n\nAn expansion of the Genetic Algorithm accessible problem domain can be obtained through more complex encoding of the solution pools by concatenating several types of heterogenously encoded genes into one chromosome. This particular approach allows for solving optimization problems that require vastly disparate definition domains for the problem parameters. For instance, in problems of cascaded controller tuning, the internal loop controller structure can belong to a conventional regulator of three parameters, whereas the external loop could implement a linguistic controller (such as a fuzzy system) which has an inherently different description. This particular form of encoding requires a specialized crossover mechanism that recombines the chromosome by section, and it is a useful tool for the modelling and simulation of complex adaptive systems, especially evolution processes.\n\nA practical variant of the general process of constructing a new population is to allow the best organism(s) from the current generation to carry over to the next, unaltered. This strategy is known as \"elitist selection\" and guarantees that the solution quality obtained by the GA will not decrease from one generation to the next.\n\nParallel implementations of genetic algorithms come in two flavors. Coarse-grained parallel genetic algorithms assume a population on each of the computer nodes and migration of individuals among the nodes. Fine-grained parallel genetic algorithms assume an individual on each processor node which acts with neighboring individuals for selection and reproduction.\nOther variants, like genetic algorithms for online optimization problems, introduce time-dependence or noise in the fitness function.\n\nGenetic algorithms with adaptive parameters (adaptive genetic algorithms, AGAs) is another significant and promising variant of genetic algorithms. The probabilities of crossover (pc) and mutation (pm) greatly determine the degree of solution accuracy and the convergence speed that genetic algorithms can obtain. Instead of using fixed values of \"pc\" and \"pm\", AGAs utilize the population information in each generation and adaptively adjust the \"pc\" and \"pm\" in order to maintain the population diversity as well as to sustain the convergence capacity. In AGA (adaptive genetic algorithm), the adjustment of \"pc\" and \"pm\" depends on the fitness values of the solutions. In \"CAGA\" (clustering-based adaptive genetic algorithm), through the use of clustering analysis to judge the optimization states of the population, the adjustment of \"pc\" and \"pm\" depends on these optimization states.\nIt can be quite effective to combine GA with other optimization methods. GA tends to be quite good at finding generally good global solutions, but quite inefficient at finding the last few mutations to find the absolute optimum. Other techniques (such as simple hill climbing) are quite efficient at finding absolute optimum in a limited region. Alternating GA and hill climbing can improve the efficiency of GA while overcoming the lack of robustness of hill climbing.\n\nThis means that the rules of genetic variation may have a different meaning in the natural case. For instance – provided that steps are stored in consecutive order – crossing over may sum a number of steps from maternal DNA adding a number of steps from paternal DNA and so on. This is like adding vectors that more probably may follow a ridge in the phenotypic landscape. Thus, the efficiency of the process may be increased by many orders of magnitude. Moreover, the inversion operator has the opportunity to place steps in consecutive order or any other suitable order in favour of survival or efficiency.\n\nA variation, where the population as a whole is evolved rather than its individual members, is known as gene pool recombination.\n\nA number of variations have been developed to attempt to improve performance of GAs on problems with a high degree of fitness epistasis, i.e. where the fitness of a solution consists of interacting subsets of its variables. Such algorithms aim to learn (before exploiting) these beneficial phenotypic interactions. As such, they are aligned with the Building Block Hypothesis in adaptively reducing disruptive recombination. Prominent examples of this approach include the mGA, GEMGA and LLGA.\n\nProblems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.\n\nAs a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).\n\nExamples of problems solved by genetic algorithms include: mirrors designed to funnel sunlight to a solar collector, antennae designed to pick up radio signals in space, walking methods for computer figures, optimal design of aerodynamic bodies in complex flowfields \n\nIn his \"Algorithm Design Manual\", Skiena advises against genetic algorithms for any task:\n\nIn 1950, Alan Turing proposed a \"learning machine\" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).\n\nAlthough Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book \"Adaptation in Natural and Artificial Systems\" (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.\n\nIn the late 1980s, General Electric started selling the world's first genetic algorithm product, a mainframe-based toolkit designed for industrial processes. \nIn 1989, Axcelis, Inc. released Evolver, the world's first commercial GA product for desktop computers. The New York Times technology writer John Markoff wrote about Evolver in 1990, and it remained the only interactive commercial genetic algorithm until 1995. Evolver was sold to Palisade in 1997, translated into several languages, and is currently in its 6th version. Since the 1990s, MATLAB has built in three derivative-free optimization heuristic algorithms (simulated annealing, particle swarm optimization, genetic algorithm) and two direct search algorithms (simplex search, pattern search).\n\nGenetic algorithms are a sub-field:\n- Evolutionary algorithms\n- Evolutionary computing\n- Metaheuristics\n- Stochastic optimization\n- Optimization\n\nEvolutionary algorithms is a sub-field of evolutionary computing.\n\n- Evolution strategies (ES, see Rechenberg, 1994) evolve individuals by means of mutation and intermediate or discrete recombination. ES algorithms are designed particularly to solve problems in the real-value domain. They use self-adaptation to adjust control parameters of the search. De-randomization of self-adaptation has led to the contemporary Covariance Matrix Adaptation Evolution Strategy (CMA-ES).\n- Evolutionary programming (EP) involves populations of solutions with primarily mutation and selection and arbitrary representations. They use self-adaptation to adjust parameters, and can include other variation operations such as combining information from multiple parents.\n- Estimation of Distribution Algorithm (EDA) substitutes traditional reproduction operators by model-guided operators. Such models are learned from the population by employing machine learning techniques and represented as Probabilistic Graphical Models, from which new solutions can be sampled or generated from guided-crossover.\n- Gene expression programming (GEP) also uses populations of computer programs. These complex computer programs are encoded in simpler linear chromosomes of fixed length, which are afterwards expressed as expression trees. Expression trees or computer programs evolve because the chromosomes undergo mutation and recombination in a manner similar to the canonical GA. But thanks to the special organization of GEP chromosomes, these genetic modifications always result in valid computer programs.\n- Genetic programming (GP) is a related technique popularized by John Koza in which computer programs, rather than function parameters, are optimized. Genetic programming often uses tree-based internal data structures to represent the computer programs for adaptation instead of the list structures typical of genetic algorithms.\n- Grouping genetic algorithm (GGA) is an evolution of the GA where the focus is shifted from individual items, like in classical GAs, to groups or subset of items. The idea behind this GA evolution proposed by Emanuel Falkenauer is that solving some complex problems, a.k.a. \"clustering\" or \"partitioning\" problems where a set of items must be split into disjoint group of items in an optimal way, would better be achieved by making characteristics of the groups of items equivalent to genes. These kind of problems include bin packing, line balancing, clustering with respect to a distance measure, equal piles, etc., on which classic GAs proved to perform poorly. Making genes equivalent to groups implies chromosomes that are in general of variable length, and special genetic operators that manipulate whole groups of items. For bin packing in particular, a GGA hybridized with the Dominance Criterion of Martello and Toth, is arguably the best technique to date.\n- Interactive evolutionary algorithms are evolutionary algorithms that use human evaluation. They are usually applied to domains where it is hard to design a computational fitness function, for example, evolving images, music, artistic designs and forms to fit users' aesthetic preference.\n\nSwarm intelligence is a sub-field of evolutionary computing.\n\n- Ant colony optimization (ACO) uses many ants (or agents) equipped with a pheromone model to traverse the solution space and find locally productive areas. Although considered an Estimation of distribution algorithm,\n- Particle swarm optimization (PSO) is a computational method for multi-parameter optimization which also uses population-based approach. A population (swarm) of candidate solutions (particles) moves in the search space, and the movement of the particles is influenced both by their own best known position and swarm's global best known position. Like genetic algorithms, the PSO method depends on information sharing among population members. In some problems the PSO is often more computationally efficient than the GAs, especially in unconstrained problems with continuous variables.\n\nEvolutionary computation is a sub-field of the metaheuristic methods.\n\n- Electimize algorithm is an evolutionary algorithm that simulates the phenomenon of electron flow and electrical conductivity. Some current research showed Electimize to be more efficient in solving NP-hard optimization problems than traditional evolutionary algorithms. The algorithm provides higher capacity in searching the solution space extensively, and identifying global optimal alternatives. Unlike other evolutionary algorithms, Electimize evaluates the quality of the values in the solution string independently.\n- Memetic algorithm (MA), often called \"hybrid genetic algorithm\" among others, is a population-based method in which solutions are also subject to local improvement phases. The idea of memetic algorithms comes from memes, which unlike genes, can adapt themselves. In some problem areas they are shown to be more efficient than traditional evolutionary algorithms.\n- Bacteriologic algorithms (BA) inspired by evolutionary ecology and, more particularly, bacteriologic adaptation. Evolutionary ecology is the study of living organisms in the context of their environment, with the aim of discovering how they adapt. Its basic concept is that in a heterogeneous environment, there is not one individual that fits the whole environment. So, one needs to reason at the population level. It is also believed BAs could be successfully applied to complex positioning problems (antennas for cell phones, urban planning, and so on) or data mining.\n- Cultural algorithm (CA) consists of the population component almost identical to that of the genetic algorithm and, in addition, a knowledge component called the belief space.\n- Differential search algorithm (DS) inspired by migration of superorganisms.\n- Gaussian adaptation (normal or natural adaptation, abbreviated NA to avoid confusion with GA) is intended for the maximisation of manufacturing yield of signal processing systems. It may also be used for ordinary parametric optimisation. It relies on a certain theorem valid for all regions of acceptability and all Gaussian distributions. The efficiency of NA relies on information theory and a certain theorem of efficiency. Its efficiency is defined as information divided by the work needed to get the information. Because NA maximises mean fitness rather than the fitness of the individual, the landscape is smoothed such that valleys between peaks may disappear. Therefore it has a certain \"ambition\" to avoid local peaks in the fitness landscape. NA is also good at climbing sharp crests by adaptation of the moment matrix, because NA may maximise the disorder (average information) of the Gaussian simultaneously keeping the mean fitness constant.\n\nMetaheuristic methods broadly fall within stochastic optimisation methods.\n\n- Simulated annealing (SA) is a related global optimization technique that traverses the search space by testing random mutations on an individual solution. A mutation that increases fitness is always accepted. A mutation that lowers fitness is accepted probabilistically based on the difference in fitness and a decreasing temperature parameter. In SA parlance, one speaks of seeking the lowest energy instead of the maximum fitness. SA can also be used within a standard GA algorithm by starting with a relatively high rate of mutation and decreasing it over time along a given schedule.\n- Tabu search (TS) is similar to simulated annealing in that both traverse the solution space by testing mutations of an individual solution. While simulated annealing generates only one mutated solution, tabu search generates many mutated solutions and moves to the solution with the lowest energy of those generated. In order to prevent cycling and encourage greater movement through the solution space, a tabu list is maintained of partial or complete solutions. It is forbidden to move to a solution that contains elements of the tabu list, which is updated as the solution traverses the solution space.\n- Extremal optimization (EO) Unlike GAs, which work with a population of candidate solutions, EO evolves a single solution and makes local modifications to the worst components. This requires that a suitable representation be selected which permits individual solution components to be assigned a quality measure (\"fitness\"). The governing principle behind this algorithm is that of \"emergent\" improvement through selectively removing low-quality components and replacing them with a randomly selected component. This is decidedly at odds with a GA that selects good solutions in an attempt to make better solutions.\n\n- The cross-entropy (CE) method generates candidate solutions via a parameterized probability distribution. The parameters are updated via cross-entropy minimization, so as to generate better samples in the next iteration.\n- Reactive search optimization (RSO) advocates the integration of sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. The word reactive hints at a ready response to events during the search through an internal online feedback loop for the self-tuning of critical parameters. Methodologies of interest for Reactive Search include machine learning and statistics, in particular reinforcement learning, active or query learning, neural networks, and metaheuristics.\n\n", "related": "\n- List of genetic algorithm applications\n- Genetic algorithms in signal processing (a.k.a. particle filters)\n- Propagation of schema\n- Universal Darwinism\n- Metaheuristics\n- Learning classifier system\n- Rule-based machine learning\n\n- Rechenberg, Ingo (1994): Evolutionsstrategie '94, Stuttgart: Fromman-Holzboog.\n- Schwefel, Hans-Paul (1974): Numerische Optimierung von Computer-Modellen (PhD thesis). Reprinted by Birkhäuser (1977).\n\n- Provides a list of resources in the genetic algorithms field\n\n- Genetic Algorithms - Computer programs that \"evolve\" in ways that resemble natural selection can solve complex problems even their creators do not fully understand An excellent introduction to GA by John Holland and with an application to the Prisoner's Dilemma\n- An online interactive Genetic Algorithm tutorial for a reader to practise or learn how a GA works: Learn step by step or watch global convergence in batch, change the population size, crossover rates/bounds, mutation rates/bounds and selection mechanisms, and add constraints.\n- A Genetic Algorithm Tutorial by Darrell Whitley Computer Science Department Colorado State University An excellent tutorial with lots of theory\n- \"Essentials of Metaheuristics\", 2009 (225 p). Free open text by Sean Luke.\n- Global Optimization Algorithms – Theory and Application\n- Genetic Algorithms in Python Tutorial with the intuition behind GAs and Python implementation.\n- Genetic Algorithms evolves to solve the prisoner's dilemma. Written by Robert Axelrod.\n"}
{"id": "52242050", "url": "https://en.wikipedia.org/wiki?curid=52242050", "title": "Multiplicative weight update method", "text": "Multiplicative weight update method\n\nThe multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed: reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory.\n\n\"Multiplicative weights\" implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered.\n\nThe earliest known version of this technique was in an algorithm named \"fictitious play\" which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of \"fictitious play\" to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Papert's earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension.\n\nIn operation research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nIn computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavan's method of pessimistic estimators for derandomization of randomized rounding algorithms; Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yao's XOR Lemma; Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases.\n\nA binary decision needs to be made based on n experts’ opinions to attain an associated payoff. In the first round, all experts’ opinions have the same weight. The decision maker will make the first decision based on the majority of the experts' prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each expert's opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down.\n\nGiven a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistakes will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most mistakes.\n\nUnlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same \"expert advice\" setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithm's goal is to limit its cumulative losses to roughly the same as the best of experts.\nThe very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm.\n\nIf formula_10, the weight of the expert's advice will remain the same. When formula_11 increases, the weight of the expert's advice will decrease. Note that some researchers fix formula_12 in weighted majority algorithm.\n\nAfter formula_6 steps, let formula_14 be the number of mistakes of expert i and formula_15 be the number of mistakes our algorithm has made. Then we have the following bound for every formula_16:\n\nIn particular, this holds for i which is the best expert. Since the best expert will have the least formula_14, it will give the best bound on the number of mistakes made by the algorithm as a whole.\n\nGiven the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50%. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction:\n\npredict \n\nwhere \n\nThe number of mistakes made by the randomized weighted majority algorithm is bounded as: \n\nwhere formula_22 and formula_23.\n\nNote that only the learning algorithm is randomized. The underlying assumption is that the examples and experts' predictions are not random. The only randomness is the randomness where the learner makes his own prediction.\nIn this randomized algorithm, formula_24 if formula_25. Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people define formula_12 in weighted majority algorithm and allow formula_27 in randomized weighted majority algorithm.\n\nThe multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event.\n\nSuppose we were given the distribution formula_28 on experts. Let formula_29 = payoff matrix of a finite two-player zero-sum game, with formula_30 rows.\n\nWhen the row player formula_31 uses plan formula_16 and the column player formula_33 uses plan formula_34, the payoff of player formula_33 is formula_36≔formula_37, assuming formula_38.\n\nIf player formula_31 chooses action formula_16 from a distribution formula_28 over the rows, then the expected result for player formula_33 selecting action formula_34 is formula_44.\n\nTo maximize formula_45, player formula_33 is should choose plan formula_34. Similarly, the expected payoff for player formula_48 is formula_49. Choosing plan formula_16 would minimize this payoff. By John Von Neumann's Min-Max Theorem, we obtain:\n\nwhere P and i changes over the distributions over rows, Q and j changes over the columns.\n\nThen, let formula_52 denote the common value of above quantities, also named as the \"value of the game\". Let formula_53 be an error parameter. To solve the zero-sum game bounded by additive error of formula_54,\n\nSo there is an algorithm solving zero-sum game up to an additive factor of δ using O(/formula_57) calls to ORACLE, with an additional processing time of O(n) per call\n\nBailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it.\n\nIn machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method.\n\nBased on current knowledge in algorithms, multiplicative weight update method was first used in Littlestone's winnow algorithm. It is used in machine learning to solve a linear program.\n\nGiven formula_58 labeled examples formula_59 where formula_60 are feature vectors, and formula_61 are their labels.\n\nThe aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that formula_62 for all formula_34. Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine formula_64 to be formula_65, the problem reduces to finding a solution to the following LP:\n\nThis is general form of LP.\n\nThe hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different.\nIt is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update.\n\nAssume the learning rate formula_69 and for formula_70, formula_71 is picked by Hedge. Then for all experts formula_16,\n\nInitialization: Fix an formula_69. For each expert, associate the weight formula_75 ≔1\nFor t=1,2,…,T:\n\nThis algorithm maintains a set of weights formula_80 over the training examples. On every iteration formula_3, a distribution formula_71 is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis formula_83 that (hopefully) has small error with respect to the distribution. Using the new hypothesis formula_83, AdaBoost generates the next weight vector formula_85. The process repeats. After T such iterations, the final hypothesis formula_86 is the output. The hypothesis formula_86 combines the outputs of the T weak hypotheses using a weighted majority vote.\n\nGiven a formula_109 matrix formula_29 and formula_111, is there a formula_112 such that formula_113?\n\nUsing the oracle algorithm in solving zero-sum problem, with an error parameter formula_115, the output would either be a point formula_112 such that formula_117 or a proof that formula_112 does not exist, i.e., there is no solution to this linear system of inequalities.\n\nGiven vector formula_119, solves the following relaxed problem\n\nIf there exists a x satisfying (1), then x satisfies (2) for all formula_121. The contrapositive of this statement is also true.\nSuppose if oracle returns a feasible solution for a formula_122, the solution formula_112 it returns has bounded width formula_124.\nSo if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of formula_125. The algorithm makes at most formula_126 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case.\n\nMultiplicative weights update is the discrete-time variant of the replicator equation (replicator dynamics), which is a commonly used model in evolutionary game theory. It converges to Nash equilibrium when applied to a congestion game.\n\nIn operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently.\n\nThe multiplicative weights algorithm is also widely applied in computational geometry, such as Clarkson's algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension.\n", "related": "NONE"}
{"id": "938663", "url": "https://en.wikipedia.org/wiki?curid=938663", "title": "Multi-task learning", "text": "Multi-task learning\n\nMulti-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called \"hints\".\n\nIn a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.\n\nIn the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.\n\nMulti-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.\n\nWithin the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly.. For example, the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains.\n\nOne can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods.\n\nRelated to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task.\n\nTraditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents.\n\nThe MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015.\n\nSuppose the training data set is formula_1, with formula_2, formula_3, where indexes task, and formula_4. Let formula_5. In this setting there is a consistent input and output space and the same loss function formula_6 for each task: . This results in the regularized machine learning problem: \nwhere formula_7 is a vector valued reproducing kernel Hilbert space with functions formula_8 having components formula_9.\n\nThe reproducing kernel for the space formula_7 of functions formula_11 is a symmetric matrix-valued function formula_12 , such that formula_13 and the following reproducing property holds: \nThe form of the kernel induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a \"separable kernel,\" which factors into separate kernels on the input space and on the tasks formula_14. In this case the kernel relating scalar components formula_15 and formula_16 is given by formula_17. For vector valued functions formula_18 we can write formula_19, where is a scalar reproducing kernel, and is a symmetric positive semi-definite formula_20 matrix. Henceforth denote formula_21 .\n\nThis factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by . Methods for non-separable kernels is an current field of research.\n\nFor the separable case, the representation theorem is reduced to formula_22. The model output on the training data is then , where is the formula_23 empirical kernel matrix with entries formula_24, and is the formula_25 matrix of rows formula_26.\n\nWith the separable kernel, equation can be rewritten as\n\nwhere is a (weighted) average of applied entry-wise to and . (The weight is zero if formula_27 is a missing observation).\n\nNote the second term in can be derived as follows:\n\nThere are three largely equivalent ways to represent task structure: through a regularizer; through an output metric, and through an output mapping.\n\nVia the regularizer formulation, one can represent a variety of task structures easily. \n- Letting formula_29 (where formula_30 is the \"T\"x\"T\" identity matrix, and formula_31 is the \"T\"x\"T\" matrix of ones) is equivalent to letting control the variance formula_32 of tasks from their mean formula_33. For example, blood levels of some biomarker may be taken on patients at formula_34 time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients.\n- Letting formula_35 , where formula_36 is equivalent to letting formula_37 control the variance measured with respect to a group mean: formula_38. (Here formula_39 the cardinality of group r, and formula_40 is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group.\n- Letting formula_41, where formula_42 is the Laplacian for the graph with adjacency matrix \"M\" giving pairwise similarities of tasks. This is equivalent to giving a larger penalty to the distance separating tasks \"t\" and \"s\" when they are more similar (according to the weight formula_43,) i.e. formula_44 regularizes formula_45.\n- All of the above choices of A also induce the additional regularization term formula_46 which penalizes complexity in f more broadly.\n\nLearning problem can be generalized to admit learning task matrix A as follows:\nChoice of formula_47 must be designed to learn matrices \"A\" of a given type. See \"Special cases\" below.\n\nRestricting to the case of convex losses and coercive penalties Ciliberto \"et al.\" have shown that although is not convex jointly in \"C\" and \"A,\" a related problem is jointly convex.\n\nSpecifically on the convex set formula_48, the equivalent problem\n\nis convex with the same minimum value. And if formula_49 is a minimizer for then formula_50 is a minimizer for .\n\nThe perturbation via the barrier formula_51 forces the objective functions to be equal to formula_52 on the boundary of formula_53 .\n\nSpectral penalties - Dinnuzo \"et al\" suggested setting \"F\" as the Frobenius norm formula_56. They optimized directly using block coordinate descent, not accounting for difficulties at the boundary of formula_57.\n\nClustered tasks learning - Jacob \"et al\" suggested to learn \"A\" in the setting where \"T\" tasks are organized in \"R\" disjoint clusters. In this case let formula_58 be the matrix with formula_59. Setting formula_60, and formula_61, the task matrix formula_62 can be parameterized as a function of formula_63: formula_64 , with terms that penalize the average, between clusters variance and within clusters variance respectively of the task predictions. M is not convex, but there is a convex relaxation formula_65. In this formulation, formula_66.\n\nNon-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases.\n\nNon-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels.\n\nUsing the principles of MTL, techniques for collaborative spam filtering that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local classifier to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.\n\nUsing boosted decision trees, one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.\n\nThe Multi-Task Learning via StructurAl Regularization (MALSAR) Matlab package implements the following multi-task learning algorithms:\n- Mean-Regularized Multi-Task Learning\n- Multi-Task Learning with Joint Feature Selection\n- Robust Multi-Task Feature Learning\n- Trace-Norm Regularized Multi-Task Learning\n- Alternating Structural Optimization\n- Incoherent Low-Rank and Sparse Learning\n- Robust Low-Rank Multi-Task Learning\n- Clustered Multi-Task Learning\n- Multi-Task Learning with Graph Structures\n\n", "related": "\n- Artificial intelligence\n- Artificial neural network\n- Automated machine learning (AutoML)\n- Evolutionary computation\n- General game playing\n- Human-based genetic algorithm\n- Kernel methods for vector output\n- Multitask optimization\n- Robot learning\n- Transfer learning\n\n- The Biosignals Intelligence Group at UIUC\n- Washington University at St. Louis Depart. of Computer Science\n\n- The Multi-Task Learning via Structural Regularization Package\n- Online Multi-Task Learning Toolkit (OMT) A general-purpose online multi-task learning toolkit based on conditional random field models and stochastic gradient descent training (C#, .NET)\n"}
{"id": "52642349", "url": "https://en.wikipedia.org/wiki?curid=52642349", "title": "AIVA", "text": "AIVA\n\nAIVA (Artificial Intelligence Virtual Artist) is an electronic composer recognized by the SACEM.\n\nCreated in February 2016, AIVA specializes in Classical and Symphonic music composition. It became the world’s first virtual composer to be recognized by a music society (SACEM).\nBy reading a large collection of existing works of classical music (written by human composers such as Bach, Beethoven, Mozart) AIVA is capable of detecting regularities in music and on this base composing on its own. The algorithm AIVA is based on deep learning and reinforcement learning architectures. Since January 2019, the company offers a commercial product, Music Engine, capable of generating short (up to 3 minutes) compositions in various styles (rock, pop, jazz, fantasy, shanty, tango, 20th century cinematic, modern cinematic, and Chinese).\n\nAIVA was presented at TED by Pierre Barreau.\n\nAIVA is a published composer; its first studio album \"Genesis\" was released in November 2016. Second album \"Among the Stars\" in 2018.\n\n- 2016 CD album « Genesis » Hv-Com – LEPM 048427. Track listing \"Genesis\":\n- 2018 CD album « Among the Stars » Hv-Com – LEPM 048708\nAvignon Symphonic Orchestra [ORAP] also performed Aiva's compositions in April 2017.\n\nThis is the preview of the score Op. n°3 for piano solo \"A little chamber music\", composed by AIVA.\n\n", "related": "\n- Applications of artificial intelligence\n- Computer music\n- Music and artificial intelligence\n"}
{"id": "41732818", "url": "https://en.wikipedia.org/wiki?curid=41732818", "title": "Qloo", "text": "Qloo\n\nQloo (pronounced \"clue\") is a company that uses artificial intelligence (AI) to understand taste and cultural correlations. It provides companies with an application programming interface (API). It received funding from Leonardo DiCaprio, Elton John, Barry Sternlicht, Pierre Lagrange and others.\n\nQloo establishes consumer preference correlations via machine learning across data spanning cultural domains including music, film, television, dining, nightlife, fashion, books and travel. The recommender system uses AI to predict correlations for further applications.\n\nQloo was founded in 2012 by chief executive officer Alex Elias and chief operating officer Jay Alger. Qloo was tested on a private website in April 2012. \nIn 2012, Qloo raised $1.4 million in seed funding from investors including Cedric the Entertainer, and venture capital firm Kindler Capital.\nQloo had a public beta release in November 2012 after its initial funding.\n\nIn 2013, the company raised an additional $1.6 million from Cross Creek Pictures founding partner Tommy Thompson, and Samih Toukan and Hussam Khoury, founders of Maktoob, an Internet services company purchased by Yahoo! for $164 million in 2009.\nOn November 14, 2013, a website and an iPhone app were announced. The company later released an Android app, and tablet versions, in mid-2014.\n\nIn 2016, Qloo secured $4.5 million in venture capital investment. The $4.5 million was split between a number of investors, including Barry Sternlicht, Pierre Lagrange and Leonardo DiCaprio. In July 2017, Qloo raised $6.5 million in funding rounds from AXA Strategic Ventures and Elton John.\n\nFollowing the investment, the founders stated in an interview with Tech Crunch that they would use the investment to expand Qloo's database. They hoped the move would secure larger contracts with corporate clients. At the time, clients already included Fortune 500 companies such as Twitter, PepsiCo and BMW.\n\nIn 2019, the company announced that it had acquired cultural recommendation service TasteDive, with Alex Elias becoming chairman of TasteDive. In September 2019, Qloo was named among the Top 14 Artificial Intelligence APIs by ProgrammableWeb.\n\nQloo calls itself a cultural AI platform to provide real-time correlation data across domains of culture and entertainment including: film, music, television, dining, nightlife, fashion, books and travel. Each category contains subcategories.\n\nQloo’s knowledge of a user's taste in one category can be utilized to offer suggestions in other categories. Users then rate the suggestions, providing it with feedback for future suggestions.\nQloo has partnerships with companies such as Expedia and iTunes.\n", "related": "NONE"}
{"id": "53279262", "url": "https://en.wikipedia.org/wiki?curid=53279262", "title": "Instance selection", "text": "Instance selection\n\nInstance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems.\n\nAlgorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality.\n\nThe literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select: algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite DROP3, ICF and LSBo. On the other hand, within the category of algorithms that select internal instances, it is possible to mention ENN and LSSm. In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have a negative impact on the data mining task. They can be used by other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by DROP3 as the first step, and the LSSm algorithm is used by LSBo.\n\nThere is also another group of algorithms that adopt different selection criteria. For example, the algorithms LDIS, CDIS and XLDIS select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as DROP3 and ICF.\n\nBesides that, there is a third category of algorithms that, instead of selecting actual instances of the dataset, select prototypes (that can be synthetic instances). In this category it is possible to include PSSA, PSDSP and PSSP. The three algorithms adopt the notion of spatial partition (a hyperrectangle) for identifying similar instances and extract prototypes for each set of similar instances. In general, these approaches can also be modified for selecting actual instances of the datasets. The algorithm ISDSP adopts a similar approach for selecting actual instances (instead of prototypes).\n", "related": "NONE"}
{"id": "53802271", "url": "https://en.wikipedia.org/wiki?curid=53802271", "title": "Machine learning control", "text": "Machine learning control\n\nMachine learning control (MLC) is a subfield of machine learning, intelligent control and control theory\nwhich solves optimal control problems with methods of machine learning.\nKey applications are complex nonlinear systems\nfor which linear control theory methods are not applicable.\n\nFour types of problems are commonly encountered.\n- Control parameter identification: MLC translates to a parameter identification if the structure of the control law is given but the parameters are unknown. One example is the genetic algorithm for optimizing coefficients of a PID controller or discrete-time optimal control.\n- Control design as regression problem of the first kind: MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known full state feedback. A neural network is commonly used technique for this task.\n- Control design as regression problem of the second kind: MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, nor the control law structure, nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful regression technique for this purpose.\n- Reinforcement learning control: The control law may be continually updated over measured performance changes (rewards) using reinforcement learning.\n\nMLC comprises, for instance, neural network control, \ngenetic algorithm based control, \ngenetic programming control,\nreinforcement learning control, \nand has methodological overlaps with other data-driven control,\nlike artificial intelligence and robot control.\n\nMLC has been successfully applied\nto many nonlinear control problems,\nexploring unknown and often unexpected actuation mechanisms.\nExample applications include\n\n- Attitude control of satellites.\n- Building thermal control.\n- Feedback turbulence control.\n- Remotely operated under water vehicle.\n- Many more engineering MLC application are summarized in the review article of PJ Fleming & RC Purshouse (2002).\n\nAs for all general nonlinear methods,\nMLC comes with no guaranteed convergence, \noptimality or robustness for a range of operating conditions.\n\n- Dimitris C Dracopoulos (August 1997) \"Evolutionary Learning Algorithms for Neural Adaptive Control\", Springer. .\n- Thomas Duriez, Steven L. Brunton & Bernd R. Noack (November 2016) \"Machine Learning Control - Taming Nonlinear Dynamics and Turbulence\", Springer. .\n", "related": "NONE"}
{"id": "53970843", "url": "https://en.wikipedia.org/wiki?curid=53970843", "title": "Machine learning in bioinformatics", "text": "Machine learning in bioinformatics\n\nMachine learning, a subfield of computer science involving the development of algorithms that learn how to make predictions based on data, has a number of emerging applications in the field of bioinformatics. Bioinformatics deals with computational and mathematical approaches for understanding and processing biological data.\n\nPrior to the emergence of machine learning algorithms, bioinformatics algorithms had to be explicitly programmed by hand which, for problems such as protein structure prediction, proves extremely difficult. Machine learning techniques such as deep learning enable the algorithm to make use of automatic feature learning which means that based on the dataset alone, the algorithm can learn how to combine multiple features of the input data into a more abstract set of features from which to conduct further learning. This multi-layered approach to learning patterns in the input data allows such systems to make quite complex predictions when trained on large datasets. In recent years, the size and number of available biological datasets have skyrocketed, enabling bioinformatics researchers to make use of these machine learning systems. Machine learning has been applied to six biological domains: genomics, proteomics, microarrays, systems biology, evolution, and text mining.\n\nGenomics involves the study of the genome, the complete DNA sequence, of organisms. While genomic sequence data has historically been sparse due to the technical difficulty in sequencing a piece of DNA, the number of available sequences is growing exponentially. However, while raw data is becoming increasingly available and accessible, the biological interpretation of this data is occurring at a much slower pace. Therefore, there is an increasing need for the development of machine learning systems that can automatically determine the location of protein-encoding genes within a given DNA sequence. This is a problem in computational biology known as gene prediction.\n\nGene prediction is commonly performed through a combination of what are known as extrinsic and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated. A number of the sequence's genes can be identified by determining which strings of bases within the sequence are homologous to known gene sequences. However, given the limitation in size of the database of known and annotated gene sequences, not all the genes in a given input sequence can be identified through homology alone. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone.\n\nMachine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history.\nIt can also be used to detect and visualize genome rearrangements.\n\nProteins, strings of amino acids, gain much of their function from protein folding in which they conform into a three-dimensional structure. This structure is composed of a number of layers of folding, including the primary structure (i.e. the flat string of amino acids), the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quartenary structure.\n\nProtein secondary structure prediction is a main focus of this subfield as the further protein foldings (tertiary and quartenary structures) are determined based on the secondary structure. Solving the true structure of a protein is an incredibly expensive and time-intensive process, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Today, through the use of automatic feature learning, the best machine learning techniques are able to achieve an accuracy of 82-84%. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84% when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88–90%.\n\nMachine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction.\n\nMicroarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in the analysis of this data, and it has been applied to expression pattern identification, classification, and genetic network induction.\n\nThis technology is especially useful for monitoring the expression of genes within a genome, aiding in diagnosing different types of cancer based on which genes are expressed. One of the main problems in this field is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, there is a large amount of irrelevant data to the task of expressed gene identification, further complicating this problem. Machine learning presents a potential solution to this problem as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest.\n\nSystems biology focuses on the study of the emergent behaviors from complex interactions of simple biological components in a system. Such components can include molecules such as DNA, RNA, proteins, and metabolites.\n\nMachine learning has been used to aid in the modelling of these complex interactions in biological systems in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the structure between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using a technique known as Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures.\n\nOther systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction.\n\nMachine learning methods for analysis of neuroimaging data are used to help diagnose stroke. Three-dimensional CNN and SVM methods are often used. \nThe increase in available biological publications led to the issue of the increase in difficulty in searching through and compiling all the relevant available information on a given topic across all sources. This task is known as knowledge extraction. This is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017.\n\nThis technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to automatic annotation of the function of genes and proteins, determination of the subcellular localization of a protein, analysis of DNA-expression arrays, large-scale protein interaction analysis, and molecule interaction analysis.\n\nAnother application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data.\n", "related": "NONE"}
{"id": "54033657", "url": "https://en.wikipedia.org/wiki?curid=54033657", "title": "Labeled data", "text": "Labeled data\n\nLabeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of that unlabeled data with meaningful tags that are informative. For example, labels might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, whether the dot in an x-ray is a tumor, etc.\n\nLabels can be obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., \"Does this photo contain a horse or a cow?\"), and are significantly more expensive to obtain than the raw unlabeled data.\n\nAfter obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.\n", "related": "NONE"}
{"id": "53631046", "url": "https://en.wikipedia.org/wiki?curid=53631046", "title": "Caffe (software)", "text": "Caffe (software)\n\nCAFFE (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.\n\nYangqing Jia created the caffe project during his PhD at UC Berkeley. Now there are many contributors to the project, and it is hosted at GitHub.\n\nCaffe supports many different types of deep learning architectures geared towards image classification and image segmentation. It supports CNN, RCNN, LSTM and fully connected neural network designs. Caffe supports GPU- and CPU-based acceleration computational kernel libraries such as NVIDIA cuDNN and Intel MKL.\n\nCaffe is being used in academic research projects, startup prototypes, and even large-scale industrial applications in vision, speech, and multimedia. Yahoo! has also integrated caffe with Apache Spark to create CaffeOnSpark, a distributed deep learning framework.\n\nIn April 2017, Facebook announced Caffe2, which included new features such as Recurrent Neural Networks.\nAt the end of March 2018, Caffe2 was merged into PyTorch.\n\n", "related": "\n- Comparison of deep learning software\n"}
{"id": "44577560", "url": "https://en.wikipedia.org/wiki?curid=44577560", "title": "Occam learning", "text": "Occam learning\n\nIn computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.\n\nOccam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.\n\nOccam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al. that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, \"parsimony\" (of the output hypothesis) implies \"predictive power\".\n\nThe succinctness of a concept formula_1 in concept class formula_2 can be expressed by the length formula_3 of the shortest bit string that can represent formula_1 in formula_2. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.\n\nLet formula_2 and formula_7 be concept classes containing target concepts and hypotheses respectively. Then, for constants formula_8 and formula_9, a learning algorithm formula_10 is an formula_11-Occam algorithm for formula_2 using formula_7 iff, given a set formula_14 of formula_15 samples labeled according to a concept formula_16, formula_10 outputs a hypothesis formula_18 such that\n- formula_19 is consistent with formula_1 on formula_21 (that is, formula_22), and\n- formula_23\nwhere formula_24 is the maximum length of any sample formula_25. An Occam algorithm is called \"efficient\" if it runs in time polynomial in formula_24, formula_15, and formula_28 We say a concept class formula_2 is \"Occam learnable\" with respect to a hypothesis class formula_7 if there exists an efficient Occam algorithm for formula_2 using formula_32\n\nOccam learnability implies PAC learnability, as the following theorem of Blumer, et al. shows:\n\nLet formula_10 be an efficient formula_11-Occam algorithm for formula_2 using formula_7. Then there exists a constant formula_37 such that for any formula_38, for any distribution formula_39, given formula_40 samples drawn from formula_39 and labelled according to a concept formula_42 of length formula_24 bits each, the algorithm formula_10 will output a hypothesis formula_45 such that formula_46 with probability at least formula_47 .Here, formula_48 is with respect to the concept formula_1 and distribution formula_39. This implies that the algorithm formula_10 is also a PAC learner for the concept class formula_2 using hypothesis class formula_7. A slightly more general formulation is as follows:\n\nLet formula_38. Let formula_10 be an algorithm such that, given formula_15 samples drawn from a fixed but unknown distribution formula_57 and labeled according to a concept formula_42 of length formula_24 bits each, outputs a hypothesis formula_60 that is consistent with the labeled samples. Then, there exists a constant formula_61 such that if formula_62, then formula_10 is guaranteed to output a hypothesis formula_60 such that formula_46 with probability at least formula_47.\n\nWhile the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about \"necessity.\" Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning. They proved that for any concept class that is \"polynomially closed under exception lists,\" PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.\n\nA concept class formula_2 is polynomially closed under exception lists if there exists a polynomial-time algorithm formula_68 such that, when given the representation of a concept formula_42 and a finite list formula_70 of \"exceptions\", outputs a representation of a concept formula_71 such that the concepts formula_1 and formula_73 agree except on the set formula_70.\n\nWe first prove the Cardinality version. Call a hypothesis formula_75 \"bad\" if formula_76, where again formula_48 is with respect to the true concept formula_1 and the underlying distribution formula_57. The probability that a set of samples formula_21 is consistent with formula_19 is at most formula_82, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in formula_83 is at most formula_84, which is less than formula_85 if formula_86. This concludes the proof of the second theorem above.\n\nUsing the second theorem, we can prove the first theorem. Since we have a formula_11-Occam algorithm, this means that any hypothesis output by formula_10 can be represented by at most formula_89 bits, and thus formula_90. This is less than formula_91 if we set formula_92 for some constant formula_37. Thus, by the Cardinality version Theorem, formula_10 will output a consistent hypothesis formula_19 with probability at least formula_96. This concludes the proof of the first theorem above.\n\nThough Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions, conjunctions with few relevant variables, and decision lists.\n\nOccam algorithms have also been shown to be successful for PAC learning in the presence of errors, probabilistic concepts, function learning and Markovian non-independent examples.\n\n", "related": "\n- Structural Risk Minimization\n- Computational learning theory\n"}
{"id": "54625345", "url": "https://en.wikipedia.org/wiki?curid=54625345", "title": "Right to explanation", "text": "Right to explanation\n\nIn the regulation of algorithms, particularly artificial intelligence and its subfield of machine learning, a right to explanation (or right to \"an\" explanation) is a right to be given an explanation for an output of the algorithm. Such rights primarily refer to individual rights to be given an explanation for decisions that significantly affect an individual, particularly legally or financially. For example, a person who applies for a loan and is denied may ask for an explanation, which could be \"Credit bureau X reports that you declared bankruptcy last year; this is the main factor in considering you too likely to default, and thus we will not give you the loan you applied for.\"\n\nSome such legal rights already exist, while the scope of a general \"right to explanation\" is a matter of ongoing debate.\n\nCredit score in the United States – more generally, credit actions – have a well-established right to explanation. Under the Equal Credit Opportunity Act (Regulation B of the Code of Federal Regulations),\nTitle 12, Chapter X, Part 1002, §1002.9, creditors are required to notify applicants of action taken in certain circumstances, and such notifications must provide specific reasons, as detailed in §1002.9(b)(2):\n\nThe official interpretation of this section details what types of statements are acceptable.\n\nCredit agencies and data analysis firms such as FICO comply with this regulation by providing a list of reasons (generally at most 4, per interpretation of regulations), consisting of a numeric (as identifier) and an associated explanation, identifying the main factors affecting a credit score. An example might be:\n\nThe European Union General Data Protection Regulation (enacted 2016, taking effect 2018), extends the automated decision-making rights in the 1995 Data Protection Directive to provide a legally disputed form of a right to an explanation, stated as such in Recital 71: \"[the data subject should have] the right ... to obtain an explanation of the decision reached\". In full:\n\nHowever, the extent to which the regulations themselves provide a \"right to explanation\" is heavily debated. There are two main strands of criticism. There are significant legal issues with the right as found in Article 22 — as recitals are not binding, and the right to an explanation is not mentioned in the binding articles of the text, having been removed during the legislative process. In addition, there are significant restrictions on the types of automated decisions that are covered — which must be both \"solely\" based on automated processing, and have legal or similarly significant effects — which significantly limits the range of automated systems and decisions to which the right would apply. In particular, the right is unlikely to apply in many of the cases of algorithmic controversy that have been picked up in the media.\n\nA second potential source of such a right has been pointed to in Article 15, the \"right of access by the data subject\". This restates a similar provision from the 1995 Data Protection Directive, allowing the data subject access to \"meaningful information about the logic involved\" in the same significant, solely automated decision-making, found in Article 22. Yet this too suffers from alleged challenges that relate to the timing of when this right can be drawn upon, as well as practical challenges that mean it may not be binding in many cases of public concern.\n\nIn France the 2016 \"Loi pour une République numérique\" (Digital Republic Act or \"loi numérique\") amends the country's administrative code to introduce a new provision for the explanation of decisions made by public sector bodies about individuals. It notes that where there is \"a decision taken on the basis of an algorithmic treatment\", the rules that define that treatment and its “principal characteristics” must be communicated to the citizen upon request, where there is not an exclusion (e.g. for national security or defence). These should include the following:\n1. the degree and the mode of contribution of the algorithmic processing to the decision- making;\n2. the data processed and its source;\n3. the treatment parameters, and where appropriate, their weighting, applied to the situation of the person concerned;\n4. the operations carried out by the treatment.\nScholars have noted that this right, while limited to administrative decisions, goes beyond the GDPR right to explicitly apply to decision support rather than decisions \"solely\" based on automated processing, as well as provides a framework for explaining specific decisions. Indeed, the GDPR automated decision-making rights in the European Union, one of the places a \"right to an explanation\" has been sought within, find their origins in French law in the late 1970s.\n\nSome argue that a \"right to explanation\" is at best unnecessary, at worst harmful, and threatens to stifle innovation. Specific criticisms include: favoring human decisions over machine decisions; being redundant with existing laws; and focusing on process over outcome.\n\nMore fundamentally, many algorithms used in machine learning are not easily explainable. For example, the output of a deep neural network depends on many layers of computations, connected in a complex way, and no one input or computation may be a dominant factor. The field of Explainable AI seeks to provide better explanations from existing algorithms, and algorithms that are more easily explainable, but it is a young and active field.\n\nSimilarly, human decisions often cannot be easily explained: they may be based on intuition or a \"gut feeling\" that is hard to put into words. Some would argue that machines should not be required to meet a higher standard than humans.\n\n", "related": "\n- Explainable artificial intelligence\n- Regulation of algorithms\n\n- Artificial Intelligence Owes You an Explanation, by John Frank Weaver, \"Slate\", May 8, 2017\n"}
{"id": "55075082", "url": "https://en.wikipedia.org/wiki?curid=55075082", "title": "BigDL", "text": "BigDL\n\nBigDL is a distributed deep learning framework for Apache Spark, created by Jason Dai at Intel.\n\nIt is hosted at GitHub.\n\n", "related": "\n- Comparison of deep learning software\n"}
{"id": "55375136", "url": "https://en.wikipedia.org/wiki?curid=55375136", "title": "Highway network", "text": "Highway network\n\nIn machine learning, a highway network is an approach to optimizing networks and increasing their depth. Highway networks use learned gating mechanisms to regulate information flow, inspired by Long Short-Term Memory (LSTM) recurrent neural networks. The gating mechanisms allow neural networks to have paths for information to follow across different layers (\"information highways\").\n\nHighway networks have been used as part of text sequence labeling and speech recognition tasks.\n\nThe model has two gates in addition to the H(W, x) gate: the transform gate T(W, x) and the carry gate C(W, x). Those two last gates are non-linear transfer functions (by convention Sigmoid function). The H(W, x) function can be any desired transfer function.\n\nThe carry gate is defined as C(W, x) = 1 - T(W, x). While the transform gate is just a gate with a sigmoid transfer function.\n\nThe structure of a hidden layer follows the equation:\n\nformula_1\nThe advantage of a Highway Network over the common deep neural networks is that solves or prevents partially the Vanishing gradient problem, thus leading to easier to optimize neural networks.\n", "related": "NONE"}
{"id": "54994687", "url": "https://en.wikipedia.org/wiki?curid=54994687", "title": "Documenting Hate", "text": "Documenting Hate\n\nDocumenting Hate is a project of ProPublica, in collaboration with a number of journalistic, academic, and computing organizations, for systematic tracking of hate crimes and bias incidents. It uses an online form to facilitate reporting of incidents by the general public. Since August 2017, it has also used machine learning and natural language processing techniques to monitor and collect news stories about hate crimes and bias incidents. , over 100 news organizations had joined the project.\n\nDocumenting Hate was created in response to ProPublica's dissatisfaction with the quality of reporting and tracking of evidence of hate crimes and bias incidents after the United States presidential election of 2016. The project was launched on 17 January 2017, after the publication on 15 November 2016 of a ProPublica news story about the difficulty of obtaining hard data on hate crimes.\n\nOn 18 August 2017, ProPublica and Google announced the creation of the Documenting Hate News Index, which uses the Google Cloud Natural Language API for automated monitoring and collection of news stories about hate crimes and bias incidents. The API uses machine learning and natural language processing techniques. The findings of the Index are integrated with reports from members of the public. The Index is a joint project of ProPublica, Google News Lab, and the data visualization studio Pitch Interactive.\n\n, thousands of incidents had been reported via Documenting Hate. , over 100 news organizations had joined the project, including the \"Boston Globe\", the \"New York Times\", \"Vox\", and the Georgetown University \"Hoya\".\n\nA policy analyst for the Center for Data Innovation (an affiliate of the Information Technology and Innovation Foundation), while supporting ProPublica's critique of the present state of hate-crime statistics, and praising ProPublica for drawing attention to the problem, has argued that a nongovernmental project like Documenting Hate cannot solve it unaided; instead, intervention at the federal level is needed.\n\n", "related": "\n- Unite the Right rally\n\n- Documenting Hate on ProPublica (www.documentinghate.com redirects to this ProPublica page)\n- Documenting Hate News Index\n- Google News Lab\n- Google Cloud Natural Language API\n- Pitch Interactive\n"}
{"id": "1363880", "url": "https://en.wikipedia.org/wiki?curid=1363880", "title": "Random forest", "text": "Random forest\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nThe first algorithm for random decision forests was created by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.\n\nAn extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered \"Random Forests\" as a trademark (, owned by Minitab, Inc.). The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.\n\nThe general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions. A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions. Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting. The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.\nThe early development of Breiman's notion of random forests was influenced by the work of Amit and\nGeman who introduced the idea of searching over a random subset of the\navailable decisions when splitting a node, in the context of growing a single\ntree. The idea of random subspace selection from Ho was also influential in the design of random forests. In this method a forest of trees is grown,\nand variation among the trees is introduced by projecting the training data\ninto a randomly chosen subspace before fitting each tree or each node. Finally, the idea of\nrandomized node optimization, where the decision at each node is selected by a\nrandomized procedure, rather than a deterministic optimization was first\nintroduced by Dietterich.\n\nThe introduction of random forests proper was first made in a paper\nby Leo Breiman. This paper describes a method of building a forest of\nuncorrelated trees using a CART like procedure, combined with randomized node\noptimization and bagging. In addition, this paper combines several\ningredients, some previously known and some novel, which form the basis of the\nmodern practice of random forests, in particular:\n\n1. Using out-of-bag error as an estimate of the generalization error.\n2. Measuring variable importance through permutation.\n\nThe report also offers the first theoretical result for random forests in the\nform of a bound on the generalization error which depends on the strength of the\ntrees in the forest and their correlation.\n\nDecision trees are a popular method for various machine learning tasks. Tree learning \"come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining\", say Hastie \"et al.\", \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".\n\nIn particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n\nThe training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set = , ..., with responses = , ..., , bagging repeatedly (\"B\" times) selects a random sample with replacement of the training set and fits trees to these samples:\n\nAfter training, predictions for unseen samples can be made by averaging the predictions from all the individual regression trees on :\n\nor by taking the majority vote in the case of classification trees.\n\nThis bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n\nAdditionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on :\n\nThe number of samples/trees, , is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees can be found using cross-validation, or by observing the \"out-of-bag error\": the mean prediction error on each training sample , using only the trees that did not have in their bootstrap sample.\nThe training and test error tend to level off after some number of trees have been fit.\n\nThe above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.\n\nTypically, for a classification problem with features, (rounded down) features are used in each split. For regression problems the inventors recommend (rounded down) with a minimum node size of 5 as the default.. In practice the best values for these parameters will depend on the problem, and they should be treated as tuning parameters.\n\nAdding one further step of randomization yields \"extremely randomized trees\", or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally \"optimal\" cut-point for each feature under consideration (based on, e.g., information gain or the Gini impurity), a \"random\" cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are formula_3 for classification and formula_4 for regression, where formula_4 is the number of features in the model. \n\nRandom forests can be used to rank the importance of variables in a regression or classification problem in a natural way. The following technique was described in Breiman's original paper and is implemented in the R package randomForest.\n\nThe first step in measuring the variable importance in a data set formula_6 is to fit a random forest to the data. During the fitting process the out-of-bag error for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).\n\nTo measure the importance of the formula_7-th feature after training, the values of the formula_7-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set. The importance score for the formula_7-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees. The score is normalized by the standard deviation of these differences.\n\nFeatures which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu \"et al.\"\n\nThis method of determining variable importance has some drawbacks. For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as partial permutations\nand growing unbiased treescan be used to solve the problem. If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.\n\nA relationship between random forests and the -nearest neighbor algorithm (-NN) was pointed out by Lin and Jeon in 2002. It turns out that both can be viewed as so-called \"weighted neighborhoods schemes\". These are models built from a training set formula_10 that make predictions formula_11 for new points by looking at the \"neighborhood\" of the point, formalized by a weight function :\n\nHere, formula_13 is the non-negative weight of the 'th training point relative to the new point in the same tree. For any particular , the weights for points formula_14 must sum to one. Weight functions are given as follows:\n\n- In -NN, the weights are formula_15 if is one of the points closest to , and zero otherwise.\n- In a tree, formula_16 if is one of the points in the same leaf as , and zero otherwise.\n\nSince a forest averages the predictions of a set of trees with individual weight functions formula_17, its predictions are\n\nThis shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of in this interpretation are the points formula_14 sharing the same leaf in any tree formula_7. In this way, the neighborhood of depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.\n\nAs part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.\nThe observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.\n\nInstead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.\n\nIn machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.\n\nLeo Breiman was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest and uniform random forest, two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n\nCentered forest is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level formula_21 is built, where formula_22 is a parameter of the algorithm.\n\nUniform forest is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n\nGiven a training sample formula_23 of formula_24-valued independent random variables distributed as the independent prototype pair formula_25, where formula_26. We aim at predicting the response formula_27, associated with the random variable formula_28, by estimating the regression function formula_29. A random regression forest is an ensemble of formula_30 randomized regression trees. Denote formula_31 the predicted value at point formula_32 by the formula_7-th tree, where formula_34 are independent random variables, distributed as a generic random variable formula_35, independent of the sample formula_36. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate formula_37.\nFor regression trees, we have formula_38, where formula_39 is the cell containing formula_32, designed with randomness formula_41 and dataset formula_36, and formula_43.\n\nThus random forest estimates satisfy, for all formula_44, formula_45. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet defined KeRF by\n\nwhich is equal to the mean of the formula_47's falling in the cells containing formula_32 in the forest. If we define the connection function of the formula_30 finite forest as formula_50, i.e. the proportion of cells shared between formula_32 and formula_52, then almost surely we have formula_53, which defines the KeRF.\n\nThe construction of Centered KeRF of level formula_21 is the same as for centered forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nUniform KeRF is built in the same way as uniform forest, except that predictions are made by formula_55, the corresponding kernel function, or connection function is\n\nPredictions given by KeRF and random forests are close if the number of points in each cell is controlled:\nAssume that there exist sequences formula_59 such that, almost surely,\nThen almost surely,\nWhen the number of trees formula_30 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\nAssume that there exist sequences formula_63 such that, almost surely\n- formula_64\n- formula_65\n- formula_66\nThen almost surely,\nAssume that formula_68, where formula_69 is a centered Gaussian noise, independent of formula_28, with finite variance formula_71. Moreover, formula_28 is uniformly distributed on formula_73 and formula_74 is Lipschitz. Scornet proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n\nProviding formula_75 and formula_76, there exists a constant formula_77 such that, for all formula_78,\nformula_79.\n\nProviding formula_75 and formula_76, there exists a constant formula_82 such that,\nformula_83.\n\nThe algorithm is often used in scientific works because of its advantages. For example, it can be used for quality assessment of Wikipedia articles.\n\n- The Original RF by Breiman and Cutler written in Fortran 77.\n- ALGLIB contains a modification of the random forest in C#, C++, Pascal, VBA.\n- party Implementation based on the conditional inference trees in R.\n- randomForest for classification and regression in R.\n- Python implementation with examples in scikit-learn.\n- Orange data mining suite includes random forest learner and can visualize the trained forest.\n- Matlab implementation.\n- SQP software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.\n- Weka RandomForest in Java library and GUI.\n- ranger A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for R.\n\n", "related": "\n- Boosting\n- Decision tree learning\n- Ensemble learning\n- Gradient boosting\n- Non-parametric statistics\n- Randomized algorithm\n\n\n- Random Forests classifier description (Leo Breiman's site)\n- Liaw, Andy & Wiener, Matthew \"Classification and Regression by randomForest\" R News (2002) Vol. 2/3 p. 18 (Discussion of the use of the random forest package for R)\n"}
{"id": "55817338", "url": "https://en.wikipedia.org/wiki?curid=55817338", "title": "Algorithmic bias", "text": "Algorithmic bias\n\nAlgorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others. Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination. This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation.\n\nAs algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.\n\nAlgorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. Problems in understanding, researching, and discovering algorithmic bias stem from the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n\nAlgorithms are difficult to define, but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output. For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence. By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more.\n\nContemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality. The term \"algorithmic bias\" describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as \"biased\". This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).\n\nBias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria. Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded. Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers. Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.\n\nBeyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores). Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. This criteria could present unanticipated outcomes for search results, such as in flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths. Algorithms may also display an \"uncertainty bias\", offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.\n\nThe earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book \"Computer Power and Human Reason\", Artificial Intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.\n\nWeizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law,\" that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including his or her biases and expectations. While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decisionmaking processes\" as data is being selected.\n\nFinally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results. Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.\n\nAn early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions.\n\nThough well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten. In theory, these biases may create new patterns of behavior, or \"scripts,\" in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.\n\nThe decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist, a process described by author Clay Shirky as \"algorithmic authority\". Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources,\" such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity.\n\nBecause of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans. This can have the effect of reducing alternative options, compromises, or flexibility. Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.\n\nConcerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,\nand Transparency in Machine Learning. Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences. In recent years, the study of the Fairness, Accountability,\nand Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAT*.\nHowever, FAT has come under serious criticism itself due to dubious funding.\n\nPre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious. Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines. Encoding pre-existing bias into software can preserve social and institutional bias, and without correction, could be replicated in all future uses of that algorithm.\n\nAn example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new UK citizens after the 1981 British Nationality Act. The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\" In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.\n\nTechnical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.\n\nA \"decontextualized algorithm\" uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.\n\nLastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury. Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.\n\nEmergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms. This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion. Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.\n\nIn 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.\n\nAdditional emergent biases include:\n\nUnpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data. In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.\n\nEmergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand. These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.\n\nApart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of UK immigration law.\n\nEmergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.\n\nRecommender systems such as those used to recommend online videos or news articles can create feedback loops. When users click on content that is suggested by algorithms, it influences the next set of suggestions. Over time this may lead to users entering a Filter Bubble and being unaware of important or useful content.\n\nCorporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.\n\nIn a 1998 paper describing Google, it was shown that the founders of the company adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\" This bias would be an \"invisible\" manipulation of the user.\n\nA series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated.\n\nIn 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew,\" but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.\n\nIn 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners. Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.\n\nWeb search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\". In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites. Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. In fact, current machine translation systems fail to reproduce the real world distribution of female workers. \n\nIn 2018, Amazon.com turned off a system it developed to screen job applications when they realized it was biased against women.\n\nAlgorithms have been criticized as a method for obscuring racial prejudices in decision-making. Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data.\n\nOne example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminals's father was a consideration in those risk assessment scores. Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.\n\nIn 2015, Google apologized when black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points. Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.\n\nBiometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.\n\nOne study that set out to examine “Risk, Race, & Recidivism: Predictive Bias and Disparate Impact” alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.\n\nIn 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.\n\nA study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on \"creditworthiness\" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.\n\nIn 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks,\" whereas \"all white men\" would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.\n\nAlthough algorithms are a great way to flag hate speech, there has been a racial bias noticed from the algorithms. Algorithms were found to be 1 and a half more likely to flag information if it was posted by an African American person and 2.2 times likely to flag information as hate speech if written in Ebonics. The study was completed at the University of Washington in the year of 2019. The researchers looked at flagged hate speech on Twitter. There are slurs and epithets that communities have re-appropriated, and people within that community aren't using offensively, such as \"queer\" and the \"N-word\", but algorithms don't understand the context in which these words are being used and will flag the content anyway. \n\nSurveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. A 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites. Additional studies of facial recognition software have found the opposite to be true when trained on non-criminal databases, with the software being the least accurate in identifying darker-skinned females.\n\nIn 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in \"The Atlantic\", arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its \"adult content\" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel \"Brokeback Mountain\".\n\nIn 2019, it was found that on Facebook, searches for \"photos of my female friends\" yielded suggestions such as \"in bikinis\" or \"at the beach\". In contrast, searches for \"photos of my male friends\" yielded no results.\n\nFacial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, a instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.\n\nThere has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images. The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being \"outed\" against their will.\n\nWhile users generate results that are \"completed\" automatically, Google has failed to remove sexist and racist autocompletion text. In \"Algorithms of Oppression: How Search Engines Reinforce Racism,\" Safiya Noble notes an example of the search for \"black girls,\" which was reported to result in pornographic images. Due to Google's algorithm, it is unable to erase pages unless they qualify as unlawful.\n\nSeveral problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.\n\nAlgorithmic processes are complex, often exceeding the understanding of the people who use them. Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.\n\nSocial scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\" Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.\n\nAn example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms. Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.\n\nAdditional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms. One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.\n\nCommercial algorithms are proprietary, and may be treated as trade secrets. Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings. This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output. Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.\n\nA significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n\nSome practitioners have tried to estimate and impute these missing sensitive categorisations in order to allow bias mitigation, for example building systems to infer ethnicity from names, however this can introduce other forms of bias if not undertaken with care. Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.\n\nAlgorithmic bias does not only include protected categories, but can also concerns characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult. Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.\n\nA study of 84 policy guidelines on ethical AI found that fairness and \"mitigation of unwanted bias\" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts. \n\nThere have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent field focuses on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion).\n\nCurrently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.\n\nEthics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the \"right to understanding\" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for \"Explainable AI\" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.\n\nFrom a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias. This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes. Others propose the need for clear liability insurance mechanisms.\n\nAmid concerns that the design of AI systems is primarily the domain of white, male engineers, a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems. For example, just 12% of machine learning engineers are women, with black AI leaders pointing to a \"diversity crisis\" in the field. Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms. \n\nThe General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.\n\nThe GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting that ... the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.\n\nThe United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\". Intended only as guidance, the report did not create any legal precedent.\n\nIn 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required \"the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.\" The task force is required to present findings and recommendations for further regulatory action in 2019.\n\nOn July 31, 2018, a draft of the Personal Data Bill was presented. The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for \"...harm resulting from any processing or any kind of processing undertaken by the fiduciary\". It defines \"any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal\" or \"any discriminatory treatment\" as a source of harm that could arise from improper use of data. It also makes special provisions for people of \"Intersex status”.\n\n- Fairness (machine learning)\n", "related": "NONE"}
{"id": "55843837", "url": "https://en.wikipedia.org/wiki?curid=55843837", "title": "Automated machine learning", "text": "Automated machine learning\n\nAutomated machine learning (AutoML) is the process of automating the process of applying machine learning to real-world problems. AutoML covers the complete pipeline from the raw dataset to the deployable machine learning model. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning. The high degree of automation in AutoML allows non-experts to make use of machine learning models and techniques without requiring to become an expert in this field first. \n\nAutomating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. \n\nIn a typical machine learning application, practitioners have a dataset consisting of input data points to train on. The raw data itself may not be in a form such that all algorithms may be applicable to it out of the box. An expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods that make the dataset amenable for machine learning. Following those preprocessing steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their machine learning model. Clearly all of those steps induce their own challenges, accumulating to a significant hurdle to get started with machine learning.\n\nA downside are the additional parameters of AutoML tools, which may need some expertise to be set themselves. Although those hyperparameters exist, AutoML simplifies the application of machine learning for non-experts dramatically.\n\nAutomated machine learning can target various stages of the machine learning process. Essentially the targets can be grouped into the fields data preparation, feature engineering, model selection, selection of evaluation metrics, and hyperparameter optimization.\n- Automated data preparation and ingestion (from raw data and miscellaneous formats)\n- Automated column type detection; e.g., boolean, discrete numerical, continuous numerical, or text\n- Automated column intent detection; e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature\n- Automated task detection; e.g., binary classification, regression, clustering, or ranking\n- Automated feature engineering\n- Feature selection\n- Feature extraction\n- Meta learning and transfer learning\n- Detection and handling of skewed data and/or missing values\n- Automated model selection\n- Hyperparameter optimization of the learning algorithm and featurization\n- Automated pipeline selection under time, memory, and complexity constraints\n- Automated selection of evaluation metrics / validation procedures\n- Automated problem checking\n- Leakage detection\n- Misconfiguration detection\n- Automated analysis of results obtained\n- User interfaces and visualizations for automated machine learning\n\n", "related": "\n- Neural architecture search\n- Hyperparameter optimization\n- Model selection\n- Neuroevolution\n- Self-tuning\n"}
{"id": "30992863", "url": "https://en.wikipedia.org/wiki?curid=30992863", "title": "Proaftn", "text": "Proaftn\n\nProaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for: (PROcédure d'Affectation Floue pour la problématique du Tri Nominal), which means in English: Fuzzy Assignment Procedure for Nominal Sorting.\nThe method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set.\n\nTo resolve the classification problems, Proaftn proceeds by the following stages:\n\nStage 1. Modeling of classes: In this stage, the prototypes of the classes are conceived using the two following steps:\n\n- Step 1. Structuring: The prototypes and their parameters (thresholds, weights, etc.) are established using the available knowledge given by the expert.\n- Step 2. Validation: We use one of the two following techniques in order to validate or adjust the parameters obtained in the first step through the assignment examples known as a training set.\nDirect technique: It consists in adjusting the parameters through the training set and with the expert intervention.\n\nIndirect technique: It consists in fitting the parameters without the expert intervention as used in machine learning approaches. \nIn multicriteria classification problem, the indirect technique is known as \"preference disaggregation analysis\". This technique requires less cognitive effort than the former technique; it uses an automatic method to determine the optimal parameters, which minimize the classification errors.\nFurthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn.\n\nStage 2. Assignment: After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes.\n\n- Site dedicated to the sorting problematic of MCDA\n", "related": "NONE"}
{"id": "49082762", "url": "https://en.wikipedia.org/wiki?curid=49082762", "title": "List of datasets for machine-learning research", "text": "List of datasets for machine-learning research\n\nThese datasets are used for machine-learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.\n\nDatasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.\n\nIn computer vision, face images have been used extensively to develop facial recognition systems, face detection, and many other projects that use images of faces.\nDatasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.\n\nDatasets of sounds and sound features.\n\nDatasets containing electric signal information requiring some sort of Signal processing for further analysis.\n\nDatasets from physical systems.\n\nDatasets from biological systems.\n\nThis section includes datasets that deals with structured data.\n\nDatasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.\n\nAs datasets come in myriad formats and can sometimes be difficult to use, there has been considerable work put into curating and standardizing the format of datasets to make them easier to use for machine learning research.\n\n- OpenML: Web platform with Python, R, Java, and other APIs for downloading hundreds of machine learning datasets, evaluating algorithms on datasets, and benchmarking algorithm performance against dozens of other algorithms.\n- PMLB: A large, curated repository of benchmark datasets for evaluating supervised machine learning algorithms. Provides classification and regression datasets in a standardized format that are accessible through a Python API.\n\n", "related": "\n- Comparison of deep learning software\n- List of manual image annotation tools\n- List of biological databases\n"}
{"id": "53587467", "url": "https://en.wikipedia.org/wiki?curid=53587467", "title": "Outline of machine learning", "text": "Outline of machine learning\n\nThe following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.\n\n- An academic discipline\n- A branch of science\n- An applied science\n-  A subfield of computer science\n-   A branch of artificial intelligence\n-   A subfield of soft computing\n\nSubfields of machine learning\n- Computational learning theory – studying the design and analysis of machine learning algorithms.\n- Grammar induction\n- Meta learning\n\nCross-disciplinary fields involving machine learning\n- Adversarial machine learning\n- Predictive analytics\n- Quantum machine learning\n- Robot learning\n- Developmental robotics\n\nApplications of machine learning\n- Bioinformatics\n- Biomedical informatics\n- Computer vision\n- Customer relationship management –\n- Data mining\n- Email filtering\n- Inverted pendulum – balance and equilibrium system.\n- Natural language processing (NLP)\n- Automatic summarization\n- Automatic taxonomy construction\n- Dialog system\n- Grammar checker\n- Language recognition\n-  Handwriting recognition\n-  Optical character recognition\n-  Speech recognition\n- Machine translation\n- Question answering\n- Speech synthesis\n- Text mining\n-  Term frequency–inverse document frequency (tf–idf)\n- Text simplification\n- Pattern recognition\n- Facial recognition system\n- Handwriting recognition\n- Image recognition\n- Optical character recognition\n- Speech recognition\n- Recommendation system\n- Collaborative filtering\n- Content-based filtering\n- Hybrid recommender systems (Collaborative and content-based filtering)\n- Search engine\n- Search engine optimization\n- Social Engineering\n\nMachine learning hardware\n- Graphics processing unit\n- Tensor processing unit\n- Vision processing unit\n\nMachine learning tools   (list)\n- Comparison of deep learning software\n- Comparison of deep learning software/Resources\n\nMachine learning framework\n\nProprietary machine learning frameworks\n- Amazon Machine Learning\n- Microsoft Azure Machine Learning Studio\n- DistBelief – replaced by TensorFlow\n\nOpen source machine learning frameworks\n- Apache Singa\n- Apache MXNet\n- Caffe\n- PyTorch\n- mlpack\n- TensorFlow\n- Torch\n- CNTK\n- Accord.Net\n\nMachine learning library   \n- Deeplearning4j\n- Theano\n- Scikit-learn\n- Keras\n\nMachine learning algorithm\n\n- Almeida–Pineda recurrent backpropagation\n- ALOPEX\n- Backpropagation\n- Bootstrap aggregating\n- CN2 algorithm\n- Constructing skill trees\n- Dehaene–Changeux model\n- Diffusion map\n- Dominance-based rough set approach\n- Dynamic time warping\n- Error-driven learning\n- Evolutionary multimodal optimization\n- Expectation–maximization algorithm\n- FastICA\n- Forward–backward algorithm\n- GeneRec\n- Genetic Algorithm for Rule Set Production\n- Growing self-organizing map\n- Hyper basis function network\n- IDistance\n- K-nearest neighbors algorithm\n- Kernel methods for vector output\n- Kernel principal component analysis\n- Leabra\n- Linde–Buzo–Gray algorithm\n- Local outlier factor\n- Logic learning machine\n- LogitBoost\n- Manifold alignment\n- Markov chain Monte Carlo (MCMC)\n- Minimum redundancy feature selection\n- Mixture of experts\n- Multiple kernel learning\n- Non-negative matrix factorization\n- Online machine learning\n- Out-of-bag error\n- Prefrontal cortex basal ganglia working memory\n- PVLV\n- Q-learning\n- Quadratic unconstrained binary optimization\n- Query-level feature\n- Quickprop\n- Radial basis function network\n- Randomized weighted majority algorithm\n- Reinforcement learning\n- Repeated incremental pruning to produce error reduction (RIPPER)\n- Rprop\n- Rule-based machine learning\n- Skill chaining\n- Sparse PCA\n- State–action–reward–state–action\n- Stochastic gradient descent\n- Structured kNN\n- T-distributed stochastic neighbor embedding\n- Temporal difference learning\n- Wake-sleep algorithm\n- Weighted majority algorithm (machine learning)\n\nMachine learning method   (list)\n- Instance-based algorithm\n- K-nearest neighbors algorithm (KNN)\n- Learning vector quantization (LVQ)\n- Self-organizing map (SOM)\n- Regression analysis\n- Logistic regression\n- Ordinary least squares regression (OLSR)\n- Linear regression\n- Stepwise regression\n- Multivariate adaptive regression splines (MARS)\n- Regularization algorithm\n- Ridge regression\n- Least Absolute Shrinkage and Selection Operator (LASSO)\n- Elastic net\n- Least-angle regression (LARS)\n- Classifiers\n- Probabilistic classifier\n-  Naive Bayes classifier\n- Binary classifier\n- Linear classifier\n- Hierarchical classifier\n\nDimensionality reduction\n- Canonical correlation analysis (CCA)\n- Factor analysis\n- Feature extraction\n- Feature selection\n- Independent component analysis (ICA)\n- Linear discriminant analysis (LDA)\n- Multidimensional scaling (MDS)\n- Non-negative matrix factorization (NMF)\n- Partial least squares regression (PLSR)\n- Principal component analysis (PCA)\n- Principal component regression (PCR)\n- Projection pursuit\n- Sammon mapping\n- t-distributed stochastic neighbor embedding (t-SNE)\n\nEnsemble learning\n- AdaBoost\n- Boosting\n- Bootstrap aggregating (Bagging)\n- Ensemble averaging – process of creating multiple models and combining them to produce a desired output, as opposed to creating just one model. Frequently an ensemble of models performs better than any individual model, because the various errors of the models \"average out.\"\n- Gradient boosted decision tree (GBDT)\n- Gradient boosting machine (GBM)\n- Random Forest\n- Stacked Generalization (blending)\n\nMeta learning\n- Inductive bias\n- Metadata\n\nReinforcement learning\n- Q-learning\n- State–action–reward–state–action (SARSA)\n- Temporal difference learning (TD)\n- Learning Automata\n\nSupervised learning\n- AODE\n- Artificial neural network\n- Association rule learning algorithms\n- Apriori algorithm\n- Eclat algorithm\n- Case-based reasoning\n- Gaussian process regression\n- Gene expression programming\n- Group method of data handling (GMDH)\n- Inductive logic programming\n- Instance-based learning\n- Lazy learning\n- Learning Automata\n- Learning Vector Quantization\n- Logistic Model Tree\n- Minimum message length (decision trees, decision graphs, etc.)\n- Nearest Neighbor Algorithm\n- Analogical modeling\n- Probably approximately correct learning (PAC) learning\n- Ripple down rules, a knowledge acquisition methodology\n- Symbolic machine learning algorithms\n- Support vector machines\n- Random Forests\n- Ensembles of classifiers\n- Bootstrap aggregating (bagging)\n- Boosting (meta-algorithm)\n- Ordinal classification\n- Information fuzzy networks (IFN)\n- Conditional Random Field\n- ANOVA\n- Quadratic classifiers\n- k-nearest neighbor\n- Boosting\n- SPRINT\n- Bayesian networks\n- Naive Bayes\n- Hidden Markov models\n- Hierarchical hidden Markov model\n\nBayesian statistics\n- Bayesian knowledge base\n- Naive Bayes\n- Gaussian Naive Bayes\n- Multinomial Naive Bayes\n- Averaged One-Dependence Estimators (AODE)\n- Bayesian Belief Network (BBN)\n- Bayesian Network (BN)\n\nDecision tree algorithm\n- Decision tree\n- Classification and regression tree (CART)\n- Iterative Dichotomiser 3 (ID3)\n- C4.5 algorithm\n- C5.0 algorithm\n- Chi-squared Automatic Interaction Detection (CHAID)\n- Decision stump\n- Conditional decision tree\n- ID3 algorithm\n- Random forest\n- SLIQ\n\nLinear classifier\n- Fisher's linear discriminant\n- Linear regression\n- Logistic regression\n- Multinomial logistic regression\n- Naive Bayes classifier\n- Perceptron\n- Support vector machine\n\nUnsupervised learning\n- Expectation-maximization algorithm\n- Vector Quantization\n- Generative topographic map\n- Information bottleneck method\n\nArtificial neural network\n- Feedforward neural network\n- Extreme learning machine\n- Convolutional neural network\n- Recurrent neural network\n- Long short-term memory (LSTM)\n- Logic learning machine\n- Self-organizing map\n\nAssociation rule learning\n- Apriori algorithm\n- Eclat algorithm\n- FP-growth algorithm\n\nHierarchical clustering\n- Single-linkage clustering\n- Conceptual clustering\n\nCluster analysis\n- BIRCH\n- DBSCAN\n- Expectation-maximization (EM)\n- Fuzzy clustering\n- Hierarchical Clustering\n- K-means clustering\n- K-medians\n- Mean-shift\n- OPTICS algorithm\n\nAnomaly detection\n- k-nearest neighbors classification (\"k\"-NN)\n- Local outlier factor\n\nSemi-supervised learning\n- Active learning – special case of semi-supervised learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.\n- Generative models\n- Low-density separation\n- Graph-based methods\n- Co-training\n- Transduction\n\nDeep learning\n- Deep belief networks\n- Deep Boltzmann machines\n- Deep Convolutional neural networks\n- Deep Recurrent neural networks\n- Hierarchical temporal memory\n- Generative Adversarial Networks\n- Deep Boltzmann Machine (DBM)\n- Stacked Auto-Encoders\n\n- Anomaly detection\n- Association rules\n- Bias-variance dilemma\n- Classification\n- Multi-label classification\n- Clustering\n- Data Pre-processing\n- Empirical risk minimization\n- Feature engineering\n- Feature learning\n- Learning to rank\n- Occam learning\n- Online machine learning\n- PAC learning\n- Regression\n- Reinforcement Learning\n- Semi-supervised learning\n- Statistical learning\n- Structured prediction\n- Graphical models\n-  Bayesian network\n-  Conditional random field (CRF)\n-  Hidden Markov model (HMM)\n- Unsupervised learning\n- VC theory\n\n- List of artificial intelligence projects\n- List of datasets for machine learning research\n\nHistory of machine learning\n- Timeline of machine learning\n\nMachine learning projects\n- DeepMind\n- Google Brain\n\nMachine learning organizations\n- Knowledge Engineering and Machine Learning Group\n\n- Artificial Intelligence and Security (AISec) (co-located workshop with CCS)\n- Conference on Neural Information Processing Systems (NIPS)\n- ECML PKDD\n- International Conference on Machine Learning (ICML)\n- ML4ALL (Machine Learning For All)\n\nBooks about machine learning\n\n- \"Machine Learning\"\n- \"Journal of Machine Learning Research\" (JMLR)\n- \"Neural Computation\"\n\n- Alberto Broggi\n- Andrei Knyazev\n- Andrew McCallum\n- Andrew Ng\n- Anuraag Jain\n- Armin B. Cremers\n- Ayanna Howard\n- Barney Pell\n- Ben Goertzel\n- Ben Taskar\n- Bernhard Schölkopf\n- Brian D. Ripley\n- Christopher G. Atkeson\n- Corinna Cortes\n- Demis Hassabis\n- Douglas Lenat\n- Eric Xing\n- Ernst Dickmanns\n- Geoffrey Hinton – co-inventor of the backpropagation and contrastive divergence training algorithms\n- Hans-Peter Kriegel\n- Hartmut Neven\n- Heikki Mannila\n- Ian Goodfellow – Father of Generative & adversarial networks\n- Jacek M. Zurada\n- Jaime Carbonell\n- Jeremy Slovak\n- Jerome H. Friedman\n- John D. Lafferty\n- John Platt – invented SMO and Platt scaling\n- Julie Beth Lovins\n- Jürgen Schmidhuber\n- Karl Steinbuch\n- Katia Sycara\n- Leo Breiman – invented bagging and random forests\n- Lise Getoor\n- Luca Maria Gambardella\n- Léon Bottou\n- Marcus Hutter\n- Mehryar Mohri\n- Michael Collins\n- Michael I. Jordan\n- Michael L. Littman\n- Nando de Freitas\n- Ofer Dekel\n- Oren Etzioni\n- Pedro Domingos\n- Peter Flach\n- Pierre Baldi\n- Pushmeet Kohli\n- Ray Kurzweil\n- Rayid Ghani\n- Ross Quinlan\n- Salvatore J. Stolfo\n- Sebastian Thrun\n- Selmer Bringsjord\n- Sepp Hochreiter\n- Shane Legg\n- Stephen Muggleton\n- Steve Omohundro\n- Tom M. Mitchell\n- Trevor Hastie\n- Vasant Honavar\n- Vladimir Vapnik – co-inventor of the SVM and VC theory\n- Yann LeCun – invented convolutional neural networks\n- Yasuo Matsuyama\n- Yoshua Bengio\n- Zoubin Ghahramani\n\n", "related": "\n- Outline of artificial intelligence\n- Outline of computer vision\n- Outline of robotics\n\n- Accuracy paradox\n- Action model learning\n- Activation function\n- Activity recognition\n- ADALINE\n- Adaptive neuro fuzzy inference system\n- Adaptive resonance theory\n- Additive smoothing\n- Adjusted mutual information\n- AIVA\n- AIXI\n- AlchemyAPI\n- AlexNet\n- Algorithm selection\n- Algorithmic inference\n- Algorithmic learning theory\n- AlphaGo\n- AlphaGo Zero\n- Alternating decision tree\n- Apprenticeship learning\n- Causal Markov condition\n- Competitive learning\n- Concept learning\n- Decision tree learning\n- Distribution learning theory\n- Eager learning\n- End-to-end reinforcement learning\n- Error tolerance (PAC learning)\n- Explanation-based learning\n- Feature\n- GloVe\n- Hyperparameter\n- IBM Machine Learning Hub\n- Inferential theory of learning\n- Learning automata\n- Learning classifier system\n- Learning rule\n- Learning with errors\n- M-Theory (learning framework)\n- Machine learning control\n- Machine learning in bioinformatics\n- Margin\n- Markov chain geostatistics\n- Markov chain Monte Carlo (MCMC)\n- Markov information source\n- Markov logic network\n- Markov model\n- Markov random field\n- Markovian discrimination\n- Maximum-entropy Markov model\n- Multi-armed bandit\n- Multi-task learning\n- Multilinear subspace learning\n- Multimodal learning\n- Multiple instance learning\n- Multiple-instance learning\n- Never-Ending Language Learning\n- Offline learning\n- Parity learning\n- Population-based incremental learning\n- Predictive learning\n- Preference learning\n- Proactive learning\n- Proximal gradient methods for learning\n- Semantic analysis\n- Similarity learning\n- Sparse dictionary learning\n- Stability (learning theory)\n- Statistical learning theory\n- Statistical relational learning\n- Tanagra\n- Transfer learning\n- Variable-order Markov model\n- Version space learning\n- Waffles\n- Weka\n- Loss function\n- Loss functions for classification\n- Mean squared error (MSE)\n- Mean squared prediction error (MSPE)\n- Taguchi loss function\n- Low-energy adaptive clustering hierarchy\n\n- Anne O'Tate\n- Ant colony optimization algorithms\n- Anthony Levandowski\n- Anti-unification (computer science)\n- Apache Flume\n- Apache Giraph\n- Apache Mahout\n- Apache SINGA\n- Apache Spark\n- Apache SystemML\n- Aphelion (software)\n- Arabic Speech Corpus\n- Archetypal analysis\n- Arthur Zimek\n- Artificial ants\n- Artificial bee colony algorithm\n- Artificial development\n- Artificial immune system\n- Astrostatistics\n- Averaged one-dependence estimators\n- Bag-of-words model\n- Balanced clustering\n- Ball tree\n- Base rate\n- Bat algorithm\n- Baum–Welch algorithm\n- Bayesian hierarchical modeling\n- Bayesian interpretation of kernel regularization\n- Bayesian optimization\n- Bayesian structural time series\n- Bees algorithm\n- Behavioral clustering\n- Bernoulli scheme\n- Bias–variance tradeoff\n- Biclustering\n- BigML\n- Binary classification\n- Bing Predicts\n- Bio-inspired computing\n- Biogeography-based optimization\n- Biplot\n- Bondy's theorem\n- Bongard problem\n- Bradley–Terry model\n- BrownBoost\n- Brown clustering\n- Burst error\n- CBCL (MIT)\n- CIML community portal\n- CMA-ES\n- CURE data clustering algorithm\n- Cache language model\n- Calibration (statistics)\n- Canonical correspondence analysis\n- Canopy clustering algorithm\n- Cascading classifiers\n- Category utility\n- CellCognition\n- Cellular evolutionary algorithm\n- Chi-square automatic interaction detection\n- Chromosome (genetic algorithm)\n- Classifier chains\n- Cleverbot\n- Clonal selection algorithm\n- Cluster-weighted modeling\n- Clustering high-dimensional data\n- Clustering illusion\n- CoBoosting\n- Cobweb (clustering)\n- Cognitive computer\n- Cognitive robotics\n- Collostructional analysis\n- Common-method variance\n- Complete-linkage clustering\n- Computer-automated design\n- Concept class\n- Concept drift\n- Conference on Artificial General Intelligence\n- Conference on Knowledge Discovery and Data Mining\n- Confirmatory factor analysis\n- Confusion matrix\n- Congruence coefficient\n- Connect (computer system)\n- Consensus clustering\n- Constrained clustering\n- Constrained conditional model\n- Constructive cooperative coevolution\n- Correlation clustering\n- Correspondence analysis\n- Cortica\n- Coupled pattern learner\n- Cross-entropy method\n- Cross-validation (statistics)\n- Crossover (genetic algorithm)\n- Cuckoo search\n- Cultural algorithm\n- Cultural consensus theory\n- Curse of dimensionality\n- DADiSP\n- DARPA LAGR Program\n- Darkforest\n- Dartmouth workshop\n- DarwinTunes\n- Data Mining Extensions\n- Data exploration\n- Data pre-processing\n- Data stream clustering\n- Dataiku\n- Davies–Bouldin index\n- Decision boundary\n- Decision list\n- Decision tree model\n- Deductive classifier\n- DeepArt\n- DeepDream\n- Deep Web Technologies\n- Defining length\n- Dendrogram\n- Dependability state model\n- Detailed balance\n- Determining the number of clusters in a data set\n- Detrended correspondence analysis\n- Developmental robotics\n- Diffbot\n- Differential evolution\n- Discrete phase-type distribution\n- Discriminative model\n- Dissociated press\n- Distributed R\n- Dlib\n- Document classification\n- Documenting Hate\n- Domain adaptation\n- Doubly stochastic model\n- Dual-phase evolution\n- Dunn index\n- Dynamic Bayesian network\n- Dynamic Markov compression\n- Dynamic topic model\n- Dynamic unobserved effects model\n- EDLUT\n- ELKI\n- Edge recombination operator\n- Effective fitness\n- Elastic map\n- Elastic matching\n- Elbow method (clustering)\n- Emergent (software)\n- Encog\n- Entropy rate\n- Erkki Oja\n- Eurisko\n- European Conference on Artificial Intelligence\n- Evaluation of binary classifiers\n- Evolution strategy\n- Evolution window\n- Evolutionary Algorithm for Landmark Detection\n- Evolutionary algorithm\n- Evolutionary art\n- Evolutionary music\n- Evolutionary programming\n- Evolvability (computer science)\n- Evolved antenna\n- Evolver (software)\n- Evolving classification function\n- Expectation propagation\n- Exploratory factor analysis\n- F1 score\n- FLAME clustering\n- Factor analysis of mixed data\n- Factor graph\n- Factor regression model\n- Factored language model\n- Farthest-first traversal\n- Fast-and-frugal trees\n- Feature Selection Toolbox\n- Feature hashing\n- Feature scaling\n- Feature vector\n- Firefly algorithm\n- First-difference estimator\n- First-order inductive learner\n- Fish School Search\n- Fisher kernel\n- Fitness approximation\n- Fitness function\n- Fitness proportionate selection\n- Fluentd\n- Folding@home\n- Formal concept analysis\n- Forward algorithm\n- Fowlkes–Mallows index\n- Frederick Jelinek\n- Frrole\n- Functional principal component analysis\n- GATTO\n- GLIMMER\n- Gary Bryce Fogel\n- Gaussian adaptation\n- Gaussian process\n- Gaussian process emulator\n- Gene prediction\n- General Architecture for Text Engineering\n- Generalization error\n- Generalized canonical correlation\n- Generalized filtering\n- Generalized iterative scaling\n- Generalized multidimensional scaling\n- Generative adversarial network\n- Generative model\n- Genetic algorithm\n- Genetic algorithm scheduling\n- Genetic algorithms in economics\n- Genetic fuzzy systems\n- Genetic memory (computer science)\n- Genetic operator\n- Genetic programming\n- Genetic representation\n- Geographical cluster\n- Gesture Description Language\n- Geworkbench\n- Glossary of artificial intelligence\n- Glottochronology\n- Golem (ILP)\n- Google matrix\n- Grafting (decision trees)\n- Gramian matrix\n- Grammatical evolution\n- Granular computing\n- GraphLab\n- Graph kernel\n- Gremlin (programming language)\n- Growth function\n- HUMANT (HUManoid ANT) algorithm\n- Hammersley–Clifford theorem\n- Harmony search\n- Hebbian theory\n- Hidden Markov random field\n- Hidden semi-Markov model\n- Hierarchical hidden Markov model\n- Higher-order factor analysis\n- Highway network\n- Hinge loss\n- Holland's schema theorem\n- Hopkins statistic\n- Hoshen–Kopelman algorithm\n- Huber loss\n- IRCF360\n- Ian Goodfellow\n- Ilastik\n- Ilya Sutskever\n- Immunocomputing\n- Imperialist competitive algorithm\n- Inauthentic text\n- Incremental decision tree\n- Induction of regular languages\n- Inductive bias\n- Inductive probability\n- Inductive programming\n- Influence diagram\n- Information Harvesting\n- Information fuzzy networks\n- Information gain in decision trees\n- Information gain ratio\n- Inheritance (genetic algorithm)\n- Instance selection\n- Intel RealSense\n- Interacting particle system\n- Interactive machine translation\n- International Joint Conference on Artificial Intelligence\n- International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics\n- International Semantic Web Conference\n- Iris flower data set\n- Island algorithm\n- Isotropic position\n- Item response theory\n- Iterative Viterbi decoding\n- JOONE\n- Jabberwacky\n- Jaccard index\n- Jackknife variance estimates for random forest\n- Java Grammatical Evolution\n- Joseph Nechvatal\n- Jubatus\n- Julia (programming language)\n- Junction tree algorithm\n- K-SVD\n- K-means++\n- K-medians clustering\n- K-medoids\n- KNIME\n- KXEN Inc.\n- K q-flats\n- Kaggle\n- Kalman filter\n- Katz's back-off model\n- Kernel adaptive filter\n- Kernel density estimation\n- Kernel eigenvoice\n- Kernel embedding of distributions\n- Kernel method\n- Kernel perceptron\n- Kernel random forest\n- Kinect\n- Klaus-Robert Müller\n- Kneser–Ney smoothing\n- Knowledge Vault\n- Knowledge integration\n- LIBSVM\n- LPBoost\n- Labeled data\n- LanguageWare\n- Language Acquisition Device (computer)\n- Language identification in the limit\n- Language model\n- Large margin nearest neighbor\n- Latent Dirichlet allocation\n- Latent class model\n- Latent semantic analysis\n- Latent variable\n- Latent variable model\n- Lattice Miner\n- Layered hidden Markov model\n- Learnable function class\n- Least squares support vector machine\n- Leave-one-out error\n- Leslie P. Kaelbling\n- Linear genetic programming\n- Linear predictor function\n- Linear separability\n- Lingyun Gu\n- Linkurious\n- Lior Ron (business executive)\n- List of genetic algorithm applications\n- List of metaphor-based metaheuristics\n- List of text mining software\n- Local case-control sampling\n- Local independence\n- Local tangent space alignment\n- Locality-sensitive hashing\n- Log-linear model\n- Logistic model tree\n- Low-rank approximation\n- Low-rank matrix approximations\n- MATLAB\n- MIMIC (immunology)\n- MXNet\n- Mallet (software project)\n- Manifold regularization\n- Margin-infused relaxed algorithm\n- Margin classifier\n- Mark V. Shaney\n- Massive Online Analysis\n- Matrix regularization\n- Matthews correlation coefficient\n- Mean shift\n- Mean squared error\n- Mean squared prediction error\n- Measurement invariance\n- Medoid\n- MeeMix\n- Melomics\n- Memetic algorithm\n- Meta-optimization\n- Mexican International Conference on Artificial Intelligence\n- Michael Kearns (computer scientist)\n- MinHash\n- Mixture model\n- Mlpy\n- Models of DNA evolution\n- Moral graph\n- Mountain car problem\n- Movidius\n- Multi-armed bandit\n- Multi-label classification\n- Multi expression programming\n- Multiclass classification\n- Multidimensional analysis\n- Multifactor dimensionality reduction\n- Multilinear principal component analysis\n- Multiple correspondence analysis\n- Multiple discriminant analysis\n- Multiple factor analysis\n- Multiple sequence alignment\n- Multiplicative weight update method\n- Multispectral pattern recognition\n- Mutation (genetic algorithm)\n- MysteryVibe\n- N-gram\n- NOMINATE (scaling method)\n- Native-language identification\n- Natural Language Toolkit\n- Natural evolution strategy\n- Nearest-neighbor chain algorithm\n- Nearest centroid classifier\n- Nearest neighbor search\n- Neighbor joining\n- Nest Labs\n- NetMiner\n- NetOwl\n- Neural Designer\n- Neural Engineering Object\n- Neural Lab\n- Neural modeling fields\n- Neural network software\n- NeuroSolutions\n- Neuro Laboratory\n- Neuroevolution\n- Neuroph\n- Niki.ai\n- Noisy channel model\n- Noisy text analytics\n- Nonlinear dimensionality reduction\n- Novelty detection\n- Nuisance variable\n- Numenta\n- One-class classification\n- Onnx\n- OpenNLP\n- Optimal discriminant analysis\n- Oracle Data Mining\n- Orange (software)\n- Ordination (statistics)\n- Overfitting\n- PROGOL\n- PSIPRED\n- Pachinko allocation\n- PageRank\n- Parallel metaheuristic\n- Parity benchmark\n- Part-of-speech tagging\n- Particle swarm optimization\n- Path dependence\n- Pattern language (formal languages)\n- Peltarion Synapse\n- Perplexity\n- Persian Speech Corpus\n- Picas (app)\n- Pietro Perona\n- Pipeline Pilot\n- Piranha (software)\n- Pitman–Yor process\n- Plate notation\n- Polynomial kernel\n- Pop music automation\n- Population process\n- Portable Format for Analytics\n- Predictive Model Markup Language\n- Predictive state representation\n- Preference regression\n- Premature convergence\n- Principal geodesic analysis\n- Prior knowledge for pattern recognition\n- Prisma (app)\n- Probabilistic Action Cores\n- Probabilistic context-free grammar\n- Probabilistic latent semantic analysis\n- Probabilistic soft logic\n- Probability matching\n- Probit model\n- Product of experts\n- Programming with Big Data in R\n- Proper generalized decomposition\n- Pruning (decision trees)\n- Pushpak Bhattacharyya\n- Q methodology\n- Qloo\n- Quality control and genetic algorithms\n- Quantum Artificial Intelligence Lab\n- Queueing theory\n- Quick, Draw!\n- R (programming language)\n- Rada Mihalcea\n- Rademacher complexity\n- Radial basis function kernel\n- Rand index\n- Random indexing\n- Random projection\n- Random subspace method\n- Ranking SVM\n- RapidMiner\n- Rattle GUI\n- Raymond Cattell\n- Reasoning system\n- Regularization perspectives on support vector machines\n- Relational data mining\n- Relationship square\n- Relevance vector machine\n- Relief (feature selection)\n- Renjin\n- Repertory grid\n- Representer theorem\n- Reward-based selection\n- Richard Zemel\n- Right to explanation\n- RoboEarth\n- Robust principal component analysis\n- RuleML Symposium\n- Rule induction\n- Rules extraction system family\n- SAS (software)\n- SNNS\n- SPSS Modeler\n- SUBCLU\n- Sample complexity\n- Sample exclusion dimension\n- Santa Fe Trail problem\n- Savi Technology\n- Schema (genetic algorithms)\n- Search-based software engineering\n- Selection (genetic algorithm)\n- Self-Service Semantic Suite\n- Semantic folding\n- Semantic mapping (statistics)\n- Semidefinite embedding\n- Sense Networks\n- Sensorium Project\n- Sequence labeling\n- Sequential minimal optimization\n- Shattered set\n- Shogun (toolbox)\n- Silhouette (clustering)\n- SimHash\n- SimRank\n- Similarity measure\n- Simple matching coefficient\n- Simultaneous localization and mapping\n- Sinkov statistic\n- Sliced inverse regression\n- Snakes and Ladders\n- Soft independent modelling of class analogies\n- Soft output Viterbi algorithm\n- Solomonoff's theory of inductive inference\n- SolveIT Software\n- Spectral clustering\n- Spike-and-slab variable selection\n- Statistical machine translation\n- Statistical parsing\n- Statistical semantics\n- Stefano Soatto\n- Stephen Wolfram\n- Stochastic block model\n- Stochastic cellular automaton\n- Stochastic diffusion search\n- Stochastic grammar\n- Stochastic matrix\n- Stochastic universal sampling\n- Stress majorization\n- String kernel\n- Structural equation modeling\n- Structural risk minimization\n- Structured sparsity regularization\n- Structured support vector machine\n- Subclass reachability\n- Sufficient dimension reduction\n- Sukhotin's algorithm\n- Sum of absolute differences\n- Sum of absolute transformed differences\n- Swarm intelligence\n- Switching Kalman filter\n- Symbolic regression\n- Synchronous context-free grammar\n- Syntactic pattern recognition\n- TD-Gammon\n- TIMIT\n- Teaching dimension\n- Teuvo Kohonen\n- Textual case-based reasoning\n- Theory of conjoint measurement\n- Thomas G. Dietterich\n- Thurstonian model\n- Topic model\n- Tournament selection\n- Training, test, and validation sets\n- Transiogram\n- Trax Image Recognition\n- Trigram tagger\n- Truncation selection\n- Tucker decomposition\n- UIMA\n- UPGMA\n- Ugly duckling theorem\n- Uncertain data\n- Uniform convergence in probability\n- Unique negative dimension\n- Universal portfolio algorithm\n- User behavior analytics\n- VC dimension\n- VIGRA\n- Validation set\n- Vapnik–Chervonenkis theory\n- Variable-order Bayesian network\n- Variable kernel density estimation\n- Variable rules analysis\n- Variational message passing\n- Varimax rotation\n- Vector quantization\n- Vicarious (company)\n- Viterbi algorithm\n- Vowpal Wabbit\n- WACA clustering algorithm\n- WPGMA\n- Ward's method\n- Weasel program\n- Whitening transformation\n- Winnow (algorithm)\n- Win–stay, lose–switch\n- Witness set\n- Wolfram Language\n- Wolfram Mathematica\n- Writer invariant\n- Xgboost\n- Yooreeka\n- Zeroth (software)\n\n- Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). \"The Elements of Statistical Learning\", Springer. .\n- Pedro Domingos (September 2015), The Master Algorithm, Basic Books,\n- Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). \"Foundations of Machine Learning\", The MIT Press. .\n- Ian H. Witten and Eibe Frank (2011). \"Data Mining: Practical machine learning tools and techniques\" Morgan Kaufmann, 664pp., .\n- David J. C. MacKay. \"Information Theory, Inference, and Learning Algorithms\" Cambridge: Cambridge University Press, 2003.\n- Richard O. Duda, Peter E. Hart, David G. Stork (2001) \"Pattern classification\" (2nd edition), Wiley, New York, .\n- Christopher Bishop (1995). \"Neural Networks for Pattern Recognition\", Oxford University Press. .\n- Vladimir Vapnik (1998). \"Statistical Learning Theory\". Wiley-Interscience, .\n- Ray Solomonoff, \"An Inductive Inference Machine\", IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.\n- Ray Solomonoff, \"An Inductive Inference Machine\" A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI.\n\n- Data Science: Data to Insights from MIT (machine learning)\n- Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford University's actual course taught by Ng, see.stanford.edu/Course/CS229 available for free].\n- mloss is an academic database of open-source machine learning software.\n"}
{"id": "54361643", "url": "https://en.wikipedia.org/wiki?curid=54361643", "title": "Hyperparameter optimization", "text": "Hyperparameter optimization\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data. The objective function takes a tuple of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance.\n\nThe traditional way of performing hyperparameter optimization has been \"grid search\", or a \"parameter sweep\", which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set\nor evaluation on a held-out validation set.\n\nSince the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.\n\nFor example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant \"C\" and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of \"reasonable\" values for each, say\n\nGrid search then trains an SVM with each pair (\"C\", γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.\n\nGrid search suffers from the curse of dimensionality, but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other.\n\nRandom Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample.\n\nBayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization, aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run.\n\nFor specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression.\n\nA different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation. \n\nEvolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution:\n\n1. Create an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100+)\n2. Evaluate the hyperparameters tuples and acquire their fitness function (e.g., 10-fold cross-validation accuracy of the machine learning algorithm with those hyperparameters)\n3. Rank the hyperparameter tuples by their relative fitness\n4. Replace the worst-performing hyperparameter tuples with new hyperparameter tuples generated through crossover and mutation\n5. Repeat steps 2-4 until satisfactory algorithm performance is reached or algorithm performance is no longer improving\n\nEvolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, deep neural network architecture search, as well as training of the weights in deep neural networks.\n\nPopulation Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. Poorly performing models are iteratively replaced with models that adopt modified hyperparameter values from a better performer. The modification allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures.\n\nRBF and spectral approaches have also been developed.\n\n- Katib is a Kubernetes-native system which includes grid search.\n- scikit-learn is a Python package which includes grid search.\n- Tune is a Python library for distributed hyperparameter tuning and supports grid search.\n- Talos includes grid search for Keras.\n- H2O AutoML provides grid search over algorithms in the H2O open source machine learning library.\n\n- hyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include random search.\n- Katib is a Kubernetes-native system which includes random search.\n- scikit-learn is a Python package which includes random search.\n- Tune is a Python library for distributed hyperparameter tuning and supports random search over arbitrary parameter distributions.\n- Talos includes a customizable random search for Keras.\n\n- Auto-sklearn is a Bayesian hyperparameter optimization layer on top of scikit-learn.\n- Ax is a Python-based experimentation platform that supports Bayesian optimization and bandit optimization as exploration strategies.\n- BOCS is a Matlab package which uses semidefinite programming for minimizing a black-box function over discrete inputs. A Python 3 implementation is also included.\n- HpBandSter is a Python package which combines Bayesian optimization with bandit-based methods.\n- Katib is a Kubernetes-native system which includes bayesian optimization.\n- mlrMBO, also with mlr, is an R package for model-based/Bayesian optimization of black-box functions.\n- scikit-optimize is a Python package or sequential model-based optimization with a scipy.optimize interface.\n- SMAC SMAC is a Python/Java library implementing Bayesian optimization.\n- tuneRanger is an R package for tuning random forests using model-based optimization.\n- optuna is a Python package for black box optimization, compatible with arbitrary functions that need to be optimized.\n\n- FAR-HO is a Python package containing Tensorflow implementations and wrappers for gradient-based hyperparamteter optimization with forward and reverse mode algorithmic differentiation.\n- XGBoost is an open-source software library which provides a gradient boosting framework for C++, Java, Python, R, and Julia.\n\n- deap is a Python framework for general evolutionary computation which is flexible and integrates with parallelization packages like scoop and pyspark, and other Python frameworks like sklearn via sklearn-deap.\n- devol is a Python package that performs Deep Neural Network architecture search using genetic programming.\n- nevergrad is a Python package which includes population control methods and particle swarm optimization.\n- Tune is a Python library for distributed hyperparameter tuning and leverages nevergrad for evolutionary algorithm support.\n\n- dlib is a C++ package with a Python API which has a parameter-free optimizer based on LIPO and trust region optimizers working in tandem.\n- Tune is a Python library for hyperparameter tuning execution and integrates with/scales many existing hyperparameter optimization libraries such as hyperopt, nevergrad, and scikit-optimize.\n- Harmonica is a Python package for spectral hyperparameter optimization.\n- hyperopt, also via hyperas and hyperopt-sklearn, are Python packages which include Tree of Parzen Estimators based distributed hyperparameter optimization.\n- Katib is a Kubernetes-native system which includes grid, random search, bayesian optimization, hyperband, and NAS based on reinforcement learning.\n- nevergrad is a Python package for gradient-free optimization using techniques such as differential evolution, sequential quadratic programming, fastGA, covariance matrix adaptation, population control methods, and particle swarm optimization.\n- nni is a Python package which includes hyperparameter tuning for neural networks in local and distributed environments. Its techniques include TPE, random, anneal, evolution, SMAC, batch, grid, and hyperband.\n- parameter-sherpa is a similar Python package which includes several techniques grid search, Bayesian and genetic Optimization\n- pycma is a Python implementation of Covariance Matrix Adaptation Evolution Strategy.\n- rbfopt is a Python package that uses a radial basis function model\n\n- Amazon Sagemaker uses Gaussian processes to tune hyperparameters.\n- BigML OptiML supports mixed search domains\n- Google HyperTune supports mixed search domains\n- Indie Solver supports multiobjective, multifidelity and constraint optimization\n- Mind Foundry OPTaaS supports mixed search domains, multiobjective, constraints, parallel optimization and surrogate models.\n- SigOpt supports mixed search domains, multiobjective, multisolution, multifidelity, constraint (linear and black-box), and parallel optimization.\n\n", "related": "\n- Automated machine learning\n- Neural architecture search\n- Meta-optimization\n- Model selection\n- Self-tuning\n- XGBoost\n"}
{"id": "56142183", "url": "https://en.wikipedia.org/wiki?curid=56142183", "title": "Paraphrasing (computational linguistics)", "text": "Paraphrasing (computational linguistics)\n\nParaphrase or Paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection. Paraphrasing is also useful in the evaluation of machine translation, as well as semantic parsing and generation of new samples to expand existing corpora.\n\nBarzilay and Lee proposed a method to generate paraphrases through the usage of monolingual parallel corpora, namely news articles covering the same event on the same day. Training consists of using multi-sequence alignment to generate sentence-level paraphrases from an unannotated corpus. This is done by\n\n- finding recurring patterns in each individual corpus, i.e. \" (injured/wounded) people, seriously\" where are variables\n- finding pairings between such patterns the represent paraphrases, i.e. \" (injured/wounded) people, seriously\" and \" were (wounded/hurt) by , among them were in serious condition\"\n\nThis is achieved by first clustering similar sentences together using n-gram overlap. Recurring patterns are found within clusters by using multi-sequence alignment. Then the position of argument words are determined by finding areas of high variability within each clusters, aka between words shared by more than 50% of a cluster's sentences. Pairings between patterns are then found by comparing similar variable words between different corpora. Finally new paraphrases can be generated by choosing a matching cluster for a source sentence, then substituting the source sentence's argument into any number of patterns in the cluster.\n\nParaphrase can also be generated through the use of phrase-based translation as proposed by Bannard and Callison-Burch. The chief concept consists of aligning phrases in a pivot language to produce potential paraphrases in the original language. For example, the phrase \"under control\" in an English sentence is aligned with the phrase \"unter kontrolle\" in its German counterpart. The phrase \"unter kontrolle\" is then found in another German sentence with the aligned English phrase being \"in check\", a paraphrase of \"under control\".\n\nThe probability distribution can be modeled as formula_1, the probability phrase formula_2 is a paraphrase of formula_3, which is equivalent to formula_4 summed over all formula_5, a potential phrase translation in the pivot language. Additionally, the sentence formula_3 is added as a prior to add context to the paraphrase. Thus the optimal paraphrase, formula_7 can be modeled as:\n\nformula_9 and formula_10 can be approximated by simply taking their frequencies. Adding formula_11 as a prior is modeled by calculating the probability of forming the formula_11 when formula_3 is substituted with \n\nThere has been success in using long short-term memory (LSTM) models to generate paraphrases. In short, the model consists of an encoder and decoder component, both implemented using variations of a stacked residual LSTM. First, the encoding LSTM takes a one-hot encoding of all the words in a sentence as input and produces a final hidden vector, which can be viewed as a representation of the input sentence. The decoding LSTM then takes the hidden vector as input and generates new sentence, terminating in an end-of-sentence token. The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent. New paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder.\n\nParaphrase recognition has been attempted by Socher et al through the use of recursive autoencoders. The main concept is to produce a vector representation of a sentence along with its components through recursively using an autoencoder. The vector representations of paraphrases should have similar vector representations; they are processed, then fed as input into a neural network for classification.\n\nGiven a sentence formula_14 with formula_15 words, the autoencoder is designed to take 2 formula_16-dimensional word embeddings as input and produce an formula_16-dimensional vector as output. The same autoencoder is applied to every pair of words in formula_11 to produce formula_19 vectors. The autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced. Given an odd number of inputs, the first vector is forwarded as is to the next level of recursion. The autoencoder is then trained to reproduce every vector in the full recursion tree including the initial word embeddings.\n\nGiven two sentences formula_20 and formula_21 of length 4 and 3 respectively, the autoencoders would produce 7 and 5 vector representations including the initial word embeddings. The euclidean distance is then taken between every combination of vectors in formula_20 and formula_21 to produce a similarity matrix formula_24. formula_11 is then subject to a dynamic min-pooling layer to produce a fixed size formula_26 matrix. Since formula_11 are not uniform in size among all potential sentences, formula_11 is split into formula_29 roughly even sections. The output is then normalized to have mean 0 and standard deviation 1 and is fed into a fully connected layer with a softmax output. The dynamic pooling to softmax model is trained using pairs of known paraphrases.\n\nSkip-thought vectors are an attempt to create a vector representation of the semantic meaning of a sentence in a similar fashion as the skip gram model. Skip-thought vectors are produced through the use of a skip-thought model which consists of three key components, an encoder and two decoders. Given a corpus of documents, the skip-thought model is trained to take a sentence as input and encode it into a skip-thought vector. The skip-thought vector is used as input for both decoders, one of which attempts to reproduce the previous sentence and the other the following sentence in its entirety. The encoder and decoder can be implemented through the use of a recursive neural network (RNN) or an LSTM.\n\nSince paraphrases carry the same semantic meaning between one another, they should have similar skip-thought vectors. Thus a simple logistic regression can be trained to a good performance with the absolute difference and component-wise product of two skip-thought vectors as input.\n\nThere are multiple methods that can be used to evaluate paraphrases. Since paraphrase recognition can be posed as a classification problem, most standard evaluations metrics such as accuracy, f1 score, or an ROC curve do relatively well. However, there is difficulty calculating f1-scores due to trouble produce a complete list of paraphrases for a given phrase along with the fact that good paraphrases are dependent upon context. A metric designed to counter these problems is ParaMetric. ParaMetric aims to calculate the precision and recall of an automatic paraphrase system by comparing the automatic alignment of paraphrases to a manual alignment of similar phrases. Since ParaMetric is simply rating the quality of phrase alignment, it can be used to rate paraphrase generation systems as well assuming it uses phrase alignment as part of its generation process. A noted drawback to ParaMetric is the large and exhaustive set of manual alignments that must be initially created before a rating can be produced.\n\nThe evaluation of paraphrase generation has similar difficulties as the evaluation of machine translation. Often the quality of a paraphrase is dependent upon its context, whether it is being used as a summary, and how it is generated among other factors. Additionally, a good paraphrase usually is lexically dissimilar from its source phrase. The simplest method used to evaluate paraphrase generation would be through the use of human judges. Unfortunately, evaluation through human judges tends to be time consuming. Automated approaches to evaluation prove to be challenging as it is essentially a problem as difficult as paraphrase recognition. While originally used to evaluate machine translations, bilingual evaluation understudy (BLEU) has been used successfully to evaluate paraphrase generation models as well. However, paraphrases often have several lexically different but equally valid solutions which hurts BLEU and other similar evaluation metrics.\n\nMetrics specifically designed to evaluate paraphrase generation include paraphrase in n-gram change (PINC) and paraphrase evaluation metric (PEM) along with the aforementioned ParaMetric. PINC is designed to be used in conjunction with BLEU and help cover its inadequacies. Since BLEU has difficulty measuring lexical dissimilarity, PINC is a measurement of the lack of n-gram overlap between a source sentence and a candidate paraphrase. It is essentially the Jaccard distance between the sentence excluding n-grams that appear in the source sentence to maintain some semantic equivalence. PEM, on the other hand, attempts to evaluate the \"adequacy, fluency, and lexical dissimilarity\" of paraphrases by returning a single value heuristic calculated using N-grams overlap in a pivot language. However, a large drawback to PEM is that must be trained using a large, in-domain parallel corpora as well as human judges. In other words, it is tantamount to training a paraphrase recognition system in order to evaluate a paraphrase generation system.\n\n", "related": "\n- Round-trip translation\n- Text simplification\n- Text normalization\n\n11. Online Paraphrasing tool for rewording articles. Paraphrasing tool\n\n- Microsoft Research Paraphrase Corpus - a dataset consisting of 5800 pairs of sentences extracted from news articles annotated to note whether a pair captures semantic equivalence\n- Paraphrase Database (PPDB) - A searchable database containing millions of paraphrases in 16 different languages\n"}
{"id": "31978226", "url": "https://en.wikipedia.org/wiki?curid=31978226", "title": "Life-time of correlation", "text": "Life-time of correlation\n\nThe life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross correlation in stochastic processes.\n\nThe correlation coefficient \"ρ\", expressed as an autocorrelation function or cross-correlation function, depends on the lag-time between the times being considered. Typically such functions, \"ρ\"(\"t\"), decay to zero with increasing lag-time, but they can assume values across all levels of correlations: strong and weak, and positive and negative as in the table.\n\nThe life-time of a correlation is defined as the length of time when the correlation coefficient is at the strong level. The durability of correlation is determined by signal (the strong level of correlation is separated from weak and negative levels). The mean life-time of correlation could measure how the durability of correlation depends on the window width size (the window is the length of time series used to calculate correlation).\n", "related": "NONE"}
{"id": "57261507", "url": "https://en.wikipedia.org/wiki?curid=57261507", "title": "RAMnets", "text": "RAMnets\n\nRAMnets is one of the oldest practical neurally inspired classification algorithm is still one of the best. The RAMnets is also known as a type of \"\"n\"-tuple recognition method\" or \"weightless neural network\".\n\nConsider (let us say \"N\") sets of n distinct bit locations are selected randomly. These are the \"n\"-tuples. The restriction of a pattern to an n-tuple can be regarded as an \"n\"-bit number which, together with the identity of the \"n\"-tuple, constitutes a `feature' of the pattern. The standard \"n\"-tuple recognizer operates simply as follows:\n\n\"A pattern is classified as belonging to the class for which it has the most features in common with at least one training pattern of that class.\nThis is the formula_1= 0 case of a more general rule whereby the class assigned to unclassified pattern u is\n\nwhere D is the set of training patterns in class c, formula_3= x for formula_4 ,formula_5 for formula_6,formula_7 is the Kronecker delta(formula_7=1 if i=j and 0 otherwise.)and formula_9is the i feature of the pattern u:\n\nHere u is the k bit of u and formula_11is the j bit location of the i n-tuple.\n\nWith C classes to distinguish, the system can be implemented as a network of NC nodes, each of which is a random access memory (RAM); hence the term \"RAMnet.\" The memory content formula_12 at address formula_13 of the i node allocated to class c is set to\n\nIn the usual formula_16 = 1 case, the 1-bit content of formula_12 is set if any pattern of D has feature formula_18 and unset otherwise. Recognition is accomplished by summing the contents of the nodes of each class at the addresses given by the features of the unclassified pattern. That is, pattern u is assigned to class\n\nThe RAMnets formed the basis of a commercial product known as WiSARD (Wilkie, Stonham and Aleksander Recognition Device) was the first artificial neural network machine to be patented.\n\nA RAM-discriminator consists of a set of one-bit word RAMs with inputs and a summing device (Σ). Any such RAM-discriminator can receive a binary pattern of X⋅n bits as input. The RAM input lines are connected to the input pattern by means of a biunivocal pseudo-random mapping. The summing device enables this network of RAMs to exhibit – just like other ANN models based on synaptic weights – generalization and noise tolerance.\n\nIn order to train the discriminator one has to set all RAM memory locations to 0 and choose a training set formed by binary patterns of X⋅n bits. For each training pattern, a 1 is stored in the memory location of each RAM addressed by this input pattern. Once the training of patterns is completed, RAM memory contents will be set to a certain number of 0’s and 1’s.\n\nThe information stored by the RAM during the training phase is used to deal with previous unseen patterns. When one of these is given as input, the RAM memory contents addressed by the input pattern are read and summed by Σ. The number thus obtained, which is called the discriminator response, is equal to the number of RAMs that output 1. r reaches the maximum if the input belongs to the training set. is equal to 0 if no \"n\"-bit component of the input pattern appears in the training set (not a single RAM outputs 1). Intermediate values of r express a kind of “similarity measure” of the input pattern with respect to the patterns in the training set.\n\nA system formed by various RAM-discriminators is called WiSARD. Each RAM-discriminator is trained on a particular class of patterns, and classification by the multi-discriminator system is performed in the following way. When a pattern is given as input, each RAM-discriminator gives a response to that input. The various responses are evaluated by an algorithm which compares them and computes the relative confidence of the highest response (e.g., the difference d between the highest response and the second highest response, divided by the highest response). A schematic representation of a RAM-discriminator and a 10 RAM-discriminator WiSARD is shown in Figure 1.\n\n", "related": "\n- Artificial Neural Network\n- Kronecker delta\n- Pattern Recognition\n- Unsupervised learning\n- Erlang distribution\n- Machine learning\n- Erlang (unit)\n\n- Michal Morciniec and Richard Rohwer(1995) \"The n-tuple Classifier: Too Good to Ignore\"\n- (This book focuses on unsupervised learning in neural networks)\n- A brief introduction to Weightless NeuralSystems (2009)\n\n1. An introductory tutorial to classifiers (introducing the basic terms, with numeric example)\n"}
{"id": "57687371", "url": "https://en.wikipedia.org/wiki?curid=57687371", "title": "Multimodal sentiment analysis", "text": "Multimodal sentiment analysis\n\nMultimodal sentiment analysis is a new dimension of the traditional text-based sentiment analysis, which goes beyond the analysis of texts, and includes other modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others.\n\nSimilar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis.\n\nFeature engineering, which involves the selection of features that are fed into machine learning algorithms, plays a key role in the sentiment classification performance. In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed.\n\nSimilar to the conventional text-based sentiment analysis, some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n-grams, which are basically a sequence of words in a given textual document. These features are applied using bag-of-words or bag-of-concepts feature representations, in which words or concepts are represented as vectors in a suitable space.\n\nSentiment and emotion characteristics are prominent in different phonetic and prosodic properties contained in audio features. Some of the most important audio features employed in multimodal sentiment analysis are mel-frequency cepstrum (MFCC), spectral centroid, spectral flux, beat histogram, beat sum, strongest beat, pause duration, and pitch. OpenSMILE and Praat are popular open-source toolkits for extracting such audio features.\n\nOne of the main advantages of analyzing videos with respect to texts alone, is the presence of rich sentiment cues in visual data. Visual features include facial expressions, which are of paramount importance in capturing sentiments and emotions, as they are a main channel of forming a person's present state of mind. Specifically, smile, is considered to be one of the most predictive visual cues in multimodal sentiment analysis. OpenFace is an open-source facial analysis toolkit available for extracting and understanding such visual features.\n\nUnlike the traditional text-based sentiment analysis, multimodal sentiment analysis undergo a fusion process in which data from different modalities (text, audio, or visual) are fused and analyzed together. The existing approaches in multimodal sentiment analysis data fusion can be grouped into three main categories: feature-level, decision-level, and hybrid fusion, and the performance of the sentiment classification depends on which type of fusion technique is employed.\n\nFeature-level fusion (sometimes known as early fusion) gathers all the features from each modality (text, audio, or visual) and joins them together into a single feature vector, which is eventually fed into a classification algorithm. One of the difficulties in implementing this technique is the integration of the heterogeneous features.\n\nDecision-level fusion (sometimes known as late fusion), feeds data from each modality (text, audio, or visual) independently into its own classification algorithm, and obtains the final sentiment classification results by fusing each result into a single decision vector. One of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data, and each modality can utilize its most appropriate classification algorithm.\n\nHybrid fusion is a combination of feature-level and decision-level fusion techniques, which exploits complementary information from both methods during the classification process. It usually involves a two-step procedure wherein feature-level fusion is initially performed between two modalities, and decision-level fusion is then applied as a second step, to fuse the initial results from the feature-level fusion, with the remaining modality.\n\nSimilar to text-based sentiment analysis, multimodal sentiment analysis can be applied in the development of different forms of recommender systems such as in the analysis of user-generated videos of movie reviews and general product reviews, to predict the sentiments of customers, and subsequently create product or service recommendations. Multimodal sentiment analysis also plays an important role in the advancement of virtual assistants through the application of natural language processing (NLP) and machine learning techniques. In the healthcare domain, multimodal sentiment analysis can be utilized to detect certain medical conditions such as stress, anxiety, or depression. Multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs, which is considered as a complicated and challenging domain, as sentiments expressed by reporters tend to be less obvious or neutral.\n", "related": "NONE"}
{"id": "8190902", "url": "https://en.wikipedia.org/wiki?curid=8190902", "title": "Anomaly detection", "text": "Anomaly detection\n\nIn data mining, anomaly detection (also outlier detection) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.\n\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not \"rare\" objects, but unexpected \"bursts\" in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.\n\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given \"normal\" training data set, and then test the likelihood of a test instance to be generated by the learnt model.\n\nAnomaly detection is applicable in a variety of domains, such as intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, and detecting ecosystem disturbances. It is often used in preprocessing to remove anomalous data from the dataset. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.\n\nSeveral anomaly detection techniques have been proposed in literature. Some of the popular techniques are:\n\n- Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept).\n- Subspace-, correlation-based and tensor-based outlier detection for high-dimensional data.\n- One-class support vector machines.\n- Replicator neural networks., autoencoders, long short-term memory neural networks\n- Bayesian Networks.\n- Hidden Markov models (HMMs).\n- Cluster analysis-based outlier detection.\n- Deviations from association rules and frequent itemsets.\n- Fuzzy logic-based outlier detection.\n- Ensemble techniques, using feature bagging, score normalization and different sources of diversity.\n\nThe performance of different methods depends a lot on the data set and parameters, and methods have little systematic advantages over another when compared across many data sets and parameters.\n\nAnomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. The counterpart of anomaly detection in intrusion detection is misuse detection.\n\n- ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them.\n\n- Anomaly detection benchmark data repository of the Ludwig-Maximilians-Universität München; Mirror at University of São Paulo.\n- ODDS – ODDS: A large collection of publicly available outlier detection datasets with ground truth in different domains.\n\n", "related": "\n- Change detection\n- Statistical process control\n- Novelty detection\n- Hierarchical temporal memory\n"}
{"id": "12656117", "url": "https://en.wikipedia.org/wiki?curid=12656117", "title": "Novelty detection", "text": "Novelty detection\n\nNovelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing. The principle is long known in neurophysiology, with roots in the orienting response research by E. N. Sokholov in the 1950s. The reverse phenomenon is habituation, i.e., the phenomenon that known patterns yield a less marked response. Early neural modeling attempts were by Yehuda Salu. An increasing body of knowledge has been collected concerning the corresponding mechanisms in the brain. In technology, the principle became important for radar detection methods during the Cold War, where unusual aircraft-reflection patterns could indicate an attack by a new type of aircraft. Today, the phenomenon plays an important role in machine learning and data science, where the corresponding methods are known as anomaly detection or outlier detection. An extensive methodological overview is given by Markou and Singh.\n\n", "related": "\n- Change detection\n- Outlier\n"}
{"id": "8880387", "url": "https://en.wikipedia.org/wiki?curid=8880387", "title": "Programming by example", "text": "Programming by example\n\nIn computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples.\n\nPbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language. Many PbE systems have been developed as research prototypes, but few have found widespread real-world application. More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol: Seahawk and Gbrowse moby. \n\nAlso the programming by demonstration (PbD) term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task. The usual distinction in literature between these terms is that in PbE the user gives a prototypical product of the computer execution, such as a row in the desired results of a query; while in PbD the user performs a sequence of actions that the computer must repeat, generalizing it to be used in different data sets. For final users, to automate a workflow in a complex tool (e.g. Photoshop), the most simple case of PbD is the macro recorder. \n\n", "related": "\n- Query by Example\n- Automated machine learning\n- Example-based machine translation\n- Inductive programming\n- Lapis (text editor), which allows simultaneous editing of similar items in multiple selections created by example\n- Programming by demonstration\n- Test-driven development\n\n- Henry Lieberman's page on Programming by Example\n- Online copy of Watch What I Do, Allen Cypher's book on Programming by Demonstration\n- Online copy of Your Wish is My Command, Henry Lieberman's sequel to Watch What I Do\n- A Visual Language for Data Mapping, John Carlson's description of an Integrated Development Environment (IDE) that used Programming by Example (desktop objects) for data mapping, and an iconic language for recording operations\n"}
{"id": "8529968", "url": "https://en.wikipedia.org/wiki?curid=8529968", "title": "Bayesian regret", "text": "Bayesian regret\n\nIn game theory, Bayesian regret is the average difference between the utility of a strategy and an ideal utility where desired outcomes are maximized. \n\nThe term \"Bayesian\" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.\n\nIn social choice theory, Bayesian regret is the average difference in social utility between the chosen candidate and the best candidate. It is only measurable if it is possible to know the voters' true numerical utility for each candidate – that is, in Monte Carlo simulations of virtual elections. \n\nIf Bayesian regret can be \"measured only if it is possible to know the voters' true numerical utility for each candidate\", it would seem to be irrelevant to two voting methods that instead grade the utility of each candidate: Majority Judgment (MJ) invites citizens to grade the suitability of each candidate for a single office: Excellent (ideal), Very Good, Good, Acceptable, Poor, or Reject (entirely unsuitable). The winner is the one who receives the highest median-grade. Since MJ allows each citizen honestly and fully to express their judgment of each candidate, it would seem to give each voter every appropriate reason to be pleased with the election. At least the voters who constitute the relevant absolute majority should be pleased. Similarly, Evaluative Proportional Representation (EPR) in Section 5.5.5 in Proportional Representation allows all the members of a legislature to be elected at the same time. Each elected member has a different weighted vote during its deliberations. At the same time, each citizen is assured that their honest vote proportionately increases the voting power of the member who received either their highest grade, remaining highest grade, or proxy vote. No vote is needlessly wasted quantitatively or qualitatively. Again, each citizen is given every appropriate reason to be please with an EPR election --no legitimate regret.\n\nThe term Bayesian is somewhat a misnomer, really meaning only \"average probabilistic\"; there is no standard or objective way to create distributions of voters and candidates.\n\nThe Bayesian regret concept was recognized as useful (and used) for comparing single-winner voting systems by Bordley and Merrill, and it also was invented independently by R. J. Weber. Bordley attributed it (and the whole idea of the usefulness of \"social\" utility, that is, summed over all people in the population) to John Harsanyi in 1955.\n\nThis term has been used to compare a random buy-and-hold strategy to professional traders' records. This same concept has received numerous different names, as the New York Times notes: \n\n\"In 1957, for example, a statistician named James Hanna called his theorem Bayesian Regret. He had been preceded by David Blackwell, also a statistician, who called his theorem Controlled Random Walks. Other, later papers had titles like 'On Pseudo Games', 'How to Play an Unknown Game', 'Universal Coding' and 'Universal Portfolios'\".\n\n", "related": "\n- Loss function\n- Regret (decision theory)\n- Social utility efficiency\n\n- Robert F. Bordley: \"A pragmatic method for evaluating election schemes through simulation\", \"Amer. Polit. Sci. Rev.\" 77 (1983) 123–141.\n- Samuel Merrill: Making multicandidate elections more democratic, Princeton Univ. Press 1988.\n- Samuel Merrill: \"A comparison of efficiency of multicandidate electoral systems\", \"Amer. J. Polit. Sci.\" 28, 1 (1984) 23–48.\n"}
{"id": "1514392", "url": "https://en.wikipedia.org/wiki?curid=1514392", "title": "Training, validation, and test sets", "text": "Training, validation, and test sets\n\nIn machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms work by making data-driven predictions or decisions, through building a mathematical model from input data.\n\nThe data used to build the final model usually comes from multiple datasets. In particular, three data sets are commonly used in different stages of the creation of the model.\n\nThe model is initially fit on a training dataset, that is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a neural net or a naive Bayes classifier) is trained on the training dataset using a supervised learning method (e.g. gradient descent or stochastic gradient descent). In practice, the training dataset often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), which is commonly denoted as the \"target\" (or \"label\"). The current model is run with the training dataset and produces a result, which is then compared with the \"target\", for each input vector in the training dataset. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\n\nSuccessively, the fitted model is used to predict the responses for the observations in a second dataset called the validation dataset. The validation dataset provides an unbiased evaluation of a model fit on the training dataset while tuning the model's hyperparameters (e.g. the number of hidden units in a neural network). Validation datasets can be used for regularization by early stopping: stop training when the error on the validation dataset increases, as this is a sign of overfitting to the training dataset.\nThis simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.\n\nFinally, the test dataset is a dataset used to provide an unbiased evaluation of a \"final\" model fit on the training dataset. If the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset.\n\nA training dataset is a dataset of examples used for learning, that is to fit the parameters (e.g., weights) of, for example, a classifier.\n\nMost approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify and exploit apparent relationships in the training data that do not hold in general.\n\nA test dataset is a dataset that is independent of the training dataset, but that follows the same probability distribution as the training dataset. If a model fit to the training dataset also fits the test dataset well, minimal overfitting has taken place (see figure below). A better fitting of the training dataset as opposed to the test dataset usually points to overfitting.\n\nA test set is therefore a set of examples used only to assess the performance (i.e. generalization) of a fully specified classifier.\n\nA validation dataset is a dataset of examples used to tune the hyperparameters (i.e. the architecture) of a classifier. It is sometimes also called the development set or the \"dev set\". In artificial neural networks, a hyperparameter is, for example, the number of hidden units. It, as well as the testing set (as mentioned above), should follow the same probability distribution as the training dataset.\n\nIn order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation dataset in addition to the training and test datasets. For example, if the most suitable classifier for the problem is sought, the training dataset is used to train the candidate algorithms, the validation dataset is used to compare their performances and decide which one to take and, finally, the test dataset is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure, and so on. The validation dataset functions as a hybrid: it is training data used for testing, but neither as part of the low-level training nor as part of the final testing.\n\nThe basic process of using a validation dataset for model selection (as part of training dataset, validation dataset, and test dataset) is:\nAn application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).\n\nMost simply, part of the original dataset can be set aside and used as a test set: this is known as the holdout method.\n\nThe terms test set and validation set are sometimes used in a way that flips their meaning in both industry and academia. In the erroneous usage, \"test set\" becomes the development set, and \"validation set\" is the independent set used to evaluate the performance of a fully specified classifier. The literature on machine learning often reverses the meaning of “validation” and “test” sets. This is the most blatant example of the terminological confusion that pervades artificial intelligence research.\n\nA dataset can be repeatedly split into a training dataset and a validation dataset: this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal datasets and using them as training/validation, and then validation/training, or repeatedly selecting a random subset as a validation dataset. To validate the model performance, sometimes an additional test dataset that was held out from cross-validation is used.\n\nAnother example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition), which splits a complete multi-class problem into a set of smaller classification problems. It serves for learning more accurate concepts due to simpler classification boundaries in subtasks and individual feature selection procedures for subtasks. When doing classification decomposition, the central choice is the order of combination of smaller classification steps, called the classification path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example, on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.\n\nCommercial tools, such as Diffgram and Supervisely, are available to create training data.\n\n", "related": "\n- Statistical classification\n- List of datasets for machine learning research\n\n- FAQ: What are the population, sample, training set, design set, validation set, and test set?\n- What is the Difference Between Test and Validation Datasets?\n- What is training, validation, and testing data-sets scenario in machine learning?\n- Is there a rule-of-thumb for how to divide a dataset into training and validation sets?\n"}
{"id": "57841422", "url": "https://en.wikipedia.org/wiki?curid=57841422", "title": "Appen (company)", "text": "Appen (company)\n\nAppen Limited (formerly known as Appen Butler Hill) is a publicly traded company listed on the Australian Securities Exchange (ASX) under the code APX.\n\nAppen provides or improves data used for the development of machine learning and artificial intelligence products. Data types include speech and natural language data, image and video data, text and alphanumeric data and relevance data to improve search and social media engines. Appen's customers use machine learning for a variety of use cases including automatic speech recognition (ASR), computer vision, increasing conversions in eCommerce, delivering more meaningful and personalized advertising, enhancing social media feeds or improving customer service capabilities with tools like chatbots and virtual assistants.\n\nFor machines to demonstrate artificial intelligence, they need to be programmed with human-quality training data that helps them learn. Appen uses crowdsourcing to collect and improve data and has access to a skilled crowd of over than 1 million part-time contractors who collect, annotate, evaluate, label, rate, test, translate and transcribe speech, image, text and video data to turn it into effective machine learning training data for a variety of use cases.\n\nThe company's global headquarters is in Chatswood, New South Wales which is 10 kilometers north of the central business district of Sydney, Australia. The United States headquarters is in Kirkland Washington which is a suburb of Seattle and there are also US offices in San Francisco, California and Detroit, Michigan. Appen also has offices in Beijing, China, Cavite, Philippines, and Exeter, England.\n\nAt the end of 2017, revenues were AUD 166.6 million and the company had more than 350 full-time employees and over 1,000,000 approved flexible workers in the Appen crowd. Tasks are performed in more than 180 languages and 130 countries.\n\nMost of the company's revenues are earned offshore and clients include eight of the top ten largest technology companies.\n\nAppen was founded in Sydney in 1996 by linguist Dr. Julie Vonwiller. She was joined by her husband Chris Vonwiller who left his job at Telstra in 2000 to join Appen full-time and is currently Non-Executive Chairman of Appen.\n\nIn 2011, Appen merged with the Butler Hill Group, which was based in Ridgefield, Connecticut and Seattle, Washington and originally founded by Lisa Braden-Harder in 1993. Lisa was a member of the pioneering team in grammar checking technology at the IBM T.J. Watson Research Center before the Butler Hill Group and stayed on as CEO until 2015. After the merger, the combined business became Appen Butler Hill and expanded its business scope to include language resources, search and text.\n\nIn 2012, Appen acquired Wikman Remer, a firm based in San Rafael, California, which developed tools and platforms for employee engagement, online moderation and curation.\n\nAppen Butler Hill was re-branded as Appen in 2013, and it went public on the ASX on January 7, 2015.\n\nIn July 2015 Mark Brayan joined Appen as CEO and continues to hold that position today.\n\nIn October 2016 Appen acquired a UK based transcription services company called Mendip Media Group (MMG)\n\nAppen also acquired Leapforce in November 2017 for U.S. $80M, adding additional capabilities in search relevance and growing its crowd to over 1,000,000 flexible workers.\n\n- Appen acquired data annotation company called Leapforce in 2017.\n- Appen acquired Figure Eight in 2019.\n\n- Australian Growth Technology Company Award 2017\n- Flex Jobs Top 100 Company With Remote Jobs in 2014, 2015, 2016, 2017 and 2018 including the #1 ranking in 2017 and the #2 ranking in 2018\n- Deloitte Asia Pacific Technology Fast 500 2017\n- Four-time winner of the Deloitte Technology Fast 50 Australia award and also named number 6 in the Deloitte Leadership Awards for companies having revenues in excess of $50 million in 2013\n- 2014 BRW Momentum Awards for Best Mid-market Business $50–100MM Category Finalist\n- Inaugural winner in 2008 of the Prime Minister's Exporter of the Year Award and Australian Export Category Winner, Information & Communication Technology Award\n- Ranked 8th largest language service provider globally by Common Sense Advisory (CSA Research) in Who's Who in Language Services and Technology: 2019 Rankings.\n- Inaugural winner in 2008 of the Prime Minister's Exporter of the Year Award and Australian Export Category Winner, Information and Communication Technology Award.\n\n- Official Website\n", "related": "NONE"}
{"id": "58714104", "url": "https://en.wikipedia.org/wiki?curid=58714104", "title": "OpenAI Five", "text": "OpenAI Five\n\nOpenAI Five is the name of a machine learning project that performs as a team of video game bots playing against human players in the competitive five-on-five video game \"Dota 2\". The system was developed by OpenAI, an American artificial intelligence (AI) research and development company founded with the mission to develop safe AI in a way that benefits humanity. OpenAI Five's first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against a professional player of the game known as Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\n\nThe company uses \"Dota 2\" as an experiment for general-purpose applied machine learning to capture the unpredictability and continuous nature of the real world. The team stated that the complex nature of the game and its strong reliance on having to work together as a team to win was a major reason it was specifically chosen. The algorithms used for the project have also been applied to other systems, such as controlling a robotic hand. The project has also been compared to a number of other similar cases of AI playing against and defeating humans, such as Watson on the television game show \"Jeopardy!\", Deep Blue in chess, and AlphaGo in the board game Go.\n\nDevelopment on the algorithms used for the bots began in November 2016. OpenAI decided to use \"Dota 2\", a competitive five-on-five video game, as a base due to it being popular on the live streaming platform Twitch, having native support for Linux, and had an application programming interface (API) available. Before becoming a team of five, the first public demonstration occurred at The International 2017 in August, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player of the game, lost against an OpenAI bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks \"like being a surgeon\". OpenAI calls the system \"reinforcement learning\", as the bots learn over time by playing against itself hundreds a times a day for months, in which they are rewarded for actions such as killing an enemy and destroying towers.\n\nBy June 2018, the ability of the bots expanded to play together as a full team of five and were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two games against professional teams, one against the Brazilian-based paiN Gaming and the other against an all-star team of former Chinese players. Although the bots lost both matches, OpenAI still considered it a successful venture, stating that playing against some of the best players in \"Dota 2\" allowed them to analyze and adjust their algorithms for future games. The bots' final public demonstration occurred in April 2019, where they won a best-of-three series against The International 2018 champions OG at a live event in San Francisco. A four-day online event to play against the bots, open to the public, occurred the same month. There, the bots played in 42,729 total games, winning all but 4,075 of them.\n\nEach OpenAI Five network contains a single layer with a 1024-unit LSTM that observes the current game state extracted from the Dota developer’s API. The neural network conducts actions via numerous possible action heads (no human data involved), and every head has meaning. For instance, the number of ticks to delay an action, what action to select – the X or Y coordinate of this action in a grid around the unit. In addition, action heads are computed independently. The AI system observes the world as a list of 20,000 numbers and takes an action by conducting a list of eight enumeration values. Also, it selects different actions and targets to understand how to encode every action and observe the world.\n\nOpenAI Five has been developed as a general-purpose reinforcement learning training system on the \"Rapid\" infrastructure. Rapid consists of two layers: it spins up thousands of machines and helps them ‘talk’ to each other and a second layer runs software. By 2018, OpenAI Five had played around 180 years worth of games in reinforcement learning running on 256 GPUs and 128,000 CPU cores, using a newly developed policy gradient method dubbed \"Proximal Policy Optimization.\"\n\nPrior to OpenAI Five, other AI versus human experiments and systems have been successfully used before, such as \"Jeopardy!\" with Watson, chess with Deep Blue, and Go with AlphaGo. In comparison with other games that have used AI systems to play against human players, \"Dota 2\" differs as explained below:\n\nLong run view: The bots run at 30 frames per second for an average match time of 45 minutes, which results in 80,000 ticks per game. OpenAI Five observes every fourth frame, generating 20,000 moves. By comparison, chess usually ends before 40 moves, while Go ends before 150 moves.\n\nPartially observed state of the game: Players and their allies can only see the map directly around them. The rest of it is covered in a fog of war which hides enemies units and their movements. Thus, playing \"Dota 2\" requires making inferences based on this incomplete data, as well as predicting what their opponent could be doing at the same time. By comparison, Chess and Go are \"full-information games\", as they do not hide elements from the opposing player.\n\nContinuous action space: Each playable character in a \"Dota 2\" game, known as a hero, can take dozens of actions that target either another unit or a position. The OpenAI Five developers allow the space into 170,000 possible actions per hero. Without counting the perpetual aspects of the game, there are an average of ~1,000 valid actions each tick. By comparison, the average number of actions in chess is 35 and 250 in Go.\n\nContinuous observation space: \"Dota 2\" is played on a large map with ten heroes, five on each team, along with dozens of buildings and non-player character (NPC) units. The OpenAI system observes the state of a game through developers’ bot API, as 20,000 numbers that constitute all information a human is allowed to get access to. A chess board is represented as about 70 lists, whereas a Go board has about 400 enumerations.\n\nOpenAI Five have received acknowledgement from the AI, tech, and video game community at large. Microsoft founder Bill Gates called it a \"big deal\", as their victories \"required teamwork and collaboration\". Chess player Garry Kasparov, who lost against the Deep Blue AI in 1997, stated that despite their losing performance at The International 2018, the bots would eventually \"get there, and sooner than expected\".\n\nAndreas Theodorou, an AI researcher at the University of Bath who uses computer games to study collaboration, says OpenAI Five was a \"big step forward\" in the AI industry, although noting that perhaps the most significant achievement was their use of transparent visualizations. In a conversation with MIT Technology Review, AI experts also considered OpenAI Five system as a significant achievement, as they noted that \"Dota 2\" was an \"extremely complicated game\", so even beating non-professional players was impressive. Inspired by the success of OpenAI five, other AI companies have begun to develop systems that will be able to compete in similar complex video games that require strategic thinking, team play, and reasoning, such as \"StarCraft\".\n\n", "related": "NONE"}
{"id": "58175832", "url": "https://en.wikipedia.org/wiki?curid=58175832", "title": "Multitask optimization", "text": "Multitask optimization\n\nMulti-task optimization is a paradigm in the optimization literature that focuses on solving multiple self-contained tasks simultaneously. The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics. \n\nThe key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate the search on the other. \n\nThe success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler to more complex tasks. In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems.\n\nThere are two common approaches for multi-task optimization: Bayesian optimization and evolutionary computation.\n\nMulti-task Bayesian optimization is a modern model-based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms. The method builds a multi-task Gaussian\nprocess model on the data originating from different searches progressing in tandem. The captured inter-task dependencies are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces.\n\nEvolutionary multi-tasking has been explored as a means of exploiting the implicit parallelism of population-based search algorithms to simultaneously progress multiple distinct optimization tasks. By mapping all tasks to a unified search space, the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer. This is induced when solutions associated with different tasks crossover. Recently, modes of knowledge transfer that are different from direct solution crossover have been explored.\n\nAlgorithms for multi-task optimization span a wide array of real-world applications. Recent studies highlight the potential for speed-ups in the optimization of engineering design parameters by conducting related designs jointly in a multi-task manner. In machine learning, the transfer of optimized features across related data sets can enhance the efficiency of the training process as well as improve the generalization capability of learned models. In addition, the concept of multi-tasking has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning.\n\nApplications have also been reported in cloud computing, with future developments geared towards cloud-based on-demand optimization services that can cater to multiple customers simultaneously.\n\n", "related": "\n- Multi-objective optimization\n- Multi-task learning\n- Multicriteria classification\n- Multiple-criteria decision analysis\n"}
{"id": "58655546", "url": "https://en.wikipedia.org/wiki?curid=58655546", "title": "Associative classifier", "text": "Associative classifier\n\nAn associative classifier (AC) is a kind of supervised learning model that uses association rules to assign a target value. The term associative classification was coined by Bing Liu et al., in which the authors defined a model made of rules \"whose right-hand side are restricted to the classification class attribute\".\n\nThe model generated by an AC and used to label new records consists of association rules, where the consequent corresponds to the class label. As such, they can also be seen as a list of \"if-then\" clauses: if the record matches some criteria (expressed in the left side of the rule, also called antecedent), it is then labeled accordingly to the class on the right side of the rule (or consequent).\n\nMost ACs read the list of rules in order, and apply the first matching rule to label the new record .\n\nThe rules of an AC inherit some of the metrics of association rules, like the support or the confidence. Metrics can be used to order or filter the rules in the model and to evaluate their quality.\n\nThe first proposal of a classification model made of association rules was CBA, although other authors had previously proposed the mining of association rules for classification. Other authors have since then proposed multiple changes to the initial model, like the addition of a redundant rule pruning phase or the exploitation of Emerging Patterns.\n\nNotable implementations include:\n\n- CMAR\n- CPAR\n- L³\n- CAEP\n- GARC\n- ADT.\n", "related": "NONE"}
{"id": "406624", "url": "https://en.wikipedia.org/wiki?curid=406624", "title": "Time series", "text": "Time series\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n\nTime series are very frequently plotted via line charts. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n\nTime series \"analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series \"forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test theories that the current values of one or more independent time series affect the current value of another time series, this type of analysis of time series is not called \"time series analysis\", which focuses on comparing values of a single time series or multiple dependent time series at different points in time. Interrupted time series analysis is the analysis of interventions on a single time series.\n\nTime series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order). Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility.)\n\nTime series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).\n\nMethods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.\n\nAdditionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.\n\nMethods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.\n\nA time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset). A data set may exhibit characteristics of both panel data and time series data. One way to tell is to ask what makes one data record unique from the other records. If the answer is the time data field, then this is a time series data set candidate. If determining a unique record requires a time data field and an additional identifier which is unrelated to time (student ID, stock symbol, country code), then it is panel data candidate. If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.\n\nThere are several types of motivation and data analysis available for time series which are appropriate for different purposes and etc.\n\nIn the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection and estimation. In the context of data mining, pattern recognition and machine learning time series analysis can be used for clustering, classification, query by content, anomaly detection as well as forecasting.\n\nThe clearest way to examine a regular time series manually is with a line chart such as the one shown for tuberculosis in the United States, made with a spreadsheet program. The number of cases was standardized to a rate per 100,000 and the percent change per year in this rate was calculated. The nearly steadily dropping line shows that the TB incidence was decreasing in most years, but the percent change in this rate varied by as much as +/- 10%, with 'surges' in 1975 and around the early 1990s. The use of both vertical axes allows the comparison of two time series in one graphic.\n\nOther techniques include:\n\n- Autocorrelation analysis to examine serial dependence\n- Spectral analysis to examine cyclic behavior which need not be related to seasonality. For example, sun spot activity varies over 11 year cycles. Other common examples include celestial phenomena, weather patterns, neural activity, commodity prices, and economic activity.\n- Separation into components representing trend, seasonality, slow and fast variation, and cyclical irregularity: see trend estimation and decomposition of time series\n\nCurve fitting is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points, possibly subject to constraints. Curve fitting can involve either interpolation, where an exact fit to the data is required, or smoothing, in which a \"smooth\" function is constructed that approximately fits the data. A related topic is regression analysis, which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization, to infer values of a function where no data are available, and to summarize the relationships among two or more variables. Extrapolation refers to the use of a fitted curve beyond the range of the observed data, and is subject to a degree of uncertainty since it may reflect the method used to construct the curve as much as it reflects the observed data.\n\nThe construction of economic time series involves the estimation of some components for some dates by interpolation between values (\"benchmarks\") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (\"reading between the lines\"). Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates. Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fit into time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression).The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set. Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.\n\nExtrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.\n\nIn general, a function approximation problem asks us to select a function among a well-defined class that closely matches (\"approximates\") a target function in a task-specific way.\nOne can distinguish two major classes of function approximation problems: First, for known target functions approximation theory is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n\nSecond, the target function, call it \"g\", may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (\"x\", \"g\"(\"x\")) is provided. Depending on the structure of the domain and codomain of \"g\", several techniques for approximating \"g\" may be applicable. For example, if \"g\" is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used. If the codomain (range or target set) of \"g\" is a finite set, one is dealing with a classification problem instead. A related problem of \"online\" time series approximation is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.\n\nTo some extent the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.\n\nIn statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.\n- Fully formed statistical models for stochastic simulation purposes, so as to generate alternative versions of the time series, representing what might happen over non-specific time-periods in the future\n- Simple or fully formed statistical models to describe the likely outcome of the time series in the immediate future, given knowledge of the most recent outcomes (forecasting).\n- Forecasting on time series is usually done using automated statistical software packages and programming languages, such as Apache Spark, Julia, Python, R, SAS, SPSS and many others.\n- Forecasting on large scale data is done using Spark which has spark-ts as a third party package.\n\nAssigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language.\n\nThis approach is based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation, the development of which was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. Kálmán, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. See Kalman filter, Estimation theory, and Digital signal processing\n\nSplitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.\n\nModels for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the \"autoregressive\" (AR) models, the \"integrated\" (I) models, and the \"moving average\" (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial \"V\" for \"vector\", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some \"forcing\" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final \"X\" for \"exogenous\".\n\nNon-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel)\n\nAmong other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.\n\nIn recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.\n\nA Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.\n\nA number of different notations are in use for time-series analysis. A common notation specifying a time series \"X\" that is indexed by the natural numbers is written\n\nAnother common notation is\nwhere \"T\" is the index set.\n\nThere are two sets of conditions under which much of the theory is built:\n- Stationary process\n- Ergodic process\n\nHowever, ideas of stationarity must be expanded to consider two important ideas: strict stationarity and second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.\n\nIn addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time–frequency representation of a time-series or signal.\n\nTools for investigating time-series data include:\n\n- Consideration of the autocorrelation function and the spectral density function (also cross-correlation functions and cross-spectral density functions)\n- Scaled cross- and auto-correlation functions to remove contributions of slow components\n- Performing a Fourier transform to investigate the series in the frequency domain\n- Use of a filter to remove unwanted noise\n- Principal component analysis (or empirical orthogonal function analysis)\n- Singular spectrum analysis\n- \"Structural\" models:\n- General State Space Models\n- Unobserved Components Models\n- Machine Learning\n- Artificial neural networks\n- Support vector machine\n- Fuzzy logic\n- Gaussian process\n- Hidden Markov model\n- Queueing theory analysis\n- Control chart\n- Shewhart individuals control chart\n- CUSUM chart\n- EWMA chart\n- Detrended fluctuation analysis\n- Dynamic time warping\n- Cross-correlation\n- Dynamic Bayesian network\n- Time-frequency analysis techniques:\n- Fast Fourier transform\n- Continuous wavelet transform\n- Short-time Fourier transform\n- Chirplet transform\n- Fractional Fourier transform\n- Chaotic analysis\n- Correlation dimension\n- Recurrence plots\n- Recurrence quantification analysis\n- Lyapunov exponents\n- Entropy encoding\n\nTime series metrics or features that can be used for time series classification or regression analysis:\n\n- Univariate linear measures\n- Moment (mathematics)\n- Spectral band power\n- Spectral edge frequency\n- Accumulated Energy (signal processing)\n- Characteristics of the autocorrelation function\n- Hjorth parameters\n- FFT parameters\n- Autoregressive model parameters\n- Mann–Kendall test\n- Univariate non-linear measures\n- Measures based on the correlation sum\n- Correlation dimension\n- Correlation integral\n- Correlation density\n- Correlation entropy\n- Approximate entropy\n- Sample entropy\n- Wavelet entropy\n- Rényi entropy\n- Higher-order methods\n- Marginal predictability\n- Dynamical similarity index\n- State space dissimilarity measures\n- Lyapunov exponent\n- Permutation methods\n- Local flow\n- Other univariate measures\n- Algorithmic complexity\n- Kolmogorov complexity estimates\n- Hidden Markov Model states\n- Rough path signature\n- Surrogate time series and surrogate correction\n- Loss of recurrence (degree of non-stationarity)\n- Bivariate linear measures\n- Maximum linear cross-correlation\n- Linear Coherence (signal processing)\n- Bivariate non-linear measures\n- Non-linear interdependence\n- Dynamical Entrainment (physics)\n- Measures for Phase synchronization\n- Measures for Phase locking\n- Similarity measures:\n- Cross-correlation\n- Dynamic Time Warping\n- Hidden Markov Models\n- Edit distance\n- Total correlation\n- Newey–West estimator\n- Prais–Winsten transformation\n- Data as Vectors in a Metrizable Space\n-  Minkowski distance\n-  Mahalanobis distance\n- Data as time series with envelopes\n-  Global standard deviation\n-  Local standard deviation\n-  Windowed standard deviation\n- Data interpreted as stochastic series\n-  Pearson product-moment correlation coefficient\n-  Spearman's rank correlation coefficient\n- Data interpreted as a probability distribution function\n-  Kolmogorov–Smirnov test\n-  Cramér–von Mises criterion\n\nTime series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)\n\n- Braided graphs\n- Line charts\n- Slope graphs\n\n- Horizon graphs\n- Reduced line chart (small multiples)\n- Silhouette graph\n- Circular silhouette graph\n\n- Durbin J., Koopman S.J. (2001), \"Time Series Analysis by State Space Methods\", Oxford University Press.\n- Priestley, M. B. (1981), \"Spectral Analysis and Time Series\", Academic Press.\n- Shumway R. H., Stoffer D. S. (2017), \"Time Series Analysis and its Applications: With R Examples (ed. 4)\", Springer,\n- Weigend A. S., Gershenfeld N. A. (Eds.) (1994), \"Time Series Prediction: Forecasting the Future and Understanding the Past\". Proceedings of the NATO Advanced Research Workshop on Comparative Time Series Analysis (Santa Fe, May 1992), Addison-Wesley.\n- Wiener, N. (1949), \"Extrapolation, Interpolation, and Smoothing of Stationary Time Series\", MIT Press.\n- Woodward, W. A., Gray, H. L. & Elliott, A. C. (2012), \"Applied Time Series Analysis\", CRC Press.\n\n- Introduction to Time series Analysis (Engineering Statistics Handbook) — A practical guide to Time series analysis.\n", "related": "NONE"}
{"id": "40409788", "url": "https://en.wikipedia.org/wiki?curid=40409788", "title": "Convolutional neural network", "text": "Convolutional neural network\n\nIn deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. They have applications in image and video recognition, recommender systems, image classification, medical image analysis, natural language processing, and financial time series.\n\nCNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"fully-connectedness\" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.\n\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\nCNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.\n\nThe name “convolutional neural\nnetwork” indicates that the network employs a mathematical operation called\nconvolution. Convolution is a specialized kind of linear operation. Convolutional\nnetworks are simply neural networks that use convolution in place of general matrix\nmultiplication in at least one of their layers.\n\nA convolutional neural network consists of an input and an output layer, as well as multiple hidden layers. The hidden layers of a CNN typically consist of a series of convolutional layers that \"convolve\" with a multiplication or other dot product. The activation function is commonly a RELU layer, and is subsequently followed by additional convolutions such as pooling layers, fully connected layers and normalization layers, referred to as hidden layers because their inputs and outputs are masked by the activation function and final convolution.\n\nThough the layers are colloquially referred to as convolutions, this is only by convention. Mathematically, it is technically a \"sliding dot product\" or cross-correlation. This has significance for the indices in the matrix, in that it affects how weight is determined at a specific index point.\n\nWhen programming a CNN, the input is a tensor with shape (number of images) x (image width) x (image height) x (image depth). Then after passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map width) x (feature map height) x (feature map channels). A convolutional layer within a neural network should have the following attributes:\n\n- Convolutional kernels defined by a width and height (hyper-parameters).\n- The number of input channels and output channels (hyper-parameter).\n- The depth of the Convolution filter (the input channels) must be equal to the number channels (depth) of the input feature map.\n\nConvolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus. Each convolutional neuron processes data only for its receptive field. Although fully connected feedforward neural networks can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture, due to the very large input sizes associated with images, where each pixel is a relevant variable. For instance, a fully connected layer for a (small) image of size 100 x 100 has 10,000 weights for \"each\" neuron in the second layer. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters. For instance, regardless of image size, tiling regions of size 5 x 5, each with the same shared weights, requires only 25 learnable parameters. In this way, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using backpropagation.\n\nConvolutional networks may include local or global pooling layers to streamline the underlying computation. Pooling layers reduce the dimensions of the data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, typically 2 x 2. Global pooling acts on all the neurons of the convolutional layer. In addition, pooling may compute a max or an average. \"Max pooling\" uses the maximum value from each of a cluster of neurons at the prior layer. \"Average pooling\" uses the average value from each of a cluster of neurons at the prior layer.\n\nFully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n\nIn neural networks, each neuron receives input from some number of locations in the previous layer. In a fully connected layer, each neuron receives input from \"every\" element of the previous layer. In a convolutional layer, neurons receive input from only a restricted subarea of the previous layer. Typically the subarea is of a square shape (e.g., size 5 by 5). The input area of a neuron is called its \"receptive field\". So, in a fully connected layer, the receptive field is the entire previous layer. In a convolutional layer, the receptive area is smaller than the entire previous layer.\n\nEach neuron in a neural network computes an output value by applying a specific function to the input values coming from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning, in a neural network, progresses by making iterative adjustments to these biases and weights.\n\nThe vector of weights and the bias are called \"filters\" and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces memory footprint because a single bias and a single vector of weights are used across all receptive fields sharing that filter, as opposed to each receptive field having its own bias and vector weighting.\n\nCNN design follows vision processing in living organisms.\n\nWork by Hubel and Wiesel in the 1950s and 1960s showed that cat and monkey visual cortexes contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field. Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space. The cortex in each hemisphere represents the contralateral visual field.\n\nTheir 1968 paper identified two basic visual cell types in the brain:\n\n- simple cells, whose output is maximized by straight edges having particular orientations within their receptive field\n- complex cells, which have larger receptive fields, whose output is insensitive to the exact position of the edges in the field.\n\nHubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.\n\nThe \"neocognitron\" was introduced by Kunihiko Fukushima in 1980.\nIt was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers in CNNs: convolutional layers, and downsampling layers. A convolutional layer contains units whose receptive fields cover a patch of the previous layer. The weight vector (the set of adaptive parameters) of such a unit is often called a filter. Units can share filters. Downsampling layers contain units whose receptive fields cover patches of previous convolutional layers. Such a unit typically computes the average of the activations of the units in its patch. This downsampling helps to correctly classify objects in visual scenes even when the objects are shifted.\n\nIn a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging, J. Weng et al. introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch. Max-pooling is often used in modern CNNs.\n\nSeveral supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron. Today, however, the CNN architecture is usually trained through backpropagation.\n\nThe neocognitron is the first CNN which requires units located at multiple network positions to have shared weights. Neocognitrons were adapted in 1988 to analyze time-varying signals.\n\nThe time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. and was the first convolutional network, as it achieved shift invariance. It did so by utilizing weight sharing in combination with Backpropagation training. Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights, instead of a local one.\n\nTDNNs are convolutional networks that share weights along the temporal dimension. They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant which performs a two dimensional convolution. Since these TDNNs operated on spectrograms the resulting phoneme recognition system was invariant to both, shifts in time and in frequency. This inspired translation invariance in image processing with CNNs. The tiling of neuron outputs can cover timed stages.\n\nTDNNs now achieve the best performance in far distance speech recognition.\n\nIn 1990 Yamaguchi et al. introduced the concept of max pooling. They did so by combining TDNNs with max pooling in order to realize a speaker independent isolated word recognition system. In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n\nA system to recognize hand-written ZIP Code numbers involved convolutions in which the kernel coefficients had been laboriously hand designed.\n\nYann LeCun et al. (1989) used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types.\n\nThis approach became a foundation of modern computer vision.\n\nLeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1998, that classifies digits, was applied by several banks to recognize hand-written numbers on checks () digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n\nSimilarly, a shift invariant neural network was proposed by W. Zhang et al. for image character recognition in 1988. The architecture and training algorithm were modified in 1991 and applied for medical image processing and automatic detection of breast cancer in mammograms.\n\nA different convolution-based design was proposed in 1988 for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.\n\nThe feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n\nAlthough CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n\nIn 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU. In 2005, another paper also emphasised the value of GPGPU for machine learning.\n\nThe first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU. Subsequent work also used GPUs, initially for other types of neural networks (different from CNNs), especially unsupervised neural networks.\n\nIn 2010, Dan Ciresan et al. at IDSIA showed that even deep standard neural networks with many layers can be quickly trained on GPU by supervised learning through the old method known as backpropagation. Their network outperformed previous machine learning methods on the MNIST handwritten digits benchmark. In 2011, they extended this GPU approach to CNNs, achieving an acceleration factor of 60, with impressive results. In 2011, they used such CNNs on GPU to win an image recognition contest where they achieved superhuman performance for the first time. Between May 15, 2011 and September 30, 2012, their CNNs won no less than four image competitions. In 2012, they also significantly improved on the best performance in the literature for multiple image databases, including the MNIST database, the NORB database, the HWDB1.0 dataset (Chinese characters) and the CIFAR10 dataset (dataset of 60000 32x32 labeled RGB images).\n\nSubsequently, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012. A very deep CNN with over 100 layers by Microsoft won the ImageNet 2015 contest.\n\nCompared to the training of CNNs using GPUs, not much attention was given to the Intel Xeon Phi coprocessor.\nA notable development is a parallelization method for training convolutional neural networks on the Intel Xeon Phi, named Controlled Hogwild with Arbitrary Order of Synchronization (CHAOS).\nCHAOS exploits both the thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.\n\nIn the past, traditional multilayer perceptron (MLP) models have been used for image recognition. However, due to the full connectivity between nodes, they suffered from the curse of dimensionality, and did not scale well with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.\n\nFor example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n\nAlso, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in image data, both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n\nConvolutional neural networks are biologically inspired variants of multilayer perceptrons that are designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n- 3D volumes of neurons. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. where each neuron inside a convolutional layer is connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.\n- Local connectivity: following the concept of receptive fields, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learned \"filters\" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear filters that become increasingly global (i.e. responsive to a larger region of pixel space) so that the network first creates representations of small parts of the input, then from them assembles representations of larger areas.\n- Shared weights: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature within their specific response field. Replicating units in this way allows for the resulting feature map to be equivariant under changes in the locations of input features in the visual field, i.e. they grant translational equivariance.\n- Pooling: In a CNN's pooling layers, feature maps are divided into rectangular sub-regions, and the features in each rectangle are independently down-sampled to a single value, commonly by taking their average or maximum value. In addition to reducing the sizes of feature maps, the pooling operation grants a degree of translational invariance to the features contained therein, allowing the CNN to be more robust to variations in their positions.\n\nTogether, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n\nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.\n\nThe convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.\n\nStacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.\n\nWhen dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n\nThe extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.\n\nThree hyperparameters control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.\n\n- The \"depth\" of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.\n- \"Stride\" controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. When the stride is 2 then the filters jump 2 pixels at a time as they slide around. Similarly, for any integer formula_1 a stride of \"S\" causes the filter to be translated by \"S\" units at a time per output. In practice, stride lengths of formula_2 are rare. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions when stride length is increased.\n- Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.\n\nThe spatial size of the output volume can be computed as a function of the input volume size formula_3, the kernel field size of the convolutional layer neurons formula_4, the stride with which they are applied formula_5, and the amount of zero padding formula_6 used on the border. The formula for calculating how many neurons \"fit\" in a given volume is given by\n\nformula_7\n\nIf this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be formula_8 when the stride is formula_9 ensures that the input volume and output volume will have the same size spatially. However, it's not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n\nA parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a \"depth slice\", the neurons in each depth slice are constrained to use the same weights and bias.\n\nSince all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume. Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.\n\nSometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n\nAnother important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which \"max pooling\" is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum.\n\nIntuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.\n\nThe pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2×2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\nformula_10\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged.\n\nIn addition to max pooling, pooling units can use other functions, such as average pooling or ℓ-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which performs better in practice.\n\nDue to the aggressive reduction in the size of the representation, there is a recent trend towards using smaller filters or discarding pooling layers altogether.\n\"Region of Interest\" pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.\n\nPooling is an important component of convolutional neural networks for object detection based on Fast R-CNN architecture.\n\nReLU is the abbreviation of rectified linear unit, which applies the non-saturating activation function formula_11. It effectively removes negative values from an activation map by setting them to zero. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.\n\nOther functions are also used to increase nonlinearity, for example the saturating hyperbolic tangent formula_12, formula_13, and the sigmoid function formula_14. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.\n\nFinally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n\nThe \"loss layer\" specifies how training penalizes the deviation between the predicted (output) and true labels and is normally the final layer of a neural network. Various loss functions appropriate for different tasks may be used.\n\nSoftmax loss is used for predicting a single class of \"K\" mutually exclusive classes. Sigmoid cross-entropy loss is used for predicting \"K\" independent probability values in formula_15. Euclidean loss is used for regressing to real-valued labels formula_16.\n\nCNNs use more hyperparameters than a standard multilayer perceptron (MLP). While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimizing.\n\nSince feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values \"v\" with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n\nThe number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n\nCommon filter shapes found in the literature vary greatly, and are usually chosen based on the dataset.\n\nThe challenge is, thus, to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset, and without overfitting.\n\nTypical values are 2×2. Very large input volumes may warrant 4×4 pooling in the lower layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.\n\nRegularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n\nBecause a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout. At each training stage, individual nodes are either \"dropped out\" of the net with probability formula_17 or kept with probability formula_18, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n\nIn the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.\n\nAt testing time after training has finished, we would ideally like to find a sample average of all possible formula_19 dropped-out networks; unfortunately this is unfeasible for large values of formula_20. However, we can find an approximation by using the full network with each node's output weighted by a factor of formula_18, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates formula_19 neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n\nBy avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.\n\nDropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability formula_17. Each unit thus receives input from a random subset of units in the previous layer.\n\nDropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n\nA major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n\nIn stochastic pooling, the conventional deterministic pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\n\nAn alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images, which delivers excellent performance on the MNIST data set. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n\nSince the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.\n\nOne of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n\nAnother simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n\nA simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.\n\nL2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n\nL1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called Elastic net regularization). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.\n\nAnother form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector formula_24 of every neuron to satisfy formula_25. Typical values of formula_26 are order of 3–4. Some papers report improvements when using this form of regularization.\n\nPooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.\n\nCurrently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.\n\nThus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.\n\nCNNs are often used in image recognition systems. In 2012 an error rate of 0.23 percent on the MNIST database was reported. Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database. Subsequently, a similar CNN called \nAlexNet won the ImageNet Large Scale Visual Recognition Challenge 2012.\n\nWhen applied to facial recognition, CNNs achieved a large decrease in error rate. Another paper reported a 97.6 percent recognition rate on \"5,600 still images of more than 10 subjects\". CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.\n\nThe ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014, a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans. The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.\n\nIn 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.\n\nCompared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space. Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream. Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies. Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines and Independent Subspace Analysis.\n\nCNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing, search query retrieval, sentence modeling, classification, prediction and other traditional NLP tasks.\n\nA CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.\n\nCNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based rational drug design. The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures, AtomNet discovers chemical features, such as aromaticity, sp carbons and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus and multiple sclerosis.\n\nCNNs can be naturally tailored to analyze a sufficiently large collection of time series data representing one-week-long human physical activity streams augmented by the rich clinical data (including the death register, as provided by, e.g., the NHANES study). A simple CNN was combined with Cox-Gompertz proportional hazards model and used to produce a proof-of-concept example of digital biomarkers of aging in the form of all-causes-mortality predictor.\n\nCNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%. It also earned a win against the program Chinook at its \"expert\" level of play.\n\nCNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play. Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.\n\nA couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.\n\nRecurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better. Dilated convolutions might enable one-dimensional convolutional neural networks to effectively learn time series dependences. Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients. Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from. CNNs can also be applied to further tasks in time series analysis (e.g., time series classification or quantile forecasting).\n\nFor many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.\n\nEnd-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.\n\nA deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs.\n\nPreliminary results were presented in 2014, with an accompanying paper in February 2015. The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.\n\nConvolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR have been obtained using CDBNs.\n\n- Caffe: A library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in C++, and has Python and MATLAB wrappers.\n- Deeplearning4j: Deep learning in Java and Scala on multi-GPU-enabled Spark. A general-purpose deep learning library for the JVM production stack running on a C++ scientific computing engine. Allows the creation of custom layers. Integrates with Hadoop and Kafka.\n- Dlib: A toolkit for making real world machine learning and data analysis applications in C++.\n- Microsoft Cognitive Toolkit: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.\n- TensorFlow: Apache 2.0-licensed Theano-like library with support for CPU, GPU, Google's proprietary tensor processing unit (TPU), and mobile devices.\n- Theano: The reference deep-learning library for Python with an API largely compatible with the popular NumPy library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.\n- Torch: A scientific computing framework with wide support for machine learning algorithms, written in C and Lua. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.\n\n- Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.\n\n", "related": "\n- Convolution\n- Deep learning\n- Natural-language processing\n- Neocognitron\n- Scale-invariant feature transform\n- Time delay neural network\n- Vision processing unit\n\n- CS231n: Convolutional Neural Networks for Visual Recognition — Andrej Karpathy's Stanford computer science course on CNNs in computer vision\n- An Intuitive Explanation of Convolutional Neural Networks — A beginner level introduction to what Convolutional Neural Networks are and how they work\n- Convolutional Neural Networks for Image Classification — Literature Survey\n"}
{"id": "43385931", "url": "https://en.wikipedia.org/wiki?curid=43385931", "title": "Data exploration", "text": "Data exploration\n\nData exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data.\n\nData exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics.\n\nThis is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using spreadsheets or similar tools to view the raw data.\n\nAll of these activities are aimed at creating a mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis.\n\nOnce this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data (data cleansing), correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality.\n\nData exploration can also refer to the ad hoc querying and visualization of data to identify potential relationships or insights that may be hidden in the data.\n\nTraditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists; the latter being a relatively new role within enterprises and larger organizations.\n\nThis area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As its most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Many common patterns include regression and classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning.\n\nBy employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques.\n\n- Trifacta – a data preparation and analysis platform\n- Paxata – self-service data preparation software\n- Alteryx – data blending and advanced data analytics software\n- Microsoft Power BI - interactive visualization and data analysis tool\n- OpenRefine - a standalone open source desktop application for data clean-up and data transformation\n- Tableau software – interactive data visualization software\n\n", "related": "\n- Exploratory data analysis\n- Machine learning\n- Data profiling\n- Data visualization\n"}
{"id": "59729969", "url": "https://en.wikipedia.org/wiki?curid=59729969", "title": "Waifu2x", "text": "Waifu2x\n\nwaifu2x is an image scaling and noise reduction program for anime-style art, but also supports photos.\n\nwaifu2x was inspired by Super-Resolution Convolutional Neural Network (SRCNN). It uses Nvidia CUDA for computing, although alternative implementations that allow for OpenCL and Vulkan have been created.\n\"waifu\" is anime slang for a character to whom one is attracted.\n\"2x\" means two-times magnification. \n\n", "related": "\n- Comparison gallery of image scaling algorithms\n\n"}
{"id": "59968610", "url": "https://en.wikipedia.org/wiki?curid=59968610", "title": "Learning curve (machine learning)", "text": "Learning curve (machine learning)\n\nIn machine learning, a learning curve (or training curve) shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much a machine learning model benefits from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, it will not benefit much from more training data.\n\nThe machine learning curve is useful for many purposes including comparing different algorithms, choosing model parameters during design, adjusting optimization to improve convergence, and determining the amount of data used for training.\n\nIn the machine learning domain, there are two connotations of learning curves differing in the x-axis of the curves, with experience of the model graphed either as the number of training examples used for learning or the number of iterations used in training the model.\n\n", "related": "\n- Overfitting\n- Bias–variance tradeoff\n- Model selection\n- Cross-validation (statistics)\n- Validity (statistics)\n- Verification and validation\n"}
{"id": "59969558", "url": "https://en.wikipedia.org/wiki?curid=59969558", "title": "Learning rate", "text": "Learning rate\n\nIn machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model \"learns\". The learning rate is often denoted by the character η or α.\n\nIn setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the direction toward the minimum is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum. \n\nIn order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newton's method. In these quasi-Newton methods, step size becomes an important component of the optimization algorithm.\n\nA learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters: decay and momentum . There are many different learning rate schedules but the most common are time-based, step-based and exponential.\n\nDecay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minima, and is controlled by a hyperparameter.\n\nMomentum is analogous to a ball rolling down a hill; we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by 'rolling over' small bumps. Momentum is controlled by a hyper parameter analogous to a ball's mass which must be chosen manually—too high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras.\n\nTime-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is:\n\nformula_1\n\nwhere formula_2 is the learning rate, formula_3 is a decay parameter and formula_4 is the iteration step.\n\nStep-based learning schedules changes the learning rate according to some pre defined steps. The decay application formula is here defined as:\n\nformula_5\n\nwhere formula_6 is the learning rate at iteration formula_4, formula_8 is the initial learning rate, formula_3 is how much the learning rate should change at each drop (0.5 corresponds to a halving) and formula_10 corresponds to the droprate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations). The \"floor\" function here drops the value of its input to 0 for all values smaller than 1.\n\nExponential learning schedules are similar to step-based but instead of steps a decreasing exponential function is used. The mathematical formula for factoring in the decay is:\n\nformula_11\n\nwhere formula_3 is a decay parameter.\n\nThe issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, Adam which are generally built into deep learning libraries such as Keras.\n\n", "related": "\n- Hyperparameter (machine learning)\n- Hyperparameter optimization\n- Stochastic gradient descent\n- Variable metric methods\n- Overfitting\n- Backpropagation\n- AutoML\n- Model selection\n- Self-tuning\n"}
{"id": "59973182", "url": "https://en.wikipedia.org/wiki?curid=59973182", "title": "Nature Machine Intelligence", "text": "Nature Machine Intelligence\n\nNature Machine Intelligence is a scientific journal dedicated to covering machine learning and artificial intelligence. It was created by Nature Research in response to the machine learning explosion of the 2010s. It launched in January 2019, and its opening was met with controversy and boycotts within the machine learning research community due to opposition to Nature publishing the journal as closed access.\n", "related": "NONE"}
{"id": "54550729", "url": "https://en.wikipedia.org/wiki?curid=54550729", "title": "Connectionist temporal classification", "text": "Connectionist temporal classification\n\nConnectionist temporal classification (CTC) is a type of neural network output and associated scoring function, for training recurrent neural networks (RNNs) such as LSTM networks to tackle sequence problems where the timing is variable. It can be used for tasks like on-line handwriting recognition or recognizing phonemes in speech audio. CTC refers to the outputs and scoring, and is independent of the underlying neural network structure. It was introduced in 2006.\n\nThe input is a sequence of observations, and the outputs are a sequence of labels, which can include blank outputs. The difficulty of training comes from there being many more observations than there are labels. For example in speech audio there can be multiple time slices which correspond to a single phoneme. Since we don't know the alignment of the observed sequence with the target labels we predict a probability distribution at each time step. A CTC network has a continuous output (e.g. softmax), which is fitted through training to model the probability of a label. CTC does not attempt to learn boundaries and timings: Label sequences are considered equivalent if they differ only in alignment, ignoring blanks. Equivalent label sequences can occur in many ways – which makes scoring a non-trivial task, but there is an efficient forward–backward algorithm for that.\n\nCTC scores can then be used with the back-propagation algorithm to update the neural network weights.\n\nAlternative approaches to a CTC-fitted neural network include a hidden Markov model (HMM).\n", "related": "NONE"}
{"id": "37815827", "url": "https://en.wikipedia.org/wiki?curid=37815827", "title": "Astrostatistics", "text": "Astrostatistics\n\nAstrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference.\n\nPractitioners are represented by the International Astrostatistics Association affiliated with the International Statistical Institute, the International Astronomical Union Working Group in Astrostatistics and Astroinformatics, the American Astronomical Society Working Group in Astroinformatics and Astrostatistics, the American Statistical Association Interest Group in Astrostatistics, and the Cosmostatistics Initiative. All of these organizations participate in the Astrostatistics and Astroinformatics Portal Web site.\n", "related": "NONE"}
{"id": "60968880", "url": "https://en.wikipedia.org/wiki?curid=60968880", "title": "Weak supervision", "text": "Weak supervision\n\nWeak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.\n\nMachine learning models and techniques are increasingly accessible to researchers and developers; the real-world usefulness of these models, however, depends on access to high-quality labeled training data. This need for labeled training data often proves to be a significant obstacle to the application of machine learning models within an organization or industry. This bottleneck effect manifests itself in various ways, including the following examples:\n\nInsufficient quantity of labeled data\n\nWhen machine learning techniques are initially used in new applications or industries, there is often not enough training data available to apply traditional processes. Some industries have the benefit of decades' worth of training data readily available; those that do not are at a significant disadvantage. In such cases, obtaining training data may be impractical, expensive, or impossible without waiting years for its accumulation.\n\nInsufficient subject-matter expertise to label data\n\nWhen labeling training data requires specific relevant expertise, creation of a usable training data set can quickly become prohibitively expensive. This issue is likely to occur, for example, in biomedical or security-related applications of machine learning.\n\nInsufficient time to label and prepare data\n\nMost of the time required to implement machine learning is spent in preparing data sets. When an industry or research field deals with problems that are, by nature, rapidly evolving, it can be impossible to collect and prepare data quickly enough for results to be useful in real-world applications. This issue could occur, for example, in fraud detection or cybersecurity applications.\n\nOther areas of machine learning exist that are likewise motivated by the demand for increased quantity and quality of labeled training data but employ different high-level techniques to approach this demand. These other approaches include active learning, semi-supervised learning, and transfer learning.\n\nWeak labels are intended to decrease the cost and increase the efficiency of human efforts expended in hand-labeling data. They can take many forms, including the following:\n\n- Imprecise or inexact labels: developers may use higher-level, less precise input from subject-matter experts to create heuristic rules, define expected distributions, or impose other constraints on the training data.\n- Inaccurate labels: developers may use inexpensive, lower-quality input through means such as crowdsourcing to obtain labels that are numerous, but not expected to be perfectly correct.\n- Existing resources: developers may take advantage of existing resources (such as knowledge bases, alternative data sets, or pre-trained models) to create labels that are helpful, though not perfectly suited for the given task.\n\nApplications of weak supervision are numerous and varied within the machine learning research community.\n\nStanford University researchers created Snorkel, an open-source system for quickly assembling training data through weak supervision. Snorkel employs the central principles of the data programming paradigm, in which developers create labeling functions, which are then used to programmatically label data, and employs supervised learning techniques to assess the accuracy of those labeling functions. In this way, potentially low-quality inputs can be used to create high-quality models.\n\nIn a joint work with Google, Stanford researchers showed that existing organizational knowledge resources could be converted into weak supervision sources and used to significantly decrease development costs and time.\n\nIn 2019, Massachusetts Institute of Technology and Google researchers released cleanlab, the first standardized Python package for machine learning and deep learning with noisy labels. Cleanlab implements confident learning, a framework of theory and algorithms for dealing with uncertainty in dataset labels, to (1) find label errors in datasets, (2) characterize label noise, and (3) standardize and simplify research in weak supervision and learning with noisy labels.\n\nResearchers at University of Massachusetts Amherst propose augmenting traditional active learning approaches by soliciting labels on features rather than instances within a data set.\n\nResearchers at Johns Hopkins University propose reducing the cost of labeling data sets by having annotators provide rationales supporting each of their data annotations, then using those rationales to train both discriminative and generative models for labeling additional data.\n\nResearchers at University of Alberta propose a method that applies traditional active learning approaches to enhance the quality of the imperfect labels provided by weak supervision.\n", "related": "NONE"}
{"id": "60992857", "url": "https://en.wikipedia.org/wiki?curid=60992857", "title": "Federated learning", "text": "Federated learning\n\nFederated learning (aka collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples. This approach stands in contrast to traditional centralized machine learning techniques where all data samples are uploaded to one server, as well as to more classical decentralized approaches which assume that local data samples are identically distributed.\n\nFederated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus addressing critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, or pharmaceutics.\n\nFederated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights of a deep neural network) between these local models at some frequency to generate a global model.\n\nFederated learning algorithms may use a central server that orchestrates the different steps of the algorithm and acts as a reference clock, or they may be peer-to-peer, where no such central server exists. In the non peer-to-peer case, a federated learning process can be broken down in multiple rounds, each consisting of 4 general steps.\nThe main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are identically distributed and roughly have the same size. None of these hypotheses are made for federated learning; instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude.\n\nTo ensure good task performance of a final, central machine learning model, federated learning relies on an iterative process broken up into an atomic set of client-server interactions known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model.\n\nIn the methodology below, we use a central server for this aggregation, while local nodes perform local training depending on the central server's orders. However, other strategies lead to the same results without central servers, in a peer-to-peer approach, using gossip methodologies.\n\nA statistical model (e.g., linear regression, neural network, boosting) is chosen to be trained on local nodes and initialized. Nodes are activated and wait for the central server to give calculation tasks.\n\nFor multiple iterations of so-called federated learning rounds, the following steps are performed:\n\nA fraction of local nodes are selected to start training on local data. They all acquire the same current statistical model from the central server. Other nodes wait for the next federated round.\n\nThe central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g. for some batch updates of gradient descent).\n\nEach node returns the locally learned incremental model updates to the central server. The central server aggregates all results and stores the new model. It also handles failures (e.g., connection lost with a node while training). The system returns to the selection phase.\n\nWhen a pre-specified termination criterion (e.g. maximal number of rounds or local accuracies higher than some target) has been met, the central server orders the end of the iterative training process. The central server contains a robust model which was trained on multiple heterogeneous data sources.\n\nThe way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches: for instance no central orchestrating server, or stochastic communication.\n\nIn particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to a several randomly-selected others, which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost.\n\nOnce the topology of the node network is chosen, one can control different parameters of the federated learning process (in opposition to the machine learning model's own hyperparameters) to optimize learning :\n\n- Number of federated learning rounds : T\n- Total number of nodes used in the process : K\n- Fraction of nodes used at each iteration for each node : C\n- Local batch size used at each learning iteration : B\n\nOther model-dependent parameters can also be tinkered with, such as :\n\n- Number of iterations for local training before pooling : N\n- Local learning rate : η\n\nThose parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, bandwidth). For instance, stochastically choosing a limited fraction C of nodes for each iteration diminishes computing cost and may prevent overfitting, in the same way that stochastic gradient descent can reduce overfitting.\n\nIn this section, we follow the exposition of Communication-Efficient Learning of Deep Networks from Decentralized Data, H. Brendan McMahan and al. 2017.\n\nTo describe the federated strategies, let us introduce some notations:\n\n- K : total number of clients;\n- k : index of clients;\n- n : number of data samples available during training for client k;\n- w : model's weight vector on client k, at the federated round t;\n- l(w, b) : loss function for weights w and batch b;\n- E : number of local epochs;\n\nDeep learning training mainly relies on variants of stochastic gradient descent, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.\n\nFederated stochastic gradient descent is the direct transposition of this algorithm to the federated setting, but by using a random fraction C of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.\n\nFederative averaging (FedAvg) is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged model's performance.\n\nFederated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoid data communication, which can require significant resources before starting centralized machine learning.\n\nFederated learning raises several statistical challenges :\n\n- Heterogeneity between the different local datasets: each node may have some bias with respect to the general population, and the size of the datasets may vary significantly;\n- Temporal heterogeneity: each local dataset's distribution may vary with time;\n- Interoperability of each node's dataset is a prerequisite;\n- Each node's dataset may require regular curations.\n- Hiding training data might allow attackers to inject backdoors into the global model .\n- Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation\n\nThe main advantage of using federated approaches to machine learning is to ensure data privacy or data secrecy. Indeed, no local data is uploaded externally, concatenated or exchanged. Since the entire database is segmented into local bits, this makes it more difficult to hack into it.\n\nWith federated learning, only machine learning parameters are exchanged. In addition, such parameters can be encrypted before sharing between learning rounds to extend privacy. Despite such protective measures, these parameters mays still leak information about the underlying data samples, for instance, by making multiple specific queries on specific datasets. Querying capability of nodes thus is a major attention point, which can be addressed using differential privacy or secure aggregation.\n\nThe generated model delivers insights based on the global patterns of nodes. However, if a participating node wishes to learn from global patterns but also adapt outcomes to its peculiar status, the federated learning methodology can be adapted to generate two models at once in a multi-task learning framework.\n\nIn the case of deep neural networks, it is possible to share some layers across the different nodes and keep some of them on each local node. Typically, first layers performing general pattern recognition are shared and trained all datasets. The last layers will remain on each local node and only be trained on the local node's dataset.\n\nWestern legal frameworks emphasize more and more on data protection and data traceability. White House 2012 Report recommended the application of a data minimization principle, which is mentioned in European GDPR. In some cases, it is impossible to transfer data from a country to another (e.g., genomic data), however international consortia are sometimes necessary for scientific advances. In such cases federated learning brings solutions to train a global model while respecting security constraints.\n\nFederated learning has started to emerge as an important research topic in 2015 and 2016, with the first publications on federative averaging in telecommunication settings. Recent publications have emphasized the development of resource allocation strategies, especially to reduce communication requirements between nodes with gossip algorithms. In addition, recent publications continue to work on the federated algorithms robustness to differential privacy attacks.\n\nFederated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with other (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node.\n\nOne of the first use cases of federated learning was implemented by Google for predictive keyboards. Under high regulatory pressure, it showed impossible to upload every user's text message to train the predictive algorithm for word guessing. Besides, such a process would hijack too much of the user's data. Despite the sometimes limited memory and computing power of smartphones, Google has made a compelling use case out of its G-board, as presented during the Google IO 2019 event.\n\nPharmaceutical research is pivoting towards a new paradigm : real world data use for generating drug leads and synthetic control arms. Generating knowledge on complex biological problems require to gather a lot of data from diverse medical institutions, which are eager to maintain control of their sensitive patient data. Federated learning, especially assisted by high traceability technologies (distributive ledgers) enable researchers to train predictive models on many sensitive data in a transparent way without uploading them. In 2019, French start-up Owkin is pioneering the development of biomedical machine learning models based on such algorithms to capture heterogeneous data from both pharmaceutical companies and medical institutions.\n\nSelf driving cars encapsulate many machine learning technologies to function: computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes.\n\nSome libraries have been made to facilitate the development of federated learning systems, including: \n\n- PySyft - a library for implementing federated learning, multi-party computation, and differential privacy within PyTorch and TensorFlow from the open-source community OpenMined\n- TensorFlow Federated - a platform within TensorFlow with high-level and low-level interfaces for federated learning\n\n- \"Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016\" at eur-lex.europa.eu. Retrieved October 18, 2019.\n", "related": "NONE"}
{"id": "61213249", "url": "https://en.wikipedia.org/wiki?curid=61213249", "title": "Distill (journal)", "text": "Distill (journal)\n\nDistill, is a peer-reviewed scientific journal covering machine learning. Articles may contain interactive graphics and so-called explorable explanations. The journal was established in March 2017 by Google, OpenAI, DeepMind, and Y Combinator Research. The editors-in-chief are Shan Carter (Google Brain), Chris Olah (OpenAI), and Arvind Satyanarayan (MIT Computer Science and Artificial Intelligence Laboratory). The journal is indexed in Ei Compendex. Its launch was criticized as overly hyped by \"The Scholarly Kitchen\", which also noted that most authors were Google employees.\n", "related": "NONE"}
{"id": "61373032", "url": "https://en.wikipedia.org/wiki?curid=61373032", "title": "Machine learning in physics", "text": "Machine learning in physics\n\nApplying classical methods of machine learning to the study of quantum systems (sometimes called \"quantum machine learning\") is the focus of an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other examples include learning Hamiltonians, learning quantum phase transitions, and automatically generating new quantum experiments. Classical machine learning is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technologies development, and computational materials design. In this context, it can be used for example as a tool to interpolate pre-calculated interatomic potentials or directly solving the Schrödinger equation with a variational method.\n\nThe ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, Bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification, Hamiltonian learning, and the characterization of an unknown unitary transformation. Other problems that have been addressed with this approach are given in the following list:\n\n- Identifying an accurate model for the dynamics of a quantum system, through the reconstruction of the Hamiltonian;\n- Extracting information on unknown states;\n- Learning unknown unitary transformations and measurements;\n- Engineering of quantum gates from qubit networks with pairwise interactions, using time dependent or independent Hamiltonians.\n- Improving the extraction accuracy of physical observables from absorption images of ultracold atoms (degenerate Fermi gas), by the generation of an ideal reference frame.\n\nQuantum machine learning can also be applied to dramatically accelerate the prediction of quantum properties of molecules and materials. This can be helpful for the computational design of new molecules or materials. Some examples include\n\n- Interpolating interatomic potentials;\n- Inferring molecular atomization energies throughout chemical compound space;\n- Accurate potential energy surfaces with restricted Boltzmann machines;\n- Automatic generation of new quantum experiments;\n- Solving the many-body, static and time-dependent Schrödinger equation;\n- Identifying phase transitions from entanglement spectra;\n- Generating adaptive feedback schemes for quantum metrology and quantum tomography.\n\nVariational circuits are a family of algorithms which utilize training based on circuit parameters and an objective function. Variational circuits are generally composed of a classical device communicating input parameters (random or pre-trained parameters) into a quantum device, along with a classical Mathematical optimization function. These circuits are very heavily dependent on the architecture of the proposed quantum device because parameter adjustments are adjusted based solely on the classical components within the device. Though the application is considerably infantile in the field of quantum machine learning, it has incredibly high promise for more efficiently generating efficient optimization functions.\n\nMachine learning techniques can be used to find a better manifold of integration for path integrals in order to avoid the sign problem.\n\n", "related": "\n- Quantum computing\n- Quantum machine learning\n- Quantum algorithm for linear systems of equations\n- Quantum annealing\n- Quantum neural network\n"}
{"id": "60951296", "url": "https://en.wikipedia.org/wiki?curid=60951296", "title": "Machine learning in video games", "text": "Machine learning in video games\n\nIn video games, various artificial intelligence techniques have been used in a variety of ways, ranging from non-player character (NPC) control to procedural content generation (PCG). Machine learning is a subset of artificial intelligence that focuses on using algorithms and statistical models to make machines act without specific programming. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems.\n\nInformation on machine learning techniques in the field of games is mostly known to public through research projects as most gaming companies choose not to publish specific information about their intellectual property. The most publicly known application of machine learning in games is likely the use of deep learning agents that compete with professional human players in complex strategy games. There has been a significant application of machine learning on games such as Atari/ALE, \"Doom\", \"Minecraft\", \"StarCraft\", and car racing. Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning.\n\nDeep learning is a subset of machine learning which focuses heavily on the use of artificial neural networks (ANN) that learn to solve complex tasks. Deep learning uses multiple layers of ANN and other techniques to progressively extract information from an input. Due to this complex layered approach, deep learning models often require powerful machines to train and run on.\n\nConvolutional neural networks (CNN) are specialized ANNs that are often used to analyze image data. These types of networks are able to learn translation invariant patterns, which are patterns that are not dependent on location. CNNs are able to learn these patterns in a hierarchy, meaning that earlier convolutional layers will learn smaller local patterns while later layers will learn larger patterns based on the previous patterns. A CNN's ability to learn visual data has made it a commonly used tool for deep learning in games.\n\nRecurrent neural networks are a type of ANN that are designed to process sequences of data in order, one part at a time rather than all at once. An RNN runs over each part of a sequence, using the current part of the sequence along with memory of previous parts of the current sequence to produce an output. These types of ANN are highly effective at tasks such as speech recognition and other problems that depend heavily on temporal order. There are several types of RNNs with different internal configurations; the basic implementation suffers from a lack of long term memory due to the vanishing gradient problem, thus it is rarely used over newer implementations.\n\nA long short-term memory (LSTM) network is a specific implementation of a RNN that is designed to deal with the vanishing gradient problem seen in simple RNNs, which would lead to them gradually \"forgetting\" about previous parts of an inputted sequence when calculating the output of a current part. LSTMs solve this problem with the addition of an elaborate system that uses an additional input/output to keep track of long term data. LSTMs have achieved very strong results across various fields, and were used by several monumental deep learning agents in games.\n\nReinforcement learning is the process of training an agent using rewards and/or punishments. The way an agent is rewarded or punished depends heavily on the problem; such as giving an agent a positive reward for winning a game or a negative one for losing. Reinforcement learning is used heavily in the field of machine learning and can be seen in methods such as Q-learning, policy search, Deep Q-networks and others. It has seen strong performance in both the field of games and robotics.\n\nNeuroevolution involves the use of both neural networks and evolutionary algorithms. Instead of using gradient descent like most neural networks, neuroevolution models make use of evolutionary algorithms to update neurons in the network. Researchers claim that this process is less likely to get stuck in a local minimum and is potentially faster than state of the art deep learning techniques.\n\nMachine learning agents have been used to take the place of a human player rather than function as NPCs, which are deliberately added into video games as part of designed gameplay. Deep learning agents have achieved impressive results when used in competition with both humans and other artificial intelligence agents.\n\nChess is a turn-based strategy game that is considered a difficult AI problem due to the computational complexity of its board space. Similar strategy games are often solved with some form of a Minimax Tree Search. These types of AI agents have been known to beat professional human players, such as the historic 1997 Deep Blue versus Garry Kasparov match. Since then, machine learning agents have shown ever greater success than previous AI agents.\n\nGo is another turn-based strategy game which is considered an even more difficult AI problem than chess. The state space of is Go is around 10^170 possible board states compared to the 10^120 board states for Chess. Prior to recent deep learning models, AI Go agents were only able to play at the level of a human amateur.\n\nGoogle's 2015 AlphaGo was the first AI agent to beat a professional Go player. AlphaGo used a deep learning model to train the weights of a Monte Carlo tree search (MCTS). The deep learning model consisted of 2 ANN, a policy network to predict the probabilities of potential moves by opponents, and a value network to predict the win chance of a given state. The deep learning model allows the agent to explore potential game states more efficiently than a vanilla MCTS. The network were initially trained on games of humans players and then were further trained by games against itself.\n\nAlphaGo Zero, another implementation of AlphaGo, was able to train entirely by playing against itself. It was able to quickly train up to the capabilities of the previous agent.\n\n\"StarCraft\" and its sequel \"\" are real-time strategy (RTS) video games that have become popular environments for AI research. Blizzard and DeepMind have worked together to release a public \"StarCraft 2\" environment for AI research to be done on. Various deep learning methods have been tested on both games, though most agents usually have trouble outperforming the default AI with cheats enable or skilled players of the game.\n\nAlphastar was the first AI agent to beat professional \"StarCraft 2\" players without any in-game advantages. The deep learning network of the agent initially received input from a simplified zoomed out version of the gamestate, but was later updated to play using a camera like other human players. The developers have not publicly released the code or architecture of their model, but have listed several state of the art machine learning techniques such as relational deep reinforcement learning, long short-term memory, auto-regressive policy heads, pointer networks, and centralized value baseline. Alphastar was initially trained with supervised learning, it watched replays of many human games in order to learn basic strategies. It then trained against different versions of itself and was improved through reinforcement learning. The final version was hugely successful, but only trained to play on a specific map in a protoss mirror matchup.\n\n\"Dota 2\" is a multiplayer online battle arena (MOBA) game. Like other complex games, traditional AI agents have not been able to compete on the same level as professional human player. The only widely published information on AI agents attempted on \"Dota 2\" is OpenAI's deep learning Five agent.\n\nOpenAI Five utilized separate LSTM networks to learn each hero. It trained using a reinforcement learning technique known as Proximal Policy Learning running on a system containing 256 GPUs and 128,000 CPU cores. Five trained for months, accumulating 180 years of game experience each day, before facing off with professional players. It was eventually able to beat the 2018 \"Dota 2\" esports champion team in a 2019 series of games.\n\n\"Planetary Annihilation\" is a real-time strategy game which focuses on massive scale war. The developers use ANNs in their default AI agent.\n\nThere have been attempts to make machine learning agents that are able to play more than one game. These \"general\" gaming agents are trained to understand games based on shared properties between them.\n\nAlphaZero is a modified version of AlphaGo Zero which is able to play Shogi, chess, and Go. The modified agent starts without only information basic rules of the game, and is also trained entirely through self-learning. DeepMind was able to train this generalized agent to be competitive with previous versions of itself on Go, as well as top agents in the other two games.\n\nMachine learning agents are often not covered in many game design courses. Previous use of machine learning agents in games may not have been very practical, as even the 2015 version of AlphaGo took hundreds of CPUs and GPUs to train to a strong level. This potentially limits the creation of highly effective deep learning agents to large corporations or extremely wealthy individuals. The extensive training time of neural network based approaches can also take weeks on these powerful machines.\n\nThe problem of effectively training ANN based models extends beyond powerful hardware environments; finding a good way to represent data and learn meaningful things from it is also often a difficult problem. ANN models often overfit to very specific data and perform poorly in more generalized cases. AlphaStar shows this weakness, despite being able to beat professional players, it is only able to do so on a single map when playing a mirror protoss matchup. OpenAI Five also shows this weakness, it was only able to beat professional player when facing a very limited hero pool out of the entire game. This example show how difficult it can be to train a deep learning agent to perform in more generalized situations.\n\nMachine learning agents have shown great success in a variety of different games. However, agents that are too competent also risk making games too difficult for new or casual players. Research has shown that challenge that is too far above a player's skill level will ruin lower player enjoyment. These highly trained agents are likely only desirable against very skilled human players who have many of hours of experience in a given game. Given these factors, highly effective deep learning agents are likely only a desired choice in games that have a large competitive scene, where they can function as an alternative practice option to a skilled human player.\n\nComputer vision focuses on training computers to gain a high-level understanding of digital images or videos. Many computer vision techniques also incorporate forms of machine learning, and have been applied on various video games. This application of computer vision focuses on interpreting game events using visual data. In some cases, artificial intelligence agents have used model-free techniques to learn to play games without any direct connection to internal game logic, solely using video data as input.\n\nAndrej Karpathy has demonstrated that relatively trivial neural network with just one hidden layer is capable of being trained to play \"Pong\" based on screen data alone.\n\nIn 2013, a team at DeepMind demonstrated the use of deep Q-learning to play a variety of Atari video games — \"Beamrider\", \"Breakout\", \"Enduro\", \"Pong\", \"Q*bert\", \"Seaquest\", and \"Space Invaders\" — from screen data.\n\n\"Doom\" (1993) is a first-person shooter (FPS) game. Student researchers from Carnegie Mellon University used computer vision techniques to create an agent that could play the game using only image pixel input from the game. The students used convolutional neural network (CNN) layers to interpret incoming image data and output valid information to a recurrent neural network which was responsible for outputting game moves.\n\nOther uses of vision-based deep learning techniques for playing games have included playing \"Super Mario Bros.\" only using image input, using deep Q-learning for training.\n\nMachine learning has seen research for use in content recommendation and generation. Procedural content generation is the process of creating data algorithmically rather than manually. This type of content is used to add replayability to games without relying on constant additions by human developers. PCG has been used in various games for different types of content generation, examples of which include weapons in \"Borderlands 2\", all world layouts in Minecraft and entire universes in \"No Man's Sky\". Common approaches to PCG include techniques that involve grammars, search-based algorithms, and logic programming. These approaches require humans to manually define the range of content possible, meaning that a human developer decides what features make up a valid piece of generated content. Machine learning is theoretically capable of learning these features when given examples to train off of, thus greatly reducing the complicated step of developers specifying the details of content design. Machine learning techniques used for content generation include Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN), Generative Adversarial networks (GAN), and K-means clustering. Not all of these techniques make use of ANNs, but the rapid development of deep learning has greatly increased the potential of techniques that do.\n\n\"Galactic Arms Race\" is a space shooter video game that uses neuroevolution powered PCG to generate unique weapons for the player. This game was a finalist in the 2010 Indie Game Challenge and its related research paper won the Best Paper Award at the 2009 IEEE Conference on Computational Intelligence and Games. The developers use a form of neuroevolution called cgNEAT to generate new content based on each player's personal preferences.\n\nEach generated item is represented by a special ANN known as a Compositional Pattern Producing Network (CPPNs). During the evolutionary phase of the game cgNEAT calculates the fitness of current items based on player usage and other gameplay metrics, this fitness score is then used decide which CPPNs will reproduce to create a new item. The ending result is the generation of new weapon effects based on the player's preference.\n\n\"Super Mario Bros.\" has been used by several researchers to simulate PCG level creation. Various attempts having used different methods. A version in 2014 used n-grams to generate levels similar to the ones it trained on, which was later improved by making use of MCTS to guide generation. These generations were often not optimal when taking gameplay metrics such as player movement into account, a separate research project in 2017 tried to resolve this problem by generating levels based on player movement using Markov Chains. These projects were not subjected to human testing and may not meet human playability standards.\n\nPCG level creation for \"The Legend of Zelda\" has been attempted by researchers at the University of California, Santa Cruz. This attempt made use of a Bayesian Network to learn high level knowledge from existing levels, while Principal Component Analysis (PCA) was used to represent the different low level features of these levels. The researchers used PCA to compare generated levels to human made levels and found that they were considered very similar. This test did not include playability or human testing of the generated levels.\n\nMusic is often seen in video games and can be a crucial element for influencing the mood of different situations and story points. Machine learning has seen use in the experimental field of music generation; it is uniquely suited to processing raw unstructured data and forming high level representations that could be applied to the diverse field of music. Most attempted methods have involved the use of ANN in some form. Methods include the use of basic feedforward neural networks, autoencoders, restricted boltzmann machines, recurrent neural networks, convolutional neural networks, generative adversarial networks (GANs), and compound architectures that use multiple methods.\n\nThe 2014 research paper on \"Variational Recurrent Auto-Encoders\" attempted to generate music based on songs from 8 different video games. This project is one of the few conducted purely on video game music. The neural network in the project was able to generate data that was very similar to the data of the games it trained off of. The generated data did not translate into good quality music.\n", "related": "NONE"}
{"id": "55631919", "url": "https://en.wikipedia.org/wiki?curid=55631919", "title": "DoNotPay", "text": "DoNotPay\n\nDoNotPay is a legal services chatbot founded by Joshua Browder, a British-American entrepreneur. The chatbot was originally built to contest parking tickets, but has expanded to include other services as well. As a \"robot lawyer,\" DoNotPay is a downloadable mobile app that makes use of artificial intelligence to provide legal services to all users free of charge. It is currently available in the United Kingdom and United States (all 50 states).\n\nDoNotPay has been featured by the BBC, NPR, NBC, \"Bloomberg\", \"Washington Times\", and many other major news outlets.\n\nDoNotPay had started off as an app for contesting parking tickets, but has since expanded to include features that help users with many different types of legal issues, ranging from consumer protection to immigration rights and other social issues. The \"robot lawyer\" makes use of automation to provide free legal consultation for the public. The application is supported by IBM's Watson computer.\n\nAs of October 2018, the app only allows for appealing small claims with a maximum limit of $25,000, but Browder plans to expand into more legal areas and add many more features in the near future. Browder claims that one of his major goals for DoNotPay is to eventually allow all members of society to have access to the same levels of legal representation. The app also allows users to file small claims with utility providers and other companies.\n\nIn 2015, DoNotPay was founded by Browder when he was 17 years old. Originally, Browder had created an app that allowed users in the United Kingdom to protest their parking tickets. Coverage for DoNotPay was then subsequently expanded to the United States, covering all 50 states.\n\nImmediately after its launch, Browder's DoNotPay application quickly became widely used by thousands of users and gained significant international media coverage. In 2016, \"The Guardian\" reported that the chatbot had successfully contested more than 250,000 parking tickets in London and New York and won 160,000 of them, all free of charge, claiming a success rate of over 60 percent.\n\nIn 2017, Browder launched 1,000 more bots to help with filling out transaction legal forms in the US and UK. DoNotPay has also expanded to include features that help users obtain refunds on flight tickets and hotel bookings, cancel free trials, sue people, and even offer legal services relating to social issues such as asylum applications and housing for the homeless.\n\nIn 2018, DoNotPay acquired Visabot, a chatbot that helps provide automated services to users seeking to obtain U.S. visas and green cards. Around the same time, DoNotPay also launched a service that helped users seek claims from Equifax during the aftermath of its security breach, a feature that has since been integrated into the DoNotPay app.\n\nAs of 2019, DoNotPay provides specialized advice for appealing parking tickets in locations such as New York City, Cambridge, Massachusetts, Chicago, Milwaukee, Sacramento, and UCSD.\n\nIn 2019, the DoNotPay application has even advised students at Stanford University, Browder's alma mater, to waive their Student Activities Fees. More recently, DoNotPay launched Free Trial Card, which gives users a virtual credit card number that can be used to sign up for free online trials such as Netflix and Spotify. As soon as the free trial period ends, the card automatically declines any charges, thus ending free trials without having to give up the cardholder's personal payment information.\n\nIn 2019, Browder obtained $4.6 million in funding from Silicon Valley investors such as Andreessen Horowitz and Founders Fund, who were early funders of Facebook.\n\n", "related": "\n- Artificial intelligence and law\n- Computational law\n- Legal expert systems\n- Legal informatics\n- Legal technology\n- Robot lawyer\n"}
{"id": "62295363", "url": "https://en.wikipedia.org/wiki?curid=62295363", "title": "Knowledge distillation", "text": "Knowledge distillation\n\nIn machine learning, knowledge distillation is the process of transferring knowledge from a large model to a smaller one. While large models (such as very deep neural networks or ensembles of many models) are able to better learn how to generalise from the data compared to smaller ones, they have the obvious drawback of being computationally expensive to evaluate, therefore limiting their applications to sufficiently powerful hardware. Knowledge distillation allows to transfer the knowledge learnt by a large model to a smaller one, that would not be able to easily learn it directly from the data itself, producing a new model that is faster to evaluate, and therefore deployable on less powerful hardware (such as a mobile device), while at the same time experiencing only a small loss of classification performance compared to the original, large model.\n\nKnowledge distillation has been successfully used in several applications of machine learning such as object detection, acoustic models, and natural language processing.\n\nWhat it is desired from a good model is good generalization capability, therefore transferring the knowledge from a large to a small model needs to somehow teach to the latter how to generalise in the same way as the former. If both models are trained on the same data, the small model will be worse at generalisation because of its inferior expressive power. However, some information of how a model generalise is encoded in the pseudo-probabilities assigned to its output: when a model correctly predicts a class, it assigns a large value to the output variable corresponding to such class, and smaller values to the other output variables. The distribution of values among the outputs for a record provides information on how the model tends to generalise, therefore the goal of successfully transferring knowledge can be achieved by training only the large model on the data, exploiting its better ability to learn generalisation, and then distilling such ability into the smaller model, that would not be able to learn it on its own, by training it to learn the soft output of the large model.\n\nModel compression, a methodology to compress the knowledge of multiple models into a single neural network, was introduced in 2006. Compression was achieved by training a smaller model on large amounts of pseudo-data labelled by a higher-performing ensemble, optimising to match the logit of the compressed model to the logit of the ensemble. Knowledge distillation is a generalisation of such approach, introduced by Geoffrey Hinton et al. in 2015, in a preprint that formulated the concept and showed some results achieved in the task of image classification.\n\nGiven a large model as a function of the vector variable formula_1, trained for a specific classification task, typically the final layer of the network is a softmax in the form\nwhere formula_3 is a parameter called \"temperature\", that for a standard softmax is normally set to 1. The softmax operator converts the logit values formula_4 to pseudo-probabilities, and higher values of temperature have the effect of generating a softer distribution of pseudo-probabilities among the output classes. Knowledge distillation consists of training a smaller network, called \"distilled model\", on a dataset called transfer set (different than the dataset used to train the large model) using as loss function the cross entropy between the output of the distilled model formula_5 and the output formula_6 produced by the large model on the same record (or the average of the individual outputs, if the large model is an ensemble), using a high value of softmax temperature formula_3 for both models\nIn this context, a high temperature increases the entropy of the output, and therefore provides more information to learn for the distilled model compared to hard targets, at the same time reducing the variance of the gradient between different records and therefore allowing higher learning rates.\n\nIf ground truth is available for the transfer set, the process can be strengthened by adding to the loss the cross entropy between the output of the distilled model (computed with formula_9) and the known label formula_10\nwhere the component of the loss with respect to the large model is weighted by a factor of formula_12 since, as the temperature increases, the gradient of the loss with respect to the model weights scales by a factor of formula_13.\n\nUnder the assumption that the logits have zero mean, it is possible to show that model compression is a special case of knowledge distillation. The gradient of the knowledge distillation loss formula_14 with respect to the logit of the distilled model formula_15 is given by\nwhere formula_17 are the logits of the large model. For large values of formula_3 this can be approximated as\nand under the zero-mean hypothesis formula_20 it becomes formula_21, which is the derivative of formula_22, i.e. the loss is equivalent to matching the logits of the two models, as done in model compression.\n\n\n- Distilling the knowledge in a neural network – Google AI\n- Knowledge distillation\n", "related": "NONE"}
{"id": "62285602", "url": "https://en.wikipedia.org/wiki?curid=62285602", "title": "Multi-agent learning", "text": "Multi-agent learning\n\nMulti-agent learning is the use of machine learning in a multi-agent system. Typically, agents improve their decisions via experience. In particular, an agent has to learn how to coordinate with the other agents.\n\nAccording to an article by Shoham et al. in 2007, it is difficult to pinpoint all relevant articles in the domain. There are some inherent difficulties about multi-agent deep reinforcement learning. The environment is not stationnary anymore, thus the Markov property is violated: transitions and rewards does not only depend on the current state of an agent.\n", "related": "NONE"}
{"id": "62683332", "url": "https://en.wikipedia.org/wiki?curid=62683332", "title": "Fairness (machine learning)", "text": "Fairness (machine learning)\n\nIn machine learning, a given algorithm is said to be fair, or to have fairness if its results are independent of some variables we consider to be sensitive and not related with it (f.e.: gender, ethnicity, sexual orientation, etc.).\n\nResearch about fairness in machine learning is a relatively recent topic. Most of the articles about it have been written in the last three years. Some of the most important facts in this topic are the following:\n- In 2018, IBM introduces AI Fairness 360, a Python library with several algorithms to reduce bias in a program, increasing its fairness.\n- Facebook made public, in 2018, their use of a tool, Fairness Flow, to detect bias in their AI. However, said tool code is not accessible, and it is not known if it really corrects this bias.\n- In 2019, Google publishes a set of tools in Github to study the effects of fairness in the long run.\n\nThe algorithms used for assuring fairness are still being improved. However, the main progress in this area is that some big corporations are realising the importance the reduction of algorithm bias will have on the society. \n\nIn classification problems, an algorithm learns a function to predict a discrete characteristic formula_1, the target variable, from known characteristics formula_2. We model formula_3 as a discrete random variable which encodes some characteristics contained or implicitly encoded in formula_2 that we consider as sensitive characteristics (gender, ethnicity, sexual orientation, etc.). We finally denote by formula_5 the prediction of the classifier.\nNow let us define three main criteria to evaluate if a given classifier is fair, that is, if its predictions are not influenced by some of these sensitive variables.\n\nWe say the random variables formula_6 satisfy independence if the sensitive characteristics formula_3 are statistically independent to the prediction formula_5, and we write formula_9.\n\nWe can also express this notion with the following formula:\nformula_10\nThis means that the probability of being classified by the algorithm in each of the groups is equal for two individuals with different sensitive characteristics.\n\nYet another equivalent expression for independence can be given using the concept of mutual information between random variables, defined as\nformula_11\nIn this formula, formula_12 of the random variable. Then formula_13 satisfy independence if formula_14.\n\nA possible relaxation of the indepence definition include introducing a positive slack formula_15 and is given by the formula:\nformula_16\n\nFinally, another possible relaxation is to require formula_17.\n\nWe say the random variables formula_18 satisfy separation if the sensitive characteristics formula_3 are statistically independent to the prediction formula_5 given the target value formula_1, and we write formula_22.\n\nWe can also express this notion with the following formula:\nformula_23\nThis means that the probability of being classified by the algorithm in each of the groups is equal for two individuals with different sensitive characteristics given that they actually belong in the same group (have the same target variable).\n\nAnother equivalent expression, in the case of a binary target rate, is that the true positive rate and the false positive rate are equal (and therefore the false negative rate and the true negative rate are equal) for every value of the sensitive characteristics:\nformula_24\nformula_25\n\nFinally, a possible relaxation of the given definitions is the difference between rates to be a positive number lower than a given slack formula_15, instead of equals to zero.\n\nWe say the random variables formula_18 satisfy sufficiency if the sensitive characteristics formula_3 are statistically independent to the target value formula_1 given the prediction formula_5, and we write formula_31.\n\nWe can also express this notion with the following formula:\nformula_32\nThis means that the probability of actually being in each of the groups is equal for two individuals with different sensitive characteristics given that they were predicted to belong to the same group.\n\nFinally, we sum up some of the main results that relate the three definitions given above:\n\n- If formula_3 and formula_1 are not statistically independent, then sufficiency and independence cannot both hold.\n- Assuming formula_1 is binary, if formula_3 and formula_1 are not statistically independent, and formula_5 and formula_1 are not statistically independent either, then independence and separation cannot both hold.\n- If formula_18 as a joint distribution has positive probability for all its possible values and formula_3 and formula_1 are not statistically independent, then separation and sufficiency cannot both hold.\n\nMost statistical measures of fairness rely on different metrics, so we will start by defining them. When working with a binary classifier, both the predicted and the actual classes can take two values: positive and negative. Now let us start explaining the different possible relations between predicted and actual outcome:\n- True positive (TP): The case where both the predicted and the actual outcome are in the positive class.\n- True negative (TN): The case where both the predicted and the actual outcome are in the negative class.\n- False positive (FP): A case predicted to be in the positive class when the actual outcome is in the negative one.\n- False negative (FN): A case predicted to be in the negative class when the actual outcome is in the positive one.\nThis relations can be easily represented with a confusion matrix, a table which describes the accuracy of a classification model. In this matrix, columns and rows represent instances of the predicted and the actual cases, respectively.\n\nBy using this relations, we can define multiple metrics which can be later used to measure the fairness of an algorithm:\n- Positive predicted value (PPV): the fraction of positive cases which were correctly predicted out of all the positive predictions. It is usually referred to as precision, and represents the probability of a positive prediction to be right. It is given by the following formula:\nformula_43\n- False discovery rate (FDR): the fraction of positive predictions which were actually negative out of all the positive predictions. It represents the probability of a positive prediction to be wrong, and it is given by the following formula:\nformula_44\n- Negative predicted value (NPV): the fraction of negative cases which were correctly predicted out of all the negative predictions. It represents the probability of a negative prediction to be right, and it is given by the following formula:\nformula_45\n- False omission rate (FOR): the fraction of negative predictions which were actually positive out of all the negative predictions. It represents the probability of a negative prediction to be wrong, and it is given by the following formula:\nformula_46\n- True positive rate (TPR): the fraction of positive cases which were correctly predicted out of all the positive cases. It is usually referred to as sensitivity or recall, and it represents the probability of the positive subjects to be classified correctly as such. It is given by the formula:\nformula_47\n- False negative rate (FNR): the fraction of positive cases which were incorrectly predicted to be negative out of all the positive cases. It represents the probability of the positive subjects to be classified incorrectly as negative ones, and it is given by the formula:\nformula_48\n- True negative rate (TNR): the fraction of negative cases which were correctly predicted out of all the negative cases. It represents the probability of the negative subjects to be classified correctly as such, and it is given by the formula:\nformula_49\n- False positive rate (FPR): the fraction of negative cases which were incorrectly predicted to be positive out of all the negative cases. It represents the probability of the negative subjects to be classified incorrectly as positive ones, and it is given by the formula:\nformula_50\n\nThe following criteria can be understood as measures of the three definitions given in the first section, or a relaxation of them. In the table to the right, we can see the relationships between them.\n\nTo define this measures specifically, we will divide them into three big groups as done in Verma et al.: definitions based on predicted outcome, on predicted and actual outcomes, and definitions based on predicted probabilities and actual outcome.\n\nWe will be working with a binary classifier and the following notation: formula_51 refers to the score given by the classifier, which is the probability of a certain subject to be in the positive or the negative class.formula_5 represents the final classification predicted by the algorithm, and its value is usually derived from formula_51, for example will be positive when formula_51 is above a certain threshold.formula_1 represents the actual outcome, that is, the real classification of the individual and, finally, formula_3 denotes the sensitive attributes of the subjects.\n\nThe definitions in this section focus on a predicted outcome formula_5 for various distributions of subjects. They are the simplest and most intuitive notions of fairness.\n\n- Group fairness, also referred to as statistical parity, demographic parity, acceptance rate and benchmarking. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal probability of being assigned to the positive predicted class. This is, if the following formula is satisfied:\nformula_58\n\n- Conditional statistical parity. Basically consists in the definition above, but restricted only to a subset of the attributes. With mathematical notation this would be:\nformula_59\n\nThis definitions not only consider de predicted outcome formula_5 but also compare it to the actual outcome formula_1.\n\n- Predictive parity, also referred to as outcome test. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV. This is, if the following formula is satisfied:\nformula_62\nformula_63\n\n- False positive error rate balance, also referred to as predictive equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have aqual FPR. This is, if the following formula is satisfied:\nformula_64\nformula_65\n\n- False negative error rate balance, also referred to as equal opportunity. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal FNR. This is, if the following formula is satisfied:\nformula_66\nformula_67\n\n- Equalized odds, also referred to as conditional procedure accuracy equality and disparate mistreatment. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal TPR and equal FPR, satisfying the formula:\nformula_68\n\n- Conditional use accuracy equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV and equal NPV, satisfying the formula:\nformula_69\n\n- Overall accuracy equality. A classifier satisfies this definition if the subject in the protected and unprotected groups have equal prediction accuracy, that is, the probability of a subject from one class to be assigned to it. This is, if it satisfies the following formula:\nformula_70\n\n- Treatment equality. A classifier satisfies this definition if the subjects in the protected and unprotected groups have an equal ratio of FN and FP, satisfying the formula:\nformula_71\n\nThese definitions are based in the actual outcome formula_1 and the predicted probability score formula_51.\n\n- Test-fairness, also known as calibration or matching conditional frequencies. A classifier satisfies this definition if individuals with the same predicted probability score formula_51 have the same probability to be classified in the positive class when they belong to either the protected or the unprotected group:\nformula_75\n\n- Well-calibration is an extension of the previous definition. It states that when individuals inside or outside the protected group have the same predicted probability score formula_51 they must have the same probability of being classified in the positive class, and this probability must be equal to formula_51:\nformula_78\n\n- Balance for positive class. A classifier satisfies this definition if the subjects constituting the positive class from both protected and unprotected groups have equal average predicted probability score formula_51. This means that the expected value of probability score for the protected and unprotected groups with positive actual outcome formula_1 is the same, satisfying the formula:\nformula_81\n\n- Balance for negative class. A classifier satisfies this definition if the subjects constituting the negative class from both protected and unprotected groups have equal average predicted probability score formula_51. This means that the expected value of probability score for the protected and unprotected groups with negative actual outcome formula_1 is the same, satisfying the formula:\nformula_84\n\nFairness can be applied to machine learning algorithms in three different ways: preprocessing the data used in the algorithm, optimization during the training, or post-processing the answers of the algorithm.\n\nUsually, the classifier is not the only problem, the dataset is also biased. The discrimination of a dataset formula_85 with respect to the group formula_86 can be defined as follows:\nformula_87\n\nThat is, an approximation to the difference between the probabilities of belonging in the positive class given that the subject has a protected characteristic different from formula_88 and equal to formula_88.\n\nAlgorithms correcting bias at preprocessing remove information concerning variables in the dataset which can result in unfair decisions of the AI, while trying to alter just the bare minimum of this data. This is not as easy as just removing the sensitive variable, because other attributes can be related to the protected one.\n\nA way to do this is by mapping each individual in the initial dataset into an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group, while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm.\nThis way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesn't belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classificator.\n\nAn example is explained in Zemel et al. where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all the information except those that can lead to biased decisions, and to obtain a prediction as accurate as possible.\n\nOn the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness.\n\nReweighing is an example of preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group.\n\nIf the dataset formula_85 was unbiased the sensitive variable formula_3 and the target variable formula_1 would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows:\nformula_93\n\nIn reality, however, the dataset is not unbiased and the variables are not statistically independent so the observed probability is:\nformula_94\n\nTo compensate for the bias, lower weights to favored objects and higher weights to unfavored objects will be assigned. For each formula_95 we get:\nformula_96\n\nWhen we have for each formula_2 a weight associated formula_98 we compute the weighted discrimination with respect to group formula_86 as follows:\nformula_100\n\nIt can be shown that after reweighting this weighted discrimination is 0.\n\nAnother approach is correcting the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm. These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group.\n\nThe main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed.\n\nThis technique obtains good results in improving fairness while keeping high accuracy and lets the programmer choose the fairness measures to improve. However, each machine learning task may need a different method to be applied and the code in the classifier needs to be modified, which is not always possible.\n\nWe train two classifiers at the same time through some gradient-based method (f.e.: gradient descent). The first one, the \"predictor\" tries to accomplish the task of predicting formula_1, the target variable, given formula_2, the input, by modifying its weights formula_103 to minimize some loss function formula_104. The second one, the \"adversary\" tries to accomplish the task of predicting formula_3, the sensitive variable, given formula_106 by modifying its weights formula_107 to minimize some loss function formula_108.\n\nAn important point here is that, in order to propagate correctly, formula_106 above must refer to the raw output of the classifier, not the discrete prediction; for example, with an artificial neural network and a classification problem, formula_106 could refer to the output of the softmax layer.\n\nThen we update formula_107 to minimize formula_112 at each training step according to the gradient formula_113 and we modify formula_103 according to the expression:\nformula_115\nwhere formula_116 is a tuneable hyperparameter that can vary at each time step.\nThe intuitive idea is that we want the \"predictor\" to try to minimize formula_117 (therefore the term formula_118) while, at the same time, maximize formula_112 (therefore the term formula_120), so that the \"adversary\" fails at predicting the sensitive variable from formula_106.\n\nThe term formula_122 prevents the \"predictor\" from moving in a direction that helps the \"adversary\" decrease its loss function.\n\nIt can be shown that training a \"predictor\" classification model with this algorithm improves demographic parity with respect to training it without the \"adversary\".\n\nThe final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive answer, while low scores are likely to get a negative answer, but we need to adjust the threshold to determine when to answer yes or no depending on our needs. Note that variations in the threshold affect the trade-off between true positive rate and true negative rate.\n\nIf the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but this type of classifiers tend to be biased, so we may need to set a different threshold for each protected group to achieve fairness. A way to do this is plotting the true positive rate against the false negative rate at various threshold settings (this is called ROC curve) and check which threshold satisfies that the rates are equal for the protected group and the rest of the individuals.\n\nThe advantages of postprocessing include that the technique can be applied after any classifiers, without modifying it, and has a good performance in fairness measures. The cons are the need to access to the protected attribute in test time and the lack of choice in the balance between accuracy and fairness.\n\nGiven a classifier let formula_123 be the probability computed by the classifiers as the probability that the instance formula_2 belongs to the positive class +. When formula_123 is close to 1 or to 0, the instance formula_2 is specified with high degree of certainty to belong to class + or - respectively. However, when formula_123 is closer to 0.5 the classification is more unclear.\n\nWe say formula_2 is a \"rejected instance\" if formula_129 with a certain formula_130 such that formula_131.\n\nThe algorithm of \"ROC\" consists on classifying the non-rejected instances following the rule above and the rejected instances as follows: if the instance is an example of a deprived group (formula_132) then label it as positive, otherwise, label it as negative.\n\nWe can optimize different measures of discrimination (link) as functions of formula_130 to find the optimal formula_130 for each problem and avoid becoming discriminatory against the privileged group.\n\n", "related": "\n- Algorithmic bias\n- Machine learning\n"}
{"id": "62817500", "url": "https://en.wikipedia.org/wiki?curid=62817500", "title": "Leakage (machine learning)", "text": "Leakage (machine learning)\n\nIn statistics and machine learning, leakage (also data leakage, or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the model's utility when run in a production environment.\n\nLeakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause modeler to select a suboptimal model, which otherwise could be outperformed by a leakage-free model.\nLeakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model: features and training examples.\n\nColumn-wise leakage is caused by the inclusion of columns which are one of: a duplicate label, a proxy for the label, or the label itself, when training the model which are not available at prediction time (anachronisms). This can include leaks which partially give away the label. \n\nFor example, including a \"MonthySalary\" column when predicting \"YearlySalary\"; or \"MinutesLate\" when predicting \"IsLate\"; or more subtly \"NumOfLatePayments\" when predicting \"ShouldGiveLoan\".\n\nRow-wise leakage leakage is caused by improper sharing of information between rows of data.\n\nData leakage types:\n- Premature featurization; leaking from premature featurization before CV/TrainTest split (must fit MinMax/ngrams/etc on only the train split, then transform the test set)\n- Duplicate rows between train/validation/test (e.g. oversampling a dataset to pad its size before splitting; e.g. different rotations/augmentations of an single image; bootstrap sampling before splitting; or duplicating rows to up sample the minority class)\n- Non-i.i.d. data\n- Time leakage (e.g. splitting a time-series dataset randomly instead of newer data in test set using a TrainTest split or rolling-origin cross validation)\n- Group leakage -- not including a grouping split column (e.g. Andrew Ng's group had 100k x-rays of 30k patients, meaning ~3 images per patient. The paper used random splitting instead of ensuring that all images of a patient was in the same split. Hence the model partially memorized the patients instead of learning to recognize pneumonia in chest x-rays. Revised paper had a drop in scores.)\n\nFor time-dependent datasets, the structure of the system being studied evolves over time (i.e. it is \"non-stationary\"). This can introduce systematic differences between the training and validation sets. For example, if a model for predicting stock values is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year. \n\n", "related": "\n- AutoML\n- Cross-validation\n- Overfitting\n- Resampling (statistics)\n- Supervised learning\n- Training, validation, and test sets\n"}
{"id": "49239240", "url": "https://en.wikipedia.org/wiki?curid=49239240", "title": "VaultML", "text": "VaultML\n\nVault AI is an Israeli–based artificial intelligence company that lays claims to have created technologies that can \"read\" movie and TV screenplays in order to predict box office and investment performance. Part of the process reportedly entails analyzing 300,000 to 400,000 elements from the script, which could be anything from plot, character development, script structure, scene events. The founders are made up of high frequency trading veterans and state they use similar approaches to predicting film performance. Vault published its 2015 film predictions for over 20 movies in early 2015 and successfully predicted correctly many box office performances throughout that year. Vault's algorithms out earned the market on a return on investment basis.\n", "related": "NONE"}
{"id": "52003586", "url": "https://en.wikipedia.org/wiki?curid=52003586", "title": "End-to-end reinforcement learning", "text": "End-to-end reinforcement learning\n\nIn end-to-end reinforcement learning, the entire process from sensors to motors in a robot or agent (called the end-to-end process) involves a single, layered or recurrent neural network without modularization, and is trained by reinforcement learning (RL). The approach has been proposed for a long time, but was reenergized by the successful results in learning to play Atari video games (2013–15) and AlphaGo (2016) by Google DeepMind.\n\nRL traditionally required explicit design of state space and action space, while the mapping from state space to action space is learned. Therefore, RL has been limited to learning only for action, and human designers have to design how to construct state space from sensor signals and to give how the motion commands are generated for each action before learning. Neural networks have been often used in RL, to provide non-linear function approximation to avoid the curse of dimensionality. Recurrent neural networks have been also employed, mainly to avoid perceptual aliasing or partially observable Markov decision process (POMDP).\n\nEnd-to-end RL extends RL from learning only for actions to learning the entire process from sensors to motors including higher-level functions that are difficult to develop independently from other functions. Higher-level functions do not connect directly with either sensors or motors, and so even giving their inputs and outputs is difficult.\n\nThe approach originated in TD-Gammon (1992). In backgammon, the evaluation of the game situation during self-play was learned through TD(formula_1) using a layered neural network. Four inputs were used for the number of pieces of a given color at a given location on the board, totaling 198 input signals. With zero knowledge built in, the network learned to play the game at an intermediate level.\n\nShibata began working with this framework in 1997. They employed Q-learning and actor-critic for continuous motion tasks, and used a recurrent neural network for memory-required tasks. They applied this framework to some real robot tasks. They demonstrated learning of various functions.\n\nBeginning around 2013, Google DeepMind showed impressive learning results in video games and game of Go (AlphaGo). They used a deep convolutional neural network that showed superior results in image recognition. They used 4 frames of almost raw RGB pixels (84x84) as inputs. The network was trained based on RL with the reward representing the sign of the change in the game score. All 49 games were learned using the same network architecture and Q-learning with minimal prior knowledge, and outperformed competing methods on almost all the games and performed at a level that is comparable or superior to a professional human game tester. It is sometimes called Deep-Q network (DQN). In AlphaGo, deep neural networks are trained not only by reinforcement learning, but also by supervised learning and Monte Carlo tree search.\n\nShibata's group showed that various functions emerge in this framework, including: \n\n- Image recognition\n- Color constancy (optical illusion)\n- Sensor motion (active recognition)\n- Hand-eye coordination and hand reaching movement\n- Explanation of brain activities\n- Knowledge transfer\n- Memory\n- Selective attention\n- Prediction\n- Exploration\n\nCommunications were established in this framework. Modes include:\n\n- Dynamic communication (negotiation)\n- Binalization of signals\n- Grounded communication using a real robot and camera\n", "related": "NONE"}
{"id": "53769524", "url": "https://en.wikipedia.org/wiki?curid=53769524", "title": "Matroid (company)", "text": "Matroid (company)\n\nMatroid, Inc. is a Computer Vision company that offers a platform for creating computer vision models, called detectors, to search visual media for objects, persons, events, emotions, and actions. Matroid provides real-time notifications once the object of interest has been detected, as well as the ability to search past events.\n\nMatroid was founded in 2016 by Reza Zadeh, a Stanford professor. The company raised $3.5 million in Series A funding from New Enterprise Associates in 2016, and an additional $10 million from Intel in 2017.\n\nOnce a detector has been trained using the Matroid GUI, it automatically finds the objects of interest in real-time video and archived footage. Users can explore detection information via reports, notifications, or a calendar interface to view events and identify trends. Matroid’s functionality is also exposed via a developer API.\n\nSupported hardware platforms:\n\n- On-cloud: www.matroid.com, allows for scaling based on workload\n\n- On-prem: contains the same functionality of www.matroid.com in a secure, offline environment for applications where data privacy and security are key concerns\n- On-device: runs on embedded devices such as cameras, sensors, etc.\n\nThe company has a range of customers in Media, Security, Healthcare, Industrial IoT, AI chip, and other industries.\nMatroid annually holds a conference, Scaled Machine Learning, where technical speakers lead discussions about running and scaling machine learning algorithms, artificial intelligence, and computing platforms, such as GPUs, CPUs, TPUs, & the nascent AI chip industry.\n\nPast speakers include Turing Award Winners, creators of Keras, TensorFlow, PyTorch, Caffe, OpenAI, Kubernetes, Horovod, Allen Institute for AI, Apache Spark, Apache Arrow, MLPerf, Matroid, and others.\n\n2019 - Matroid was selected by Gartner, Inc. as a “Cool Vendor” for Cool Vendors in AI Core Technologies.\n\n2018 - Matroid announced a partnership with HP for their on-prem platform. Matroid certified a selection of HP Z computers as Computer-Vision-Ready (CV-Ready) for monitoring video streams. \n\n2018 - Oracle announced their software integration with Matroid to provide real-time and analytics based on people monitoring.\n\n2016 - Matroid was awarded a Best Paper Award at KDD 2016. \n\nTogether with Stanford Hospital and hospitals in Hong Kong, India, and Nepal, Matroid used computer vision in the field of Ophthalmology. The company created a model that learns to predict glaucoma from areas of the eye previously ignored during diagnosis, specifically the Lamina Cribrosa, as no established automated metrics existed for this region yet. Matroid is able to detect glaucoma on OCT scans of the eye, with an F1 score of 96% and similar AUC and accuracy.\n\nFusionNet was released as a leading neural networks architecture at the Princeton ModelNet competition.  It is a fusion of three convolutional neural networks, one trained on pixel representation and two networks trained on voxelized objects. It exploits the strength of each component network in order to improve the classification performance. Each component network of FusionNet considers multiple views or orientations of each object before classifying it. While it is intuitive that one can get more information from multiple views of the object than a single view, it is not trivial to put the information together in order to enhance the accuracy. Matroid used information from 20 views for pixel representation and 60 CAD object orientations for voxel representation before predicting the object class. FusionNet outperformed the current leading submission on the Princeton ModelNet leaderboard in both the 10 class and the 40 class datasets.\n\nMatroid released a book with co-author Bharath Ramsundar, TensorFlow for Deep Learning. It introduces the fundamentals of machine learning through TensorFlow and explains how to use TensorFlow to build systems capable of detecting objects in images, understanding human text, and predicting the properties of potential medicines.\n", "related": "NONE"}
{"id": "60929882", "url": "https://en.wikipedia.org/wiki?curid=60929882", "title": "Flux (machine-learning framework)", "text": "Flux (machine-learning framework)\n\nFlux is an open-source machine-learning library and ecosystem written in Julia. Its current stable release is v0.10.3. Flux uses Julia's language features, such as just-ahead-of-time compilation, to provide an interface to users. It has a layer-stacking-based interface for simpler models, and can be readily integrated with other Julia packages. For example, GPU support is supplied transparently by CuArrays.jl, due to Julia's multiple dispatch. This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl, and thus are more limited by the functionality present in the underlying implementation, which is often in C or C++.\n\nThis feature has been used, for example, to implement support for Neural Differential Equations, by fusing Flux and DifferentialEquations.jl into DiffEqFlux.jl.\n\nFlux supports recurrent and convolutional networks. It is also capable of Differentiable programming through its source-to-source Automatic differentiation package, Zygote.\n\nJulia is a popular machine-learning language in Github and Flux is appointed as its most highly regarded machine-learning repository. A demonstration compiling Julia code to run in Google's Tensor processing unit received praise from Google Brain AI lead Jeff Dean.\n\nFlux was employed to the first application of machine-learning to data encrypted with Homomorphic encryption without ever decrypting it. This kind of application is envisioned to be central for privacy to future API using machine-learning models.\n\nFlux.jl is an intermediate representation for running high level programs on CUDA hardware. It was the predecessor to CUDAnative.jl which is also a GPU programming language.\n\n", "related": "\n- Differentiable programming\n- Comparison of deep-learning software\n"}
{"id": "49119569", "url": "https://en.wikipedia.org/wiki?curid=49119569", "title": "Comparison of deep-learning software", "text": "Comparison of deep-learning software\n\nThe following table compares notable software frameworks, libraries and computer programs for deep learning.\n\n- Neural Engineering Object (NENGO) – A graphical and scripting software for simulating large-scale neural systems\n- Numenta Platform for Intelligent Computing – Numenta's open source implementation of their hierarchical temporal memory model\n\n", "related": "\n- Comparison of numerical-analysis software\n- Comparison of statistical packages\n- List of datasets for machine-learning research\n- List of numerical-analysis software\n"}
{"id": "42571226", "url": "https://en.wikipedia.org/wiki?curid=42571226", "title": "Torch (machine learning)", "text": "Torch (machine learning)\n\nTorch is an open-source machine learning library, \na scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation. As of 2018, Torch is no longer in active development. However, PyTorch is actively developed as of August 2019.\n\nThe core package of Torch is codice_1. It provides a flexible N-dimensional array or Tensor, which supports basic routines for indexing, slicing, transposing, type-casting, resizing, sharing storage and cloning. This object is used by most other packages and thus forms the core object of the library. The Tensor also supports mathematical operations like codice_2, codice_3, codice_4, statistical distributions like uniform, normal and multinomial, and BLAS operations like dot product, matrix-vector multiplication, matrix-matrix multiplication, matrix-vector product and matrix product.\n\nThe following exemplifies using torch via its REPL interpreter:\n\n> a = torch.randn(3,4)\n\n> =a\n-0.2381 -0.3401 -1.7844 -0.2615\n-1.0434 2.2291 1.0525 0.8465\n[torch.DoubleTensor of dimension 3x4]\n\n> a[1][2]\n-0.34010116549482\n> a:narrow(1,1,2)\n-0.2381 -0.3401 -1.7844 -0.2615\n[torch.DoubleTensor of dimension 2x4]\n\n> a:index(1, torch.LongTensor{1,2})\n-0.2381 -0.3401 -1.7844 -0.2615\n[torch.DoubleTensor of dimension 2x4]\n\n> a:min()\n-1.7844365427828 \nThe codice_1 package also simplifies object oriented programming and serialization by providing various convenience functions which are used throughout its packages. The codice_6 function can be used to create object factories (classes). When the constructor is called, torch initializes and sets a Lua table with the user-defined metatable, which makes the table an object.\n\nObjects created with the torch factory can also be serialized, as long as they do not contain references to objects that cannot be serialized, such as Lua coroutines, and Lua \"userdata\". However, \"userdata\" can be serialized if it is wrapped by a table (or metatable) that provides codice_7 and codice_8 methods.\n\nThe codice_9 package is used for building neural networks. It is divided into modular objects that share a common codice_10 interface. Modules have a codice_11 and codice_12 method that allow them to feedforward and backpropagate, respectively. Modules can be joined together using module composites, like codice_13, codice_14 and codice_15 to create complex task-tailored graphs. Simpler modules like codice_16, codice_17 and codice_18 make up the basic component modules. This modular interface provides first-order automatic gradient differentiation. What follows is an example use-case for building a multilayer perceptron using Modules:\n\n> mlp = nn.Sequential()\n> mlp:add( nn.Linear(10, 25) ) -- 10 input, 25 hidden units\n> mlp:add( nn.Tanh() ) -- some hyperbolic tangent transfer function\n> mlp:add( nn.Linear(25, 1) ) -- 1 output\n> =mlp:forward(torch.randn(10))\n-0.1815\n[torch.Tensor of dimension 1]\nLoss functions are implemented as sub-classes of codice_19, which has a similar interface to codice_10. It also has codice_11 and codice_12 methods for computing the loss and backpropagating gradients, respectively. Criteria are helpful to train neural network on classical tasks. Common criteria are the Mean Squared Error criterion implemented in codice_23 and the cross-entropy criterion implemented in codice_24. What follows is an example of a Lua function that can be iteratively called to train \nan codice_25 Module on input Tensor codice_26, target Tensor codice_27 with a scalar codice_28: \n\nfunction gradUpdate(mlp, x, y, learningRate)\nend\nIt also has codice_29 class for training a neural network using Stochastic gradient descent, although the codice_30 package provides much more options in this respect, like momentum and weight decay regularization.\n\nMany packages other than the above official packages are used with Torch. These are listed in the torch cheatsheet. These extra packages provide a wide range of utilities such as parallelism, asynchronous input/output, image processing, and so on. They can be installed with LuaRocks, the Lua package manager which is also included with the Torch distribution.\n\nTorch is used by the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. Torch has been extended for use on Android and iOS. It has been used to build hardware implementations for data flows like those found in neural networks.\n\nFacebook has released a set of extension modules as open source software.\n\n", "related": "\n- Comparison of deep learning software\n- PyTorch\n"}
{"id": "33520809", "url": "https://en.wikipedia.org/wiki?curid=33520809", "title": "Theano (software)", "text": "Theano (software)\n\nTheano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.\nIn Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.\n\nTheano is an open source project primarily developed by a Montreal Institute for Learning Algorithms (MILA) at the Université de Montréal.\n\nThe name of the software references the ancient philosopher Theano, long associated with the development of the golden mean.\n\nOn 28 September 2017, Pascal Lamblin posted a message from Yoshua Bengio, \nHead of MILA: major development would cease after the 1.0 release due to competing offerings by strong industrial players. Theano 1.0.0 was then released on 15 November 2017.\n\nOn 17 May 2018, Chris Fonnesbeck wrote on behalf of the PyMC development team that the PyMC developers will officially assume control of Theano maintenance once they step down.\n\nThe following code is the original Theano's example. It defines a computational graph with 2 scalars and of type \"double\" and an operation between them (addition) and then creates a Python function \"f\" that does the actual computation.\nimport theano\nfrom theano import tensor\n\n1. Declare two symbolic floating-point scalars\na = tensor.dscalar()\nb = tensor.dscalar()\n\n1. Create a simple expression\nc = a + b\n\n1. Convert the expression into a callable object that takes (a, b)\n2. values as input and computes a value for c\nf = theano.function([a, b], c)\n\n1. Bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\nassert 4.0 == f(1.5, 2.5)\n", "related": "\n- Comparison of deep learning software\n- Differentiable programming\n\n- (GitHub)\n- Theano at Deep Learning, Université de Montréal\n"}
{"id": "32472154", "url": "https://en.wikipedia.org/wiki?curid=32472154", "title": "Deep learning", "text": "Deep learning\n\nDeep learning (also known as deep structured learning or differential programming) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n\nDeep learning architectures such as deep neural networks, deep belief networks, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, bioinformatics, drug design, medical image analysis, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.\n\nArtificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.\n\nDeep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\n\nMost modern deep learning models are based on artificial neural networks, specifically, Convolutional Neural Networks (CNN)s, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.\n\nIn deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level \"on its own\". (Of course, this does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.)\n\nThe word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial \"credit assignment path\" (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\n\nDeep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.\n\nFor supervised learning tasks, deep learning methods eliminate feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.\n\nDeep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.\n\nDeep neural networks are generally interpreted in terms of the universal approximation theorem or probabilistic inference.\n\nThe classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.\n\nThe universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then deep neural network is not a universal approximator.\n\nThe probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.\n\nThe term \"Deep Learning\" was introduced to the machine learning community by Rina Dechter in 1986, and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.\n\nThe first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described already a deep network with 8 layers trained by the group method of data handling algorithm.\n\nOther deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980. In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.\n\nBy 1991 such systems were used for recognizing isolated 2-D hand-written digits, while recognizing 3-D objects was done by matching 2-D images with a handcrafted 3-D object model. Weng \"et al.\" suggested that a human brain does not use a monolithic 3-D object model and in 1992 they published Cresceptron, a method for performing 3-D object recognition in cluttered scenes. Because it directly used natural images, Cresceptron started the beginning of general-purpose visual learning for natural 3D worlds. Cresceptron is a cascade of layers similar to Neocognitron. But while Neocognitron required a human programmer to hand-merge features, Cresceptron learned an open number of features in each layer without supervision, where each feature is represented by a convolution kernel. Cresceptron segmented each learned object from a cluttered scene through back-analysis through the network. Max pooling, now often adopted by deep neural networks (e.g. ImageNet tests), was first used in Cresceptron to reduce the position resolution by a factor of (2x2) to 1 through the cascade for better generalization.\n\nIn 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.\n\nIn 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.\n\nSimpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural network's (ANN) computational cost and a lack of understanding of how the brain wires its biological networks.\n\nBoth shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.\n\nMost speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.\n\nThe principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.\n\nMany aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.\n\nIn 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh\n\nDeep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.\n\nThe impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.\n\nThe 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009-2010, contrasted the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.\n\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.\n\nAdvances in hardware have enabled renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Google Brain used Nvidia GPUs to create capable DNNs. While there, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.\n\nIn 2012, a team led by George E. Dahl won the \"Merck Molecular Activity Challenge\" using multi-task deep neural networks to predict the biomolecular target of one drug. In 2014, Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the \"Tox21 Data Challenge\" of NIH, FDA and NCATS.\n\nSignificant additional impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades, and GPU implementations of NNs for years, including CNNs, fast implementations of CNNs with max-pooling on GPUs in the style of Ciresan and colleagues were needed to progress on computer vision. In 2011, this approach achieved for the first time superhuman performance in a visual pattern recognition contest. Also in 2011, it won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest. Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records. In October 2012, a similar system by Krizhevsky et al. won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. In November 2012, Ciresan et al.'s system also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic. In 2013 and 2014, the error rate on the ImageNet task using deep learning was further reduced, following a similar trend in large-scale speech recognition. The Wolfram Image Identification project publicized these improvements.\n\nImage classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.\n\nSome researchers assess that the October 2012 ImageNet victory anchored the start of a \"deep learning revolution\" that has transformed the AI industry.\n\nIn March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.\n\nArtificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\n\nAn ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\n\nTypically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n\nThe original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\n\nNeural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nAs of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, playing \"Go\" ).\n\nA deep neural network (DNN) is an artificial neural network (ANN) with multiple layers between the input and output layers. The DNN finds the correct mathematical manipulation to turn the input into the output, whether it be a linear relationship or a non-linear relationship. The network moves through the layers calculating the probability of each output. For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, and complex DNN have many layers, hence the name \"deep\" networks.\n\nDNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives. The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.\n\nDeep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.\n\nDNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights. That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\n\nRecurrent neural networks (RNNs), in which data can flow in any direction, are used for applications such as language modeling. Long short-term memory is particularly effective for this use.\n\nConvolutional deep neural networks (CNNs) are used in computer vision. CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).\n\nAs with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\n\nDNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or weight decay (formula_1-regularization) or sparsity (formula_2-regularization) can be applied during training to combat overfitting. Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies. Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.\n\nDNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples) speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.\n\nAlternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.\n\nLarge-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates is competitive with traditional speech recognizers on certain tasks.\n\nThe initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences. Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n\nThe debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003-2007, accelerated progress in eight major areas:\n\n- Scale-up/out and accelerated DNN training and decoding\n- Sequence discriminative training\n- Feature processing by deep models with solid understanding of the underlying mechanisms\n- Adaptation of DNNs and related deep models\n- Multi-task and transfer learning by DNNs and related deep models\n- CNNs and how to design them to best exploit domain knowledge of speech\n- RNN and its rich LSTM variants\n- Other types of deep models including tensor-based models and integrated deep generative/discriminative models.\n\nAll major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.\n\nA common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.\n\nDeep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011.\n\nDeep learning-trained vehicles now interpret 360° camera views. Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n\nClosely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of a) identifying the style period of a given painting, b) Neural Style Transfer - capturing the style of a given artwork and applying it in a visually pleasing manner to an arbitrary photograph or video, and c) generating striking imagery based on random visual input fields.\n\nNeural networks have been used for implementing language models since the early 2000s. LSTM helped to improve machine translation and language modeling.\n\nOther key techniques in this field are negative sampling and word embedding. Word embedding, such as \"word2vec\", can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN. Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing. Deep neural architectures provide the best results for constituency parsing, sentiment analysis, information retrieval, spoken language understanding, machine translation, contextual entity linking, writing style recognition, Text classification and others.\n\nRecent developments generalize word embedding to sentence embedding.\n\nGoogle Translate (GT) uses a large end-to-end long short-term memory network. Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples.\" It translates \"whole sentences at a time, rather than pieces. Google Translate supports over one hundred languages. The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\". GT uses English as an intermediate between most language pairs.\n\nA large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects. Research has explored use of deep learning to predict the biomolecular targets, off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.\n\nAtomNet is a deep learning system for structure-based rational drug design. AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus and multiple sclerosis.\n\nIn 2019 generative neural networks were used to produce molecules that were validated experimentally all the way into mice.\n\nDeep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.\n\nRecommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations. Multiview deep learning has been applied for learning user preferences from multiple domains. The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n\nAn autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.\n\nIn medical informatics, deep learning was used to predict sleep quality based on data from wearables and predictions of health complications from electronic health record data. Deep learning has also showed efficacy in healthcare.\n\nDeep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement\n\nFinding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server. Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n\nDeep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization. These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\" which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n\nDeep learning is being successfully applied to financial fraud detection and anti-money laundering. \"Deep anti-money laundering detection system can spot and recognize relationships and similarities between data and, further down the road, learn to detect anomalies or classify and predict specific events\". The solution leverages both supervised learning techniques, such as the classification of suspicious transactions, and unsupervised learning, e.g. anomaly detection.\n\nThe United States Department of Defense applied deep learning to train robots in new tasks through observation.\n\nDeep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.\"\n\nA variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.\n\nAlthough a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.\n\nFacebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.\n\nGoogle's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.\n\nIn 2015, Blippar demonstrated a mobile augmented reality application that uses deep learning to recognize objects in real time.\n\nIn 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.\n\nAs of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”\n\nDeep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n\nA main criticism concerns the lack of theory surrounding some methods. Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear. (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.\n\nOthers point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed for realizing this goal entirely. Research psychologist Gary Marcus noted:\"Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.\"As an alternative to this emphasis on the limits of deep learning, one author speculated that it might be possible to train a machine vision stack to perform the sophisticated task of discriminating between \"old master\" and amateur figure drawings, and hypothesized that such a sensitivity might represent the rudiments of a non-trivial machine empathy. This same author proposed that this would be in line with anthropology, which identifies a concern with aesthetics as a key element of behavioral modernity.\n\nIn further reference to the idea that artistic sensitivity might inhere within relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on \"The Guardian's\" website.\n\nSome deep learning architectures display problematic behaviors, such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images and misclassifying minuscule perturbations of correctly classified images. Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures. These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar decompositions of observed entities and events. Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition and artificial intelligence (AI).\n\nAs deep learning moves from the lab into the world, research and experience shows that artificial neural networks are vulnerable to hacks and deception. By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such a manipulation is termed an “adversarial attack.” In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system. One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.\n\nAnother group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.\n\nANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.\n\nAnother group demonstrated that certain sounds could make the Google Now voice command system open a particular web address that would download malware.\n\nIn “data poisoning,” false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.\n\nMost Deep Learning systems rely on training and verification data that is generated and/or annotated by humans. It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such. The philosopher Rainer Mühlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork. Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether of not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture. This user interface is a mechanism to generate \"a constant stream of  verification data\" to further train the network in real-time. As Mühlhoff argues, involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as \"human-aided artificial intelligence\".\n\nShallowing refers to reducing a pre-trained DNN to a smaller network with the same or similar performance. Training of DNN with further shallowing can produce more efficient systems than just training of smaller networks from scratch. Shallowing is the rebirth of pruning that developed in the 1980-1990s. The main approach to pruning is to gradually remove network elements (synapses, neurons, blocks of neurons, or layers) that have little impact on performance evaluation. Various indicators of sensitivity are used that estimate the changes of performance after pruning. The simplest indicators use just values of transmitted signals and the synaptic weights (the zeroth order). More complex indicators use mean absolute values of partial derivatives of the cost function, \nor even the second derivatives. The shallowing allows to reduce the necessary resources and makes the skills of neural network more explicit. It is used for image classification, for development of security systems, for accelerating DNN execution on mobile devices, and for other applications. It has been demonstrated that using linear postprocessing, such as supervised PCA, improves DNN performance after shallowing.\n\n", "related": "\n- Applications of artificial intelligence\n- Comparison of deep learning software\n- Compressed sensing\n- Echo state network\n- List of artificial intelligence projects\n- Liquid state machine\n- List of datasets for machine learning research\n- Reservoir computing\n- Sparse coding\n"}
{"id": "47012074", "url": "https://en.wikipedia.org/wiki?curid=47012074", "title": "Neural Designer", "text": "Neural Designer\n\nNeural Designer is a software tool for data analytics based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.\n\nIn 2015, Neural Designer was chosen by the European Commission, within the Horizon 2020 program, as a disruptive technology in the ICT field.\n\nNeural Designer performs descriptive, diagnostic, predictive and prescriptive data analytics. It implements deep architectures with multiple non-linear layers and contains utilities to solve function regression, pattern recognition, time series and autoencoding problems.\n\nThe input to Neural Designer is a data set, and the output from it is a predictive model. That result takes the form of an explicit mathematical expression, which can be exported to any computer language or system.\n\n- Weka: free machine learning and data mining software.\n- RapidMiner: free and commercial machine learning framework implemented in Java.\n- KNIME: free and commercial machine learning and data mining software.\n\n", "related": "\n- Artificial intelligence\n- Artificial neural network\n- Comparison of deep learning software\n- Data mining\n- Deep learning\n- Machine learning\n- Predictive analytics\n"}
{"id": "1643246", "url": "https://en.wikipedia.org/wiki?curid=1643246", "title": "Numenta", "text": "Numenta\n\nNumenta is a machine intelligence company that has developed a cohesive theory, core software, technology and applications based on the principles of the neocortex. The company was founded on February 4, 2005 by Palm founder Jeff Hawkins with his longtime business partner Donna Dubinsky and Stanford graduate student Dileep George. Numenta is headquartered in Redwood City, California and is privately funded.\n\nNumenta has developed a number of example applications to demonstrate the applicability of its technology. Its first commercial product, Grok, offers anomaly detection for IT analytics, giving insight into IT systems to identify unusual behavior and reduce business downtime. Grok has since been licensed to their strategic partner, Avik Partners. Other applications include stock monitoring, geospatial tracking, and rogue behavior.\n\nIn addition, Numenta has created NuPIC (Numenta Platform for Intelligent Computing) as an open source project and maintains an open source community discussion forum.\n\nThe company name comes from the Latin \"mentis\" (\"pertaining to the mind\") genitive of \"mēns\" (\"mind\").\n\nNumenta's machine intelligence technology is called hierarchical temporal memory (HTM), and is a computational theory of the neocortex. This theory was first described in the book \"On Intelligence\", written in 2004 by Jeff Hawkins and co-author Sandra Blakeslee. At the core of HTM are time-based learning algorithms that store and recall temporal patterns. The HTM algorithms are documented and available through its open source project, NuPIC. The HTM technology is suited to address a number of problems, particularly those with the following characteristics: streaming data, underlying patterns in data change over time, subtle patterns, time-based patterns.\n\nNumenta focuses on large-scale brain theory and simulation. Numenta researchers work with experimentalists and published results to derive an understanding of the neocortex. Their main research focus areas are cortical columns, sequence learning and sparse distributed representations. They have written a number of peer-reviewed journal papers and research reports on these topics.\n\nNumenta is a technology provider and does not create go-to-market solutions for specific use cases. The company licenses their technology and application code to developers, organizations and companies who wish to build upon their technology.\" Numenta has several different types of licenses, including open source licenses, trial licenses and commercial licenses. Developers can use Numenta technology within NuPIC using the AGPL v3 open source license.\n\nThe following commercial applications are available using NuPIC: \n- Grok – anomaly detection for IT servers.\n- Cortical.io – advanced natural language processing.\n\nThe following tools are available on NuPIC:\n- HTM Studio – find anomalies in time series.\n- Numenta Anomaly Benchmark – compare HTM anomalies with other anomaly detection techniques.\n\nThe following example applications are available on NuPIC:\n\n- HTM for Stocks – example of tracking anomalies in the stock market (sample code)\n- Rogue behavior detection – example of finding anomalies in human behavior (white paper and sample code)\n- Geospatial tracking – example of finding anomalies in objects moving through space and time (white paper and sample code)\n\nNumenta works with strategic partners, who license their technology and build products using HTM. Cortical.io is using HTM for natural language processing, the partnership was announced in May 2015. Avik Partners has licensed their Grok for IT Analytics application to monitor IT servers. The partnership was announced in August 2015. Numenta also partners with various research institutions and universities.\n\nThe Numenta Platform for Intelligent Computing is an open source platform and community for machine intelligence based on HTM theory. NuPIC is an implementation of HTM and can be used to analyze streaming data. Numenta first announced in June 2013 that it would open-source its HTM technology, the core of its software and algorithms. This was accompanied by the new Numenta.org website and a mailing list for community members.\n\nCommunity members are contributors from around the world, and topics on the mailing list have included both discussions of the HTM theory and details of software development. The mission of NuPIC is to build and support a community that is interested in machine learning and machine intelligence based on modeling the neocortex and its principles.\n\nNumenta has hosted a series of hackathons, the first one in 2013, to bring community members together to collaborate on NuPIC and its applications.\n\nNuPIC codebase is based on 2.7.x Python that is scheduled to be obsoleted in 2020. There is no active plan to update it to support a modern version of Python at the moment. , a PyTorch version of NuPIC remains in unsteady development.\n", "related": "NONE"}
{"id": "11273721", "url": "https://en.wikipedia.org/wiki?curid=11273721", "title": "Hierarchical temporal memory", "text": "Hierarchical temporal memory\n\nHierarchical temporal memory (HTM) is a biologically constrained theory (or model) of intelligence, originally described in the 2004 book \"On Intelligence\" by Jeff Hawkins with Sandra Blakeslee. HTM is based on neuroscience and the physiology and interaction of pyramidal neurons in the neocortex of the mammalian (in particular, human) brain.\n\nAt the core of HTM are learning algorithms that can store, learn, infer, and recall high-order sequences. Unlike most other machine learning methods, HTM continuously learns (in an unsupervised process) time-based patterns in unlabeled data. HTM is robust to noise, and has high capacity (it can learn multiple patterns simultaneously). When applied to computers, HTM is well suited for prediction, anomaly detection, classification, and ultimately sensorimotor applications.\n\nThe theory has been tested and implemented in software through example applications from Numenta and a few commercial applications from Numenta's partners.\n\nA typical HTM network is a tree-shaped hierarchy of \"levels\" (not to be confused with the \"layers\" of the neocortex, as described ). These levels are composed of smaller elements called \"region\"s (or nodes). A single level in the hierarchy possibly contains several regions. Higher hierarchy levels often have fewer regions. Higher hierarchy levels can reuse patterns learned at the lower levels by combining them to memorize more complex patterns.\n\nEach HTM region has the same basic function. In learning and inference modes, sensory data (e.g. data from the eyes) comes into bottom-level regions. In generation mode, the bottom level regions output the generated pattern of a given category. The top level usually has a single region that stores the most general and most permanent categories (concepts); these determine, or are determined by, smaller concepts at lower levels—concepts that are more restricted in time and space. When set in inference mode, a region (in each level) interprets information coming up from its \"child\" regions as probabilities of the categories it has in memory.\n\nEach HTM region learns by identifying and memorizing spatial patterns—combinations of input bits that often occur at the same time. It then identifies temporal sequences of spatial patterns that are likely to occur one after another.\n\nHTM is an evolving theory (or model): it is not yet a complete theory, as our knowledge of the neocortex is incomplete. So new findings on the neocortex are progressively incorporated into the HTM model, which change over time in response. The new findings do not necessarily invalidate the previous parts of the model, so ideas from one generation are not necessarily excluded in its successive one. Because of the evolving nature of the theory, there have been several generations of HTM algorithms, which are briefly described below.\n\nThe first generation of HTM algorithms (or the first version of the HTM theory) is sometimes referred to as \"zeta 1\".\n\nDuring \"training\", a node (or region) receives a temporal sequence of spatial patterns as its input. The learning process consists of two stages:\n\n1. The spatial pooling identifies (in the input) frequently observed patterns and memorizes them as \"coincidences\". Patterns that are significantly similar to each other are treated as the same coincidence. A large number of possible input patterns are reduced to a manageable number of known coincidences.\n2. The temporal pooling partitions coincidences that are likely to follow each other in the training sequence into temporal groups. Each group of patterns represents a \"cause\" of the input pattern (or \"name\" in \"On Intelligence\").\n\nThe concepts of \"spatial pooling\" and \"temporal pooling\" are still quite important in the current HTM theory. Temporal pooling is not yet well understood, and its meaning has changed over time (as the HTM theory evolved).\n\nDuring inference, the node calculates the set of probabilities that a pattern belongs to each known coincidence. Then it calculates the probabilities that the input represents each temporal group. The set of probabilities assigned to the groups is called a node's \"belief\" about the input pattern. (In a simplified implementation, node's belief consists of only one winning group). This belief is the result of the inference that is passed to one or more \"parent\" nodes in the next higher level of the hierarchy.\n\n\"Unexpected\" patterns to the node do not have a dominant probability of belonging to any one temporal group, but have nearly equal probabilities of belonging to several of the groups. If sequences of patterns are similar to the training sequences, then the assigned probabilities to the groups will not change as often as patterns are received. The output of the node will not change as much, and a resolution in time is lost.\n\nIn a more general scheme, the node's belief can be sent to the input of any node(s) in any level(s), but the connections between the nodes are still fixed. The higher-level node combines this output with the output from other child nodes thus forming its own input pattern.\n\nSince resolution in space and time is lost in each node as described above, beliefs formed by higher-level nodes represent an even larger range of space and time. This is meant to reflect the organization of the physical world as it is perceived by human brain. Larger concepts (e.g. causes, actions and objects) are perceived to change more slowly and consist of smaller concepts that change more quickly. Jeff Hawkins postulates that brains evolved this type of hierarchy to match, predict, and affect the organization of the external world.\n\nMore details about the functioning of Zeta 1 HTM can be found in Numenta's old documentation.\n\nThe second generation of HTM learning algorithms, often referred to as cortical learning algorithms (CLA), was drastically different from zeta 1. It relies on a data structure called sparse distributed representations (that is, a data structure whose elements are binary, 1 or 0, and whose number of 1 bits is small compared to the number of 0 bits) to represent the brain activity and a more biologically-realistic neuron model (often also referred to as cell, in the context of the HTM theory). There are two core components in this HTM theory: a spatial pooling algorithm, which outputs sparse distributed representations (SDR), and a sequence memory algorithm, which learns to represent and predict complex sequences.\n\nIn this new generation, the layers and minicolumns of the cerebral cortex are addressed and partially modeled. Each HTM layer (not to be confused with an HTM level of an HTM hierarchy, as described above) consists of a number of highly interconnected minicolumns. An HTM layer creates a sparse distributed representation from its input, so that a fixed percentage of \"minicolumns\" are active at any one time. A minicolumn is understood as a group of cells that have the same receptive field. Each minicolumn has a number of cells that are able to remember several previous states. A cell can be in one of three states: \"active\", \"inactive\" and \"predictive\" state.\n\nThe receptive field of each minicolumn is a fixed number of inputs that are randomly selected from a much larger number of node inputs. Based on the (specific) input pattern, some minicolumns will be more or less associated with the active input values. Spatial pooling selects a relatively constant number of the most active minicolumns and inactivates (inhibits) other minicolumns in the vicinity of the active ones. Similar input patterns tend to activate a stable set of minicolumns. The amount of memory used by each layer can be increased to learn more complex spatial patterns or decreased to learn simpler patterns.\n\nAs mentioned above, a cell (or a neuron) of a minicolumn, at any point in time, can be in an active, inactive or predictive state. Initially, cells are inactive.\n\nIf one or more cells in the active minicolumn are in the \"predictive\" state (see below), they will be the only cells to become active in the current time step. If none of the cells in the active minicolumn are in the predictive state (which happens during the initial time step or when the activation of this minicolumn was not expected), all cells are made active.\n\nWhen a cell becomes active, it gradually forms connections to nearby cells that tend to be active during several previous time steps. Thus a cell learns to recognize a known sequence by checking whether the connected cells are active. If a large number of connected cells are active, this cell switches to the \"predictive\" state in anticipation of one of the few next inputs of the sequence.\n\nThe output of a layer includes minicolumns in both active and predictive states. Thus minicolumns are active over longer periods of time, which leads to greater temporal stability seen by the parent layer.\n\nCortical learning algorithms are able to learn continuously from each new input pattern, therefore no separate inference mode is necessary. During inference, HTM tries to match the stream of inputs to fragments of previously learned sequences. This allows each HTM layer to be constantly predicting the likely continuation of the recognized sequences. The index of the predicted sequence is the output of the layer. Since predictions tend to change less frequently than the input patterns, this leads to increasing temporal stability of the output in higher hierarchy levels. Prediction also helps to fill in missing patterns in the sequence and to interpret ambiguous data by biasing the system to infer what it predicted.\n\nCortical learning algorithms are currently being offered as commercial SaaS by Numenta (such as Grok).\n\nThe following question was posed to Jeff Hawkins September 2011 with regard to cortical learning algorithms: \"How do you know if the changes you are making to the model are good or not?\" To which Jeff's response was \"There are two categories for the answer: one is to look at neuroscience, and the other is methods for machine intelligence. In the neuroscience realm there are many predictions that we can make, and those can be tested. If our theories explain a vast array of neuroscience observations then it tells us that we’re on the right track. In the machine learning world they don’t care about that, only how well it works on practical problems. In our case that remains to be seen. To the extent you can solve a problem that no one was able to solve before, people will take notice.\"\n\nThe third generation builds on the second generation and adds in a theory of sensorimotor inference in the neocortex. This theory proposes that cortical columns at every level of the hierarchy can learn complete models of objects over time and that features are learned at specific locations on the objects. The theory was expanded in 2018 and referred to as the Thousand Brains Theory. \n\nHTM attempts to implement the functionality that is characteristic of a hierarchically related group of cortical regions in the neocortex. A \"region\" of the neocortex corresponds to one or more \"levels\" in the HTM hierarchy, while the hippocampus is remotely similar to the highest HTM level. A single HTM node may represent a group of cortical columns within a certain region.\n\nAlthough it is primarily a functional model, several attempts have been made to relate the algorithms of the HTM with the structure of neuronal connections in the layers of neocortex. The neocortex is organized in vertical columns of 6 horizontal layers. The 6 layers of cells in the neocortex should not be confused with levels in an HTM hierarchy.\n\nHTM nodes attempt to model a portion of cortical columns (80 to 100 neurons) with approximately 20 HTM \"cells\" per column. HTMs model only layers 2 and 3 to detect spatial and temporal features of the input with 1 cell per column in layer 2 for spatial \"pooling\", and 1 to 2 dozen per column in layer 3 for temporal pooling. A key to HTMs and the cortex's is their ability to deal with noise and variation in the input which is a result of using a \"sparse distributive representation\" where only about 2% of the columns are active at any given time.\n\nAn HTM attempts to model a portion of the cortex's learning and plasticity as described above. Differences between HTMs and neurons include:\n- strictly binary signals and synapses\n- no direct inhibition of synapses or dendrites (but simulated indirectly)\n- currently only models layers 2/3 and 4 (no 5 or 6)\n- no \"motor\" control (layer 5)\n- no feed-back between regions (layer 6 of high to layer 1 of low)\n\nIntegrating memory component with neural networks has a long history dating back to early research in distributed representations and self-organizing maps. For example, in sparse distributed memory (SDM), the patterns encoded by neural networks are used as memory addresses for content-addressable memory, with \"neurons\" essentially serving as address encoders and decoders.\n\nComputers store information in \"dense\" representations such as a 32-bit word, where all combinations of 1s and 0s are possible.\nBy contrast, brains use \"sparse\" distributed representations (SDRs). The human neocortex has roughly 16 billion neurons, but at any given time only a small percent are active. The activities of neurons are like bits in a computer, and so the representation is sparse. Similar to SDM developed by NASA in the 80s and vector space models used in Latent semantic analysis, HTM uses sparse distributed representations.\n\nThe SDRs used in HTM are binary representations of data consisting of many bits with a small percentage of the bits active (1s); a typical implementation might have 2048 columns and 64K artificial neurons where as few as 40 might be active at once. Although it may seem less efficient for the majority of bits to go \"unused\" in any given representation, SDRs have two major advantages over traditional dense representations. First, SDRs are tolerant of corruption and ambiguity due to the meaning of the representation being shared (\"distributed\") across a small percentage (\"sparse\") of active bits. In a dense representation, flipping a single bit completely changes the meaning, while in an SDR a single bit may not affect the overall meaning much. This leads to the second advantage of SDRs: because the meaning of a representation is distributed across all active bits, similarity between two representations can be used as a measure of semantic similarity in the objects they represent. That is, if two vectors in an SDR have 1s in the same position, then they are semantically similar in that attribute. The bits in SDRs have semantic meaning, and that meaning is distributed across the bits.\n\nThe semantic folding theory builds on these SDR properties to propose a new model for language semantics, where words are encoded into word-SDRs and the similarity between terms, sentences, and texts can be calculated with simple distance measures.\n\nLikened to a Bayesian network, an HTM comprises a collection of nodes that are arranged in a tree-shaped hierarchy. Each node in the hierarchy discovers an array of causes in the input patterns and temporal sequences it receives. A Bayesian belief revision algorithm is used to propagate feed-forward and feedback beliefs from child to parent nodes and vice versa. However, the analogy to Bayesian networks is limited, because HTMs can be self-trained (such that each node has an unambiguous family relationship), cope with time-sensitive data, and grant mechanisms for covert attention.\n\nA theory of hierarchical cortical computation based on Bayesian belief propagation was proposed earlier by Tai Sing Lee and David Mumford. While HTM is mostly consistent with these ideas, it adds details about handling invariant representations in the visual cortex.\n\nLike any system that models details of the neocortex, HTM can be viewed as an artificial neural network. The tree-shaped hierarchy commonly used in HTMs resembles the usual topology of traditional neural networks. HTMs attempt to model cortical columns (80 to 100 neurons) and their interactions with fewer HTM \"neurons\". The goal of current HTMs is to capture as much of the functions of neurons and the network (as they are currently understood) within the capability of typical computers and in areas that can be made readily useful such as image processing. For example, feedback from higher levels and motor control are not attempted because it is not yet understood how to incorporate them and binary instead of variable synapses are used because they were determined to be sufficient in the current HTM capabilities.\n\nLAMINART and similar neural networks researched by Stephen Grossberg attempt to model both the infrastructure of the cortex and the behavior of neurons in a temporal framework to explain neurophysiological and psychophysical data. However, these networks are, at present, too complex for realistic application.\n\nHTM is also related to work by Tomaso Poggio, including an approach for modeling the ventral stream of the visual cortex known as HMAX. Similarities of HTM to various AI ideas are described in the December 2005 issue of the Artificial Intelligence journal.\n\nNeocognitron, a hierarchical multilayered neural network proposed by Professor Kunihiko Fukushima in 1987, is one of the first Deep Learning Neural Networks models.\n\nThe Numenta Platform for Intelligent Computing (NuPIC) is one of several available HTM implementations. Some are provided by Numenta, while some are developed and maintained by the HTM open source community.\n\nNuPIC includes implementations of Spatial Pooling and Temporal Memory in both C++ and Python. It also includes 3 APIs. Users can construct HTM systems using direct implementations of the algorithms, or construct a Network using the Network API, which is a flexible framework for constructing complicated associations between different Layers of cortex.\n\nNuPIC 1.0 was released on July 2017, after which the codebase was put into maintenance mode. Current research continues in Numenta research codebases.\n\nThe following commercial applications are available using NuPIC: \n- Grok – anomaly detection for IT servers, see www.grokstream.com\n- Cortical.io – advanced natural language processing, see www.cortical.io\nThe following tools are available on NuPIC:\n- HTM Studio – find anomalies in time series using your own data, see www.numenta.com/htm-studio/\n- Numenta Anomaly Benchmark – compare HTM anomalies with other anomaly detection techniques, see https://numenta.com/numenta-anomaly-benchmark/\nThe following example applications are available on NuPIC, see http://numenta.com/applications/:\n- HTM for stocks – example of tracking anomalies in the stock market (sample code)\n- Rogue behavior detection – example of finding anomalies in human behavior (white paper and sample code)\n- Geospatial tracking – example of finding anomalies in objectives moving through space and time (white paper and sample code)\n\n", "related": "\n- Neocognitron\n- Deep learning\n- Convolutional neural network\n- Strong AI\n- Artificial consciousness\n- Cognitive architecture\n- \"On Intelligence\"\n- Memory-prediction framework\n- Belief revision\n- Belief propagation\n- Bionics\n- List of artificial intelligence projects\n- Memory Network\n- Neural Turing Machine\n- Multiple trace theory\n\n- Hierarchical hidden Markov model\n- Bayesian networks\n- Neural networks\n\n- Cortical Learning Algorithm overview (Accessed May 2013)\n- HTM Cortical Learning Algorithms (PDF Sept. 2011)\n- Numenta, Inc.\n- HTM Cortical Learning Algorithms Archive\n- Association for Computing Machinery talk from 2009 by Subutai Ahmad from Numenta\n- OnIntelligence.org Forum, an Internet forum for the discussion of relevant topics, especially relevant being the Models and Simulation Topics forum.\n- Hierarchical Temporal Memory (Microsoft PowerPoint presentation)\n- Cortical Learning Algorithm Tutorial: CLA Basics, talk about the cortical learning algorithm (CLA) used by the HTM model on YouTube\n\n- Pattern Recognition by Hierarchical Temporal Memory by Davide Maltoni, April 13, 2011\n- Vicarious Startup rooted in HTM by Dileep George\n- The Gartner Fellows: Jeff Hawkins Interview by Tom Austin, \"Gartner\", March 2, 2006\n- Emerging Tech: Jeff Hawkins reinvents artificial intelligence by Debra D'Agostino and Edward H. Baker, \"CIO Insight\", May 1, 2006\n- \"Putting your brain on a microchip\" by Stefanie Olsen, \"CNET News.com\", May 12, 2006\n- \"The Thinking Machine\" by Evan Ratliff, Wired, March 2007\n- Think like a human by Jeff Hawkins, IEEE Spectrum, April 2007\n- Neocortex – Memory-Prediction Framework — Open Source Implementation with GNU General Public License\n- Hierarchical Temporal Memory related Papers and Books\n"}
{"id": "49830146", "url": "https://en.wikipedia.org/wiki?curid=49830146", "title": "Microsoft Cognitive Toolkit", "text": "Microsoft Cognitive Toolkit\n\nMicrosoft Cognitive Toolkit, previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.\n\n", "related": "\n- Comparison of deep learning software\n- ML.NET\n- Open Neural Network Exchange\n"}
{"id": "41755648", "url": "https://en.wikipedia.org/wiki?curid=41755648", "title": "DeepMind", "text": "DeepMind\n\nDeepMind Technologies is a UK artificial intelligence company founded in September 2010, and acquired by Google in 2014. The company is based in London, with research centres in Canada, France, and the United States. In 2015, it became a wholly owned subsidiary of Alphabet Inc.\n\nThe company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.\n\nThe company made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.\n\nThe start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in 2010. Hassabis and Legg first met at University College London's Gatsby Computational Neuroscience Unit.\n\nDuring one of the interviews, Demis Hassabis said that the start-up began working on artificial intelligence technology by teaching it how to play old games from the seventies and eighties, which are relatively primitive compared to the ones that are available today. Some of those games included \"Breakout\", \"Pong\" and \"Space Invaders\". AI was introduced to one game at a time, without any prior knowledge of its rules. After spending some time on learning the game, AI would eventually become an expert in it. “The cognitive processes which the AI goes through are said to be very like those a human who had never seen the game would use to understand and attempt to master it.” The goal of the founders is to create a general-purpose AI that can be useful and effective for almost anything.\n\nMajor venture capital firms Horizons Ventures and Founders Fund invested in the company, as well as entrepreneurs Scott Banister, Peter Thiel, and Elon Musk. Jaan Tallinn was an early investor and an adviser to the company. On 26 January 2014, Google announced the company had acquired DeepMind for $500 million, and that it had agreed to take over DeepMind Technologies. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013. The company was afterwards renamed Google DeepMind and kept that name for about two years.\n\nIn 2014, DeepMind received the \"Company of the Year\" award from Cambridge Computer Laboratory.\n\nIn September 2015, DeepMind and the Royal Free NHS Trust signed their initial Information Sharing Agreement (ISA) to co-develop a clinical task management app, Streams.\n\nAfter Google's acquisition the company established an artificial intelligence ethics board. The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board. DeepMind, together with Amazon, Google, Facebook, IBM and Microsoft, is a founding member of Partnership on AI, an organization devoted to the society-AI interface. DeepMind has opened a new unit called DeepMind Ethics and Society and focused on the ethical and societal questions raised by artificial intelligence featuring prominent philosopher Nick Bostrom as advisor. In October 2017, DeepMind launched a new research team to investigate AI ethics.\n\nIn December 2019, Co-founder Suleyman announced he would be leaving DeepMind to join Google, working in a policy role.\n\nDeepMind Technologies' goal is to \"solve intelligence\", which they are trying to achieve by combining \"the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms\".\nThey are trying to formalize intelligence in order to not only implement it into machines, but also understand the human brain, as Demis Hassabis explains:\n\nGoogle Research has released a paper in 2016 regarding AI Safety and avoiding undesirable behaviour during the AI learning process. Deepmind has also released several publications via its website. In 2017 DeepMind released GridWorld, an open-source testbed for evaluating whether an algorithm learns to disable its kill switch or otherwise exhibits certain undesirable behaviours.\n\nTo date, the company has published research on computer systems that are able to play games, and developing these systems, ranging from strategy games such as Go to arcade games. According to Shane Legg, human-level machine intelligence can be achieved \"when a machine can learn to play a really wide range of games from perceptual stream input and output, and transfer understanding across games[...].\"\n\nResearch describing an AI playing seven different Atari 2600 video games (the \"Pong\" game in \"Video Olympics\", \"Breakout\", \"Space Invaders\", \"Seaquest\", \"Beamrider\", \"Enduro\", and \"Q*bert\") reportedly led to the company's acquisition by Google. Hassabis has mentioned the popular e-sport game \"StarCraft\" as a possible future challenge, since it requires a high level of strategic thinking and handling imperfect information. The first demonstration of the DeepMind progress in \"StarCraft II\" occurred on 24 January 2019, on StarCrafts Twitch channel and DeepMind's YouTube channel.\n\nIn July 2018, researchers from DeepMind trained one of its systems to play the famous computer game \"Quake III Arena\".\n\nAs opposed to other AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within its scope, DeepMind claims that its system is not pre-programmed: it learns from experience, using only raw pixels as data input. Technically it uses deep learning on a convolutional neural network, with a novel form of Q-learning, a form of model-free reinforcement learning. They test the system on video games, notably early arcade games, such as \"Space Invaders\" or \"Breakout\". Without altering the code, the AI begins to understand how to play the game, and after some time plays, for a few games (most notably \"Breakout\"), a more efficient game than any human ever could.\n\nIn 2013, DeepMind demonstrated an AI system could surpass human abilities in games such as Pong, Breakout, Space Invaders, Seaquest, Beamrider, Enduro and Q*bert. DeepMind's AI had been applied to video games made in the 1970s and 1980s; work was ongoing for more complex 3D games such as \"Doom\", which first appeared in the early 1990s.\n\nIn October 2015, a computer Go program called AlphaGo, developed by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero. This is the first time an artificial intelligence (AI) defeated a professional Go player. Previously, computers were only known to have played Go at \"amateur\" level. Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force.\n\nIn March 2016 it beat Lee Sedol—a 9th dan Go player and one of the highest ranked players in the world—with a score of 4-1 in a five-game match.\n\nIn the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who at the time continuously held the world No. 1 ranking for two years. It used a supervised learning protocol, studying large numbers of games played by humans against each other.\n\nIn 2017, an improved version, AlphaGo Zero, defeated AlphaGo 100 games to 0. AlphaGo Zero's strategies were self-taught. AlphaGo Zero was able to beat its predecessor after just three days with less processing power than AlphaGo; in comparison, the original AlphaGo needed months to learn how to play.\n\nLater that year, AlphaZero, a modified version of AlphaGo Zero but for handling any two-player game of perfect information, gained superhuman abilities at chess and shogi. Like AlphaGo Zero, AlphaZero learned solely through self-play.\n\nAlphaGo technology was developed based on the deep reinforcement learning approach. This makes AlphaGo different from the rest of AI technologies on the market. With that said, AlphaGo's ‘brain’ was introduced to various moves based on the historical tournament data. The number of moves was increased gradually until it eventually processed over 30 million of them. The aim was to have the system mimic the human player and eventually become better. It played against itself and learned not only from its own defeats but wins as well; thus, it learned to improve itself over the time and increased its winning rate as a result.\n\nAlphaGo used two deep neural networks: a policy network to evaluate move probabilities and a value network to assess positions. The policy network trained via supervised learning, and was subsequently refined by policy-gradient reinforcement learning. The value network learned to predict winners of games played by the policy network against itself. After training these networks employed a lookahead Monte Carlo tree search (MCTS), using the policy network to identify candidate high-probability moves, while the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) evaluated tree positions.\n\nZero trained using reinforcement learning in which the system played millions of games against itself. Its only guide was to increase its win rate. It did so without learning from games played by humans. Its only input features are the black and white stones from the board. It uses a single neural network, rather than separate policy and value networks. Its simplified tree search relies upon this neural network to evaluate positions and sample moves, without Monte Carlo rollouts. A new reinforcement learning algorithm incorporates lookahead search inside the training loop. AlphaGo Zero employed around 15 people and millions in computing resources. Ultimately, it needed much less computing power than AlphaGo, running on four specialized AI processors (Google TPUs), instead of AlphaGo's 48.\n\nIn 2016 DeepMind turned its artificial intelligence to protein folding, one of the toughest problems in science. In December 2018, DeepMind's AlphaFold won the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) by successfully predicting the most accurate structure for 25 out of 43 proteins. “This is a lighthouse project, our first major investment in terms of people and resources into a fundamental, very important, real-world scientific problem,” Hassabis said to \"The Guardian\".\n\nAlso in 2016, DeepMind introduced WaveNet, a text-to-speech system. It was originally too computationally intensive for use in consumer products, but in late 2017 it became ready for use in consumer applications such as Google Assistant. In 2018 Google launched a commercial text-to-speech product, Cloud Text-to-Speech, based on WaveNet.\n\nIn 2018, DeepMind introduced a more efficient model called WaveRNN co-developed with Google AI. In 2019, Google started to roll it out to Google Duo users.\n\nIn January 2019, DeepMind introduced AlphaStar, a program playing the real-time strategy game \"StarCraft II\". AlphaStar used reinforcement learning based on replays from human players, and then played against itself to enhance its skills. At the time of the presentation, AlphaStar had knowledge equivalent to 200 years of playing time. It won 10 consecutive matches against two professional players, although it had the unfair advantage of being able to see the entire field, unlike a human player who has to move the camera manually. A preliminary version in which that advantage was fixed lost a subsequent match.\n\nIn July 2019, AlphaStar began playing against random humans on the public 1v1 European multiplayer ladder. Unlike the first iteration of AlphaStar, which played only Protoss v. Protoss, this one played as all of the game's races, and had earlier unfair advantages fixed. By October 2019, AlphaStar reached Grandmaster level on the StarCraft II ladder on all three StarCraft races, becoming the first AI to reach the top league of a widely popular esport without any game restrictions.\n\nGoogle has stated that DeepMind algorithms have greatly increased the efficiency of cooling its data centers. In addition, DeepMind (alongside other Alphabet AI researchers) assists Google Play's personalized app recommendations. DeepMind has also collaborated with the Android team at Google for the creation of two new features which will be available to people with devices running Android Pie, the ninth installment of Google's mobile operating system. These features, Adaptive Battery and Adaptive Brightness, use machine learning to conserve energy and make devices running the operating system easier to use. It is the first time DeepMind has used these techniques on such a small scale, with typical machine learning applications requiring orders of magnitude more computing power.\n\nIn July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced to develop AI applications for healthcare. DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.\n\nIn August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.\n\nThere are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records. Staff at the Royal Free Hospital were reported as saying in December 2017 that access to patient data through the app had saved a ‘huge amount of time’ and made a ‘phenomenal’ difference to the management of patients with acute kidney injury. Test result data is sent to staff's mobile phones and alerts them to change in the patient's condition. It also enables staff to see if someone else has responded, and to show patients their results in visual form.\n\nIn November 2017, DeepMind announced a research partnership with the Cancer Research UK Centre at Imperial College London with the goal of improving breast cancer detection by applying machine learning to mammography. Additionally, in February 2018, DeepMind announced it was working with the U.S. Department of Veterans Affairs in an attempt to use machine learning to predict the onset of acute kidney injury in patients, and also more broadly the general deterioration of patients during a hospital stay so that doctors and nurses can more quickly treat patients in need.\n\nDeepMind developed an app called Streams, which sends alerts to doctors about patients at risk of acute risk injury. On 13 November 2018, DeepMind announced that its health division and the Streams app would be absorbed into Google Health. Privacy advocates said the announcement betrayed patient trust and appeared to contradict previous statements by DeepMind that patient data would not be connected to Google accounts or services. A spokesman for DeepMind said that patient data would still be kept separate from Google services or projects.\n\nIn April 2016, \"New Scientist\" obtained a copy of a data sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust. The latter operates three London hospitals where an estimated 1.6 million patients are treated annually. The agreement shows DeepMind Health had access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion in order to conduct research to seek better outcomes in various health conditions.\n\nA complaint was filed to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted. In May 2016, \"New Scientist\" published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency.\n\nIn May 2017, \"Sky News\" published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her \"considered opinion\" the data-sharing agreement between DeepMind and the Royal Free took place on an \"inappropriate legal basis\". The Information Commissioner's Office ruled in July 2017 that the Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind.\n\nIn October 2017, DeepMind announced a new research unit, DeepMind Ethics & Society. Their goal is to fund external research of the following themes: privacy, transparency, and fairness; economic impacts; governance and accountability; managing AI risk; AI morality and values; and how AI can address the world's challenges. As a result, the team hopes to further understand the ethical implications of AI and aid society to seeing AI can be beneficial.\n\nThis new subdivision of DeepMind is a completely separate unit from the partnership of leading companies using AI, academia, civil society organizations and nonprofits of the name Partnership on Artificial Intelligence to Benefit People and Society of which DeepMind is also a part.\n\n", "related": "\n- Glossary of artificial intelligence\n"}
{"id": "51650259", "url": "https://en.wikipedia.org/wiki?curid=51650259", "title": "Keras", "text": "Keras\n\nKeras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System), and its primary author and maintainer is François Chollet, a Google engineer. Chollet also is the author of the XCeption deep neural network model.\n\nIn 2017, Google's TensorFlow team decided to support Keras in TensorFlow's core library. Chollet explained that Keras was conceived to be an interface rather than a standalone machine learning framework. It offers a higher-level, more intuitive set of abstractions that make it easy to develop deep learning models regardless of the computational backend used. Microsoft added a CNTK backend to Keras as well, available as of CNTK v2.0.\n\nKeras contains numerous implementations of commonly used neural-network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools to make working with image and text data easier to simplify the coding necessary for writing deep neural network code. The code is hosted on GitHub, and community support forums include the GitHub issues page, and a Slack channel.\n\nIn addition to standard neural networks, Keras has support for convolutional and recurrent neural networks. It supports other common utility layers like dropout, batch normalization, and pooling.\n\nKeras allows users to productize deep models on smartphones (iOS and Android), on the web, or on the Java Virtual Machine. It also allows use of distributed training of deep-learning models on clusters of Graphics processing units (GPU) and tensor processing units (TPU) principally in conjunction with CUDA.\n\nKeras claims over 250,000 individual users as of mid-2018. Keras was the 10th most cited tool in the KDnuggets 2018 software poll and registered a 22% usage.\n\n", "related": "\n- Comparison of deep-learning software\n"}
{"id": "52513310", "url": "https://en.wikipedia.org/wiki?curid=52513310", "title": "Apache MXNet", "text": "Apache MXNet\n\nApache MXNet is an open-source deep learning software framework, used to train, and deploy deep neural networks. It is scalable, allowing for fast model training, and supports a flexible programming model and multiple programming languages (including C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl, and Wolfram Language.)\n\nThe MXNet library is portable and can scale to multiple GPUs and multiple machines. MXNet is supported by public cloud providers including Amazon Web Services (AWS) and Microsoft Azure. Amazon has chosen MXNet as its deep learning framework of choice at AWS. Currently, MXNet is supported by Intel, Baidu, Microsoft, Wolfram Research, and research institutions such as Carnegie Mellon, MIT, the University of Washington, and the Hong Kong University of Science and Technology.\n\nApache MXNet is a lean, flexible, and ultra-scalable deep learning framework that supports state of the art in deep learning models, including convolutional neural networks (CNNs) and long short-term memory networks (LSTMs).\n\nMXNet is designed to be distributed on dynamic cloud infrastructure, using a distributed parameter server (based on research at Carnegie Mellon University, Baidu, and Google), and can achieve almost linear scale with multiple GPUs or CPUs.\n\nMXNet supports both imperative and symbolic programming, which makes it easier for developers that are used to imperative programming to get started with deep learning. It also makes it easier to track, debug, save checkpoints, modify hyperparameters, such as learning rate or perform early stopping.\n\nSupports C++ for the optimized backend to get the most of the GPU or CPU available, and Python, R, Scala, Clojure, Julia, Perl, MATLAB and JavaScript for a simple frontend for the developers.\n\nSupports an efficient deployment of a trained model to low-end devices for inference, such as mobile devices (using Amalgamation), Internet of things devices (using AWS Greengrass), serverless computing (using AWS Lambda) or containers. These low-end environments can have only weaker CPU or limited memory (RAM), and should be able to use the models that were trained on a higher-level environment (GPU based cluster, for example).\n\n", "related": "\n- Comparison of deep learning software\n- Differentiable programming\n"}
{"id": "52801963", "url": "https://en.wikipedia.org/wiki?curid=52801963", "title": "AlexNet", "text": "AlexNet\n\nAlexNet is the name of a convolutional neural network (CNN), designed by Alex Krizhevsky, and published with Ilya Sutskever and Krizhevsky's doctoral advisor Geoffrey Hinton.\n\nAlexNet competed in the ImageNet Large Scale Visual Recognition Challenge on September 30, 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of graphics processing units (GPUs) during training.\n\nAlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU. A deep CNN of Dan Ciresan et al. (2011) at IDSIA was already 60 times faster and achieved superhuman performance in August 2011. Between May 15, 2011 and September 10, 2012, their CNN won no fewer than four image competitions. They also significantly improved on the best performance in the literature for multiple image databases. \n\nAccording to the AlexNet paper, Ciresan's earlier net is \"somewhat similar.\" Both were originally written with CUDA to run with GPU support. In fact, both are actually just variants of the CNN designs introduced by Yann LeCun et al. (1989) who applied the backpropagation algorithm to a variant of Kunihiko Fukushima's original CNN architecture called \"neocognitron.\" The architecture was later modified by J. Weng's method called max-pooling.\n\nIn 2015, AlexNet was outperformed by Microsoft's very deep CNN with over 100 layers, which won the ImageNet 2015 contest.\n\nAlexNet contained eight layers; the first five were convolutional layers, some of them followed by max-pooling layers, and the last three were fully connected layers. It used the non-saturating ReLU activation function, which showed improved training performance over tanh and sigmoid.\n\nAlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning. , the AlexNet paper has been cited over 47,000 times.\n\nAlex Krizhevsky (born in Ukraine, raised in Canada) is a computer scientist most noted for his work on artificial neural networks and deep learning. \nShortly after having won the ImageNet challenge 2012 through AlexNet, he and his colleagues sold their startup DNN Research Inc. to Google.\nKrizhevsky left Google in September 2017 when he lost interest in the work. At the company Dessa, Krizhevsky will advise and help research new deep-learning techniques.\nMany of his numerous papers on machine learning and computer vision are frequently cited by other researchers.\n", "related": "NONE"}
{"id": "48508507", "url": "https://en.wikipedia.org/wiki?curid=48508507", "title": "TensorFlow", "text": "TensorFlow\n\nTensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google.  \n\nTensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache License 2.0 on November 9, 2015.\n\nStarting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications. Google assigned multiple computer scientists, including Jeff Dean, to simplify and refactor the codebase of DistBelief into a faster, more robust application-grade library, which became TensorFlow. In 2009, the team, led by Geoffrey Hinton, had implemented generalized backpropagation and other improvements which allowed generation of neural networks with substantially higher accuracy, for instance a 25% reduction in errors in speech recognition.\n\nTensorFlow is Google Brain's second-generation system. Version 1.0.0 was released on February 11, 2017. While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units). TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.\n\nIts flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.\n\nTensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as \"tensors\". During the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.\n\nIn December 2017, developers from Google, Cisco, RedHat, CoreOS, and CaiCloud introduced Kubeflow at a conference. Kubeflow allows operation and deployment of TensorFlow on Kubernetes.\n\nIn March 2018, Google announced TensorFlow.js version 1.0 for machine learning in JavaScript.\n\nIn Jan 2019, Google announced TensorFlow 2.0. It became officially available in Sep 2019.\n\nIn May 2019, Google announced TensorFlow Graphics for deep learning in computer graphics.\n\nIn May 2016, Google announced its Tensor processing unit (TPU), an application-specific integrated circuit (a hardware chip) built specifically for machine learning and tailored for TensorFlow. TPU is a programmable AI accelerator designed to provide high throughput of low-precision arithmetic (e.g., 8-bit), and oriented toward using or running models rather than training them. Google announced they had been running TPUs inside their data centers for more than a year, and had found them to deliver an order of magnitude better-optimized performance per watt for machine learning.\n\nIn May 2017, Google announced the second-generation, as well as the availability of the TPUs in Google Compute Engine. The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs, provide up to 11.5 petaflops.\n\nIn May 2018, Google announced the third-generation TPUs delivering up to 420 teraflops of performance and 128 GB high bandwidth memory (HBM). Cloud TPU v3 Pods offer 100+ petaflops of performance and 32 TB HBM.\n\nIn February 2018, Google announced that they were making TPUs available in beta on the Google Cloud Platform.\n\nIn July 2018, the Edge TPU was announced. Edge TPU is Google’s purpose-built ASIC chip designed to run TensorFlow Lite machine learning (ML) models on small client computing devices such as smartphones known as edge computing.\n\nIn May 2017, Google announced a software stack specifically for mobile development, TensorFlow Lite. In January 2019, TensorFlow team released a developer preview of the mobile GPU inference engine with OpenGL ES 3.1 Compute Shaders on Android devices and Metal Compute Shaders on iOS devices. In May 2019, Google announced that their TensorFlow Lite Micro (also known as TensorFlow Lite for Microcontrollers) and ARM's uTensor would be merging.\n\nIn October 2017, Google released the Google Pixel 2 which featured their Pixel Visual Core (PVC), a fully programmable image, vision and AI processor for mobile devices. The PVC supports TensorFlow for machine learning (and Halide for image processing).\n\nGoogle officially released RankBrain on October 26, 2015, backed by TensorFlow.\n\nGoogle also released , which is a TensorFlow Jupyter notebook environment that requires no setup to use.\n\nOn March 1, 2018, Google released its Machine Learning Crash Course (MLCC). Originally designed to help equip Google employees with practical artificial intelligence and machine learning fundamentals, Google rolled out its free TensorFlow workshops in several cities around the world before finally releasing the course to the public.\n\nTensorFlow provides stable Python (for version 3.7 across all platforms) and C APIs; and without API backwards compatibility guarantee: C++, Go, Java, JavaScript and Swift (early release). Third-party packages are available for C#, Haskell, Julia, MATLAB, R, Scala, Rust, OCaml, and Crystal.\n\n\"New language support should be built on top of the C API. However, [..] not all functionality is available in C yet.\" Some more functionality is provided by the Python API.\n\nAmong the applications for which TensorFlow is the foundation, are automated image-captioning software, such as DeepDream.\n", "related": "\n- Comparison of deep learning software\n- Differentiable programming\n\n"}
{"id": "49594059", "url": "https://en.wikipedia.org/wiki?curid=49594059", "title": "ND4J (software)", "text": "ND4J (software)\n\nND4J (short for \"N-Dimensional Arrays for Java\") is a scientific computing and linear algebra library, written in the programming language Java, operating on the Java virtual machine (JVM), and compatible with other languages such as Scala, Kotlin and Clojure. ND4J was contributed to the Eclipse Foundation in October 2017.\n\nND4J is for performing linear algebra and matrix manipulation in a production environment, integrating with Apache Hadoop and Spark to work with distributed central processing units (CPUs) or graphics processing units (GPUs). It supports n-dimensional arrays for JVM-based languages.\n\nND4J is free and open-source software, released under Apache License 2.0, and developed mostly by the group in San Francisco that built Deeplearning4j. It was created under an Apache Software Foundation license.\n\nND4J's operations include distributed parallel versions. Operation can occur in a cluster and process massive amounts of data. Matrix manipulation occurs in parallel on CPUs or GPUs over cloud computing, and can work in Apache Spark or Hadoop clusters.\n\nA usability gap has separated Java, Scala, Kotlin and Clojure programmers from powerful tools in data analysis such as NumPy or Matlab. Libraries like Breeze, the numerical processing library for Scala, don't support n-dimensional arrays, or tensors, which are necessary for deep learning and other tasks. Libraries like Colt and Parallel Colt use or have dependencies with GPL in the license, making them unsuitable for commercial use. ND4J was built to address those functional and licenses issues.\n\n", "related": "\n- NumPy\n- SciPy\n"}
{"id": "55690224", "url": "https://en.wikipedia.org/wiki?curid=55690224", "title": "Netomi", "text": "Netomi\n\nNetomi, formerly msg.ai, is an American artificial intelligence company and developer of human–computer interaction technologies.\n\nFounded in May 2015 by Puneet Mehta, Netomi was recruited by the global media and advertising agency, Universal McCann, to assist clients Heinz and BMW.\n\nNetomi worked with Sony Pictures to launch the first ever bot on Facebook Messenger for a $100M film, “Goosebumps” and subsequently joined Y Combinator as a member of the Winter 2016 class.\n\nIn 2016 the company received investments from several venture capital firms, including Index Ventures, Y Combinator, Bowery Capital, Salesforce Ventures and private entrepreneurs such as the founders of Google DeepMind and OpenAI.\n\nNetomi later partnered with Facebook’s Creative Shop and the Tommy Hilfiger fashion brand to develop a brand-specific bot with the goal of outperforming existing retail shopping bots.\n\nIn 2016 there was an update to the platform to incorporate multivariate testing. This type of testing, unlike traditional A/B testing, permits the monitoring of user interaction with the bot, adjustments to the bot’s tone, and experiments with the use of media.\n\nIn 2018, the company changed its name to Netomi.\n\nNetomi's deep reinforcement learning platform allows for conversational AI and chatbots which engage through personalized interactions at scale and one-to-one relationships throughout the entire user experience.\n\nNetomi utilizes artificial intelligence to automate customized messages and engage in natural dialogues with deep reinforcement learning. This allows the bot to interact in a conversational manner and in a number of ways, including offering product recommendations based on user preferences, answering questions regarding availability and pricing, guiding customers towards a purchase, and providing assistance to complex issues.\n\nNetomi has collaborated with messaging platforms, creative agencies, and technology providers to build conversational AI for brands such as Heinz, BMW, Tommy Hilfiger, Signal, and The Anne Frank House.\n\n", "related": "\n- Artificial intelligence\n- Chatbot\n"}
{"id": "54022970", "url": "https://en.wikipedia.org/wiki?curid=54022970", "title": "PyTorch", "text": "PyTorch\n\nPyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook's AI Research lab (FAIR). It is free and open-source software released under the Modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface. \n\nA number of pieces of Deep Learning software are built on top of PyTorch, including Uber's Pyro, HuggingFace's Transformers, and Catalyst.\n\nPyTorch provides two high-level features:\n- Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)\n- Deep neural networks built on a tape-based autodiff system\n\nFacebook operates both \"PyTorch\" and \"Convolutional Architecture for Fast Feature Embedding\" (Caffe2), but models defined by the two frameworks were mutually incompatible. The Open Neural Network Exchange (ONNX) project was created by Facebook and Microsoft in September 2017 for converting models between frameworks. Caffe2 was merged into PyTorch at the end of March 2018.\n\nPyTorch defines a class called Tensor (codice_1) to store and operate on homogeneous multidimensional rectangular arrays of numbers. PyTorch Tensors are similar to NumPy Arrays, but can also be operated on a CUDA-capable Nvidia GPU. PyTorch supports various sub-types of Tensors.\n\nPyTorch uses a method called automatic differentiation. A recorder records what operations have performed, and then it replays it backward to compute the gradients. This method is especially powerful when building neural networks to save time on one epoch by calculating differentiation of the parameters at the forward pass.\n\ncodice_3 is a module that implements various optimization algorithms used for building neural networks. Most of the commonly used methods are already supported, so there is no need to build them from scratch.\n\nPyTorch autograd makes it easy to define computational graphs and take gradients, but raw autograd can be a bit too low-level for defining complex neural networks. This is where the codice_4 module can help.\n\n", "related": "\n- Comparison of deep learning software\n- Differentiable programming\n- Torch (machine learning)\n- Tensor\n"}
{"id": "56173403", "url": "https://en.wikipedia.org/wiki?curid=56173403", "title": "Deep Image Prior", "text": "Deep Image Prior\n\nDeep Image Prior is a type of convolutional neural network used to enhance a given image with no prior training data other than the image itself.\nA neural network is randomly initialized and used as prior to solve inverse problems such as noise reduction, super-resolution, and inpainting. Image statistics is captured by the structure of a convolutional image generator rather than by any previously learned capabilities.\n\nInverse problems such as noise reduction, super-resolution, and inpainting can be formulated as the optimization task formula_1, where formula_2 is an image, formula_3 a corrupted representation of that image, formula_4 is a task-dependent data term, and R(x) is the regularizer. This forms an energy minimization problem.\n\nDeep neural networks learn a generator/decoder formula_5 which maps a random code vector formula_6 to an image formula_2.\n\nThe image corruption method used to generate formula_3 is selected for the specific application.\n\nIn this approach, the formula_9 prior is replaced with the implicit prior captured by the neural network (where formula_10 for images that can be produced by a deep neural networks and formula_11 otherwise). This yields the equation for the minimizer formula_12 and the result of the optimization process formula_13.\n\nThe minimizer formula_14 (typically a gradient descent) starts from a randomly initialized parameters and descends into a local best result to yield the formula_15 restoration function.\n\nA parameter θ may be used to recover any image, including its noise. However, the network is reluctant to pick up noise because it contains high impedance while useful signal offers low impedance. This results in the θ parameter approaching a good-looking local optimum so long as the number of iterations in the optimization process remains low enough not to overfit data.\n\nThe principle of denoising is to recover an image formula_2 from a noisy observation formula_3, where formula_18. The distribution formula_19 is sometimes known (e.g.: profiling sensor and photon noise) and may optionally be incorporated into the model, though this process works well in blind denoising.\n\nThe quadratic energy function formula_20 is used as the data term, plugging it into the equation for formula_14 yields the optimization problem formula_22.\n\nSuper-resolution is used to generate a higher resolution version of image x. The data term is set to formula_23 where d(·) is a downsampling operator such as Lanczos that decimates the image by a factor t.\n\nInpainting is used to reconstruct a missing area in an image formula_3. These missing pixels are defined as the binary mask formula_25. The data term is defined as formula_26 (where formula_27 is the Hadamard product).\n\nThis approach may be extended to multiple images. A straightforward example mentioned by the author is the reconstruction of an image to obtain natural light and clarity from a flash-no-flash pair. Video reconstruction is possible but it requires optimizations to take into account the spatial differences.\n\n- A reference implementation rewritten in Python 3.6 with the PyTorch 0.4.0 library was released by the author under the Apache 2.0 license: deep-image-prior\n- A Tensorflow-based implementation written in Python 2 and released under the CC-SA 3.0 license: deep-image-prior-tensorflow\n- A Keras-based implementation written in Python 2 and released under the GPLv3: machine_learning_denoising\n", "related": "NONE"}
{"id": "56086085", "url": "https://en.wikipedia.org/wiki?curid=56086085", "title": "Deep Instinct", "text": "Deep Instinct\n\nDeep Instinct is a cybersecurity company that applies deep learning to cybersecurity. The company implements advanced Artificial Intelligence to the task of preventing and detecting malware.\n\nDeep Instinct was founded in 2014 by Guy Caspi, Dr. Eli David, and Nadav Maman. The headquarters of the company is located in New York City.\n\nNVIDIA is an investor.\n\nIn April 2019, Deep Instinct commissioned an art project, titled \"The Persistence of Chaos\", by Chinese artist, Guo O Dong, consisting of a laptop infected with 6 pieces of malware that represented $95 billion in damages. The art was auctioned with a final bid of $1,345,000.\n", "related": "NONE"}
{"id": "57179040", "url": "https://en.wikipedia.org/wiki?curid=57179040", "title": "U-Net", "text": "U-Net\n\nU-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg, Germany. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.\n\nThe U-Net architecture stems from the so-called “fully convolutional network” first proposed by Long and Shelhamer.\n\nThe main idea is to supplement a usual contracting network by successive layers, where pooling operations are replaced by upsampling operators. Hence these layers increase the resolution of the output. What's more, a successive convolutional layer can then learn to assemble a precise output based on this information.\n\nOne important modification in U-Net is that there are a large number of feature channels in the upsampling part, which allow the network to propagate context information to higher resolution layers. As a consequence, the expansive path is more or less symmetric to the contracting part, and yields a u-shaped architecture. The network only uses the valid part of each convolution without any fully connected layers. To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the input image. This tiling strategy is important to apply the network to large images, since otherwise the resolution would be limited by the GPU memory.\n\nU-Net was created by Olaf Ronneberger, Philipp Fischer, Thomas Brox in 2015 at the paper “U-Net: Convolutional Networks for Biomedical Image Segmentation”. It's an improvement and development of FCN: Evan Shelhamer, Jonathan Long, Trevor Darrell (2014). \"Fully convolutional networks for semantic segmentation\".\n\nThe network consists of a contracting path and an expansive path, which gives it the u-shaped architecture. The contracting path is a typical convolutional network that consists of repeated application of convolutions, each followed by a rectified linear unit (ReLU) and a max pooling operation. During the contraction, the spatial information is reduced while feature information is increased. The expansive pathway combines the feature and spatial information through a sequence of up-convolutions and concatenations with high-resolution features from the contracting path.\n\nThere are many applications of U-Net in biomedical image segmentation, such as brain image segmentation (<nowiki>\"BRATS\"</nowiki>) and liver image segmentation (\"siliver07\"). Variations of the U-Net have also been applied for medical image reconstruction. Here are some variants and applications of U-Net as follows:\n\n1. Pixel-wise regression using U-Net and its application on pansharpening;\n2. 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation;\n3. TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation.\n\njakeret (2017): \"Tensorflow Unet\"\n\nU-Net source code from Pattern Recognition and Image Processing at Computer Science Department of the University of Freiburg, Germany.\n\nThe basic articles on the system have been cited 3693, 7049, 442 and 22 times respectively on Google Scholar as of December 24, 2018.\n", "related": "NONE"}
{"id": "56641018", "url": "https://en.wikipedia.org/wiki?curid=56641018", "title": "Deepfake", "text": "Deepfake\n\nDeepfakes (a portmanteau of \"deep learning\" and \"fake\") are synthetic media in which a person in an existing image or video is replaced with someone else's likeness. While the act of faking content is a not new, deepfakes leverage powerful techniques from machine learning and artificial intelligence to manipulate or generate visual and audio content with a high potential to deceive. The main machine learning methods used to create deepfakes are based on deep learning and involve training generative neural network architectures, such as autoencoders or generative adversarial networks (GANs).\n\nDeepfakes have garnered widespread attention for their uses in celebrity pornographic videos, revenge porn, fake news, hoaxes, and financial fraud. This has elicited responses from both industry and government to detect and limit their use.\n\nPhoto manipulation was developed in the 19th century and soon applied to motion pictures. Technology steadily improved during the 20th century, and more quickly with digital video. \n\nDeepfake technology has been developed by researchers at academic institutions beginning in the 1990s, and later by amateurs in online communities. More recently the methods have been adopted by industry.\n\nAcademic research related to deepfakes lies predominantly within the field of computer vision, a subfield of computer science. An early landmark project was the Video Rewrite program, published in 1997, which modified existing video footage of a person speaking to depict that person mouthing the words contained in a different audio track. It was the first system to fully automate this kind of facial reanimation, and it did so using machine learning techniques to make connections between the sounds produced by a video's subject and the shape of the subject's face.\n\nContemporary academic projects have focused on creating more realistic videos and on improving techniques. The “Synthesizing Obama” program, published in 2017, modifies video footage of former president Barack Obama to depict him mouthing the words contained in a separate audio track. The project lists as a main research contribution its photorealistic technique for synthesizing mouth shapes from audio. The Face2Face program, published in 2016, modifies video footage of a person's face to depict them mimicking the facial expressions of another person in real time. The project lists as a main research contribution the first method for re-enacting facial expressions in real time using a camera that does not capture depth, making it possible for the technique to be performed using common consumer cameras.\n\nIn August 2018, researchers at the University of California, Berkeley published a paper introducing a fake dancing app that can create the impression of masterful dancing ability using AI. This project expands the application of deepfakes to the entire body; previous works focused on the head or parts of the face.\n\nThe term deepfakes originated around the end of 2017 from a Reddit user named \"deepfakes\". He, as well as others in the Reddit community r/deepfakes, shared deepfakes they created; many videos involved celebrities’ faces swapped onto the bodies of actresses in pornographic videos, while non-pornographic content included many videos with actor Nicolas Cage’s face swapped into various movies. \n\nOther online communities remain, including Reddit communities that do not share pornography, such as r/SFWdeepfakes (short for \"safe for work deepfakes\"), in which community members share deepfakes depicting celebrities, politicians, and others in non-pornographic scenarios. Other online communities continue to share pornography on platforms that have not banned deepfake pornography.\n\nIn January 2018, a proprietary desktop application called FakeApp was launched. This app allows users to easily create and share videos with their faces swapped with each other. As of 2019, FakeApp has been superseded by open-source alternatives such as Faceswap and the command line-based DeepFaceLab.\n\nLarger companies are also starting to use deepfakes. The mobile app giant Momo created the application Zao which allows users to superimpose their face on TV and movie clips with a single picture. The Japanese AI company DataGrid made a full body deepfake that can create a person from scratch. They intend to use these for fashion and apparel.\n\nAudio deepfakes, and AI software capable of detecting deepfakes and cloning human voices after 5 seconds of listening time also exist.\n\nMobile deepfakes, a mobile deepfake app, Impressions, was launched in March of 2020. It was the first app for the creation of celebrity deepfake videos from mobile phones.\n\nDeepfakes rely on a type of neural network called an autoencoder. These consist of an encoder, which reduces an image to a lower dimensional latent space, and a decoder, which reconstructs the image from the latent representation. Deepfakes utilize this architecture by having a universal encoder which encodes a person in to the latent space. The latent representation contains key features about their facial features and body posture. This can then be decoded with a model trained specifically for the target. This means the target's detailed information will be superimposed on the underlying facial and body features of the original video, represented in the latent space.\n\nA popular upgrade to this architecture attaches a generative adversarial network to the decoder. A GAN trains a generator, in this case the decoder, and a discriminator in an adversarial relationship. The generator creates new images from the latent representation of the source material, while the discriminator attempts to determine whether or not the image is generated. This causes the generator to create images that mimic reality extremely well as any defects would be caught by the discriminator. Both algorithms improve constantly in a zero sum game. This makes deepfakes difficult to combat as they are constantly evolving; any time a defect is determined, it can be corrected.\n\nMany deepfakes on the internet feature pornography of people, often female celebrities whose likeness is typically used without their consent. Deepfake pornography prominently surfaced on the Internet in 2017, particularly on Reddit. The first one that captured attention was the Daisy Ridley deepfake, which was featured in several articles. Other prominent pornographic deepfakes were of various other celebrities. As of October 2019, most of the deepfake subjects on the internet were British and American Actresses. However, around a quarter of the subjects are South Korean, the majority of which are K-pop stars.\n\nIn June 2019, a downloadable Windows and Linux application called DeepNude was released which used neural networks, specifically generative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing $50. On June 27 the creators removed the application and refunded consumers.\n\nDeepfakes have been used to misrepresent well-known politicians in videos. In separate videos, the face of the Argentine President Mauricio Macri has been replaced by the face of Adolf Hitler, and Angela Merkel's face has been replaced with Donald Trump's. In April 2018, Jordan Peele collaborated with Buzzfeed to create a deepfake of Barack Obama with Peele's voice; it served as a public service announcement to increase awareness of deepfakes. In January 2019, Fox affiliate KCPQ aired a deepfake of Trump during his Oval Office address, mocking his appearance and skin color.\n\nIn June 2019, the United States House Intelligence Committee held hearings on the potential malicious use of deepfakes to sway elections.\n\nDuring the 2020 Delhi Legislative Assembly election campaign, the Delhi Bharatiya Janata Party used similar technology to distribute a version of an English-language campaign advertisement by its leader, Manoj Tiwari, translated into Haryanvi to target Haryana voters. A voiceover was provided by an actor, and AI trained using video of Tiwari speeches was used to lip-sync the video to the new voiceover. A party staff member described it as a \"positive\" use of deepfake technology, which allowed them to \"convincingly approach the target audience even if the candidate didn't speak the language of the voter.\"\n\nIn March 2018 the multidisciplinary artist Joseph Ayerle published the videoartwork \"Un'emozione per sempre 2.0\" (English title: \"The Italian Game\"). The artist worked with Deepfake technology to create a synthetic version of 80s moviestar Ornella Muti, travelling in time from 1978 to 2018. The artist used Ornella Muti's time travel to explore generational reflections, while also investigating questions about the role of provocation in the world of art.\n\nThere has been speculation about deepfakes being used for creating digital actors for future films. Digitally constructed/altered humans have already been used in films before, and deepfakes could contribute new developments in the near future. Amateur deepfake technology has already been used to insert faces into existing films, such as the insertion of Harrison Ford's young face onto Han Solo's face in \"\", and techniques similar to those used by deepfakes were used for the acting of Princess Leia in \"Rogue One.\"\n\nDeepfakes have begun to see use in popular social media platforms, notably through Zao, a Chinese deepfake app that allows users to substitute their own faces onto those of characters in scenes from films and television shows such as \"Romeo + Juliet\" and \"Game of Thrones\". The app originally faced scrutiny over its invasive user data and privacy policy, after which the company put out a statement claiming it would revise the policy. in January 2020 Facebook announced that it was introducing new measures to counter this on its platforms. \n\nAudio deepfakes have been used as part of social engineering scams, fooling people into thinking they are receiving instructions from a trusted individual. In 2019, a U.K.-based energy firm's CEO was scammed over the phone when he was ordered to transfer €220,000 into a Hungarian bank account by an individual who used audio deepfake technology to impersonate the voice of the firm's parent company's chief executive. The perpetrator reportedly called three times and requested a second payment but was turned down when the CEO realized the phone number of the caller was Austrian and that the money was not being reimbursed as he was told it would be.\n\nThough fake photos have long been plentiful, faking motion pictures has been more difficult, and the presence of deepfakes increases the difficulty of classifying videos as genuine or not. AI researcher Alex Champandard has said people should know how fast things can be corrupted with deepfake technology, and that the problem is not a technical one, but rather one to be solved by trust in information and journalism. The primary pitfall is that humanity could fall into an age in which it can no longer be determined whether a medium's content corresponds to the truth.\n\nSimilarly, computer science associate professor Hao Li of the University of Southern California states that deepfakes created for malicious use, such as fake news, will be even more harmful if nothing is done to spread awareness of deepfake technology. Li predicts that genuine videos and deepfakes will become indistinguishable in as soon as half a year, as of October 2019, due to rapid advancement in artificial intelligence and computer graphics.\n\nMost of the academic research surrounding Deepfake seeks to detect the videos. The most popular technique is to use algorithms similar to the ones used to build the deepfake to detect them. By recognizing patterns in how Deepfakes are created the algorithm is able to pick up subtle inconsistencies. Researchers have developed automatic systems that examine videos for errors such as irregular blinking patterns of lighting. This technique has also been criticized for creating a \"Moving Goal post\" where anytime the algorithms for detecting get better, so do the Deepfakes. The Deepfake Detection Challenge, hosted by a coalition of leading tech companies, hope to accelerate the technology for identifying manipulated content.\n\nOther techniques use Blockchain to verify the source of the media. Videos will have to be verified through the ledger before they are shown on social media platforms. With this technology, only videos from trusted sources would be approved, decreasing the spread of possibly harmful Deepfake media.\n\nSince 2017, Samantha Cole of Vice published a series of articles covering news surrounding deepfake pornography. On January 31st, 2018, Gfycat began removing all deepfakes from its site. On Reddit, the r/deepfakes subreddit was banned on February 7, 2018, due to the policy violation of \"involuntary pornography\". In the same month, representatives from Twitter stated that they would suspend accounts suspected of posting non-consensual deepfake content. Chat site Discord has taken action against deepfakes in the past, and has taken a general stance against deepfakes.. In September 2018, Google added \"involuntary synthetic pornographic imagery” to its ban list, allowing anyone to request the block of results showing their fake nudes.\n\nIn February 2018, Pornhub said that it would ban deepfake videos on its website because it is considered “non consensual content” which violates their terms of service. They also stated previously to Mashable that they will take down content flagged as deepfakes. Writers from Motherboard from Buzzfeed News reported that searching “deepfakes” on Pornhub still returned multiple recent deepfake videos. \n\nFacebook has previously stated that they would not remove deepfakes from their platforms. The videos will instead be flagged as fake by third-parties and then have a lessened priority in user's feeds. This response was prompted in June 2019 after a deepfake featuring a 2016 video of Mark Zuckerberg circulated on Facebook and Instagram.\n\nIn the United States, there have been some responses to the problems posed by deepfakes. In 2018, the Malicious Deep Fake Prohibition Act was introduced to the US Senate, and in 2019 the DEEPFAKES Accountability Act was introduced in the House of Representatives. Several states have also introduced legislation regarding deepfakes, including Virginia, Texas, California, and New York. On October 3, 2019, California governor Gavin Newsom signed into law Assembly Bills No. 602 and No. 730. Assembly Bill No. 602 provides individuals targeted by sexually explicit deepfake content made without their consent with a cause of action against the content's creator. Assembly Bill No. 730 prohibits the distribution of malicious deepfake audio or visual media targeting a candidate running for public office within 60 days of their election.\n\nIn November 2019 China announced that deepfakes and other synthetically faked footage should bear a clear notice about its fakeness starting in 2020. Failure to comply could be considered a crime the Cyberspace Administration of China stated on its website. The Chinese government seems to be reserving the right to prosecute both users and online video platforms failing to abide by the rules.\n\nIn the United Kingdom, producers of deepfake material can be prosecuted for harassment, but there are calls to make deepfake a specific crime; in the United States, where charges as varied as identity theft, cyberstalking, and revenge porn have been pursued, the notion of a more comprehensive statute has also been discussed.\n\nIn Canada, the Communications Security Establishment released a report which said that deepfakes could be used to interfere in Canadian politics, particularly to discredit politicians and influence voters. There are multiple ways for citizens in Canada to deal with deepfakes if they are targeted by them.\n\n- \"Picaper\" by Jack Wodhams. The 1986 Mid-December issue of \"Analog\" magazine published the novelette \"Picaper\" by Jack Wodhams. Its plot revolves around digitally enhanced or digitally generated videos produced by skilled hackers serving unscrupulous lawyers and political figures.\n- A Philosophical Investigation. In the 1992 techno-thriller \"A Philosophical Investigation\" by Philip Kerr, \"Wittgenstein\", the main character and a serial killer, makes use of both a software similar to Deepfake and a virtual reality suit for having sex with an avatar of the female police lieutenant Isadora \"Jake\" Jakowicz assigned to catch him.\n- Rising Sun. The 1993 film \"Rising Sun\" starring Sean Connery and Wesley Snipes depicts another character, Jingo Asakuma, who reveals that a computer disc has digitally altered personal identities to implicate a competitor.\n- The Capture. Deepfake technology is part of the plot of the 2019 BBC One drama \"The Capture\". The series follows British ex-soldier Shaun Emery, who is accused of assaulting and abducting his barrister. Expertly doctored CCTV footage is used to set him up and mislead the police investigating him.\n\n", "related": "\n- Synthetic media\n\n- Deepfake Detection Challenge (DFDC)\n"}
{"id": "57410739", "url": "https://en.wikipedia.org/wiki?curid=57410739", "title": "SqueezeNet", "text": "SqueezeNet\n\nSqueezeNet is the name of a deep neural network for computer vision that was released in 2016. SqueezeNet was developed by researchers at DeepScale, University of California, Berkeley, and Stanford University. In designing SqueezeNet, the authors' goal was to create a smaller neural network with fewer parameters that can more easily fit into computer memory and can more easily be transmitted over a computer network.\n\nSqueezeNet was originally released on February 22, 2016. This original version of SqueezeNet was implemented on top of the Caffe deep learning software framework. Shortly thereafter, the open-source research community ported SqueezeNet to a number of other deep learning frameworks. On February 26, 2016, Eddie Bell released a port of SqueezeNet for the Chainer deep learning framework. On March 2, 2016, Guo Haria released a port of SqueezeNet for the Apache MXNet framework. On June 3, 2016, Tammy Yang released a port of SqueezeNet for the Keras framework. In 2017, companies including Baidu, Xilinx, Imagination Technologies, and Synopsys demonstrated SqueezeNet running on low-power processing platforms such as smartphones, FPGAs, and custom processors.\n\nAs of 2018, SqueezeNet ships \"natively\" as part of the source code of a number of deep learning frameworks such as PyTorch, Apache MXNet, and Apple CoreML. In addition, 3rd party developers have created implementations of SqueezeNet that are compatible with frameworks such as TensorFlow. Below is a summary of frameworks that support SqueezeNet.\n\nSqueezeNet was originally described in a paper entitled \"SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size.\" AlexNet is a deep neural network that has 240MB of parameters, and SqueezeNet has just 5MB of parameters. However, it's important to note that SqueezeNet is not a \"squeezed version of AlexNet.\" Rather, SqueezeNet is an entirely different DNN architecture than AlexNet. What SqueezeNet and AlexNet have in common is that both of them achieve approximately the same level of accuracy when evaluated on the ImageNet image classification validation dataset.\n\nModel compression (e.g. quantization and pruning of model parameters) can be applied to a deep neural network after it has been trained. In the SqueezeNet paper, the authors demonstrated that a model compression technique called Deep Compression can be applied to SqueezeNet to further reduce the size of the parameter file from 5MB to 500KB. Deep Compression has also been applied to other DNNs such as AlexNet and VGG.\n\nSome of the members of the original SqueezeNet team have continued to develop resource-efficient deep neural networks for a variety of applications. A few of these works are noted in the following table. As with the original SqueezeNet model, the open-source research community has ported and adapted these newer \"squeeze\"-family models for compatibility with multiple deep learning frameworks.\nIn addition, the open-source research community has extended SqueezeNet to other applications, including semantic segmentation of images and style transfer.\n", "related": "NONE"}
{"id": "360030", "url": "https://en.wikipedia.org/wiki?curid=360030", "title": "Question answering", "text": "Question answering\n\nQuestion answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.\n\nA question answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question answering systems can pull answers from an unstructured collection of natural language documents.\n\nSome examples of natural language document collections used for question answering systems include:\n\n- a local collection of reference texts\n- internal organization documents and web pages\n- compiled newswire reports\n- a set of Wikipedia pages\n- a subset of World Wide Web pages\n\nQuestion answering research attempts to deal with a wide range of question types including: fact, list, definition, \"How\", \"Why\", hypothetical, semantically constrained, and cross-lingual questions.\n\n- \"Closed-domain\" question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, \"closed-domain\" might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. Question answering systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease\n- \"Open-domain\" question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.\n\nTwo early question answering systems were BASEBALL and LUNAR. BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both question answering systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.\n\nSHRDLU was a highly successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the \"blocks world\"), and it offered the possibility of asking the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\n\nIn the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus.\n\nThe 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\n\nSpecialized natural language question answering systems have been developed, such as EAGLi for health and life scientists, and Wolfram Alpha, an online computational knowledge engine that answers factual queries directly by computing the answer from externally sourced curated data.\n\nAs of 2001, question answering systems typically included a \"question classifier\" module that determines the type of question and the type of answer. A \"multiagent\" question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).\n\nQuestion answering is very dependent on a good search corpus—for without documents containing the answer, there is little any question answering system can do. It thus makes sense that larger collection sizes generally lend well to better question answering performance, unless the question domain is orthogonal to the collection. The notion of data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, leading to two benefits:\n\n1. By having the right information appear in many forms, the burden on the question answering system to perform complex NLP techniques to understand the text is lessened.\n2. Correct answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones.\n\nSome question answering systems rely heavily on automated reasoning. There are a number of question answering systems designed in Prolog, a logic programming language associated with artificial intelligence.\n\nIn information retrieval, an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from computational linguistics, information retrieval and knowledge representation for finding answers.\n\nThe system takes a natural language question as an input rather than a set of keywords, for example, \"When is the national day of China?\" The sentence is then transformed into a query through its logical form. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\n\nKeyword extraction is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. \"Who\", \"Where\" or \"How many\", these words tell the system that the answers should be of type \"Person\", \"Location\", \"Number\" respectively. In the example above, the word \"When\" indicates that the answer should be of type \"Date\". POS (part-of-speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is \"Chinese National Day\", the predicate is \"is\" and the adverbial modifier is \"when\", therefore the answer type is \"Date\". Unfortunately, some interrogative words like \"Which\", \"What\" or \"How\" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context.\n\nOnce the question type has been identified, an information retrieval system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as \"Who\" or \"Where\", a named-entity recogniser is used to find relevant \"Person\" and \"Location\" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.\n\nA vector space model can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. An inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is \"1st Oct.\"\n\nAn open source math-aware question answering system based on Ask Platypus and Wikidata was published in 2018. The system takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as succinct answer. The resulting formula is translated into a computable form, allowing the user to insert values for the variables. Names and values of variables and common constants are retrieved from Wikidata if available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set.\n\nQuestion answering systems have been extended in recent years to encompass additional domains of knowledge For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current question answering research topics include:\n\n- interactivity—clarification of questions or answers\n- answer reuse or caching\n- semantic parsing\n- answer presentation\n- knowledge representation and reasoning\n- social media analysis with question answering systems\n- sentiment analysis\n- utilization of thematic roles\n- semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts\n- utilization of linguistic resources, such as WordNet, FrameNet, and the similar\n- Image captioning for visual question answering\n\nIBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.\n\nFacebook Research has made their DrQA system available under an open source license. This system has been used for open domain question answering using Wikipedia as knowledge source.\n\n- Dragomir R. Radev, John Prager, and Valerie Samn. Ranking suspected answers to natural language questions using predictive annotation. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.\n- John Prager, Eric Brown, Anni Coden, and Dragomir Radev. Question-answering by predictive annotation. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.\n- L. Fortnow, Steve Homer (2002/2003). A Short History of Computational Complexity. In D. van Dalen, J. Dawson, and A. Kanamori, editors, \"The History of Mathematical Logic\". North-Holland, Amsterdam.\n\n- Question Answering Evaluation at NTCIR\n- Question Answering Evaluation at TREC\n- Question Answering Evaluation at CLEF\n- Quiz Question Answers\n- Online Question Answering System\n", "related": "NONE"}
{"id": "53194499", "url": "https://en.wikipedia.org/wiki?curid=53194499", "title": "Chainer", "text": "Chainer\n\nChainer is an open source deep learning framework written purely in Python on top of Numpy and CuPy Python libraries. The development is led by Japanese venture company Preferred Networks in partnership with IBM, Intel, Microsoft, and Nvidia.\n\nChainer is notable for its early adoption of \"define-by-run\" scheme, as well as its performance on large scale systems. The first version was released in June 2015 and has gained large popularity in Japan since then. Furthermore, in 2017, it was listed by KDnuggets in top 10 open source machine learning Python projects.\n\nIn December 2019, Preferred Networks announced the transition of its development effort from Chainer to PyTorch and it will only provide maintenance patches after releasing v7.\n\nChainer was the first deep learning framework to introduce the define-by-run approach. The traditional procedure to train a network was in two phases: define the fixed connections between mathematical operations (such as matrix multiplication and nonlinear activations) in the network, and then run the actual training calculation. This is called the define-and-run or static-graph approach. Theano and TensorFlow are among the notable frameworks that took this approach. In contrast, in the define-by-run or dynamic-graph approach, the connection in a network is not determined when the training is started. The network is determined during the training as the actual calculation is performed.\n\nOne of the advantages of this approach is that it's intuitive and flexible. If the network has complicated control flows such as conditionals and loops, in the define-and-run approach, specially designed operations for such constructs are needed. On the other hand, in the define-by-run approach, programming language's native constructs such as if statements and for loops can be used to describe such flow. This flexibility is especially useful to implement recurrent neural networks.\n\nAnother advantage is ease of debugging. In the define-and-run approach, if an error (such as numeric error) has occurred in the training calculation, it's often difficult to inspect the fault, because the code written to define the network and the actual place of the error are separated. In the define-by-run approach, you can just suspend the calculation with the language's built-in debugger and inspect the data that flows on your code of the network.\n\nDefine-by-run has gained popularity since the introduction by Chainer and is now implemented in many other frameworks, including PyTorch and TensorFlow.\n\nChainer has four extension libraries, ChainerMN, ChainerRL, ChainerCV and ChainerUI. ChainerMN enables Chainer to be used on multiple GPUs with performance significantly faster than other deep learning frameworks. A supercomputer running Chainer on 1024 GPUs processed 90 epochs of ImageNet dataset on ResNet-50 network in 15 minutes, which is four times faster than the previous record held by Facebook. ChainerRL adds state of art deep reinforcement learning algorithms, and ChainerUI is a management and visualization tool.\n\nChainer is used as the framework for PaintsChainer, a service which does automatic colorization of black and white, line only, draft drawings with minimal user input.\n\n", "related": "\n- Comparison of deep learning software\n- Machine learning\n- Artificial neural network\n"}
{"id": "57741272", "url": "https://en.wikipedia.org/wiki?curid=57741272", "title": "Rnn (software)", "text": "Rnn (software)\n\nrnn is an open-source machine learning framework that implements recurrent neural network architectures, such as LSTM and GRU, natively in the R programming language, that has been downloaded over 50,000 times (from the RStudio servers alone).\n\nThe rnn package is distributed through the Comprehensive R Archive Network under the open-source GPL v3 license.\n\nThe below example from the rnn documentation show how to train a recurrent neural network to solve the problem of bit-by-bit binary addition.\n\nThe sigmoid functions and derivatives used in the package were originally included in the package, from version 0.8.0 onwards, these were released in a separate R package sigmoid, with the intention to enable more general use. The sigmoid package is a dependency of the rnn package and therefore automatically installed with it.\n\nWith the release of version 0.3.0 in April 2016 the use in production and research environments became more widespread. The package was reviewed several months later on the R blog The Beginner Programmer as \"R provides a simple and very user friendly package named rnn for working with recurrent neural networks.\", which further increased usage.\n\nThe book Neural Networks in R by Balaji Venkateswaran and Giuseppe Ciaburro uses rnn to demonstrate recurrent neural networks to R users. It is also used in the r-exercises.com course \"Neural network exercises\".\n\nThe RStudio CRAN mirror download logs\n, with a total of over 30,000 downloads since the first release, according to RDocumentation.org, this puts the package in the 15th percentile of most popular R packages\n\n- Repository on GitHub\n- rnn package on CRAN\n", "related": "NONE"}
{"id": "56785877", "url": "https://en.wikipedia.org/wiki?curid=56785877", "title": "Deep Learning Studio", "text": "Deep Learning Studio\n\nDeep Learning Studio is a software tool that aims to simplify the creation of deep learning models used in artificial intelligence. It is compatible with a number of open-source programming frameworks popularly used in artificial neural networks, including MXNet and Google's TensorFlow.\n\nPrior to the release of Deep Learning Studio in January 2017, proficiency in Python, among other programming languages, was essential in developing effective deep learning models. Deep Learning Studio sought to simplify the model creation process through a visual, drag-and-drop interface and the application of pre-trained learning models on available data.\n\nIrving, TX-based Deep Cognition Inc. is the developer behind Deep Learning Studio. In 2017, the software allowed Deep Cognition to become a finalist for Best Innovation in Deep Learning in the Alconics Awards, which are given annually to the best artificial intelligence software.\n\nDeep Cognition launched version 2.0 of Deep Learning Studio at NVIDIA's GTC 2018 Conference in San Jose, CA.\n\nFremont, CA-based computing products supplier Exxact Corp provides desktop computers specifically built to handle Deep Learning Studio workloads.\n\nDeep Learning Studio is available in two versions: Desktop and Cloud, both of which are free software. The Desktop version is available on Windows and Ubuntu. The Cloud version is available in single-user and multi-user configurations. A Deep Cognition account is needed to access the Cloud version. Account registration is free.\n\nDeep Learning Studio can import existing Keras models; it also takes a data set as an input.\n\nDeep Learning Studio's AutoML feature allows automatic generation of deep learning models. More advanced users may choose to generate their own models using various types of layers and neural networks.\n\nDeep Learning Studio also has a library of loss functions and optimizers for use in hyperparameter tuning, a traditionally complicated area in neural network programming.\n\nGenerated models can be trained using either CPUs or GPUs. Trained models can then be used for predictive analytics.\n\nDeep Learning Studio has been cited for being a user-friendly deep learning tool.\n\n", "related": "\n- Artificial intelligence\n- Artificial neural network\n- Data mining\n- Deep learning\n- Machine learning\n- Predictive analytics\n\n- Official website\n- Official blog\n"}
{"id": "58469254", "url": "https://en.wikipedia.org/wiki?curid=58469254", "title": "CLEVER score", "text": "CLEVER score\n\nThe CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) score is a way of measuring the robustness of an artificial neural network towards adversarial attacks.\nIt was developed by a team at the MIT-IBM Watson AI Lab and first presented at the 2018 International Conference on Learning Representations.\n", "related": "NONE"}
{"id": "58497153", "url": "https://en.wikipedia.org/wiki?curid=58497153", "title": "Datasets.load", "text": "Datasets.load\n\ndatasets.load is an R package and RStudio plugin, that provides both a Graphical User Interface (GUI), as well as a Command Line Interface for loading datasets. Normally, R only makes visible datasets of packages that are loaded, datasets.load shows the list of all installed datasets on the local library, including datasets included with packages that are not loaded. It is one of the top 10% of most downloaded R packages.\n\nR functionality is extendible using extensions call R packages. The central place from which to install packages is the Central R Archive network (CRAN). From CRAN, R packages can be installed using the command (here to install datasets.load):\nOnce installed, R packages can be loaded using the command:\nAfter a package has been loaded, objects available from the package - such as functions and datasets - can be accessed.\n\nThe datasets from all the loaded packages can listed using the command:\nHowever, datasets from packages that are not loaded, are not listed. As a result, many R users have access to datasets on their local install that are never used. The datasets.load packages addresses this by listed all datasets that are in any packages installed (loaded or not loaded).\n\nA video demonstrating the GUI usage is available via the thumbnail on the right and on YouTube.\n\nThe usage of datasets.load's Command Line Interface is demonstrated in the code snippet below.\n\nThe basic functionality of datasets.load is to expose all installed datasets to the user, including datasets in packages that are not loaded. There is a Command Line Interface (CLI) which can be used from any R terminal.\n\nIn addition to the CLI, there is also a Graphical User Interface for RStudio using RStudio Addins.\n\nThe initial release on CRAN of version 0.1.0 took place in December 2016, and averaged a download rate of 1,000 times per month, from the RStudio servers alone. With the release of version 0.3.0 in 2018, the download rate increased to 2,000 times per month, putting the package in the 9th percentile of most popular R packages. The package was reviewed in the 2017 article \"R Packages worth a look\" on Data Analytics & R, which further increased usage. It is also frequently preinstalled on university computers, in order to help make R more accessible to students. \n\nVersion 1.0.0 was released on 12 December 2019. \n\nThe RStudio CRAN mirror download logs\n, with a total of over 55,000 downloads since the first release\n, according to RDocumentation.org, this puts the package in the 9th percentile of most popular R packages. On Rdocumentation.org it is listed as the second most downloaded package under the keyword datasets (after the base R package datasets), which is the most popular keyword.\n\n- Repository on GitHub\n- datasets.load package on CRAN\n", "related": "NONE"}
{"id": "57388949", "url": "https://en.wikipedia.org/wiki?curid=57388949", "title": "ML.NET", "text": "ML.NET\n\nML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.\n\nML.NET brings model-based Machine Learning analytic and prediction capabilities to existing .NET developers. The framework is built upon .NET Core and .NET Standard inheriting the ability to run cross-platform on Linux, Windows and macOS. Although the ML.NET framework is new, its origins began in 2002 as a Microsoft Research project named TMSN (text mining search and navigation) for use internally within Microsoft products. It was later renamed to TLC (the learning code) around 2011. ML.NET was derived from the TLC library and has largely surpassed its parent says Dr. James McCaffrey, Microsoft Research.\n\nDevelopers can train a Machine Learning Model or reuse an existing Model by a 3rd party and run it on any environment offline. This means developers do not need to have a background in Data Science to use the framework. Support for the open-source Open Neural Network Exchange (ONNX) Deep Learning model format was introduced from build 0.3 in ML.NET. The release included other notable enhancements such as Factorization Machines, LightGBM, Ensembles, LightLDA transform and OVA. The ML.NET integration of TensorFlow is enabled from the 0.5 release. Support for x86 & x64 applications was added to build 0.7 including enhanced recommendation capabilities with Matrix Factorization. A full roadmap of planned features have been made available on the official GitHub repo.\n\nThe first stable 1.0 release of the framework was announced at Build (developer conference) 2019. It included the addition of a Model Builder tool and AutoML (Automated Machine Learning) capabilities. Build 1.3.1 introduced a preview of Deep Neural Network training using C# bindings for Tensorflow and a Database loader which enables model training on databases. The 1.4.0 preview added ML.NET scoring on ARM processors and Deep Neural Network training with GPU's for Windows and Linux. \n\nMicrosoft's paper on machine learning with ML.NET demonstrated it is capable of training sentiment analysis models using large datasets while achieving high accuracy. It's results showed 95% accuracy on Amazon's 9GB review dataset. \n\nThe ML.NET CLI is a Command-line interface which uses ML.NET AutoML to perform model training and pick the best algorithm for the data. The ML.NET Model Builder preview is an extension for Visual Studio that uses ML.NET CLI and ML.NET AutoML to output the best ML.NET Model using a GUI.\n\nAI fairness and explainability has been an area of debate for AI Ethicists in recent years. A major issue for Machine Learning applications is the black box effect where end users and the developers of an application are unsure of how an algorithm came to a decision or whether the dataset contains bias. Build 0.8 included model explainability API's that had been used internally in Microsoft. It added the capability to understand the feature importance of models with the addition of 'Overall Feature Importance' and 'Generalized Additive Models'.\n\nWhen there are several variables that contribute to the overall score, it is possible to see a breakdown of each variable and which features had the most impact on the final score. The official documentation demonstrates that the scoring metrics can be output for debugging purposes. During training & debugging of a model, developers can preview and inspect live filtered data. This is possible using the Visual Studio DataView tools.\n\nMicrosoft Research announced the popular Infer.NET model-based machine learning framework used for research in academic institutions since 2008 has been released open source and is now part of the ML.NET framework. The Infer.NET framework utilises probabilistic programming to describe probabilistic models which has the added advantage of interpretability. The Infer.NET namespace has since been changed to Microsoft.ML.Probabilistic consistent with ML.NET namespaces.\n\nMicrosoft acknowledged that the Python programming language is popular with Data Scientists, so it has introduced NimbusML the experimental Python bindings for ML.NET. This enables users to train and use machine learning models in Python. It was made open source similar to Infer.NET.\n\nML.NET allows users to export trained models to the Open Neural Network Exchange (Onnx) format. This establishes an opportunity to use models in different environments that don't use ML.NET. It would be possible to run these models in the client side of a browser using ONNX.js, a javascript client-side framework for deep learning models created in the Onnx format.\n\nAlong with the rollout of the ML.NET preview, Microsoft rolled out free AI tutorials and courses to help developers understand techniques needed to work with the framework.\n\n", "related": "\n- Scikit-learn\n- Accord.NET\n- Infer.NET\n- NimbusML\n- LightGBM\n- TensorFlow\n- Microsoft Cognitive Toolkit\n- List of numerical analysis software\n- List of numerical libraries for .NET framework\n\n- GitHub repo:\n- GitHub repo:\n"}
{"id": "59067677", "url": "https://en.wikipedia.org/wiki?curid=59067677", "title": "MLOps", "text": "MLOps\n\nMLOps (a compound of “machine learning” and “operations”) is a practice for collaboration and communication between data scientists and operations professionals to help manage production ML (or deep learning) lifecycle. Similar to the DevOps or DataOps approaches, MLOps looks to increase automation and improve the quality of production ML while also focusing on business and regulatory requirements. While MLOps also started as a set of best practices, it is slowly evolving into an independent approach to ML lifecycle management. MLOps applies to the entire lifecycle - from integrating with model generation (software development lifecycle, continuous integration/continuous delivery), orchestration, and deployment, to health, diagnostics, governance, and business metrics.\n\nThe challenges of the ongoing use of machine learning in applications were highlighted in a 2015 paper titled, Hidden Technical Debt in Machine Learning Systems. \n\nThe predicted growth in machine learning includes an estimated doubling of ML pilots and implementations from 2017 to 2018, and again from 2018 to 2020. Spending on machine learning is estimated to reach $57.6 billion by 2021, a compound annual growth rate (CAGR) of 50.1%.\n\nReports show a majority (up to 88%) of corporate AI initiatives are struggling to move beyond test stages. However, those organizations that actually put AI and machine learning into production saw a 3-15% profit margin increases. \n\nIn 2018, MLOps and approaches to it began to gain traction among AI/ML experts, companies, and technology journalists as a solution that can address the complexity and growth of machine learning in businesses. \n\nThere are a number of barriers that prevent organizations from successfully implementing ML across the enterprise, including difficulties with:\n- Deployment and automation\n- Reproducibility of models and predictions\n- Diagnostics\n- Governance and regulatory compliance\n- Scalability\n- Collaboration\n- Business uses\n\nA standard practice, such as MLOps, takes into account each of the aforementioned areas, which can help enterprises optimize workflows and avoid issues during implementation. \n\nA common architecture of an MLOps system would include data science platforms where models are constructed and the analytical engines were computations are performed, with the MLOps tool orchestrating the movement of machine learning models, data and outcomes between the systems.\n\n", "related": "\n- AIOps\n"}
{"id": "59892172", "url": "https://en.wikipedia.org/wiki?curid=59892172", "title": "Neural Style Transfer", "text": "Neural Style Transfer\n\nNeural Style Transfer (NST) refers to a class of software algorithms that manipulate digital images, or videos, to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks in order to perform the image transformation. Common uses for NST are the creation of artificial artwork from photographs, for example by transferring the appearance of famous paintings to user supplied photographs. Several notable mobile apps use NST techniques for this purpose, including DeepArt and Prisma.\n\nNST is an example of image stylization, a problem studied for over two decades within the field of non-photorealistic rendering. Prior to NST, the transfer of image style was performed using machine learning techniques based on image analogy. Given a training pair of images–a photo and an artwork depicting that photo–a transformation could be learned and then applied to create a new artwork from a new photo, by analogy. The drawback of this method is that such a training pair rarely exists in practice. For example, original source material (photos) are rarely available for famous artworks.\n\nNST requires no such pairing; only a single example of artwork is needed for the algorithm to transfer its style.\n\nNST was first published in the paper \"A Neural Algorithm of Artistic Style\" by Gatys et al., originally released to ArXiv 2015, and subsequently accepted by the peer-reviewed Computer Vision and Pattern Recognition (CVPR) in 2016.\n\nThe core innovation of NST is the use of deep learning to disentangle the representation of the content (structure) of an image, from the appearance (style) in which it is depicted. The original paper used a convolutional neural network (CNN) VGG-19 architecture that has been pre-trained to perform object recognition using the ImageNet dataset.\n\nThe process of NST assumes an input image formula_1 and an example style image formula_2.\n\nThe image formula_1 is fed through the CNN, and network activations are sampled at a late convolution layer of the VGG-19 architecture. Let formula_4 be the resulting output sample, called the 'content' of the input formula_1.\n\nThe style image formula_2 is then fed through the same CNN, and network activations are sampled at the early to middle layers of the CNN. These activations are encoded into a Gramian matrix representation, call it formula_7 to denote the 'style' of formula_2.\n\nThe goal of NST is to synthesize an output image formula_9 that exhibits the content of formula_1 applied with the style of formula_2, i.e. formula_12 and formula_13.\n\nAn iterative optimization (usually gradient descent) then gradually updates formula_9 to minimize the loss function error:\n\nformula_15,\n\nwhere formula_16 is the L2 distance. The constant formula_17 controls the level of the stylization effect.\n\nImage formula_9 is initially approximated by adding a small amount of white noise to input image formula_1 and feeding it through the CNN. Then we successively backpropagate this loss through the network with the CNN weights fixed in order to update the pixels of formula_9. After several thousand epochs of training, an formula_9 (hopefully) emerges that matches the style of formula_2 and the content of formula_1.\n\nAlgorithms are typically implemented for GPUs, so that training takes a few minutes.\n\nNST has also been extended to videos.\n\nSubsequent work improved the speed of NST for images.\n\nIn a paper by Fei-Fei Li et al. adopted a different regularized loss metric and accelerated method for training to produce results in real time (three times faster than Gatys). Their idea was to use not the \"pixel-based loss\" defined above but rather a 'perceptual loss' measuring the differences between higher level layers within the CNN. They used a symmetric encoder-decoder CNN. Training uses a similar loss function to the basic NST method but also regularizes the output for smoothness using a total variation (TV) loss. Once trained, the network may be used to transform an image into the style used during training, using a single feed-forward pass of the network. However the network is restricted to the single style in which it has been trained.\n\nIn a work by Chen Dongdong et al. they explored the fusion of optical flow information into feedforward networks in order to improve the temporal coherence of the output.\n\nMost recently, feature transform based NST methods have been explored for fast stylization that are not coupled to single specific style and enable user-controllable \"blending\" of styles, for example the Whitening and Coloring Transform (WCT).\n", "related": "NONE"}
{"id": "59982100", "url": "https://en.wikipedia.org/wiki?curid=59982100", "title": "StyleGAN", "text": "StyleGAN\n\nStyleGAN is a novel generative adversarial network (GAN) introduced by Nvidia researchers in December 2018, and open sourced in February 2019. \n\nStyleGAN depends on Nvidia's CUDA software, GPUs and on TensorFlow.\n\nThe website This Person Does Not Exist showcases fully automated human image synthesis by endlessly generating images that look like facial portraits of human faces. The website was published in February 2019 by Phillip Wang. The technology has drawn comparison with deep fakes and its potential usage for sinister purposes has been bruited.\n\n- The original 2018 Nvidia StyleGAN paper 'A Style-Based Generator Architecture for Generative Adversarial Networks' at arXiv.org\n- StyleGAN code at GitHub.com\n- This Person Does Not Exist\n- Face editing with Generative Adversarial Networks - youtube\n", "related": "NONE"}
{"id": "60105148", "url": "https://en.wikipedia.org/wiki?curid=60105148", "title": "Deep reinforcement learning", "text": "Deep reinforcement learning\n\nDeep reinforcement learning (DRL) uses deep learning and reinforcement learning principles to create efficient algorithms applied on areas like robotics, video games, NLP (computer science), computer vision, education, transportation, finance and healthcare. Implementing deep learning architectures (deep neural networks) with reinforcement learning algorithms (Q-learning, actor critic, etc.) is capable of scaling to previously unsolvable problems. That is because DRL is able to learn from raw sensors or image signals as input. A remarkable milestone in DQN is that agent uses end-to-end reinforcement learning with convolutional neural networks for playing ATARI games. \n", "related": "NONE"}
{"id": "61004524", "url": "https://en.wikipedia.org/wiki?curid=61004524", "title": "Amazon SageMaker", "text": "Amazon SageMaker\n\nAmazon SageMaker is a cloud machine-learning platform that was launched in November 2017. SageMaker enables developers to create, train, and deploy machine-learning (ML) models in the cloud. SageMaker also enables developers to deploy ML models on embedded systems and edge-devices.\n\nSageMaker enables developers to operate at a number of levels of abstraction when training and deploying machine learning models. At its highest level of abstraction, SageMaker provides pre-trained ML models that can be deployed as-is. In addition, SageMaker provides a number of built-in ML algorithms that developers can train on their own data. Further, SageMaker provides managed instances of TensorFlow and Apache MXNet, where developers can create their own ML algorithms from scratch. Regardless of which level of abstraction is used, a developer can connect their SageMaker-enabled ML models to other AWS services, such as the Amazon DynamoDB database for structured data storage, AWS Batch for offline batch processing, or Amazon Kinesis for real-time processing.\n\nA number of interfaces are available for developers to interact with SageMaker. First, there is a web API that remotely controls a SageMaker server instance. While the web API is agnostic to the programming language used by the developer, Amazon provides SageMaker API bindings for a number of languages, including Python, JavaScript, Ruby, and Java. In addition, SageMaker provides managed Jupyter Notebook instances for interactively programming SageMaker and other applications.\n\n- 2017-11-29: SageMaker is launched at the AWS re:Invent conference.\n- 2018-02-27: Managed TensorFlow and MXNet deep neural network training and inference are now supported within SageMaker.\n- 2018-02-28: SageMaker automatically scales model inference to multiple server instances.\n- 2018-07-13: SageMaker adds support for recurrent neural network training, word2vec training, multi-class linear learner training, and distributed deep neural network training in Chainer with Layer-wise Adaptive Rate Scaling (LARS).\n- 2018-07-17: AWS Batch Transform enables high-throughput non-realtime machine learning inference in SageMaker.\n- 2018-11-08: Support for training and inference of Object2Vec word embeddings.\n- 2018-11-27: SageMaker Ground Truth \"makes it much easier for developers to label their data using human annotators through Mechanical Turk, third party vendors, or their own employees.\"\n- 2018-11-28: SageMaker Reinforcement Learning (RL) \"enables developers and data scientists to quickly and easily develop reinforcement learning models at scale.\"\n- 2018-11-28: SageMaker Neo enables deep neural network models to be deployed from SageMaker to edge-devices such as smartphones and smart cameras.\n- 2018-11-29: The AWS Marketplace for SageMaker is launched. The AWS Marketplace enables 3rd-party developers to buy and sell machine learning models that can be trained and deployed in SageMaker.\n- 2019-01-27: SageMaker Neo is released as open-source software.\n\n- NASCAR is using SageMaker to train deep neural networks on 70 years of video data.\n- Carsales.com uses SageMaker to train and deploy machine learning models to analyze and approve automotive classified ad listings.\n- Avis Budget Group and Slalom Consulting are using SageMaker to develop \"a practical on-site solution that could address the over- and under-utilization of cars in real-time using an optimization engine built in Amazon SageMaker.\"\n- Volkswagen Group uses SageMaker to develop and deploy machine learning in its manufacturing plants.\n- Peak and Footasylum use SageMaker in a recommendation engine for footwear.\n\nIn 2019, CIOL named SageMaker one of the \"5 Best Machine Learning Platforms For Developers,\" alongside IBM Watson, Microsoft Azure Machine Learning, Apache PredictionIO, and ai-one.\n\n", "related": "\n- Amazon Web Services\n- Amazon Lex\n- Amazon Polly\n- Amazon Rekognition\n- Amazon Mechanical Turk\n- Timeline of Amazon Web Services\n"}
{"id": "54294875", "url": "https://en.wikipedia.org/wiki?curid=54294875", "title": "FaceApp", "text": "FaceApp\n\nFaceApp is a mobile application for iOS and Android developed by Russian company Wireless Lab. The app generates highly realistic transformations of faces in photographs by using neural networks based on artificial intelligence. The app can transform a face to make it smile, look younger, look older, or change gender.\n\nFaceApp was launched on iOS in January 2017 and on Android in February 2017. There are multiple options to manipulate the photo uploaded such as editor options of adding an impression, make-up, smiles, hair colors, hairstyles, glasses, age or beards. Filters, lens blur and backgrounds along with overlays, tattoos, and vignettes are also a part of the app. The gender change transformations of FaceApp have attracted particular interest from the LGBT and transgender communities, due to their ability to realistically simulate the appearance of a person as the opposite gender.\n\nIn 2019, FaceApp attracted criticism in both the press and on social media over the privacy of user data. Among the concerns raised were allegations that FaceApp stored users' photos on their servers, and that their terms of use allowed them to use users' likenesses and photos for commercial purposes. In response to questions, the company's founder, Yaroslav Goncharov, stated that user data and uploaded images were not being transferred to Russia but instead processed on servers running in the Google Cloud Platform and Amazon Web Services. According to Goncharov, user photos were only stored on servers to save bandwidth when applying multiple filters, and were deleted shortly after being uploaded. US senator Chuck Schumer expressed \"serious concerns regarding both the protection of the data that is being aggregated as well as whether users are aware of who may have access to it\" and called for an FBI investigation into the app. The specific section of the apps terms of service that drew concern were as follows:\n\nA \"hot\" transformation was available in the app in 2017 supposedly making its users appear more physically attractive, but this was accused of racism for lightening the skin color of black people and making them look more European. The feature was briefly renamed \"spark\" before being removed. Founder and chief executive Yaroslav Goncharov apologised, describing the situation as \"an unfortunate side-effect of the underlying neural network caused by the training set bias, not intended behaviour\" and announcing that a \"complete fix\" was being worked on. In August that year, FaceApp once again faced criticism when it featured \"ethnicity filters\" depicting \"White\", \"Black\", \"Asian\", and \"Indian\". The filters were immediately removed from the app.\n\n", "related": "\n- Face of the Future\n- Deepfake\n\n- Official website\n"}
{"id": "57169104", "url": "https://en.wikipedia.org/wiki?curid=57169104", "title": "Large memory storage and retrieval neural network", "text": "Large memory storage and retrieval neural network\n\nA large memory storage and retrieval neural network (LAMSTAR) is a fast deep learning neural network of many layers that can use many filters simultaneously. These filters may be nonlinear, stochastic, logic, non-stationary, or even non-analytical. They are biologically motivated and learn continuously.\n\nA LAMSTAR neural network may serve as a dynamic neural network in spatial or time domains or both. Its speed is provided by Hebbian link-weights that integrate the various and usually different filters (preprocessing functions) into its many layers and to dynamically rank the significance of the various layers and functions relative to a given learning task. This vaguely imitates biological learning that integrates various preprocessors (cochlea, retina, \"etc.\") and cortexes (auditory, visual, \"etc.\") and their various regions. Its deep learning capability is further enhanced by using inhibition, correlation and by its ability to cope with incomplete data, or \"lost\" neurons or layers even amidst a task. It is fully transparent due to its link weights. The link-weights allow dynamic determination of innovation and redundancy, and facilitate the ranking of layers, of filters or of individual neurons relative to a task.\n\nLAMSTAR has been applied to many domains, including medical and financial predictions, adaptive filtering of noisy speech in unknown noise, still-image recognition, video image recognition, software security and adaptive control of non-linear systems. LAMSTAR had a much faster learning speed and somewhat lower error rate than a CNN based on ReLU-function filters and max pooling, in 20 comparative studies.\n\nThese applications demonstrate delving into aspects of the data that are hidden from shallow learning networks and the human senses, such as in the cases of predicting onset of sleep apnea events, of an electrocardiogram of a fetus as recorded from skin-surface electrodes placed on the mother's abdomen early in pregnancy, of financial prediction or in blind filtering of noisy speech.\n\nLAMSTAR was proposed in 1996 and was further developed Graupe and Kordylewski from 1997–2002. A modified version, known as LAMSTAR 2, was developed by Schneider and Graupe in 2008.\n", "related": "NONE"}
{"id": "51545339", "url": "https://en.wikipedia.org/wiki?curid=51545339", "title": "Apache SINGA", "text": "Apache SINGA\n\nApache SINGA is an Apache top-level project for developing an open source machine learning library. It provides a flexible architecture for scalable distributed training, is extensible to run over a wide range of hardware, and has a focus on health-care applications.\n\nThe SINGA project was initiated by the DB System Group at National University of Singapore in 2014, in collaboration with the database group of Zhejiang University, in order to support complex analytics at scale, and make database systems more intelligent and autonomic. It focused on distributed deep learning by partitioning the model and data onto nodes in a cluster and parallelize the training. The prototype was accepted by Apache Incubator in March 2015, and graduated as a top-level project in October 2019. Seven versions have been released as shown in the following table. Since V1.0, SINGA is general to support traditional machine learning models such as logistic regression. Companies like NetEase, yzBigData,Shentilium and others are using SINGA for their applications, including healthcare and finance.\n\nSINGA's software stack includes three major components, namely, core, IO and model. The following figure illustrates these components together with the hardware. The core component provides memory management and tensor operations; IO has classes for reading (and writing) data from (to) disk and network; The model component provides data structures and algorithms for machine learning models, e.g., layers for neural network models, optimizers/initializer/metric/loss for general machine learning models.\n\nWorkload: we use a deep convolutional neural network, ResNet-50 as the application. ResNet-50 has 50 convolution layers for image classification. It requires 3.8 GFLOPs to pass a single image (of size 224x224) through the network. The input image size is 224x224.\n\nHardware: we use p2.8xlarge instances from AWS, each of which has 8 Nvidia Tesla K80 GPUs, 96 GB GPU memory in total, 32 vCPU, 488 GB main memory, 10 Gbit/s network bandwidth.\n\nMetric: we measure the time per iteration for different number of workers to evaluate the scalability of SINGA. The batch size is fixed to be 32 per GPU. Synchronous training scheme is applied. As a result, the effective batch size is $32N$, where N is the number of GPUs. We compare with a popular open source system which uses the parameter server topology. The first GPU is selected as the server. In the following figure, bars are for the throughput and lines are for the communication cost.\n\nRafiki is a sub module of SINGA for providing machine learning analytics service.\n\nTo get started with SINGA, there are some tutorials available as Jupyter notebooks. The tutorials cover the following:\n\n- Core classes\n- Model classes\n- Linear Regression\n- Multi-layer Perceptron\n- Convolutional Neural Network (CNN)\n- Recurrent Neural Networks (RNN)\n- Restricted Boltzmann Machine (RBM)\n\nThere is also an online course about SINGA.\n\n", "related": "\n- List of Apache Software Foundation projects\n- Comparison of deep learning software\n"}
{"id": "61835858", "url": "https://en.wikipedia.org/wiki?curid=61835858", "title": "Conversica", "text": "Conversica\n\nConversica is a US-based cloud software technology company. Conversica offers a suite of Intelligent Virtual Assistants for business, with a focus on Customer Experience business functions (Marketing, Sales, Customer Success, Account Management and Payments). Powered by Artificial Intelligence, the Intelligent Virtual Assistants (IVA) interact with leads and customers in a human-like way, like an entry level employee. The IVA software interacts over multiple channels, including email and SMS text messages, and in multiple languages. Conversica is a pioneer in providing AI-driven lead engagement software for marketing and sales organizations. Conversica is headquartered in Silicon Valley, (Foster City, California).\n\n2007: The Company was founded by Ben Brigham in Bellingham, Washington as AutoFerret.com. The company initially produced a Customer Relationship Management (CRM) product, targeted at automotive dealerships. This soon expanded to lead generation, and then lead validation and qualification. The AI for which Conversica is known today was born out of a need to follow-up on and filter out low-quality leads, and in time it was clear that this was where the real value of the product lay. The focus of the company shifted toward this automated lead engagement technology.\n\n2010: The Company started commercially selling AVA, the first Automated Virtual Assistant for sales, and the company name is changed to AVA.ai®. Early customers for AVA are automotive dealerships. and subsequent history of the company has been iteration and refinement of the AI-lead interaction. As the company moved away from generating leads themselves, and providing the CRM themselves, it became necessary to integrate with existing CRMs and Marketing Automation platforms, such as DealerSocket, VinSolutions and Salesforce. \n\n2013: Company raises $16m Series A funding, lead by Kennet Partners, names Mark Bradley as CEO of AVA.ai. Headquarters move from Bellingham, Washington to Foster City, California. Other key executives joining the business include David Marod (Automotive Sales) and William Webb-Purkis (Product Management). Expands sales office in Kansas City, Missouri. \n\n2014: Company changes name from AVA.ai to Conversica. \n\n2015: Alex Terry joins Conversica as CEO. Other key executives joining the business include Jason Lund (CFO), Dr. Werner Koepf (SVP Engineering). Business expands to include material number of customers in additional industry verticals, including technology, education, and financial services.\n\n2016: Company raises $34m Series B funding, lead by Providence Strategic Growth. Conversica launches second type of IVA - a Customer Success Assistant to help drive expansion revenue from existing customers. \n\n2017: Conversica expands intelligent automation platform and IVAs to support additional communication channels (e-mail and SMS text messaging) and additional communication languages. Conversica opens new technology center of excellence in Seattle, Washington to expand AI and Machine Learning capabilities. Dr. Sid Reddy joins Conversica as Chief Scientist.\n\n2018: Company raises $31m Series C funding, lead by Providence Strategic Growth. Conversica acquires Intelligens.ai, providing a regional presence in Latin America with an office in Las Condes, Santiago, Chile. Launches AI-powered Admissions Assistant for Higher Education industry. Rashmi Vittal joins Conversica as CMO. \n\n2019: Conversica was selected by Fast Company magazine as one of the Top 10 Most Innovative AI Companies in the World, and was named Marketo's \"Technology Partner of the Year\". The company officially expanded into the EMEA region with the opening of a London office. As of August 2019, Conversica has over 50 different integrations with third parties. In October Conversica won three awards at the fourth annual Global Annual Achievement Awards for Artificial Intelligence. Also that month, Alex Terry stepped down from his role as CEO, to be replaced by former Janrain leader Jim Kaskade.\n\nConversica's Intelligent Virtual Assistants are AI assistants who communicate with leads, prospects, customers, employees and other persons of interest (Contacts) in a human-like manner, via email and SMS text. The IVAs are built on an Intelligent Automation platform that leverages natural language understanding, natural language processing, natural language generation, deep learning and machine learning. The IVAs interact in a number of languages, including English, French, German, Spanish, Portuguese, and Japanese. The Assistants are generally deployed alongside sales and marketing, customer success, account management and higher education admissions teams, as part of an augmented workforce. The Intelligent Automation platform integrates with over 50 external systems, including CRM, Marketing Automation, and other systems of record. A partial list of integration partners includes: Salesforce, Marketo, Oracle, HubSpot, DealerSocket, Reynolds & Reynolds, CDK Global, VinSolutions and many more.\n", "related": "NONE"}
{"id": "62590682", "url": "https://en.wikipedia.org/wiki?curid=62590682", "title": "Deepfake pornography", "text": "Deepfake pornography\n\nDeepfake pornography, or simply fake pornography, is a type of synthetic porn that is generated by using deepfake technology. Many deepfakes on the internet feature pornography of people, often female celebrities whose likeness is typically used without their consent.\n\nDeepfake pornography prominently surfaced on the Internet in 2017, particularly on Reddit. The first one that captured attention was the Daisy Ridley deepfake, which was featured in several articles. Other prominent pornographic deepfakes were of various other celebrities.\n\nIn December 2017, Samantha Cole published an article about r/deepfakes in \"Vice\" that drew the first mainstream attention to deepfakes being shared in online communities. Six weeks later, Cole wrote in a follow-up article about the large increase in AI-assisted fake pornography. Since 2017, Samantha Cole of Vice has published a series of articles covering news surrounding deepfake pornography.\n\nSince then, multiple social media outlets have banned or made efforts to restrict deepfake pornography. Most notably, the r/deepfakes subreddit on Reddit was banned on February 7, 2018, due to the policy violation of \"involuntary pornography\". In the same month, representatives from Twitter stated that they would suspend accounts suspected of posting non-consensual deepfake content. \n\nScarlett Johansson, a frequent subject of deepfake porn, spoke publicly about the subject to \"The Washington Post\" in December 2018. In a prepared statement, she expressed that despite concerns, she would not attempt to remove any of her deepfakes, due to her belief that they do not affect her public image and that differing laws across countries and the nature of internet culture make any attempt to remove the deepfakes \"a lost cause\". While celebrities like herself are protected by their fame, however, she believes that deepfakes pose a grave threat to women of lesser prominence who could have their reputations damaged by depiction in involuntary deepfake pornography or revenge porn.\n\nIn June 2019, a downloadable Windows and Linux application called DeepNude was released which used neural networks, specifically generative adversarial networks, to remove clothing from images of women. The app had both a paid and unpaid version, the paid version costing $50. On June 27 the creators removed the application and refunded consumers.\n\nOn January 31st, 2018, Gfycat began removing all deepfakes from its site.\n\nIn February 2018, r/deepfakes was banned by Reddit for sharing involuntary pornography. Other websites have also banned the use of deepfakes for involuntary pornography, including the social media platform Twitter and the pornography site Pornhub. However, some websites have not yet banned Deepfake content, including 4chan and 8chan.\n\nAlso in February 2018, Pornhub said that it would ban deepfake videos on its website because it is considered “non consensual content” which violates their terms of service. They also stated previously to Mashable that they will take down content flagged as deepfakes. Writers from Motherboard from Buzzfeed News reported that searching “deepfakes” on Pornhub still returned multiple recent deepfake videos.\n\nChat site Discord has taken action against deepfakes in the past, and has taken a general stance against deepfakes.. In September 2018, Google added \"involuntary synthetic pornographic imagery” to its ban list, allowing anyone to request the block of results showing their fake nudes.\n\n- Deepfake\n- Media synthesis (AI)\n", "related": "NONE"}
{"id": "63359350", "url": "https://en.wikipedia.org/wiki?curid=63359350", "title": "Region Based Convolutional Neural Networks", "text": "Region Based Convolutional Neural Networks\n\nRegion Based Convolutional Neural Networks (R-CNN) are a family of machine learning models for computer vision and specifically object detection.\n\nThe original goal of R-CNN was to take an input image and produce a set of bounding boxes as output, where the each bounding box contains an object and also the category (e.g. car or pedestrian) of the object. More recently, R-CNN has been extended to perform other computer vision tasks. The following covers some of the versions of R-CNN that have been developed.\n\n- November 2013: R-CNN. Given an input image, R-CNN begins by applying a mechanism called Selective Search to extract regions of interest (ROI), where each ROI is a rectangle that may represent the boundary of an object in image. Depending on the scenario, there may be as many as two thousand ROIs. After that, each ROI is fed through a neural network to produce output features. For each ROI's output features, a collection of support-vector machine classifiers is used to determine what type of object (if any) is contained within the ROI.\n- April 2015: Fast R-CNN. While the original R-CNN independently computed the neural network features on each of as many as two thousand regions of interest, Fast R-CNN runs the neural network once on the whole image. At the end of the network is a novel method called ROIPooling, which slices out each ROI from the network's output tensor, reshapes it, and classifies it. As in the original R-CNN, the Fast R-CNN uses Selective Search to generate its region proposals.\n- June 2015: Faster R-CNN. While Fast R-CNN used Selective Search to generate ROIs, Faster R-CNN integrates the ROI generation into the neural network itself.\n- March 2017: Mask R-CNN. While previous versions of R-CNN focused on object detection, Mask R-CNN adds instance segmentation. Mask R-CNN also replaced ROIPooling with a new method called ROIAlign, which can represent fractions of a pixel.\n- June 2019: Mesh R-CNN adds the ability to generate a 3D mesh from a 2D image.\n\nRegion Based Convolutional Neural Networks have been used for tracking objects from a drone-mounted camera, locating text in an image, and enabling object detection in Google Lens. Mask R-CNN serves as one of seven tasks in the MLPerf Training Benchmark, which is a competition to speed up the training of neural networks.\n", "related": "NONE"}
{"id": "1510134", "url": "https://en.wikipedia.org/wiki?curid=1510134", "title": "Autoregressive integrated moving average", "text": "Autoregressive integrated moving average\n\nIn statistics and econometrics, and in particular in time series analysis, an autoregressive integrated moving average (ARIMA) model is a generalization of an autoregressive moving average (ARMA) model. Both of these models are fitted to time series data either to better understand the data or to predict future points in the series (forecasting). ARIMA models are applied in some cases where data show evidence of non-stationarity, where an initial differencing step (corresponding to the \"integrated\" part of the model) can be applied one or more times to eliminate the non-stationarity.\n\nThe AR part of ARIMA indicates that the evolving variable of interest is regressed on its own lagged (i.e., prior) values. The MA part indicates that the regression error is actually a linear combination of error terms whose values occurred contemporaneously and at various times in the past. The I (for \"integrated\") indicates that the data values have been replaced with the difference between their values and the previous values (and this differencing process may have been performed more than once). The purpose of each of these features is to make the model fit the data as well as possible.\n\nNon-seasonal ARIMA models are generally denoted ARIMA(\"p\",\"d\",\"q\") where parameters \"p\", \"d\", and \"q\" are non-negative integers, \"p\" is the order (number of time lags) of the autoregressive model, \"d\" is the degree of differencing (the number of times the data have had past values subtracted), and \"q\" is the order of the moving-average model. Seasonal ARIMA models are usually denoted ARIMA(\"p\",\"d\",\"q\")(\"P\",\"D\",\"Q\"), where \"m\" refers to the number of periods in each season, and the uppercase \"P\",\"D\",\"Q\" refer to the autoregressive, differencing, and moving average terms for the seasonal part of the ARIMA model.\n\nWhen two out of the three terms are zeros, the model may be referred to based on the non-zero parameter, dropping \"AR\", \"I\" or \"MA\" from the acronym describing the model. For example, ARIMA (1,0,0) is AR(1), ARIMA(0,1,0) is I(1), and ARIMA(0,0,1) is MA(1).\n\nARIMA models can be estimated following the Box–Jenkins approach.\n\nGiven a time series data \"X\" where \"t\" is an integer index and the \"X\" are real numbers, an formula_1 model is given by\n\nor equivalently by\n\nwhere formula_4 is the lag operator, the formula_5 are the parameters of the autoregressive part of the model, the formula_6 are the parameters of the moving average part and the formula_7 are error terms. The error terms formula_7 are generally assumed to be independent, identically distributed variables sampled from a normal distribution with zero mean.\n\nAssume now that the polynomial formula_9 has a unit root (a factor formula_10) of multiplicity \"d\". Then it can be rewritten as:\n\nAn ARIMA(\"p\",\"d\",\"q\") process expresses this polynomial factorisation property with \"p\"=\"p'−d\", and is given by:\n\nand thus can be thought as a particular case of an ARMA(\"p+d\",\"q\") process having the autoregressive polynomial with \"d\" unit roots. (For this reason, no ARIMA model with \"d\" > 0 is wide sense stationary.)\n\nThe above can be generalized as follows.\n\nThis defines an ARIMA(\"p\",\"d\",\"q\") process with drift formula_14.\n\nThe explicit identification of the factorisation of the autoregression polynomial into factors as above, can be extended to other cases, firstly to apply to the moving average polynomial and secondly to include other special factors. For example, having a factor formula_15 in a model is one way of including a non-stationary seasonality of period \"s\" into the model; this factor has the effect of re-expressing the data as changes from \"s\" periods ago. Another example is the factor formula_16, which includes a (non-stationary) seasonality of period 2. The effect of the first type of factor is to allow each season's value to drift separately over time, whereas with the second type values for adjacent seasons move together.\n\nIdentification and specification of appropriate factors in an ARIMA model can be an important step in modelling as it can allow a reduction in the overall number of parameters to be estimated, while allowing the imposition on the model of types of behaviour that logic and experience suggest should be there.\n\nDifferencing in statistics is a transformation applied to time-series data in order to make it stationary. A stationary time series' properties do not depend on the time at which the series is observed.\n\nIn order to difference the data, the difference between consecutive observations is computed. Mathematically, this is shown as\n\nDifferencing removes the changes in the level of a time series, eliminating trend and seasonality and consequently stabilizing the mean of the time series.\n\nSometimes it may be necessary to difference the data a second time to obtain a stationary time series, which is referred to as second order differencing:\n\nAnother method of differencing data is seasonal differencing, which involves computing the difference between an observation and the corresponding observation in the previous season e.g a year. This is shown as:\n\nThe differenced data is then used for the estimation of an ARMA model.\n\nSome well-known special cases arise naturally or are mathematically equivalent to other popular forecasting models. For example:\n- An ARIMA(0, 1, 0) model (or I(1) model) is given by formula_20 — which is simply a random walk.\n- An ARIMA(0, 1, 0) with a constant, given by formula_21 — which is a random walk with drift.\n- An ARIMA(0, 0, 0) model is a white noise model.\n- An ARIMA(0, 1, 2) model is a Damped Holt's model.\n- An ARIMA(0, 1, 1) model without constant is a basic exponential smoothing model.\n- An ARIMA(0, 2, 2) model is given by formula_22 — which is equivalent to Holt's linear method with additive errors, or double exponential smoothing.\n\nTo determine the order of a non-seasonal ARIMA model, a useful criterion is the Akaike information criterion (AIC) . It is written as\n\nwhere \"L \"is the likelihood of the data, \"p \"is the order of the autoregressive part and \"q \"is the order of the moving average part. The \"k\" represents the intercept of the ARIMA model. For AIC, if \"k\" = 1 then there is an intercept in the ARIMA model (\"c \"≠ 0) and if \"k \"= 0 then there is no intercept in the ARIMA model (\"c \"= 0).\n\nThe corrected AIC for ARIMA models can be written as\n\nThe Bayesian Information Criterion can be written as\n\nThe objective is to minimize the AIC, AICc or BIC values for a good model. The lower the value of one of these criteria for a range of models being investigated, the better the model will suit the data. The AIC and the BIC are used for two completely different purposes. While the AIC tries to approximate models towards the reality of the situation, the BIC attempts to find the perfect fit. The BIC approach is often criticized as there never is a perfect fit to real-life complex data; however, it is still a useful method for selection as it penalizes models more heavily for having more parameters than the AIC would.\n\nAICc can only be used to compare ARIMA models with the same orders of differencing. For ARIMAs with different orders of differencing, RMSE can be used for model comparison.\n\nThe ARIMA model can be viewed as a \"cascade\" of two models. The first is non-stationary:\n\nwhile the second is wide-sense stationary:\n\nNow forecasts can be made for the process formula_28, using a generalization of the method of autoregressive forecasting.\n\nThe forecast intervals (confidence intervals for forecasts) for ARIMA models are based on assumptions that the residuals are uncorrelated and normally distributed. If either of these assumptions does not hold, then the forecast intervals may be incorrect. For this reason, researchers plot the ACF and histogram of the residuals to check the assumptions before producing forecast intervals.\n\n95% forecast interval: formula_29, where formula_30 is the variance of formula_31.\n\nFor formula_32, formula_33 for all ARIMA models regardless of parameters and orders.\n\nFor ARIMA(0,0,q), formula_34\n\nIn general, forecast intervals from ARIMA models will increase as the forecast horizon increases.\n\nA number of variations on the ARIMA model are commonly employed. If multiple time series are used then the formula_36 can be thought of as vectors and a VARIMA model may be appropriate. Sometimes a seasonal effect is suspected in the model; in that case, it is generally considered better to use a SARIMA (seasonal ARIMA) model than to increase the order of the AR or MA parts of the model. If the time-series is suspected to exhibit long-range dependence, then the \"d\" parameter may be allowed to have non-integer values in an autoregressive fractionally integrated moving average model, which is also called a Fractional ARIMA (FARIMA or ARFIMA) model.\n\nVarious packages that apply methodology like Box–Jenkins parameter optimization are available to find the right parameters for the ARIMA model.\n\n- EViews: has extensive ARIMA and SARIMA capabilities.\n- Julia: contains an ARIMA implementation in the TimeModels package\n- Mathematica: includes ARIMAProcess function.\n- MATLAB: the Econometrics Toolbox includes ARIMA models and regression with ARIMA errors\n- NCSS: includes several procedures for codice_1 fitting and forecasting.\n- Python: the \"statsmodels\" package includes models for time series analysis – univariate time series analysis: AR, ARIMA – vector autoregressive models, VAR and structural VAR – descriptive statistics and process models for time series analysis.\n- R: the standard R \"stats\" package includes an \"arima\" function, which is documented in \"ARIMA Modelling of Time Series\". Besides the \"ARIMA(p,d,q)\" part, the function also includes seasonal factors, an intercept term, and exogenous variables (\"xreg\", called \"external regressors\"). The CRAN task view on Time Series is the reference with many more links. The \"forecast\" package in R can automatically select an ARIMA model for a given time series with the auto.arima() function. The package can also simulate seasonal and non-seasonal ARIMA models with its simulate.Arima() function. It also has a function Arima(), which is a wrapper for the arima from the \"stats\" package.\n- Ruby: the \"statsample-timeseries\" gem is used for time series analysis, including ARIMA models and Kalman Filtering.\n- SAFE TOOLBOXES: includes ARIMA modelling and regression with ARIMA errors.\n- SAS: includes extensive ARIMA processing in its Econometric and Time Series Analysis system: SAS/ETS.\n- IBM SPSS: includes ARIMA modeling in its Statistics and Modeler statistical packages. The default Expert Modeler feature evaluates a range of seasonal and non-seasonal autoregressive (\"p\"), integrated (\"d\"), and moving average (\"q\") settings and seven exponential smoothing models. The Expert Modeler can also transform the target time-series data into its square root or natural log. The user also has the option to restrict the Expert Modeler to ARIMA models, or to manually enter ARIMA nonseasonal and seasonal \"p\", \"d\", and \"q\" settings without Expert Modeler. Automatic outlier detection is available for seven types of outliers, and the detected outliers will be accommodated in the time-series model if this feature is selected.\n- SAP: the APO-FCS package in SAP ERP from SAP allows creation and fitting of ARIMA models using the Box–Jenkins methodology.\n- SQL Server Analysis Services: from Microsoft includes ARIMA as a Data Mining algorithm.\n- Stata includes ARIMA modelling (using its arima command) as of Stata 9.\n- Teradata Vantage has the ARIMA function as part of its Machine learning engine.\n- TOL (Time Oriented Language) is designed to model ARIMA models (including SARIMA, ARIMAX and DSARIMAX variants) .\n- Scala: spark-timeseries library contains ARIMA implementation for Scala, Java and Python. Implementation is designed to run on Apache Spark.\n- PostgreSQL/MadLib: Time Series Analysis/ARIMA.\n- X-12-ARIMA: from the US Bureau of the Census\n\n", "related": "\n- Autocorrelation\n- ARMA\n- Partial autocorrelation\n- Finite impulse response\n- Infinite impulse response\n\n\n- The US Census Bureau uses ARIMA for \"seasonally adjusted\" data (programs, docs, and papers here)\n- Lecture notes on ARIMA models (Robert Nau, Duke University)\n"}
{"id": "1236458", "url": "https://en.wikipedia.org/wiki?curid=1236458", "title": "Bellman equation", "text": "Bellman equation\n\nA Bellman equation, named after Richard E. Bellman, is a necessary condition for optimality associated with the mathematical optimization method known as dynamic programming. It writes the \"value\" of a decision problem at a certain point in time in terms of the payoff from some initial choices and the \"value\" of the remaining decision problem that results from those initial choices. This breaks a dynamic optimization problem into a sequence of simpler subproblems, as Bellman's “principle of optimality” prescribes.\n\nThe Bellman equation was first applied to engineering control theory and to other topics in applied mathematics, and subsequently became an important tool in economic theory; though the basic concepts of dynamic programming are prefigured in John von Neumann and Oskar Morgenstern's \"Theory of Games and Economic Behavior\" and Abraham Wald's \"sequential analysis\".\n\nAlmost any problem that can be solved using optimal control theory can also be solved by analyzing the appropriate Bellman equation. However, the term 'Bellman equation' usually refers to the dynamic programming equation associated with discrete-time optimization problems. In continuous-time optimization problems, the analogous equation is a partial differential equation that is usually called the Hamilton–Jacobi–Bellman equation.\n\nTo understand the Bellman equation, several underlying concepts must be understood. First, any optimization problem has some objective: minimizing travel time, minimizing cost, maximizing profits, maximizing utility, etc. The mathematical function that describes this objective is called the \"objective function\".\n\nDynamic programming breaks a multi-period planning problem into simpler steps at different points in time. Therefore, it requires keeping track of how the decision situation is evolving over time. The information about the current situation that is needed to make a correct decision is called the \"state\". For example, to decide how much to consume and spend at each point in time, people would need to know (among other things) their initial wealth. Therefore, wealth formula_1 would be one of their \"state variables\", but there would probably be others.\n\nThe variables chosen at any given point in time are often called the \"control variables\". For example, given their current wealth, people might decide how much to consume now. Choosing the control variables now may be equivalent to choosing the next state; more generally, the next state is affected by other factors in addition to the current control. For example, in the simplest case, today's wealth (the state) and consumption (the control) might exactly determine tomorrow's wealth (the new state), though typically other factors will affect tomorrow's wealth too.\n\nThe dynamic programming approach describes the optimal plan by finding a rule that tells what the controls should be, given any possible value of the state. For example, if consumption (\"c\") depends \"only\" on wealth (\"W\"), we would seek a rule formula_2 that gives consumption as a function of wealth. Such a rule, determining the controls as a function of the states, is called a \"policy function\" (See Bellman, 1957, Ch. III.2).\n\nFinally, by definition, the optimal decision rule is the one that achieves the best possible value of the objective. For example, if someone chooses consumption, given wealth, in order to maximize happiness (assuming happiness \"H\" can be represented by a mathematical function, such as a utility function and is something defined by wealth), then each level of wealth will be associated with some highest possible level of happiness, formula_3. The best possible value of the objective, written as a function of the state, is called the \"value function\".\n\nRichard Bellman showed that a dynamic optimization problem in discrete time can be stated in a recursive, step-by-step form known as backward induction by writing down the relationship between the value function in one period and the value function in the next period. The relationship between these two value functions is called the \"Bellman equation\". In this approach, the optimal policy in the last time period is specified in advance as a function of the state variable's value at that time, and the resulting optimal value of the objective function is thus expressed in terms of that value of the state variable. Next, the next-to-last period's optimization involves maximizing the sum of that period's period-specific objective function and the optimal value of the future objective function, giving that period's optimal policy contingent upon the value of the state variable as of the next-to-last period decision. This logic continues recursively back in time, until the first period decision rule is derived, as a function of the initial state variable value, by optimizing the sum of the first-period-specific objective function and the value of the second period's value function, which gives the value for all the future periods. Thus, each period's decision is made by explicitly acknowledging that all future decisions will be optimally made.\n\nLet the state at time formula_4 be formula_5. For a decision that begins at time 0, we take as given the initial state formula_6. At any time, the set of possible actions depends on the current state; we can write this as formula_7, where the action formula_8 represents one or more control variables. We also assume that the state changes from formula_9 to a new state formula_10 when action formula_11 is taken, and that the current payoff from taking action formula_11 in state formula_9 is formula_14. Finally, we assume impatience, represented by a discount factor formula_15.\n\nUnder these assumptions, an infinite-horizon decision problem takes the following form:\n\nsubject to the constraints\n\nNotice that we have defined notation formula_18 to denote the optimal value that can be obtained by maximizing this objective function subject to the assumed constraints. This function is the \"value function\". It is a function of the initial state variable formula_6, since the best value obtainable depends on the initial situation.\n\nThe dynamic programming method breaks this decision problem into smaller subproblems. Richard Bellman's \"principle of optimality\" describes how to do this:Principle of Optimality: An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)\nIn computer science, a problem that can be broken apart like this is said to have optimal substructure. In the context of dynamic game theory, this principle is analogous to the concept of subgame perfect equilibrium, although what constitutes an optimal policy in this case is conditioned on the decision-maker's opponents choosing similarly optimal policies from their points of view.\n\nAs suggested by the \"principle of optimality\", we will consider the first decision separately, setting aside all future decisions (we will start afresh from time 1 with the new state formula_20). Collecting the future decisions in brackets on the right, the above infinite-horizon decision problem is equivalent to:\n\nsubject to the constraints\n\nHere we are choosing formula_23, knowing that our choice will cause the time 1 state to be formula_24. That new state will then affect the decision problem from time 1 on. The whole future decision problem appears inside the square brackets on the right.\n\nSo far it seems we have only made the problem uglier by separating today's decision from future decisions. But we can simplify by noticing that what is inside the square brackets on the right is \"the value\" of the time 1 decision problem, starting from state formula_24.\n\nTherefore, we can rewrite the problem as a recursive definition of the value function:\n\nThis is the Bellman equation. It can be simplified even further if we drop time subscripts and plug in the value of the next state:\n\nThe Bellman equation is classified as a functional equation, because solving it means finding the unknown function \"V\", which is the \"value function\". Recall that the value function describes the best possible value of the objective, as a function of the state \"x\". By calculating the value function, we will also find the function \"a\"(\"x\") that describes the optimal action as a function of the state; this is called the \"policy function\".\n\nIn the deterministic setting, other techniques besides dynamic programming can be used to tackle the above optimal control problem. However, the Bellman Equation is often the most convenient method of solving \"stochastic\" optimal control problems.\n\nFor a specific example from economics, consider an infinitely-lived consumer with initial wealth endowment formula_29 at period formula_30. He has an instantaneous utility function formula_31 where formula_32 denotes consumption and discounts the next period utility at a rate of formula_33. Assume that what is not consumed in period formula_4 carries over to the next period with interest rate formula_35. Then the consumer's utility maximization problem is to choose a consumption plan formula_36\n\nand\n\nThe first constraint is the capital accumulation/law of motion specified by the problem, while the second constraint is a transversality condition that the consumer does not carry debt at the end of his life. The Bellman equation is\n\nAlternatively, one can treat the sequence problem directly using, for example, the Hamiltonian equations.\n\nNow, if the interest rate varies from period to period, the consumer is faced with a stochastic optimization problem. Let the interest \"r\" follow a Markov process with probability transition function formula_39 where formula_40 denotes the probability measure governing the distribution of interest rate next period if current interest rate is formula_35. In this model the consumer decides his current period consumption after the current period interest rate is announced.\n\nRather than simply choosing a single sequence <math>\\\n", "related": "NONE"}
{"id": "35188921", "url": "https://en.wikipedia.org/wiki?curid=35188921", "title": "Bellman pseudospectral method", "text": "Bellman pseudospectral method\n\nThe Bellman pseudospectral method is a pseudospectral method for optimal control based on Bellman's principle of optimality. It is part of the larger theory of pseudospectral optimal control, a term coined by Ross. The method is named after Richard E. Bellman. It was introduced by Ross et al.\nfirst as a means to solve multiscale optimal control problems, and later expanded to obtain suboptimal solutions for general optimal control problems.\n\nThe multiscale version of the Bellman pseudospectral method is based on the spectral convergence property of the Ross–Fahroo pseudospectral methods. That is, because the Ross–Fahroo pseudospectral method converges at an exponentially fast rate, pointwise convergence to a solution is obtained at very low number of nodes even when the solution has high-frequency components. This aliasing phenomenon in optimal control was first discovered by Ross et al. Rather than use signal processing techniques to anti-alias the solution, Ross et al. proposed that Bellman's principle of optimality can be applied to the converged solution to extract information between the nodes. Because the Gauss–Lobatto nodes cluster at the boundary points, Ross et al. suggested that if the node density around the initial conditions satisfy the Nyquist–Shannon sampling theorem, then the complete solution can be recovered by solving the optimal control problem in a recursive fashion over piecewise segments known as Bellman segments.\n\nIn an expanded version of the method, Ross et al., proposed that method could also be used to generate feasible solutions that were not necessarily optimal. In this version, one can apply the Bellman pseudospectral method at even lower number of nodes even under the knowledge that the solution may not have converged to the optimal one. In this situation, one obtains a feasible solution.\n\nA remarkable feature of the Bellman pseudospectral method is that it automatically determines several measures of suboptimality based on the original pseudospectral cost and the cost generated by the sum of the Bellman segments.\n\nOne of the computational advantages of the Bellman pseudospectral method is that it allows one to escape Gaussian rules in the distribution of node points. That is, in a standard pseudospectral method, the distribution of node points are Gaussian (typically Gauss-Lobatto for finite horizon and Gauss-Radau for infinite horizon). The Gaussian points are sparse in the middle of the interval (middle is defined in a shifted sense for infinite-horizon problems) and dense at the boundaries. The second-order accumulation of points near the boundaries have the effect of wasting nodes. The Bellman pseudospectral method takes advantage of the node accumulation at the initial point to anti-alias the solution and discards the remainder of the nodes. Thus the final distribution of nodes is non-Gaussian and dense while the computational method retains a sparse structure.\n\nThe Bellman pseudospectral method was first applied by Ross et al. to solve the challenging problem of very low thrust trajectory optimization. It has been successfully applied to solve a practical problem of generating very high accuracy solutions to a trans-Earth-injection problem of bringing a space capsule from a lunar orbit to a pin-pointed Earth-interface condition for successful reentry.\n\nThe Bellman pseudospectral method is most commonly used as an additional check on the optimality of a pseudospectral solution generated by the Ross–Fahroo pseudospectral methods. That is, in addition to the use of Pontryagin's minimum principle in conjunction with the solutions obtained by the Ross–Fahroo pseudospectral methods, the Bellman pseudospectral method is used as a primal-only test on the optimality of the computed solution.\n\n", "related": "\n- Legendre pseudospectral method\n- Chebyshev pseudospectral method\n- Pseudospectral knotting method\n"}
{"id": "125297", "url": "https://en.wikipedia.org/wiki?curid=125297", "title": "Dynamic programming", "text": "Dynamic programming\n\nDynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\n\nIf sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.\n\nIn terms of mathematical optimization, dynamic programming usually refers to simplifying a decision by breaking it down into a sequence of decision steps over time. This is done by defining a sequence of value functions \"V\", \"V\", ..., \"V\" taking \"y\" as an argument representing the state of the system at times \"i\" from 1 to \"n\". The definition of \"V\"(\"y\") is the value obtained in state \"y\" at the last time \"n\". The values \"V\" at earlier times \"i\" = \"n\" −1, \"n\" − 2, ..., 2, 1 can be found by working backwards, using a recursive relationship called the Bellman equation. For \"i\" = 2, ..., \"n\", \"V\" at any state \"y\" is calculated from \"V\" by maximizing a simple function (usually the sum) of the gain from a decision at time \"i\" − 1 and the function \"V\" at the new state of the system if this decision is made. Since \"V\" has already been calculated for the needed states, the above operation yields \"V\" for those states. Finally, \"V\" at the initial state of the system is the value of the optimal solution. The optimal values of the decision variables can be recovered, one by one, by tracking back the calculations already performed.\n\nIn control theory, a typical problem is to find an admissible control formula_1 which causes the system formula_2 to follow an admissible trajectory formula_3 on a continuous time interval formula_4 that minimizes a cost function\nThe solution to this problem is an optimal control law or policy formula_6, which produces an optimal trajectory formula_3 and an optimized loss function formula_8. The latter obeys the fundamental equation of dynamic programming:\na partial differential equation known as the Hamilton–Jacobi–Bellman equation, in which formula_10 and formula_11. One finds the minimizing formula_12 in terms of formula_13, formula_14, and the unknown function formula_15 and then substitutes the result into the Hamilton–Jacobi–Bellman equation to get the partial differential equation to be solved with boundary condition formula_16. In practice, this generally requires numerical techniques for some discrete approximation to the exact optimization relationship.\n\nAlternatively, the continuous process can be approximated by a discrete system, which leads to a following recurrence relation analog to the Hamilton–Jacobi–Bellman equation:\nat the formula_18-th stage of formula_19 equally spaced discrete time intervals, and where formula_20 and formula_21 denote discrete approximations to formula_22 and formula_23. This functional equation is known as the Bellman equation, which can be solved for an exact solution of the discrete approximation of the optimization equation.\n\nIn economics, the objective is generally to maximize (rather than minimize) some dynamic social welfare function. In Ramsey's problem, this function relates amounts of consumption to levels of utility. Loosely speaking, the planner faces the trade-off between contemporaneous consumption and future consumption (via investment in capital stock that is used in production), known as intertemporal choice. Future consumption is discounted at a constant rate formula_24. A discrete approximation to the transition equation of capital is given by\nwhere formula_26 is consumption, formula_18 is capital, and formula_22 is a production function satisfying the Inada conditions. An initial capital stock formula_29 is assumed.\n\nLet formula_30 be consumption in period , and assume consumption yields utility formula_31 as long as the consumer lives. Assume the consumer is impatient, so that he discounts future utility by a factor each period, where formula_32 be capital in period . Assume initial capital is a given amount formula_33, and suppose that this period's capital and consumption determine next period's capital as formula_34, where is a positive constant and formula_35 subject to formula_36 for all formula_37\n\nWritten this way, the problem looks complicated, because it involves solving for all the choice variables formula_38. (The capital formula_39 is not a choice variable—the consumer's initial capital is taken as given.)\n\nThe dynamic programming approach to solve this problem involves breaking it apart into a sequence of smaller decisions. To do so, we define a sequence of \"value functions\" formula_40, for formula_41 which represent the value of having any amount of capital at each time . There is (by assumption) no utility from having capital after death, formula_42.\n\nThe value of any quantity of capital at any previous time can be calculated by backward induction using the Bellman equation. In this problem, for each formula_37, the Bellman equation is\n\nThis problem is much simpler than the one we wrote down before, because it involves only two decision variables, formula_30 and formula_47. Intuitively, instead of choosing his whole lifetime plan at birth, the consumer can take things one step at a time. At time , his current capital formula_48 is given, and he only needs to choose current consumption formula_30 and saving formula_47.\n\nTo actually solve this problem, we work backwards. For simplicity, the current level of capital is denoted as . formula_51 is already known, so using the Bellman equation once we can calculate formula_52, and so on until we get to formula_53, which is the \"value\" of the initial decision problem for the whole lifetime. In other words, once we know formula_54, we can calculate formula_55, which is the maximum of formula_56, where formula_57 is the choice variable and formula_58.\n\nWorking backwards, it can be shown that the value function at time formula_59 is\n\nwhere each formula_61 is a constant, and the optimal amount to consume at time formula_59 is\n\nwhich can be simplified to\n\nWe see that it is optimal to consume a larger fraction of current wealth as one gets older, finally consuming all remaining wealth in period , the last period of life.\n\nThere are two key attributes that a problem must have in order for dynamic programming to be applicable: optimal substructure and overlapping sub-problems. If a problem can be solved by combining optimal solutions to \"non-overlapping\" sub-problems, the strategy is called \"divide and conquer\" instead. This is why merge sort and quick sort are not classified as dynamic programming problems.\n\n\"Optimal substructure\" means that the solution to a given optimization problem can be obtained by the combination of optimal solutions to its sub-problems. Such optimal substructures are usually described by means of recursion. For example, given a graph \"G=(V,E)\", the shortest path \"p\" from a vertex \"u\" to a vertex \"v\" exhibits optimal substructure: take any intermediate vertex \"w\" on this shortest path \"p\". If \"p\" is truly the shortest path, then it can be split into sub-paths \"p\" from \"u\" to \"w\" and \"p\" from \"w\" to \"v\" such that these, in turn, are indeed the shortest paths between the corresponding vertices (by the simple cut-and-paste argument described in \"Introduction to Algorithms\"). Hence, one can easily formulate the solution for finding shortest paths in a recursive manner, which is what the Bellman–Ford algorithm or the Floyd–Warshall algorithm does.\n\n\"Overlapping\" sub-problems means that the space of sub-problems must be small, that is, any recursive algorithm solving the problem should solve the same sub-problems over and over, rather than generating new sub-problems. For example, consider the recursive formulation for generating the Fibonacci series: \"F\" = \"F\" + \"F\", with base case \"F\" = \"F\" = 1. Then \"F\" = \"F\" + \"F\", and \"F\" = \"F\" + \"F\". Now \"F\" is being solved in the recursive sub-trees of both \"F\" as well as \"F\". Even though the total number of sub-problems is actually small (only 43 of them), we end up solving the same problems over and over if we adopt a naive recursive solution such as this. Dynamic programming takes account of this fact and solves each sub-problem only once.\n\nThis can be achieved in either of two ways:\n\n- \"Top-down approach\": This is the direct fall-out of the recursive formulation of any problem. If the solution to any problem can be formulated recursively using the solution to its sub-problems, and if its sub-problems are overlapping, then one can easily memoize or store the solutions to the sub-problems in a table. Whenever we attempt to solve a new sub-problem, we first check the table to see if it is already solved. If a solution has been recorded, we can use it directly, otherwise we solve the sub-problem and add its solution to the table.\n- \"Bottom-up approach\": Once we formulate the solution to a problem recursively as in terms of its sub-problems, we can try reformulating the problem in a bottom-up fashion: try solving the sub-problems first and use their solutions to build-on and arrive at solutions to bigger sub-problems. This is also usually done in a tabular form by iteratively generating solutions to bigger and bigger sub-problems by using the solutions to small sub-problems. For example, if we already know the values of \"F\" and \"F\", we can directly calculate the value of \"F\".\n\nSome programming languages can automatically memoize the result of a function call with a particular set of arguments, in order to speed up call-by-name evaluation (this mechanism is referred to as \"call-by-need\"). Some languages make it possible portably (e.g. Scheme, Common Lisp, Perl or D). Some languages have automatic memoization built in, such as tabled Prolog and J, which supports memoization with the \"M.\" adverb. In any case, this is only possible for a referentially transparent function. Memoization is also encountered as an easily accessible design pattern within term-rewrite based languages such as Wolfram Language.\n\nDynamic programming is widely used in bioinformatics for the tasks such as sequence alignment, protein folding, RNA structure prediction and protein-DNA binding. The first dynamic programming algorithms for protein-DNA binding were developed in the 1970s independently by Charles DeLisi in USA and Georgii Gurskii and Alexander Zasedatelev in USSR. Recently these algorithms have become very popular in bioinformatics and computational biology, particularly in the studies of nucleosome positioning and transcription factor binding.\n\nFrom a dynamic programming point of view, Dijkstra's algorithm for the shortest path problem is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the Reaching method.\n\nIn fact, Dijkstra's explanation of the logic behind the algorithm, namely\nis a paraphrasing of Bellman's famous Principle of Optimality in the context of the shortest path problem.\n\nUsing dynamic programming in the calculation of the \"n\"th member of the Fibonacci sequence improves its performance greatly. Here is a naïve implementation, based directly on the mathematical definition:\n\nNotice that if we call, say, codice_1, we produce a call tree that calls the function on the same value many different times:\n\n1. codice_1\n2. codice_3\n3. codice_4\n4. codice_5\n5. codice_6\n\nIn particular, codice_7 was calculated three times from scratch. In larger examples, many more values of codice_8, or \"subproblems\", are recalculated, leading to an exponential time algorithm.\n\nNow, suppose we have a simple map object, \"m\", which maps each value of codice_8 that has already been calculated to its result, and we modify our function to use it and update it. The resulting function requires only O(\"n\") time instead of exponential time (but requires O(\"n\") space):\n\nThis technique of saving values that have already been calculated is called \"memoization\"; this is the top-down approach, since we first break the problem into subproblems and then calculate and store values.\n\nIn the bottom-up approach, we calculate the smaller values of codice_8 first, then build larger values from them. This method also uses O(\"n\") time since it contains a loop that repeats n − 1 times, but it only takes constant (O(1)) space, in contrast to the top-down approach which requires O(\"n\") space to store the map.\n\nIn both examples, we only calculate codice_7 one time, and then use it to calculate both codice_12 and codice_13, instead of computing it every time either of them is evaluated.\n\nThe above method actually takes formula_65 time for large n because addition of two integers with formula_66 bits each takes formula_66 time. (The \"n\" fibonacci number has formula_66 bits.) Also, there is a closed form for the Fibonacci sequence, known as Binet's formula, from which the formula_19-th term can be computed in approximately formula_70 time, which is more efficient than the above dynamic programming technique. However, the simple recurrence directly gives the matrix form that leads to an approximately formula_71 algorithm by fast matrix exponentiation.\n\nConsider the problem of assigning values, either zero or one, to the positions of an matrix, with even, so that each row and each column contains exactly zeros and ones. We ask how many different assignments there are for a given formula_19. For example, when , four possible solutions are\n\nThere are at least three possible approaches: brute force, backtracking, and dynamic programming.\n\nBrute force consists of checking all assignments of zeros and ones and counting those that have balanced rows and columns ( zeros and ones). As there are formula_74 possible assignments, this strategy is not practical except maybe up to formula_75.\n\nBacktracking for this problem consists of choosing some order of the matrix elements and recursively placing ones or zeros, while checking that in every row and column the number of elements that have not been assigned plus the number of ones or zeros are both at least . While more sophisticated than brute force, this approach will visit every solution once, making it impractical for larger than six, since the number of solutions is already 116,963,796,250 for  = 8, as we shall see.\n\nDynamic programming makes it possible to count the number of solutions without visiting them all. Imagine backtracking values for the first row – what information would we require about the remaining rows, in order to be able to accurately count the solutions obtained for each first row value? We consider boards, where , whose formula_18 rows contain formula_77 zeros and formula_77 ones. The function \"f\" to which memoization is applied maps vectors of \"n\" pairs of integers to the number of admissible boards (solutions). There is one pair for each column, and its two components indicate respectively the number of zeros and ones that have yet to be placed in that column. We seek the value of formula_79 (formula_19 arguments or one vector of formula_19 elements). The process of subproblem creation involves iterating over every one of formula_82 possible assignments for the top row of the board, and going through every column, subtracting one from the appropriate element of the pair for that column, depending on whether the assignment for the top row contained a zero or a one at that position. If any one of the results is negative, then the assignment is invalid and does not contribute to the set of solutions (recursion stops). Otherwise, we have an assignment for the top row of the board and recursively compute the number of solutions to the remaining board, adding the numbers of solutions for every admissible assignment of the top row and returning the sum, which is being memoized. The base case is the trivial subproblem, which occurs for a board. The number of solutions for this board is either zero or one, depending on whether the vector is a permutation of formula_83 and formula_84 pairs or not.\n\nFor example, in the first two boards shown above the sequences of vectors would be\nThe number of solutions is\n\nLinks to the MAPLE implementation of the dynamic programming approach may be found among the external links.\n\nConsider a checkerboard with \"n\" × \"n\" squares and a cost function codice_14 which returns a cost associated with square codice_15 (codice_16 being the row, codice_17 being the column). For instance (on a 5 × 5 checkerboard),\nThus codice_18\n\nLet us say there was a checker that could start at any square on the first rank (i.e., row) and you wanted to know the shortest path (the sum of the minimum costs at each visited rank) to get to the last rank; assuming the checker could move only diagonally left forward, diagonally right forward, or straight forward. That is, a checker on codice_19 can move to codice_20, codice_21 or codice_22.\nThis problem exhibits optimal substructure. That is, the solution to the entire problem relies on solutions to subproblems. Let us define a function codice_23 as\n\nStarting at rank codice_24 and descending to rank codice_25, we compute the value of this function for all the squares at each successive rank. Picking the square that holds the minimum value at each rank gives us the shortest path between rank codice_24 and rank codice_25.\n\nThe function codice_23 is equal to the minimum cost to get to any of the three squares below it (since those are the only squares that can reach it) plus codice_14. For instance:\n\nNow, let us define codice_23 in somewhat more general terms:\n\nThe first line of this equation deals with a board modeled as squares indexed on codice_25 at the lowest bound and codice_24 at the highest bound. The second line specifies what happens at the last rank; providing a base case. The third line, the recursion, is the important part. It represents the codice_33 terms in the example. From this definition we can derive straightforward recursive code for codice_34. In the following pseudocode, codice_24 is the size of the board, codice_14 is the cost function, and codice_37 returns the minimum of a number of values:\n\nThis function only computes the path cost, not the actual path. We discuss the actual path below. This, like the Fibonacci-numbers example, is horribly slow because it too exhibits the overlapping sub-problems attribute. That is, it recomputes the same path costs over and over. However, we can compute it much faster in a bottom-up fashion if we store path costs in a two-dimensional array codice_38 rather than using a function. This avoids recomputation; all the values needed for array codice_38 are computed ahead of time only once. Precomputed values for codice_15 are simply looked-up whenever needed.\n\nWe also need to know what the actual shortest path is. To do this, we use another array codice_41; a \"predecessor array\". This array records the path to any square codice_42. The predecessor of codice_42 is modeled as an offset relative to the index (in codice_38) of the precomputed path cost of codice_42. To reconstruct the complete path, we lookup the predecessor of codice_42, then the predecessor of that square, then the predecessor of that square, and so on recursively, until we reach the starting square. Consider the following code:\n\nNow the rest is a simple matter of finding the minimum and printing it.\n\nIn genetics, sequence alignment is an important application where dynamic programming is essential. Typically, the problem consists of transforming one sequence into another using edit operations that replace, insert, or remove an element. Each operation has an associated cost, and the goal is to find the sequence of edits with the lowest total cost.\n\nThe problem can be stated naturally as a recursion, a sequence A is optimally edited into a sequence B by either:\n\n1. inserting the first character of B, and performing an optimal alignment of A and the tail of B\n2. deleting the first character of A, and performing the optimal alignment of the tail of A and B\n3. replacing the first character of A with the first character of B, and performing optimal alignments of the tails of A and B.\n\nThe partial alignments can be tabulated in a matrix, where cell (i,j) contains the cost of the optimal alignment of A[1..i] to B[1..j]. The cost in cell (i,j) can be calculated by adding the cost of the relevant operations to the cost of its neighboring cells, and selecting the optimum.\n\nDifferent variants exist, see Smith–Waterman algorithm and Needleman–Wunsch algorithm.\n\nThe Tower of Hanoi or Towers of Hanoi is a mathematical game or puzzle. It consists of three rods, and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical shape.\n\nThe objective of the puzzle is to move the entire stack to another rod, obeying the following rules:\n\n- Only one disk may be moved at a time.\n- Each move consists of taking the upper disk from one of the rods and sliding it onto another rod, on top of the other disks that may already be present on that rod.\n- No disk may be placed on top of a smaller disk.\n\nThe dynamic programming solution consists of solving the functional equation\n\nwhere n denotes the number of disks to be moved, h denotes the home rod, t denotes the target rod, not(h,t) denotes the third rod (neither h nor t), \";\" denotes concatenation, and\n\nFor n=1 the problem is trivial, namely S(1,h,t) = \"move a disk from rod h to rod t\" (there is only one disk left).\n\nThe number of moves required by this solution is 2 − 1. If the objective is to maximize the number of moves (without cycling) then the dynamic programming functional equation is slightly more complicated and 3 − 1 moves are required.\n\nThe following is a description of the instance of this famous puzzle involving N=2 eggs and a building with H=36 floors: \n\nTo derive a dynamic programming functional equation for this puzzle, let the state of the dynamic programming model be a pair s = (n,k), where\n\nFor instance, \"s\" = (2,6) indicates that two test eggs are available and 6 (consecutive) floors are yet to be tested. The initial state of the process is \"s\" = (\"N\",\"H\") where \"N\" denotes the number of test eggs available at the commencement of the experiment. The process terminates either when there are no more test eggs (\"n\" = 0) or when \"k\" = 0, whichever occurs first. If termination occurs at state \"s\" = (0,\"k\") and \"k\" > 0, then the test failed.\n\nNow, let\n\nThen it can be shown that\n\nwith \"W\"(\"n\",0) = 0 for all \"n\" > 0 and \"W\"(1,\"k\") = \"k\" for all \"k\". It is easy to solve this equation iteratively by systematically increasing the values of \"n\" and \"k\".\n\nAn interactive online facility is available for experimentation with this model as well as with other versions of this puzzle (e.g., when the objective is to minimize the expected value of the number of trials.)\n\nNotice that the above solution takes formula_88 time with a DP solution. This can be improved to formula_89 time by binary searching on the optimal formula_90 in the above recurrence, since formula_91 is increasing in formula_90 while formula_93 is decreasing in formula_90, thus a local minimum of formula_95 is a global minimum. Also, by storing the optimal formula_90 for each cell in the DP table and referring to its value for the previous cell, the optimal formula_90 for each cell can be found in constant time, improving it to formula_98 time. However, there is an even faster solution that involves a different parametrization of the problem:\n\nLet formula_18 be the total number of floors such that the eggs break when dropped from the formula_18th floor (The example above is equivalent to taking formula_101).\n\nLet formula_102 be the minimum floor from which the egg must be dropped to be broken.\n\nLet formula_103 be the maximum number of values of formula_102 that are distinguishable using formula_13 tries and formula_19 eggs.\n\nThen formula_107 for all formula_108.\n\nLet formula_109 be the floor from which the first egg is dropped in the optimal strategy.\n\nIf the first egg broke, formula_102 is from formula_111 to formula_109 and distinguishable using at most formula_113 tries and formula_114 eggs.\n\nIf the first egg did not break, formula_102 is from formula_116 to formula_18 and distinguishable using formula_113 tries and formula_19 eggs.\n\nTherefore, formula_120.\n\nThen the problem is equivalent to finding the minimum formula_90 such that formula_122.\n\nTo do so, we could compute formula_123 in order of increasing formula_13, which would take formula_125 time.\n\nThus, if we separately handle the case of formula_126, the algorithm would take formula_127 time.\n\nBut the recurrence relation can in fact be solved, giving formula_128, which can be computed in formula_129 time using the identity formula_130 for all formula_131.\n\nSince formula_132 for all formula_133, we can binary search on formula_13 to find formula_90, giving an formula_136 algorithm.\n\nMatrix chain multiplication is a well-known example that demonstrates utility of dynamic programming. For example, engineering applications often have to multiply a chain of matrices. It is not surprising to find matrices of large dimensions, for example 100×100. Therefore, our task is to multiply matrices . As we know from basic linear algebra, matrix multiplication is not commutative, but is associative; and we can multiply only two matrices at a time. So, we can multiply this chain of matrices in many different ways, for example:\n\nand so on. There are numerous ways to multiply this chain of matrices. They will all produce the same final result, however they will take more or less time to compute, based on which particular matrices are multiplied. If matrix A has dimensions m×n and matrix B has dimensions n×q, then matrix C=A×B will have dimensions m×q, and will require m*n*q scalar multiplications (using a simplistic matrix multiplication algorithm for purposes of illustration).\n\nFor example, let us multiply matrices A, B and C. Let us assume that their dimensions are m×n, n×p, and p×s, respectively. Matrix A×B×C will be of size m×s and can be calculated in two ways shown below:\n\n1. Ax(B×C) This order of matrix multiplication will require nps + mns scalar multiplications.\n2. (A×B)×C This order of matrix multiplication will require mnp + mps scalar calculations.\n\nLet us assume that m = 10, n = 100, p = 10 and s = 1000. So, the first way to multiply the chain will require 1,000,000 + 1,000,000 calculations. The second way will require only 10,000+100,000 calculations. Obviously, the second way is faster, and we should multiply the matrices using that arrangement of parenthesis.\n\nTherefore, our conclusion is that the order of parenthesis matters, and that our task is to find the optimal order of parenthesis.\n\nAt this point, we have several choices, one of which is to design a dynamic programming algorithm that will split the problem into overlapping problems and calculate the optimal arrangement of parenthesis. The dynamic programming solution is presented below.\n\nLet's call m[i,j] the minimum number of scalar multiplications needed to multiply a chain of matrices from matrix i to matrix j (i.e. A × ... × A, i.e. i<=j). We split the chain at some matrix k, such that i <= k < j, and try to find out which combination produces minimum m[i,j].\n\nThe formula is:\n\nwhere \"k\" ranges from \"i\" to \"j\" − 1.\n\n- is the row dimension of matrix i,\n- is the column dimension of matrix k,\n- is the column dimension of matrix j.\n\nThis formula can be coded as shown below, where input parameter \"chain\" is the chain of matrices, i.e. :\n\nSo far, we have calculated values for all possible , the minimum number of calculations to multiply a chain from matrix \"i\" to matrix \"j\", and we have recorded the corresponding \"split point\". For example, if we are multiplying chain , and it turns out that and , that means that the optimal placement of parenthesis for matrices 1 to 3 is and to multiply those matrices will require 100 scalar calculation.\n\nThis algorithm will produce \"tables\" \"m\"[, ] and \"s\"[, ] that will have entries for all possible values of i and j. The final solution for the entire chain is m[1, n], with corresponding split at s[1, n]. Unraveling the solution will be recursive, starting from the top and continuing until we reach the base case, i.e. multiplication of single matrices.\n\nTherefore, the next step is to actually split the chain, i.e. to place the parenthesis where they (optimally) belong. For this purpose we could use the following algorithm:\n\nOf course, this algorithm is not useful for actual multiplication. This algorithm is just a user-friendly way to see what the result looks like.\n\nTo actually multiply the matrices using the proper splits, we need the following algorithm:\nThe term \"dynamic programming\" was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another. By 1953, he refined this to the modern meaning, referring specifically to nesting smaller decision problems inside larger decisions, and the field was thereafter recognized by the IEEE as a systems analysis and engineering topic. Bellman's contribution is remembered in the name of the Bellman equation, a central result of dynamic programming which restates an optimization problem in recursive form.\n\nBellman explains the reasoning behind the term \"dynamic programming\" in his autobiography, \"Eye of the Hurricane: An Autobiography\":\n\nThe word \"dynamic\" was chosen by Bellman to capture the time-varying aspect of the problems, and because it sounded impressive. The word \"programming\" referred to the use of the method to find an optimal \"program\", in the sense of a military schedule for training or logistics. This usage is the same as that in the phrases \"linear programming\" and \"mathematical programming\", a synonym for mathematical optimization.\n\nThe above explanation of the origin of the term is lacking. As Russell and Norvig in their book have written, referring to the above story: \"This cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953.\" Also, there is a comment in a speech by Harold J. Kushner, where he remembers Bellman. Quoting Kushner as he speaks of Bellman: \"On the other hand, when I asked him the same question, he replied that he was trying to upstage Dantzig's linear programming by adding dynamic. Perhaps both motivations were true.\"\n\n- Recurrent solutions to lattice models for protein-DNA binding\n- Backward induction as a solution method for finite-horizon discrete-time dynamic optimization problems\n- Method of undetermined coefficients can be used to solve the Bellman equation in infinite-horizon, discrete-time, discounted, time-invariant dynamic optimization problems\n- Many string algorithms including longest common subsequence, longest increasing subsequence, longest common substring, Levenshtein distance (edit distance)\n- Many algorithmic problems on graphs can be solved efficiently for graphs of bounded treewidth or bounded clique-width by using dynamic programming on a tree decomposition of the graph.\n- The Cocke–Younger–Kasami (CYK) algorithm which determines whether and how a given string can be generated by a given context-free grammar\n- Knuth's word wrapping algorithm that minimizes raggedness when word wrapping text\n- The use of transposition tables and refutation tables in computer chess\n- The Viterbi algorithm (used for hidden Markov models, and particularly in part of speech tagging)\n- The Earley algorithm (a type of chart parser)\n- The Needleman–Wunsch algorithm and other algorithms used in bioinformatics, including sequence alignment, structural alignment, RNA structure prediction\n- Floyd's all-pairs shortest path algorithm\n- Optimizing the order for chain matrix multiplication\n- Pseudo-polynomial time algorithms for the subset sum, knapsack and partition problems\n- The dynamic time warping algorithm for computing the global distance between two time series\n- The Selinger (a.k.a. System R) algorithm for relational database query optimization\n- De Boor algorithm for evaluating B-spline curves\n- Duckworth–Lewis method for resolving the problem when games of cricket are interrupted\n- The value iteration method for solving Markov decision processes\n- Some graphic image edge following selection methods such as the \"magnet\" selection tool in Photoshop\n- Some methods for solving interval scheduling problems\n- Some methods for solving the travelling salesman problem, either exactly (in exponential time) or approximately (e.g. via the bitonic tour)\n- Recursive least squares method\n- Beat tracking in music information retrieval\n- Adaptive-critic training strategy for artificial neural networks\n- Stereo algorithms for solving the correspondence problem used in stereo vision\n- Seam carving (content-aware image resizing)\n- The Bellman–Ford algorithm for finding the shortest distance in a graph\n- Some approximate solution methods for the linear search problem\n- Kadane's algorithm for the maximum subarray problem\n- Optimization of electric generation expansion plans in the Wein Automatic System Planning (WASP) package\n\n", "related": "\n- Convexity in economics\n- Greedy algorithm\n- Non-convexity (economics)\n- Stochastic programming\n- Stochastic dynamic programming\n\n- . An accessible introduction to dynamic programming in economics. The link contains sample programs.\n- . Includes an extensive bibliography of the literature in the area, up to the year 1954.\n- . Dover paperback edition (2003), .\n- . Especially pp. 323–69.\n- .\n\n- An Introduction to Dynamic Programming\n- Dynamic Optimization Online Course\n- A Tutorial on Dynamic programming\n- MIT course on algorithms – Includes a video lecture on DP along with lecture notes, see lecture 15.\n- More DP Notes\n- King, Ian, 2002 (1987), \"A Simple Introduction to Dynamic Programming in Macroeconomic Models.\" An introduction to dynamic programming as an important tool in economic theory.\n- Dynamic Programming: from novice to advanced A TopCoder.com article by Dumitru on Dynamic Programming\n- Algebraic Dynamic Programming – a formalized framework for dynamic programming, including an entry-level course to DP, University of Bielefeld\n- Dreyfus, Stuart, \"Richard Bellman on the birth of Dynamic Programming.\"\n- Dynamic programming tutorial\n- A Gentle Introduction to Dynamic Programming and the Viterbi Algorithm\n- Tabled Prolog BProlog and XSB\n- Online interactive dynamic programming modules including, shortest path, traveling salesman, knapsack, false coin, egg dropping, bridge and torch, replacement, chained matrix products, and critical path problem.\n"}
{"id": "730585", "url": "https://en.wikipedia.org/wiki?curid=730585", "title": "Hamilton–Jacobi–Bellman equation", "text": "Hamilton–Jacobi–Bellman equation\n\nIn optimal control theory, the Hamilton–Jacobi–Bellman (HJB) equation gives a necessary and sufficient condition for optimality of a control with respect to a loss function. It is, in general, a nonlinear partial differential equation in the value function, which means its solution the value function itself. Once the solution is known, it can be used to obtain the optimal control by taking the maximizer/minimizer of the Hamiltonian involved in the HJB equation.\n\nThe equation is a result of the theory of dynamic programming which was pioneered in the 1950s by Richard Bellman and coworkers. The connection to the Hamilton–Jacobi equation from classical physics was first drawn by Rudolf Kálmán. In discrete-time problems, the equation is usually referred to as the Bellman equation.\n\nWhile classical variational problems, for example the brachistochrone problem, can be solved using the Hamilton–Jacobi–Bellman equation, the method can be applied to a broader spectrum of problems. Further it can be generalized to stochastic systems, in which case the HJB equation is a second-order partial differential equation. A major drawback, however, is that the HJB equation admits classical solutions only for a sufficiently smooth value function, which is not guaranteed in most situations. Instead, the notion of a viscosity solution is required, in which conventional derivatives are replaced by (set-valued) subderivatives.\n\nConsider the following problem in deterministic optimal control over the time period formula_1:\n\nwhere C[ ] is the scalar cost rate function and \"D\"[ ] is a function that gives the economic value or utility at the final state, \"x\"(\"t\") is the system state vector, \"x\"(0) is assumed given, and \"u\"(\"t\") for 0 ≤ \"t\" ≤ \"T\" is the control vector that we are trying to find.\n\nThe system must also be subject to\n\nwhere \"F\"[ ] gives the vector determining physical evolution of the state vector over time.\n\nFor this simple system (letting formula_4), the Hamilton–Jacobi–Bellman partial differential equation is\n\nsubject to the terminal condition\n\nwhere formula_7 denotes the partial derivative of formula_8 with respect to the time variable formula_9. Here formula_10 denotes the dot product of the vectors formula_11 and formula_12 and formula_13 the gradient of formula_8 with respect to the variables formula_15.\n\nThe unknown scalar formula_16 in the above partial differential equation is the Bellman value function, which represents the cost incurred from starting in state formula_15 at time formula_9 and controlling the system optimally from then until time formula_19.\n\nIntuitively, the HJB equation can be derived as follows. If formula_20 is the optimal cost-to-go function (also called the 'value function'), then by Richard Bellman's principle of optimality, going from time \"t\" to \"t\" + \"dt\", we have\n\nNote that the Taylor expansion of the first term on the right-hand side is\n\nwhere formula_23 denotes the terms in the Taylor expansion of higher order than one in little-\"o\" notation. Then if we subtract formula_20 from both sides, divide by \"dt\", and take the limit as \"dt\" approaches zero, we obtain the HJB equation defined above. \n\nThe HJB equation is usually solved backwards in time, starting from formula_25 and ending at formula_26.\n\nWhen solved over the whole of state space and formula_27 is continuously differentiable, the HJB equation is a necessary and sufficient condition for an optimum when the terminal state is unconstrained. If we can solve for formula_8 then we can find from it a control formula_29 that achieves the minimum cost.\n\nIn general case, the HJB equation does not have a classical (smooth) solution. Several notions of generalized solutions have been developed to cover such situations, including viscosity solution (Pierre-Louis Lions and Michael Crandall), minimax solution (), and others.\n\nThe idea of solving a control problem by applying Bellman's principle of optimality and then working out backwards in time an optimizing strategy can be generalized to stochastic control problems. Consider similar as above\n\nnow with formula_31 the stochastic process to optimize and formula_32 the steering. By first using Bellman and then expanding formula_33 with Itô's rule, one finds the stochastic HJB equation\n\nwhere formula_35 represents the stochastic differentiation operator, and subject to the terminal condition\n\nNote that the randomness has disappeared. In this case a solution formula_37 of the latter does not necessarily solve the primal problem, it is a candidate only and a further verifying argument is required. This technique is widely used in Financial Mathematics to determine optimal investment strategies in the market (see for example Merton's portfolio problem).\n\nAs an example, we can look at a system with linear stochastic dynamics and quadratic cost. If the system dynamics is given by\nand the cost accumulates at rate formula_39, the HJB equation is given by\nwith optimal action given by\nAssuming a quadratic form for the value function, we obtain the usual Riccati equation for the Hessian of the value function as is usual for Linear-quadratic-Gaussian control.\n\n", "related": "\n- Bellman equation, discrete-time counterpart of the Hamilton–Jacobi–Bellman equation.\n- Pontryagin's maximum principle, necessary but not sufficient condition for optimum, by maximizing a Hamiltonian, but this has the advantage over HJB of only needing to be satisfied over the single trajectory being considered.\n\n"}
{"id": "1125883", "url": "https://en.wikipedia.org/wiki?curid=1125883", "title": "Markov decision process", "text": "Markov decision process\n\nA Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, \"Dynamic Programming and Markov Processes\". They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of the Markov chains.\n\nAt each time step, the process is in some state formula_1, and the decision maker may choose any action formula_2 that is available in state formula_1. The process responds at the next time step by randomly moving into a new state formula_4, and giving the decision maker a corresponding reward formula_5.\n\nThe probability that the process moves into its new state formula_4 is influenced by the chosen action. Specifically, it is given by the state transition function formula_7. Thus, the next state formula_4 depends on the current state formula_1 and the decision maker's action formula_2. But given formula_1 and formula_2, it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.\n\nMarkov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. \"wait\") and all rewards are the same (e.g. \"zero\"), a Markov decision process reduces to a Markov chain.\n\nA Markov decision process is a 4-tuple formula_13, where\n\n- formula_14 is a finite set of states,\n- formula_15 is a finite set of actions (alternatively, formula_16 is the finite set of actions available from state formula_1),\n- formula_18 is the probability that action formula_2 in state formula_1 at time formula_21 will lead to state formula_4 at time formula_23,\n- formula_24 is the immediate reward (or expected immediate reward) received after transitioning from state formula_1 to state formula_4, due to action formula_2\n\nThe core problem of MDPs is to find a \"policy\" for the decision maker: a function formula_30 that specifies the action formula_31 that the decision maker will choose when in state formula_1. Once a Markov decision process is combined with a policy in this way, this fixes the action for each state and the resulting combination behaves like a Markov chain (since the action chosen in state formula_1 is completely determined by formula_31 and formula_35 reduces to formula_36, a Markov transition matrix).\n\nThe goal is to choose a policy formula_30 that will maximize some cumulative function of the random rewards, typically the expected discounted sum over a potentially infinite horizon:\n\nwhere formula_41 is the discount factor satisfying formula_42, which is usually close to 1. (For example, formula_43 for some discount rate r.)\n\nBecause of the Markov property, the optimal policy for this particular problem can indeed be written as a function of formula_1 only, as assumed above.\n\nThe discount-factor motivates the decision maker to favor taking actions early, rather not postpone them indefinitely.\n\nThe solution for an MDP is a policy which describes the best action for each state in the MDP, known as the optimal policy. This optimal policy can be found through a variety of methods, like dynamic programming.\n\nSome dynamic programming solutions require knowledge of the state transition function formula_45 and the reward function formula_46. Others can solve for the optimal policy of an MDP using experimentation alone.\n\nConsider the case in which state transition function formula_45 and reward function formula_46 for an MDP are given, and we seek the optimal policy formula_49 that maximizes the expected discounted reward.\n\nThe standard family of algorithms to calculate this optimal policy requires storage for two arrays indexed by state: \"value\" formula_50, which contains real values, and \"policy\" formula_30, which contains actions. At the end of the algorithm, formula_30 will contain the solution and formula_53 will contain the discounted sum of the rewards to be earned (on average) by following that solution from state formula_1.\n\nThe algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update \na new estimation of the optimal policy and state value using an older estimation of those values.\n\nTheir order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution.\n\nIn value iteration , which is also called backward induction,\nthe formula_30 function is not used; instead, the value of formula_31 is calculated within formula_53 whenever it is needed. Substituting the calculation of formula_31 into the calculation of formula_53 gives the combined step:\n\nwhere formula_63 is the iteration number. Value iteration starts at formula_64 and formula_65 as a guess of the value function. It then iterates, repeatedly computing formula_66 for all states formula_1, until formula_50 converges with the left-hand side equal to the right-hand side (which is the \"Bellman equation\" for this problem). Lloyd Shapley's 1953 paper on stochastic games included as a special case the value iteration method for MDPs, but this was recognized only later on.\n\nIn policy iteration , step one is performed once, and then step two is repeated until it converges. Then step one is again performed once and so on.\n\nInstead of repeating step two to convergence, it may be formulated and solved as a set of linear equations. These equations are merely obtained by making formula_69 in the step two equation. Thus, repeating step two to convergence can be interpreted as solving the linear equations by Relaxation (iterative method)\n\nThis variant has the advantage that there is a definite stopping condition: when the array formula_30 does not change in the course of applying step 1 to all states, the algorithm is completed.\n\nPolicy iteration is usually slower than value iteration for a large number of possible states.\n\nIn modified policy iteration (; ), step one is performed once, and then step two is repeated several times. Then step one is again performed once and so on.\n\nIn this variant, the steps are preferentially applied to states which are in some way important – whether based on the algorithm (there were large changes in formula_50 or formula_30 around those states recently) or based on use (those states are near the starting state, or otherwise of interest to the person or program using the algorithm).\n\nA Markov decision process is a stochastic game with only one player.\n\nThe solution above assumes that the state formula_1 is known when action is to be taken; otherwise formula_31 cannot be calculated. When this assumption is not true, the problem is called a partially observable Markov decision process or POMDP.\n\nA major advance in this area was provided by Burnetas and Katehakis in \"Optimal adaptive policies for Markov decision processes\". In this work, a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward were constructed under the assumptions of finite state-action spaces and irreducibility of the transition law. These policies prescribe that the choice of actions, at each state and time period, should be based on indices that are inflations of the right-hand side of the estimated average reward optimality equations.\n\nIf the probabilities or rewards are unknown, the problem is one of reinforcement learning.\n\nFor this purpose it is useful to define a further function, which corresponds to taking the action formula_2 and then continuing optimally (or according to whatever policy one currently has):\n\nWhile this function is also unknown, experience during learning is based on formula_77 pairs (together with the outcome formula_4; that is, \"I was in state formula_1 and I tried doing formula_2 and formula_4 happened\"). Thus, one has an array formula_82 and uses experience to update it directly. This is known as Q-learning.\n\nReinforcement learning can solve Markov decision processes without explicit specification of the transition probabilities; the values of the transition probabilities are needed in value and policy iteration. In reinforcement learning, instead of explicit specification of the transition probabilities, the transition probabilities are accessed through a simulator that is typically restarted many times from a uniformly random initial state. Reinforcement learning can also be combined with function approximation to address problems with a very large number of states.\n\nAnother application of MDP process in machine learning theory is called learning automata. This is also one type of reinforcement learning if the environment is stochastic. The first detail learning automata paper is surveyed by Narendra and Thathachar (1974), which were originally described explicitly as finite state automata. Similar to reinforcement learning, a learning automata algorithm also has the advantage of solving the problem when probability or rewards are unknown. The difference between learning automata and Q-learning is that the former technique omits the memory of Q-values, but updates the action probability directly to find the learning result. Learning automata is a learning scheme with a rigorous proof of convergence.\n\nIn learning automata theory, a stochastic automaton consists of:\n- a set \"x\" of possible inputs,\n- a set Φ = { Φ, ..., Φ } of possible internal states,\n- a set α = { α, ..., α } of possible outputs, or actions, with \"r\" ≤ \"s\",\n- an initial state probability vector \"p\"(0) = ≪ \"p\"(0), ..., \"p\"(0) ≫,\n- a computable function \"A\" which after each time step \"t\" generates \"p\"(\"t\" + 1) from \"p\"(\"t\"), the current input, and the current state, and\n- a function \"G\": Φ → α which generates the output at each time step.\nThe states of such an automaton correspond to the states of a \"discrete-state discrete-parameter Markov process\". At each time step \"t\" = 0,1,2,3..., the automaton reads an input from its environment, updates P(\"t\") to P(\"t\" + 1) by \"A\", randomly chooses a successor state according to the probabilities P(\"t\" + 1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton.\n\nOther than the rewards, a Markov decision process formula_83 can be understood in terms of Category theory. Namely, let formula_84 denote the free monoid with generating set \"A\". Let Dist denote the Kleisli category of the Giry monad. Then a functor formula_85 encodes both the set \"S\" of states and the probability function \"P\".\n\nIn this way, Markov decision processes could be generalized from monoids (categories with one object) to arbitrary categories. One can call the result formula_86 a \"context-dependent Markov decision process\", because moving from one object to another in formula_87 changes the set of available actions and the set of possible states.\n\nIn the MDPs, an optimal policy is a policy which maximizes the probability-weighted summation of future rewards. Therefore, an optimal policy consists of several actions which belong to a finite set of actions. In fuzzy Markov decision processes (FMDPs), first, the value function is computed as regular MDPs (i.e., with a finite set of actions); then, the policy is extracted by a fuzzy inference system. In other words, the value function is utilized as an input for the fuzzy inference system, and the policy is the output of the fuzzy inference system.\n\nIn discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes, decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision making process for a system that has continuous dynamics, i.e., the system dynamics is defined by partial differential equations (PDEs).\n\nIn order to discuss the continuous-time Markov decision process, we introduce two sets of notations:\n\nIf the state space and action space are finite,\n- formula_88: State space;\n- formula_84: Action space;\n- formula_90: formula_91, transition rate function;\n- formula_92: formula_93, a reward function.\n\nIf the state space and action space are continuous,\n- formula_94: state space;\n- formula_95: space of possible control;\n- formula_96: formula_97, a transition rate function;\n- formula_98: formula_99, a reward rate function such that formula_100, where formula_101 is the reward function we discussed in previous case.\n\nLike the discrete-time Markov decision processes, in continuous-time Markov decision processes we want to find the optimal \"policy\" or \"control\" which could give us the optimal expected integrated reward:\nwhere formula_103\n\nIf the state space and action space are finite, we could use linear programming to find the optimal policy, which was one of the earliest approaches applied. Here we only consider the ergodic model, which means our continuous-time MDP becomes an ergodic continuous-time Markov chain under a stationary policy. Under this assumption, although the decision maker can make a decision at any time at the current state, he could not benefit more by taking more than one action. It is better for him to take an action only at the time when system is transitioning from the current state to another state. Under some conditions,(for detail check Corollary 3.14 of \"Continuous-Time Markov Decision Processes\"), if our optimal value function formula_104 is independent of state formula_63, we will have the following inequality:\nIf there exists a function formula_107, then formula_108 will be the smallest formula_109 satisfying the above equation. In order to find formula_108, we could use the following linear programming model:\n- Primal linear program(P-LP)\n- Dual linear program(D-LP)\nformula_113 is a feasible solution to the D-LP if formula_113 is\nnonnative and satisfied the constraints in the D-LP problem. A\nfeasible solution formula_115 to the D-LP is said to be an optimal\nsolution if\nfor all feasible solution formula_113 to the D-LP.\nOnce we have found the optimal solution formula_115, we can use it to establish the optimal policies.\n\nIn continuous-time MDP, if the state space and action space are continuous, the optimal criterion could be found by solving Hamilton–Jacobi–Bellman (HJB) partial differential equation.\nIn order to discuss the HJB equation, we need to reformulate\nour problem\n\nformula_120 is the terminal reward function, formula_121 is the\nsystem state vector, formula_122 is the system control vector we try to\nfind. formula_123 shows how the state vector changes over time.\nThe Hamilton–Jacobi–Bellman equation is as follows:\nWe could solve the equation to find the optimal control formula_122, which could give us the optimal value formula_104\n\nContinuous-time Markov decision processes have applications in queueing systems, epidemic processes, and population processes.\n\nThe terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor formula_127 or formula_128, while the other focuses on minimization problems from engineering and navigation, using the terms control, cost, cost-to-go, and calling the discount factor formula_129. In addition, the notation for the transition probability varies.\n\nIn addition, transition probability is sometimes written formula_130, formula_131 or, rarely, formula_132\n\nConstrained Markov decision processes (CMDPs) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs.\n\n- There are multiple costs incurred after applying an action instead of one.\n- CMDPs are solved with linear programs only, and dynamic programming does not work.\n- The final policy depends on the starting state.\n\nThere are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics.\n\n", "related": "\n- Probabilistic automata\n- Quantum finite automata\n- Partially observable Markov decision process\n- Dynamic programming\n- Bellman equation for applications to economics.\n- Hamilton–Jacobi–Bellman equation\n- Optimal control\n- Recursive economics\n- Mabinogion sheep problem\n- Stochastic games\n- Q-learning\n\n- Appendix contains abridged\n- MDP Toolbox for MATLAB, GNU Octave, Scilab and R The Markov Decision Processes (MDP) Toolbox.\n- MDP Toolbox for Matlab – An excellent tutorial and Matlab toolbox for working with MDPs.\n- MDP Toolbox for Python A package for solving MDPs\n- POMDPs.jl A flexible interface for defining and solving MDPs in Julia with a variety of solvers\n- Reinforcement Learning An Introduction by Richard S. Sutton and Andrew G. Barto\n- SPUDD A structured MDP solver for download by Jesse Hoey\n- Learning to Solve Markovian Decision Processes by Satinder P. Singh\n- Optimal Adaptive Policies for Markov Decision Processes by Burnetas and Katehakis (1997).\n"}
{"id": "362565", "url": "https://en.wikipedia.org/wiki?curid=362565", "title": "Optimal control", "text": "Optimal control\n\nOptimal control theory is a branch of applied mathematics that deals with finding a control law for a dynamical system over a period of time such that an objective function is optimized. It has numerous applications in both science and engineering. For example, the dynamical system might be a spacecraft with controls corresponding to rocket thrusters, and the objective might be to reach the moon with minimum fuel expenditure. Or the dynamical system could be a nation's economy, with the objective to minimize unemployment; the controls in this case could be fiscal and monetary policy.\n\nOptimal control is an extension of the calculus of variations, and is a mathematical optimization method for deriving control policies. The method is largely due to the work of Lev Pontryagin and Richard Bellman in the 1950s, after contributions to calculus of variations by Edward J. McShane. Optimal control can be seen as a control strategy in control theory.\n\nOptimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved. A control problem includes a cost functional that is a function of state and control variables. An optimal control is a set of differential equations describing the paths of the control variables that minimize the cost function. The optimal control can be derived using Pontryagin's maximum principle (a necessary condition also known as Pontryagin's minimum principle or simply Pontryagin's Principle), or by solving the Hamilton–Jacobi–Bellman equation (a sufficient condition).\n\nWe begin with a simple example. Consider a car traveling in a straight line on a hilly road. The question is, how should the driver press the accelerator pedal in order to \"minimize\" the total traveling time? In this example, the term \"control law\" refers specifically to the way in which the driver presses the accelerator and shifts the gears. The \"system\" consists of both the car and the road, and the \"optimality criterion\" is the minimization of the total traveling time. Control problems usually include ancillary constraints. For example, the amount of available fuel might be limited, the accelerator pedal cannot be pushed through the floor of the car, speed limits, etc.\n\nA proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, and initial conditions of the system. Constraints are often interchangeable with the cost function.\n\nAnother related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time and fuel.\n\nA more abstract framework goes as follows. Minimize the continuous-time cost functional\n\nsubject to the first-order dynamic constraints (the state equation)\n\nthe algebraic \"path constraints\"\n\nand the boundary conditions\n\nwhere formula_5 is the \"state\", formula_6 is the \"control\", formula_7 is the independent variable (generally speaking, time), formula_8 is the initial time, and formula_9 is the terminal time. The terms formula_10 and formula_11 are called the \"endpoint cost \" and \"Lagrangian\", respectively. Furthermore, it is noted that the path constraints are in general \"inequality\" constraints and thus may not be active (i.e., equal to zero) at the optimal solution. It is also noted that the optimal control problem as stated above may have multiple solutions (i.e., the solution may not be unique). Thus, it is most often the case that any solution formula_12 to the optimal control problem is \"locally minimizing\".\n\nA special case of the general nonlinear optimal control problem given in the previous section is the \"linear quadratic\" (LQ) optimal control problem. The LQ problem is stated as follows. Minimize the \"quadratic\" continuous-time cost functional\n\nSubject to the \"linear\" first-order dynamic constraints\n\nand the initial condition\n\nA particular form of the LQ problem that arises in many control system problems is that of the \"linear quadratic regulator\" (LQR) where all of the matrices (i.e., formula_16, formula_17, formula_18, and formula_19) are \"constant\", the initial time is arbitrarily set to zero, and the terminal time is taken in the limit formula_20 (this last assumption is what is known as \"infinite horizon\"). The LQR problem is stated as follows. Minimize the infinite horizon quadratic continuous-time cost functional\n\nSubject to the \"linear time-invariant\" first-order dynamic constraints\n\nand the initial condition\n\nIn the finite-horizon case the matrices are restricted in that formula_18 and formula_19 are positive semi-definite and positive definite, respectively. In the infinite-horizon case, however, the matrices formula_18 and formula_19 are not only positive-semidefinite and positive-definite, respectively, but are also \"constant\". These additional restrictions on\nformula_18 and formula_19 in the infinite-horizon case are enforced to ensure that the cost functional remains positive. Furthermore, in order to ensure that the cost function is \"bounded\", the additional restriction is imposed that the pair formula_30 is \"controllable\". Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the \"control energy\" (measured as a quadratic form).\n\nThe infinite horizon problem (i.e., LQR) may seem overly restrictive and essentially useless because it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved \"after\" the zero output one is. In fact, it can be proved that this secondary LQR problem can be solved in a very straightforward manner. It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form\n\nwhere formula_32 is a properly dimensioned matrix, given as\n\nand formula_34 is the solution of the differential Riccati equation. The differential Riccati equation is given as\n\nFor the finite horizon LQ problem, the Riccati equation is integrated backward in time using the terminal boundary condition\n\nFor the infinite horizon LQR problem, the differential Riccati equation is replaced with the \"algebraic\" Riccati equation (ARE) given as\n\nUnderstanding that the ARE arises from infinite horizon problem, the matrices formula_16, formula_17, formula_18, and formula_19 are all \"constant\". It is noted that there are in general multiple solutions to the algebraic Riccati equation and the \"positive definite\" (or positive semi-definite) solution is the one that is used to compute the feedback gain. The LQ (LQR) problem was elegantly solved by Rudolf Kalman.\n\nOptimal control problems are generally nonlinear and therefore, generally do not have analytic solutions (e.g., like the linear-quadratic optimal control problem). As a result, it is necessary to employ numerical methods to solve optimal control problems. In the early years of optimal control ( 1950s to 1980s) the favored approach for solving optimal control problems was that of \"indirect methods\". In an indirect method, the calculus of variations is employed to obtain the first-order optimality conditions. These conditions result in a two-point (or, in the case of a complex problem, a multi-point) boundary-value problem. This boundary-value problem actually has a special structure because it arises from taking the derivative of a Hamiltonian. Thus, the resulting dynamical system is a Hamiltonian system of the form\n\nwhere\n\nis the \"augmented Hamiltonian\" and in an indirect method, the boundary-value problem is solved (using the appropriate boundary or \"transversality\" conditions). The beauty of using an indirect method is that the state and adjoint (i.e., formula_44) are solved for and the resulting solution is readily verified to be an extremal trajectory. The disadvantage of indirect methods is that the boundary-value problem is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints). A well-known software program that implements indirect methods is BNDSCO.\n\nThe approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called \"direct methods\". In a direct method, the state and/or control are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization). Simultaneously, the cost functional is approximated as a \"cost function\". Then, the coefficients of the function approximations are treated as optimization variables and the problem is \"transcribed\" to a nonlinear optimization problem of the form:\n\nMinimize\n\nsubject to the algebraic constraints\n\nDepending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or quasilinearization method), moderate (e.g. pseudospectral optimal control) or may be quite large (e.g., a direct collocation method). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands to tens of thousands of variables and constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the boundary-value problem. It is, however, the fact that the NLP is easier to solve than the boundary-value problem. The reason for the relative ease of computation, particularly of a direct collocation method, is that the NLP is \"sparse\" and many well-known software programs exist (e.g., SNOPT) to solve large sparse NLPs. As a result, the range of problems that can be solved via direct methods (particularly direct \"collocation methods\" which are very popular these days) is significantly larger than the range of problems that can be solved via indirect methods. In fact, direct methods have become so popular these days that many people have written elaborate software programs that employ these methods. In particular, many such programs include \"DIRCOL\", SOCS, OTIS, GESOP/ASTOS, DITAN. and PyGMO/PyKEP. In recent years, due to the advent of the MATLAB programming language, optimal control software in MATLAB has become more common. Examples of academically developed MATLAB software tools implementing direct methods include \"RIOTS\",\"DIDO\", \"DIRECT\", FALCON.m , and \"GPOPS,\" while an example of an industry developed MATLAB tool is \"PROPT\". These software tools have increased significantly the opportunity for people to explore complex optimal control problems both for academic research and industrial problems. Finally, it is noted that general-purpose MATLAB optimization environments such as TOMLAB have made coding complex optimal control problems significantly easier than was previously possible in languages such as C and FORTRAN.\n\nThe examples thus far have shown continuous time systems and control solutions. In fact, as optimal control solutions are now often implemented digitally, contemporary control theory is now primarily concerned with discrete time systems and solutions. The Theory of Consistent Approximations provides conditions under which solutions to a series of increasingly accurate discretized optimal control problem converge to the solution of the original, continuous-time problem. Not all discretization methods have this property, even seemingly obvious ones. For instance, using a variable step-size routine to integrate the problem's dynamic equations may generate a gradient which does not converge to zero (or point in the right direction) as the solution is approached. The direct method \"RIOTS\" is based on the Theory of Consistent Approximation.\n\nA common solution strategy in many optimal control problems is to solve for the costate (sometimes called the shadow price) formula_47. The costate summarizes in one number the marginal value of expanding or contracting the state variable next turn. The marginal value is not only the gains accruing to it next turn but associated with the duration of the program. It is nice when formula_47 can be solved analytically, but usually, the most one can do is describe it sufficiently well that the intuition can grasp the character of the solution and an equation solver can solve numerically for the values.\n\nHaving obtained formula_47, the turn-t optimal value for the control can usually be solved as a differential equation conditional on knowledge of formula_47. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly. Usually, the strategy is to solve for thresholds and regions that characterize the optimal control and use a numerical solver to isolate the actual choice values in time.\n\nConsider the problem of a mine owner who must decide at what rate to extract ore from their mine. They own rights to the ore from date formula_51 to date formula_52. At date formula_51 there is formula_54 ore in the ground, and the time-dependent amount of ore formula_55 left in the ground declines at the rate of formula_56 that the mine owner extracts it. The mine owner extracts ore at cost formula_57 (the cost of extraction increasing with the square of the extraction speed and the inverse of the amount of ore left) and sells ore at a constant price formula_58. Any ore left in the ground at time formula_52 cannot be sold and has no value (there is no \"scrap value\"). The owner chooses the rate of extraction varying with time formula_56 to maximize profits over the period of ownership with no time discounting.\n\n", "related": "\n- Active inference\n- Bellman equation\n- Bellman pseudospectral method\n- Brachistochrone\n- DIDO\n- DNSS point\n- Dynamic programming\n- Gauss pseudospectral method\n- Generalized filtering\n- GPOPS-II\n- JModelica.org (Modelica-based open source platform for dynamic optimization)\n- Kalman filter\n- Linear-quadratic regulator\n- Model Predictive Control\n- PID controller\n- PROPT (Optimal Control Software for MATLAB)\n- Pseudospectral optimal control\n- Pursuit-evasion games\n- Sliding mode control\n- SNOPT\n- Stochastic control\n- Trajectory optimization\n\n- Optimal Control Course Online\n- Dr. Benoît CHACHUAT: Automatic Control Laboratory – Nonlinear Programming, Calculus of Variations and Optimal Control.\n- DIDO - MATLAB tool for optimal control\n- GEKKO - Python package for optimal control\n- GESOP – Graphical Environment for Simulation and OPtimization\n\n- GPOPS-II – General-Purpose MATLAB Optimal Control Software\n- PROPT – MATLAB Optimal Control Software\n- OpenOCL – Open Optimal Control Library\n- Elmer G. Wiens: Optimal Control – Applications of Optimal Control Theory Using the Pontryagin Maximum Principle with interactive models.\n- Pontryagin's Principle Illustrated with Examples\n- On Optimal Control by Yu-Chi Ho\n- Pseudospectral optimal control: Part 1\n- Pseudospectral optimal control: Part 2\n"}
{"id": "243102", "url": "https://en.wikipedia.org/wiki?curid=243102", "title": "Optimal substructure", "text": "Optimal substructure\n\nIn computer science, a problem is said to have optimal substructure if an optimal solution can be constructed from optimal solutions of its subproblems. This property is used to determine the usefulness of dynamic programming and greedy algorithms for a problem. \n\nTypically, a greedy algorithm is used to solve a problem with optimal substructure if it can be proven by induction that this is optimal at each step. Otherwise, provided the problem exhibits overlapping subproblems as well, dynamic programming is used. If there are no appropriate greedy algorithms and the problem fails to exhibit overlapping subproblems, often a lengthy but straightforward search of the solution space is the best alternative.\n\nIn the application of dynamic programming to mathematical optimization, Richard Bellman's Principle of Optimality is based on the idea that in order to solve a dynamic optimization problem from some starting period \"t\" to some ending period \"T\", one implicitly has to solve subproblems starting from later dates \"s\", where \"t\n", "related": "NONE"}
{"id": "1281850", "url": "https://en.wikipedia.org/wiki?curid=1281850", "title": "Q-learning", "text": "Q-learning\n\n\"Q\"-learning is a model-free reinforcement learning algorithm to learn a policy telling an agent what action to take under what circumstances. It does not require a model (hence the connotation \"model-free\") of the environment, and it can handle problems with stochastic transitions and rewards, without requiring adaptations.\n\nFor any finite Markov decision process (FMDP), \"Q\"-learning finds an optimal policy in the sense maximizing the expected value of the total reward over any and all successive steps, starting from the current state. \"Q\"-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. \"Q\" names the function that returns the reward used to provide the reinforcement and can be said to stand for the \"quality\" of an action taken in a given state.\n\nReinforcement learning involves an agent, a set of \"states\" , and a set of \"actions\" per state. By performing an action formula_1, the agent transitions from state to state. Executing an action in a specific state provides the agent with a \"reward\" (a numerical score).\n\nThe goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state.\n\nAs an example, consider the process of boarding a train, in which the reward is measured by the negative of the total time spent boarding (alternatively, the cost of boarding the train is equal to the boarding time). One strategy is to enter the train door as soon as they open, minimizing the initial wait time for yourself. If the train is crowded, however, then you will have a slow entry after the initial action of entering the door as people are fighting you to depart the train as you attempt to board. The total boarding time, or cost, is then:\n\n- 0 seconds wait time + 15 seconds fight time\n\nOn the next day, by random chance (exploration), you decide to wait and let other people depart first. This initially results in a longer wait time. However, time fighting other passengers is less. Overall, this path has a higher reward than that of the previous day, since the total boarding time is now:\n\n- 5 second wait time + 0 second fight time.\n\nThrough exploration, despite the initial (patient) action resulting in a larger cost (or negative reward) than in the forceful strategy, the overall cost is lower, thus revealing a more rewarding strategy.\n\nThe weight for a step from a state formula_2 steps into the future is calculated as formula_3, where formula_4 (the \"discount factor\") is a number between 0 and 1 (formula_5) and has the effect of valuing rewards received earlier higher than those received later (reflecting the value of a \"good start\"). formula_6 may also be interpreted as the probability to succeed (or survive) at every step formula_2.\n\nThe algorithm, therefore, has a function that calculates the quality of a state-action combination:\n\nBefore learning begins, is initialized to a possibly arbitrary fixed value (chosen by the programmer). Then, at each time formula_9 the agent selects an action formula_10, observes a reward formula_11, enters a new state formula_12 (that may depend on both the previous state formula_13 and the selected action), and formula_14 is updated. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the old value and the new information:\n\nwhere \"formula_16\" is the reward received when moving from the state formula_17 to the state formula_12, and formula_19 is the learning rate (formula_20).\n\nAn episode of the algorithm ends when state formula_12 is a final or \"terminal state\". However, \"Q\"-learning can also learn in non-episodic tasks. If the discount factor is lower than 1, the action values are finite even if the problem can contain infinite loops.\n\nFor all final states formula_22, formula_23 is never updated, but is set to the reward value formula_24 observed for state formula_22. In most cases, formula_26 can be taken to equal zero.\n\nThe learning rate or \"step size\" determines to what extent newly acquired information overrides old information. A factor of 0 makes the agent learn nothing (exclusively exploiting prior knowledge), while a factor of 1 makes the agent consider only the most recent information (ignoring prior knowledge to explore possibilities). In fully deterministic environments, a learning rate of formula_27 is optimal. When the problem is stochastic, the algorithm converges under some technical conditions on the learning rate that require it to decrease to zero. In practice, often a constant learning rate is used, such as formula_28 for all formula_9.\n\nThe discount factor determines the importance of future rewards. A factor of 0 will make the agent \"myopic\" (or short-sighted) by only considering current rewards, i.e. formula_11 (in the update rule above), while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the action values may diverge. For , without a terminal state, or if the agent never reaches one, all environment histories become infinitely long, and utilities with additive, undiscounted rewards generally become infinite. Even with a discount factor only slightly lower than 1, \"Q\"-function learning leads to propagation of errors and instabilities when the value function is approximated with an artificial neural network. In that case, starting with a lower discount factor and increasing it towards its final value accelerates learning.\n\nSince \"Q\"-learning is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. High initial values, also known as \"optimistic initial conditions\", can encourage exploration: no matter what action is selected, the update rule will cause it to have lower values than the other alternative, thus increasing their choice probability. The first reward formula_24 can be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of formula_14. This allows immediate learning in case of fixed deterministic rewards. A model that incorporates \"reset of initial conditions\" (RIC) is expected to predict participants' behavior better than a model that assumes any \"arbitrary initial condition\" (AIC). RIC seems to be consistent with human behaviour in repeated binary choice experiments.\n\n\"Q\"-learning at its simplest stores data in tables. This approach falters with increasing numbers of states/actions since the likelihood of the agent visiting a particular state and performing a particular action is increasingly small.\n\n\"Q\"-learning can be combined with function approximation. This makes it possible to apply the algorithm to larger problems, even when the state space is continuous.\n\nOne solution is to use an (adapted) artificial neural network as a function approximator. Function approximation may speed up learning in finite problems, due to the fact that the algorithm can generalize earlier experiences to previously unseen states.\n\nAnother technique to decrease the state/action space quantizes possible values. Consider the example of learning to balance a stick on a finger. To describe a state at a certain point in time involves the position of the finger in space, its velocity, the angle of the stick and the angular velocity of the stick. This yields a four-element vector that describes one state, i.e. a snapshot of one state encoded into four values. The problem is that infinitely many possible states are present. To shrink the possible space of valid actions multiple values can be assigned to a bucket. The exact distance of the finger from its starting position (-Infinity to Infinity) is not known, but rather whether it is far away or not (Near, Far).\n\n\"Q\"-learning was introduced by Chris Watkins in 1989. A convergence proof was presented by Watkins and Dayan in 1992.\n\nWatkins was addressing “Learning from delayed rewards”, the title of his PhD Thesis. Eight years earlier in 1981 the same problem, under the name of “Delayed reinforcement learning”, was solved by Bozinovski's Crossbar Adaptive Array (CAA). The memory matrix W =||w(a,s)|| was the same as the eight years later Q-table of Q-learning. The architecture introduced the term “state evaluation” in reinforcement learning. The crossbar learning algorithm, written in mathematical pseudocode in the paper, in each iteration performs the following computation:\n- In state s perform action a;\n- Receive consequence state s’;\n- Compute state evaluation v(s’);\n- Update crossbar value w’(a,s) = w(a,s) + v(s’).\n\nThe term “secondary reinforcement” is borrowed from animal learning theory, to model state values via backpropagation: the state value v(s’) of the consequence situation is backpropagated to the previously encountered situations. CAA computes state values vertically and actions horizontally (the \"crossbar\"). Demonstration graphs showing delayed reinforcement learning contained states (desirable, undesirable, and neutral states), which were computed by the state evaluation function. This learning system was a forerunner of the Q-learning algorithm.\n\nIn 2014 Google DeepMind patented an application of Q-learning to deep learning, titled \"deep reinforcement learning\" or \"deep Q-learning\" that can play Atari 2600 games at expert human levels.\n\nThe DeepMind system used a deep convolutional neural network, with layers of tiled convolutional filters to mimic the effects of receptive fields. Reinforcement learning is unstable or divergent when a nonlinear function approximator such as a neural network is used to represent Q. This instability comes from the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and the data distribution, and the correlations between Q and the target values.\n\nThe technique used \"experience replay,\" a biologically inspired mechanism that uses a random sample of prior actions instead of the most recent action to proceed. This removes correlations in the observation sequence and smooths changes in the data distribution. Iterative updates adjust Q towards target values that are only periodically updated, further reducing correlations with the target.\n\nBecause the future maximum approximated action value in Q-learning is evaluated using the same Q function as in current action selection policy, in noisy environments Q-learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this. Double Q-learning is an off-policy reinforcement learning algorithm, where a different policy is used for value evaluation than what is used to select the next action.\n\nIn practice, two separate value functions are trained in a mutually symmetric fashion using separate experiences, formula_33 and formula_34. The double Q-learning update step is then as follows:\n\nNow the estimated value of the discounted future is evaluated using a different policy, which solves the overestimation issue.\n\nThis algorithm was later modified in 2015 and combined with deep learning, as in the DQN algorithm, resulting in Double DQN, which outperforms the original DQN algorithm.\n\nDelayed Q-learning is an alternative implementation of the online \"Q\"-learning algorithm, with probably approximately correct (PAC) learning.\n\nGreedy GQ is a variant of \"Q\"-learning to use in combination with (linear) function approximation. The advantage of Greedy GQ is that convergence is guaranteed even when function approximation is used to estimate the action values.\n\n", "related": "\n- Reinforcement learning\n- Temporal difference learning\n- SARSA\n- Iterated prisoner's dilemma\n- Game theory\n\n- Watkins, C.J.C.H. (1989). Learning from Delayed Rewards. PhD thesis, Cambridge University, Cambridge, England.\n- Strehl, Li, Wiewiora, Langford, Littman (2006). PAC model-free reinforcement learning\n- \"Reinforcement Learning: An Introduction\" by Richard Sutton and Andrew S. Barto, an online textbook. See \"6.5 Q-Learning: Off-Policy TD Control\".\n- Piqle: a Generic Java Platform for Reinforcement Learning\n- Reinforcement Learning Maze, a demonstration of guiding an ant through a maze using \"Q\"-learning.\n- \"Q\"-learning work by Gerald Tesauro\n"}
{"id": "17312749", "url": "https://en.wikipedia.org/wiki?curid=17312749", "title": "Recursive competitive equilibrium", "text": "Recursive competitive equilibrium\n\nIn macroeconomics, recursive competitive equilibrium (RCE) is an equilibrium concept. It has been widely used in exploring a wide variety of economic issues including business-cycle fluctuations, monetary and fiscal policy, trade related phenomena, and regularities in asset price co-movements. This is the equilibrium associated with dynamic programs that represent the decision problem when agents must distinguish between aggregate and individual state variables. These state variables embody the prior and current information of the economy. The decisions and the realizations of exogenous uncertainty determine the values of the state variables in the next sequential time period. Hence the problem is recursive. A RCE is characterized by time invariant functions of a limited number of 'state variables', which summarize the effects of past decisions and current information. These functions (decision rules) include (a) a pricing function, (b) a value function, (c) a period allocation policy specifying the individual's decision, (d) period allocation policy specifying the decision of each firm and (e) a function specifying the law of motion of the capital stock. Since decisions are made with all relevant information available, it is a rational expectations equilibrium.\n", "related": "NONE"}
{"id": "66294", "url": "https://en.wikipedia.org/wiki?curid=66294", "title": "Reinforcement learning", "text": "Reinforcement learning\n\nReinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n\nReinforcement learning, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In the operations research and control literature, reinforcement learning is called \"approximate dynamic programming,\" or \"neuro-dynamic programming.\" The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.\n\nBasic reinforcement is modeled as a Markov decision process:\n\n- a set of environment and agent states, ;\n- a set of actions, , of the agent;\n- formula_1 is the probability of transition (at time formula_2) from state formula_3 to state formula_4 under action formula_5.\n- formula_6 is the immediate reward after transition from formula_3 to formula_4 with action formula_5.\n- rules that describe what the agent observes\n\nRules are often stochastic. The observation typically involves the scalar, immediate reward associated with the last transition. In many works, the agent is assumed to observe the current environmental state (\"full observability\"). If not, the agent has \"partial observability\". Sometimes the set of actions available to the agent is restricted (a zero balance cannot be reduced. For example, if the current value of the agent is 3 and the state transition reduces the value by 4, the transition will not be allowed).\n\nA reinforcement learning agent interacts with its environment in discrete time steps. At each time , the agent receives an observation formula_10, which typically includes the reward formula_11. It then chooses an action formula_12 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state formula_13 and the reward formula_14 associated with the \"transition\" formula_15 is determined. The goal of a reinforcement learning agent is to collect as much reward as possible. The agent can (possibly randomly) choose any action as a function of the history.\n\nWhen the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of \"regret\". In order to act near optimally, the agent must reason about the long term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.\n\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and Go (AlphaGo).\n\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:\n- A model of the environment is known, but an analytic solution is not available;\n- Only a simulation model of the environment is given (the subject of simulation-based optimization);\n- The only way to collect information about the environment is to interact with it.\nThe first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\n\nThe exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space MDPs in Burnetas and Katehakis (1997).\n\nReinforcement learning requires clever exploration mechanisms. Randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\n\nOne such method is formula_16-greedy, where formula_17 is a parameter controlling the amount of exploration vs. exploitation. With probability formula_18, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability formula_16, exploration is chosen, and the action is chosen uniformly at random. formula_16 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.\n\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\n\nThe agent's action selection is modeled as a map called \"policy\":\n\nThe policy map gives the probability of taking action formula_5 when in state formula_3. There are also non-probabilistic policies.\n\nValue function formula_25 is defined as the \"expected return\" starting with state formula_3, i.e. formula_27, and successively following policy formula_28. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.\n\nwhere the random variable formula_30 denotes the return, and is defined as the sum of future discounted rewards\n\nwhere formula_11 is the reward at step formula_2, formula_34 is the discount-rate.\n\nThe algorithm must find a policy with maximum expected return. From the theory of MDPs it is known that, without loss of generality, the search can be restricted to the set of so-called \"stationary\" policies. A policy is \"stationary\" if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to \"deterministic\" stationary policies. A \"deterministic stationary\" policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\n\nThe brute force approach entails two steps:\n- For each possible policy, sample returns while following it\n- Choose the policy with the largest expected return\n\nOne problem with this is that the number of policies can be large, or even infinite. Another is that variance of the returns may be large, which requires many samples to accurately estimate the return of each policy.\n\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\n\nValue function approaches attempt to find a policy that maximizes the return by maintaining a set of estimates of expected returns for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\n\nThese methods rely on the theory of MDPs, where optimality is defined in a sense that is stronger than the above one: A policy is called optimal if it achieves the best expected return from \"any\" initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found amongst stationary policies.\n\nTo define optimality in a formal manner, define the value of a policy formula_28 by\n\nwhere formula_30 stands for the return associated with following formula_28 from the initial state formula_3. Defining formula_40 as the maximum possible value of formula_41, where formula_28 is allowed to change,\n\nA policy that achieves these optimal values in each state is called \"optimal\". Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected return formula_44, since formula_45, where formula_46 is a state randomly sampled from the distribution formula_47.\n\nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state formula_3, an action formula_5 and a policy formula_28, the action-value of the pair formula_51 under formula_28 is defined by\n\nwhere formula_30 now stands for the random return associated with first taking action formula_5 in state formula_3 and following formula_28, thereafter.\n\nThe theory of MDPs states that if formula_58 is an optimal policy, we act optimally (take the optimal action) by choosing the action from formula_59 with the highest value at each state, formula_3. The \"action-value function\" of such an optimal policy (formula_61) is called the \"optimal action-value function\" and is commonly denoted by formula_62. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\n\nAssuming full knowledge of the MDP, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions formula_63 (formula_64) that converge to formula_62. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) MDPs. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\n\nMonte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: \"policy evaluation\" and \"policy improvement\".\n\nMonte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy formula_28, the goal is to compute the function values formula_67 (or a good approximation to them) for all state-action pairs formula_51. Assuming (for simplicity) that the MDP is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair formula_51 can be computed by averaging the sampled returns that originated from formula_51 over time. Given sufficient time, this procedure can thus construct a precise estimate formula_71 of the action-value function formula_72. This finishes the description of the policy evaluation step.\n\nIn the policy improvement step, the next policy is obtained by computing a \"greedy\" policy with respect to formula_71: Given a state formula_3, this new policy returns an action that maximizes formula_75. In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.\n\nProblems with this procedure include:\n- The procedure may spend too much time evaluating a suboptimal policy.\n- It uses samples inefficiently in that a long trajectory improves the estimate only of the \"single\" state-action pair that started the trajectory.\n- When the returns along the trajectories have \"high variance\", convergence is slow.\n- It works in episodic problems only;\n- It works in small, finite MDPs only.\n\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of \"generalized policy iteration\" algorithms. Many \"actor critic\" methods belong to this category.\n\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation. The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method, may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\n\nIn order to address the fifth issue, \"function approximation methods\" are used. \"Linear function approximation\" starts with a mapping formula_76 that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair formula_51 are obtained by linearly combining the components of formula_78 with some \"weights\" formula_79:\n\nThe algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\n\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.\n\nThe problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency. Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called formula_81 parameter formula_82 that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\n\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\n\nGradient-based methods (\"policy gradient methods\") start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector formula_79, let formula_84 denote the policy associated to formula_79. Defining the performance function by\n\nunder mild conditions this function will be differentiable as a function of the parameter vector formula_79. If the gradient of formula_88 was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method (which is known as the likelihood ratio method in the simulation-based optimization literature). Policy search methods have been used in the robotics context. Many policy search methods may get stuck in local optima (as they are based on local search).\n\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\n\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, \"actor–critic methods\" have been proposed and performed well on various problems.\n\nBoth the asymptotic and finite-sample behavior of most algorithms is well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\n\nEfficient exploration of large MDPs is largely unexplored (except for the case of bandit problems). Although finite-time performance bounds appeared for many algorithms, these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\n\nFor incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\n\nResearch topics include \n- adaptive methods that work with fewer (or no) parameters under a large number of conditions\n- addressing the exploration problem in large MDPs\n- large-scale empirical evaluations\n- learning and acting under partial information (e.g., using predictive state representation)\n- modular and hierarchical reinforcement learning\n- improving existing value-function and policy search methods\n- algorithms that work well with large (or continuous) action spaces\n- transfer learning\n- lifelong learning\n- efficient sample-based planning (e.g., based on Monte Carlo tree search).\n- bug detection in software projects\n\nMultiagent or distributed reinforcement learning is a topic of interest. Applications are expanding.\n- Actor-critic reinforcement learning\nReinforcement learning algorithms such as TD learning are under investigation as a model for dopamine-based learning in the brain. In this model, the dopaminergic projections from the substantia nigra to the basal ganglia function as the prediction error. Reinforcement learning has been used as a part of the model for human skill learning, especially in relation to the interaction between implicit and explicit learning in skill acquisition (the first publication on this application was in 1995–1996).\n\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space. The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.\n\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.\n\nIn apprenticeship learning, an expert demonstrates the target behavior. The system tries to recover the policy via observation.\n\n", "related": "\n- Temporal difference learning\n- Q-learning\n- State–action–reward–state–action (SARSA)\n- Fictitious play\n- Learning classifier system\n- Optimal control\n- Dynamic treatment regimes\n- Error-driven learning\n- Multi-agent system\n- Distributed artificial intelligence\n\n- Reinforcement Learning Repository\n- Reinforcement Learning and Artificial Intelligence (RLAI, Rich Sutton's lab at the University of Alberta)\n- Autonomous Learning Laboratory (ALL, Andrew Barto's lab at the University of Massachusetts Amherst)\n- Hybrid reinforcement learning\n- Real-world reinforcement learning experiments at Delft University of Technology\n- Stanford University Andrew Ng Lecture on Reinforcement Learning\n- Dissecting Reinforcement Learning Series of blog post on RL with Python code\n"}
{"id": "10584297", "url": "https://en.wikipedia.org/wiki?curid=10584297", "title": "State–action–reward–state–action", "text": "State–action–reward–state–action\n\nState–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name \"Modified Connectionist Q-Learning\" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.\n\nThis name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent \"S\", the action the agent chooses \"A\", the reward \"R\" the agent gets for choosing this action, the state \"S\" that the agent enters after taking that action, and finally the next action \"A\" the agent chooses in its new state. The acronym for the quintuple (s, a, r, s, a) is SARSA. Some authors use a slightly different convention and write the quintuple (s, a, r, s, a), depending to which time step the reward is formally assigned. The rest of the article uses the former convention.\n\nA SARSA agent interacts with the environment and updates the policy based on actions taken, hence this is known as an \"on-policy learning algorithm\". The Q value for a state-action is updated by an error, adjusted by the learning rate alpha. Q values represent the possible reward received in the next time step for taking action \"a\" in state \"s\", plus the discounted future reward received from the next state-action observation.\n\nWatkin's Q-learning updates an estimate of the optimal state-action value function formula_2 based on the maximum reward of available actions. While SARSA learns the Q values associated with taking the policy it follows itself, Watkin's Q-learning learns the Q values associated with taking the optimal policy while following an exploration/exploitation policy.\n\nSome optimizations of Watkin's Q-learning may be applied to SARSA.\n\nThe learning rate determines to what extent newly acquired information overrides old information. A factor of 0 will make the agent not learn anything, while a factor of 1 would make the agent consider only the most recent information.\n\nThe discount factor determines the importance of future rewards. A factor of 0 makes the agent \"opportunistic\" by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the formula_3 values may diverge.\n\nSince SARSA is an iterative algorithm, it implicitly assumes an initial condition before the first update occurs. A low (infinite) initial value, also known as \"optimistic initial conditions\", can encourage exploration: no matter what action takes place, the update rule causes it to have higher values than the other alternative, thus increasing their choice probability. In 2013 it was suggested that the first reward could be used to reset the initial conditions. According to this idea, the first time an action is taken the reward is used to set the value of . This allows immediate learning in case of fixed deterministic rewards. This resetting-of-initial-conditions (RIC) approach seems to be consistent with human behavior in repeated binary choice experiments.\n", "related": "NONE"}
{"id": "52759748", "url": "https://en.wikipedia.org/wiki?curid=52759748", "title": "Stochastic dynamic programming", "text": "Stochastic dynamic programming\n\nOriginally introduced by Richard E. Bellman in , stochastic dynamic programming is a technique for modelling and solving problems of decision making under uncertainty. Closely related to stochastic programming and dynamic programming, stochastic dynamic programming represents the problem under scrutiny in the form of a Bellman equation. The aim is to compute a policy prescribing how to act optimally in the face of uncertainty.\n\nA gambler has $2, she is allowed to play a game of chance 4 times and her goal is to maximize her probability of ending up with a least $6. If the gambler bets $formula_1 on a play of the game, then with probability 0.4 she wins the game, recoup the initial bet, and she increases her capital position by $formula_1; with probability 0.6, she loses the bet amount $formula_1; all plays are pairwise independent. On any play of the game, the gambler may not bet more money than she has available at the beginning of that play.\n\nStochastic dynamic programming can be employed to model this problem and determine a betting strategy that, for instance, maximizes the gambler's probability of attaining a wealth of at least $6 by the end of the betting horizon.\n\nNote that if there is no limit to the number of games that can be played, the problem becomes a variant of the well known St. Petersburg paradox.\n\nConsider a discrete system defined on formula_4 stages in which each stage formula_5 is characterized by\n- an initial state formula_6, where formula_7 is the set of feasible states at the beginning of stage formula_8;\n- a decision variable formula_9, where formula_10 is the set of feasible actions at stage formula_8 – note that formula_10 may be a function of the initial state formula_13;\n- an immediate cost/reward function formula_14, representing the cost/reward at stage formula_8 if formula_13 is the initial state and formula_17 the action selected;\n- a state transition function formula_18 that leads the system towards state formula_19.\n\nLet formula_20 represent the optimal cost/reward obtained by following an \"optimal policy\" over stages formula_21. Without loss of generality in what follow we will consider a reward maximisation setting. In deterministic dynamic programming one usually deals with functional equations taking the following structure\nwhere formula_19 and the boundary condition of the system is \nThe aim is to determine the set of optimal actions that maximise formula_25. Given the current state formula_13 and the current action formula_17, we \"know with certainty\" the reward secured during the current stage and – thanks to the state transition function formula_28 – the future state towards which the system transitions.\n\nIn practice, however, even if we know the state of the system at the beginning of the current stage as well as the decision taken, the state of the system at the beginning of the next stage and the current period reward are often random variables that can be observed only at the end of the current stage.\n\nStochastic dynamic programming deals with problems in which the current period reward and/or the next period state are random, i.e. with multi-stage stochastic systems. The decision maker's goal is to maximise expected (discounted) reward over a given planning horizon.\n\nIn their most general form, stochastic dynamic programs deal with functional equations taking the following structure\nwhere\n- formula_20 is the maximum expected reward that can be attained during stages formula_21, given state formula_13 at the beginning of stage formula_8;\n- formula_17 belongs to the set formula_35 of feasible actions at stage formula_8 given initial state formula_13;\n- formula_38 is the discount factor;\n- formula_39 is the conditional probability that the state at the beginning of stage formula_8 is formula_41 given current state formula_13 and selected action formula_17.\n\nMarkov decision process represent a special class of stochastic dynamic programs in which the underlying stochastic process is a stationary process that features the Markov property.\n\nGambling game can be formulated as a Stochastic Dynamic Program as follows: there are formula_44 games (i.e. stages) in the planning horizon\n- the state formula_45 in period formula_8 represents the initial wealth at the beginning of period formula_8;\n- the action given state formula_45 in period formula_8 is the bet amount formula_1;\n- the transition probability formula_51 from state formula_52 to state formula_53 when action formula_54 is taken in state formula_52 is easily derived from the probability of winning (0.4) or losing (0.6) a game.\n\nLet formula_56 be the probability that, by the end of game 4, the gambler has at least $6, given that she has $formula_45 at the beginning of game formula_8. \n- the immediate profit incurred if action formula_1 is taken in state formula_45 is given by the expected value formula_61.\n\nTo derive the functional equation, define formula_62 as a bet that attains formula_56, then at the beginning of game formula_64\n- if formula_65 it is impossible to attain the goal, i.e. formula_66 for formula_65;\n- if formula_68 the goal is attained, i.e. formula_69 for formula_68;\n- if formula_71 the gambler should bet enough to attain the goal, i.e. formula_72 for formula_71.\n\nFor formula_74 the functional equation is formula_75, where formula_62 ranges in formula_77; the aim is to find formula_78.\n\nGiven the functional equation, an optimal betting policy can be obtained via forward recursion or backward recursion algorithms, as outlined below.\n\nStochastic dynamic programs can be solved to optimality by using backward recursion or forward recursion algorithms. Memoization is typically employed to enhance performance. However, like deterministic dynamic programming also its stochastic variant suffers from the curse of dimensionality. For this reason approximate solution methods are typically employed in practical applications.\n\nGiven a bounded state space, \"backward recursion\" begins by tabulating formula_79 for every possible state formula_80 belonging to the final stage formula_4. Once these values are tabulated, together with the associated optimal state-dependent actions formula_82, it is possible to move to stage formula_83 and tabulate formula_84 for all possible states belonging to the stage formula_83. The process continues by considering in a \"backward\" fashion all remaining stages up to the first one. Once this tabulation process is complete, formula_86 – the value of an optimal policy given initial state formula_45 – as well as the associated optimal action formula_88 can be easily retrieved from the table. Since the computation proceeds in a backward fashion, it is clear that backward recursion may lead to computation of a large number of states that are not necessary for the computation of formula_86.\n\nGiven the initial state formula_45 of the system at the beginning of period 1, \"forward recursion\" computes formula_86 by progressively expanding the functional equation (\"forward pass\"). This involves recursive calls for all formula_92 that are necessary for computing a given formula_93. The value of an optimal policy and its structure are then retrieved via a (\"backward pass\") in which these suspended recursive calls are resolved. A key difference from backward recursion is the fact that formula_94 is computed only for states that are relevant for the computation of formula_86. Memoization is employed to avoid recomputation of states that have been already considered.\n\nWe shall illustrate forward recursion in the context of the Gambling game instance previously discussed. We begin the \"forward pass\" by considering\nformula_96\n\nAt this point we have not computed yet formula_97, which are needed to compute formula_78; we proceed and compute these items. Note that formula_99, therefore one can leverage memoization and perform the necessary computations only once.\n\n- Computation of formula_97\nformula_101\n\nformula_102\n\nformula_103\n\nformula_104\n\nformula_105\n\nWe have now computed formula_106 for all formula_80 that are needed to compute formula_78. However, this has led to additional suspended recursions involving formula_109. We proceed and compute these values.\n\n- Computation of formula_109\nformula_111\n\nformula_112\n\nformula_113\n\nformula_114\n\nformula_115\n\nformula_116\n\nSince stage 4 is the last stage in our system, formula_117 represent boundary conditions that are easily computed as follows.\n\n- Boundary conditions\nformula_118\n\nAt this point it is possible to proceed and recover the optimal policy and its value via a \"backward pass\" involving, at first, stage 3\n\n- Backward pass involving formula_119\nformula_120\n\nformula_121\n\nformula_122\n\nformula_123\n\nformula_124\n\nformula_125\n\nand, then, stage 2.\n\n- Backward pass involving formula_126\nformula_127\n\nformula_128\n\nformula_129\n\nformula_130\n\nformula_131\n\nWe finally recover the value formula_132 of an optimal policy\n\nformula_133\n\nThis is the optimal policy that has been previously illustrated. Note that there are multiple optimal policies leading to the same optimal value formula_134; for instance, in the first game one may either bet $1 or $2.\n\nGamblersRuin.java is a standalone Java 8 implementation of the above example.\n\nAn introduction to approximate dynamic programming is provided by .\n\n- . Dover paperback edition (2003).\n- . In two volumes.\n\n", "related": "\n- Dynamic programming\n- Stochastic process\n- Stochastic programming\n- Control theory\n- Stochastic control\n- Reinforcement learning\n"}
{"id": "1209759", "url": "https://en.wikipedia.org/wiki?curid=1209759", "title": "Temporal difference learning", "text": "Temporal difference learning\n\nTemporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.\n\nWhile Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:\n\nTemporal difference methods are related to the temporal difference model of animal learning.\n\nThe tabular TD(0) method is one of the simplest TD methods. It is a special case of more general stochastic approximation methods. It estimates the state value function of a finite-state Markov decision process (MDP) under a policy formula_1. Let formula_2 denote the state value function of the MDP with states formula_3, rewards formula_4 and discount rate formula_5 under the policy formula_6:\nWe drop the action from the notion for convenience\n\nformula_2 satisfies the Hamilton-Jacobi-Bellman Equation: \n\nso formula_10 is an unbiased estimate for formula_11. This observation motivates the following algorithm for estimating formula_2.\n\nThe algorithm starts by initializing a table formula_13 arbitrarily, with one value for each state of the MDP. A positive learning rate formula_14 is chosen.\n\nWe then repeatedly evaluate the policy formula_1, obtain a reward formula_16 and update the value function for the old state using the rule: \nwhere formula_18 and formula_19are the old and new states, respectively.\n\nThe value formula_20 is known as the TD target.\n\nTD-Lambda is a learning algorithm invented by Richard S. Sutton based on earlier work on temporal difference learning by Arthur Samuel. This algorithm was famously applied by Gerald Tesauro to create TD-Gammon, a program that learned to play the game of backgammon at the level of expert human players.\n\nThe lambda (formula_21) parameter refers to the trace decay parameter, with formula_22. Higher settings lead to longer lasting traces; that is, a larger proportion of credit from a reward can be given to more distant states and actions when formula_21 is higher, with formula_24 producing parallel learning to Monte Carlo RL algorithms.\n\nThe TD algorithm has also received attention in the field of neuroscience. Researchers discovered that the firing rate of dopamine neurons in the ventral tegmental area (VTA) and substantia nigra (SNc) appear to mimic the error function in the algorithm. The error function reports back the difference between the estimated reward at any given state or time step and the actual reward received. The larger the error function, the larger the difference between the expected and actual reward. When this is paired with a stimulus that accurately reflects a future reward, the error can be used to associate the stimulus with the future reward.\n\nDopamine cells appear to behave in a similar manner. In one experiment measurements of dopamine cells were made while training a monkey to associate a stimulus with the reward of juice. Initially the dopamine cells increased firing rates when the monkey received juice, indicating a difference in expected and actual rewards. Over time this increase in firing back propagated to the earliest reliable stimulus for the reward. Once the monkey was fully trained, there was no increase in firing rate upon presentation of the predicted reward. Continually, the firing rate for the dopamine cells decreased below normal activation when the expected reward was not produced. This mimics closely how the error function in TD is used for reinforcement learning.\n\nThe relationship between the model and potential neurological function has produced research attempting to use TD to explain many aspects of behavioral research. It has also been used to study conditions such as schizophrenia or the consequences of pharmacological manipulations of dopamine on learning.\n\n", "related": "\n- Q-learning\n- SARSA\n- Rescorla-Wagner model\n- PVLV\n\n- Imran Ghory. Reinforcement Learning in Board Games.\n- S. P. Meyn, 2007. Control Techniques for Complex Networks, Cambridge University Press, 2007. See final chapter, and appendix with abridged Meyn & Tweedie.\n\n- Scholarpedia Temporal difference Learning\n- TD-Gammon\n- TD-Networks Research Group\n- Connect Four TDGravity Applet (+ mobile phone version) – self-learned using TD-Leaf method (combination of TD-Lambda with shallow tree search)\n- Self Learning Meta-Tic-Tac-Toe Example web app showing how temporal difference learning can be used to learn state evaluation constants for a minimax AI playing a simple board game.\n- Reinforcement Learning Problem, document explaining how temporal difference learning can be used to speed up Q-learning\n- TD-Simulator Temporal difference simulator for classical conditioning\n"}
{"id": "12617694", "url": "https://en.wikipedia.org/wiki?curid=12617694", "title": "Value function", "text": "Value function\n\nThe value function of an optimization problem gives the value attained by the objective function at a solution, while only depending on the parameters of the problem. In a controlled dynamical system, the value function represents the optimal payoff of the system over the interval [t, T] when started at the time-t state variable x(t)=x. If the objective function represents some cost that is to be minimized, the value function can be interpreted as the cost to finish the optimal program, and is thus referred to as \"cost-to-go function.\" In an economic context, where the objective function usually represents utility, the value function is conceptually equivalent to the indirect utility function.\n\nIn a problem of optimal control, the value function is defined as the supremum of the objective function taken over the set of admissible controls. Given formula_1, a typical optimal control problem is\nsubject to\nwith initial state variable formula_4. The objective function formula_5 is to be maximized over all admissible controls formula_6, where formula_7 is a Lebesgue measurable function from formula_8 to some prescribed arbitrary set in formula_9. The value function is then defined as \n\nIf the optimal pair of control and state trajectories is formula_10, then formula_11. In economics, the function formula_12 is called a policy function.\n\nBellman's principle of optimality roughly states that any optimal policy at time formula_13, formula_14 taking the current state formula_15 as \"new\" initial condition must be optimal for the remaining problem. If the value function happens to be continuously differentiable, this gives rise to an important functional recurrence equation known as Hamilton–Jacobi–Bellman equation,\nwhere the maximand on the right-hand side is the Hamiltonian, with formula_17 playing the role of the costate variables. The value function is a viscosity solution to the Hamilton–Jacobi–Bellman equation.\n\nIn an online closed-loop approximate optimal control, the value function is also a Lyapunov function that establishes global asymptotic stability of the closed-loop system.\n\n", "related": "NONE"}
