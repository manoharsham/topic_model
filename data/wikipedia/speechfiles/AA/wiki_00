{"id": "3271816", "url": "https://en.wikipedia.org/wiki?curid=3271816", "title": "Speech technology", "text": "Speech technology\n\nSpeech technology relates to the technologies designed to duplicate and respond to the human voice. They have many uses. These include aid to the voice-disabled, the hearing-disabled, and the blind, along with communication with computers without a keyboard. They enhance game software and aid in marketing goods or services by telephone.\n\nThe subject includes several subfields:\n\n- Speech synthesis\n- Speech recognition\n- Speaker recognition\n- Speaker verification\n- Speech encoding\n- Multimodal interaction\n\n", "related": "\n- Communication aids\n- Language technology\n- Speech interface guideline\n- Speech processing\n- \"Speech Technology\" (magazine)\n"}
{"id": "5289626", "url": "https://en.wikipedia.org/wiki?curid=5289626", "title": "TIMIT", "text": "TIMIT\n\nTIMIT is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. Each transcribed element has been delineated in time.\n\nTIMIT was designed to further acoustic-phonetic knowledge and automatic speech recognition systems. It was commissioned by DARPA and corpus design was a joint effort between the Massachusetts Institute of Technology, SRI International, and Texas Instruments (TI). The speech was recorded at TI, transcribed at MIT, and verified and prepared for publishing by the National Institute of Standards and Technology (NIST). There is also a telephone bandwidth version called NTIMIT (Network TIMIT). \n\nTIMIT and NTIMIT are not freely available — either membership of the Linguistic Data Consortium, or a monetary payment, is required for access to the dataset.\n\nThe TIMIT telephone corpus was an early attempt to create a database with speech samples. It was published in the year 1988 on CD-ROM and contains of only 10 sentences. Each sentence is 30 seconds long and is spoken by 630 different speakers. It was the first notable attempt in creating and distributing a speech corpus and the overall project has produced costs of 1.5 million US$.\n\nThe full name of the project is DARPA-TIMIT Acoustic-Phonetic Continuous Speech Corpus and the acronym TIMIT stands for Texas Instruments/Massachusetts Institute of Technology. The main reason why a corpus of telephone speech was created was to train speech recognition software. In the Blizzard challenge, different software has the obligation to convert audio recordings into textual data and the TIMIT corpus was used as a standardized baseline.\n\n", "related": "\n- Comparison of datasets in machine learning\n\n- TIMIT Acoustic-Phonetic Continuous Speech Corpus\n"}
{"id": "14477647", "url": "https://en.wikipedia.org/wiki?curid=14477647", "title": "Quack.com", "text": "Quack.com\n\nQuack.com was an early voice portal company. The domain name later was used for Quack, an iPad search application from AOL.\n\nIt was founded in 1998 by Steven Woods, Jeromy Carriere and Alex Quilici as a Pittsburgh, Pennsylvania, USA, based voice portal infrastructure company named Quackware. Quack was the first company to try to create a voice portal: a consumer-based destination \"site\" in which consumers could not only access information by voice alone, but also complete transactions. Quackware launched a beta phone service in 1999 that allowed consumers to purchase books from sites such as Amazon and CDs from sites such as CDNow by answering a short set of questions. Quack followed with a set of information services from movie listings (inspired by, but expanding upon, Moviefone) to news, weather and stock quotes. This concept introduced a series of lookalike startups including Tellme Networks which raised more money than any Internet startup in history on a similar concept.\n\nQuack received venture funding in 1999 and moved operations to Mountain View in Silicon Valley, California in 1999. \nA deal with Lycos was announced in May 2000.\nIn September 2000 Quack was acquired for $200 million by America Online (AOL) and moved onto the Netscape campus with what was left of the Netscape team.\n\nQuack was attacked in the Canadian press for being representative of the Canadian \"brain drain\" to the US during the Internet bubble, focusing its recruiting efforts on the University of Waterloo, hiring more than 50 engineers from Waterloo in less than 10 months. Quack competitor Tellme Networks raised enormous funds in what became a highly competitive market in 2000, with the emergence of more than a dozen additional competitors in a 12-month period.\n\nFollowing its acquisition by America Online in an effort led by Ted Leonsis to bring Quack into AOL Interactive, the Quack voice service became AOLbyPhone as one of AOL's \"web properties\" along with MapQuest, Moviefone and others.\n\nQuack secured several patents that underlie the technical challenges of delivering interactive voice services. Constructing a voice portal required integrations and innovations not only in speech recognition and speech generation, but also in databases, application specification, constraint-based reasoning and artificial intelligence and computational linguistics. \"Quack\"'s name derived from the company goal of providing not only voice-based services, but more broadly \"Quick Ubiquitous Access to Consumer Knowledge\".\n\nThe patents assigned to Quack.com include: System and method for voice access to Internet-based information, System and method for advertising with an Internet Voice Portal and recognizing the axiom that in interactive voice systems one must \"know the set of possible answers to a question before asking it\". System and method for determining if one web site has the same information as another web site.\n\nQuack.com was spoofed in \"The Simpsons\" in March 2002 in the episode \"Blame It on Lisa\" in which a \"ComQuaak\" sign is replaced by another equally crazy telecom company name.\n\nIn July 2010, quack.com became the focus of a new AOL iPad application, that was a web search experience. The product delivers web results and blends in picture, video and Twitter results. It enables you to preview the web results before you go to the site, search within each result, and flip through the results pages, making full use of the iPad's touch screen features. The iPad app was free via iTunes, but support discontinued in 2012.\n\n- iTunes App Link\n", "related": "NONE"}
{"id": "1032254", "url": "https://en.wikipedia.org/wiki?curid=1032254", "title": "Speaker recognition", "text": "Speaker recognition\n\nSpeaker recognition is the identification of a person from characteristics of voices. It is used to answer the question \"Who is speaking?\" The term voice recognition can refer to \"speaker recognition\" or speech recognition. Speaker verification (also called speaker authentication) contrasts with identification, and \"speaker recognition\" differs from \"speaker diarisation\" (recognizing when the same speaker is speaking). \n\nRecognizing the speaker can simplify the task of translating speech in systems that have been trained on specific voices or it can be used to authenticate or verify the identity of a speaker as part of a security process. Speaker recognition has a history dating back some four decades as of 2019 and uses the acoustic features of speech that have been found to differ between individuals. These acoustic patterns reflect both anatomy and learned behavioral patterns.\n\nThere are two major applications of speaker recognition technologies and methodologies. If the speaker claims to be of a certain identity and the voice is used to verify this claim, this is called \"verification\" or \"authentication\". On the other hand, identification is the task of determining an unknown speaker's identity. In a sense, speaker verification is a 1:1 match where one speaker's voice is matched to a particular template whereas speaker identification is a 1:N match where the voice is compared against multiple templates.\n\nFrom a security perspective, identification is different from verification. Speaker verification is usually employed as a \"gatekeeper\" in order to provide access to a secure system. These systems operate with the users' knowledge and typically require their cooperation. Speaker identification systems can also be implemented covertly without the user's knowledge to identify talkers in a discussion, alert automated systems of speaker changes, check if a user is already enrolled in a system, etc.\n\nIn forensic applications, it is common to first perform a speaker identification process to create a list of \"best matches\" and then perform a series of verification processes to determine a conclusive match.\nOne of the earliest training technologies to commercialize was implemented in Worlds of Wonder's 1987 Julie doll. At that point, speaker independence was an intended breakthrough, and systems required a training period. A 1987 ad for the doll carried the tagline \"Finally, the doll that understands you.\" - despite the fact that it was described as a product \"which children could train to respond to their voice.\" The term voice recognition, even a decade later, referred to speaker independence.\n\nEach speaker recognition system has two phases: Enrollment and verification. During enrollment, the speaker's voice is recorded and typically a number of features are extracted to form a voice print, template, or model. In the verification phase, a speech sample or \"utterance\" is compared against a previously created voice print. For identification systems, the utterance is compared against multiple voice prints in order to determine the best match(es) while verification systems compare an utterance against a single voice print. Because of the process involved, verification is faster than identification.\n\nSpeaker recognition systems fall into two categories: text-dependent and text-independent.\nText-Dependent:\n\nIf the text must be the same for enrollment and verification this is called text-dependent recognition. In a text-dependent system, prompts can either be common across all speakers (e.g. a common pass phrase) or unique. In addition, the use of shared-secrets (e.g.: passwords and PINs) or knowledge-based information can be employed in order to create a multi-factor authentication scenario.\n\nText-Independent:\n\nText-independent systems are most often used for speaker identification as they require very little if any cooperation by the speaker. In this case the text during enrollment and test is different. In fact, the enrollment may happen without the user's knowledge, as in the case for many forensic applications. As text-independent technologies do not compare what was said at enrollment and verification, verification applications tend to also employ speech recognition to determine what the user is saying at the point of authentication.\n\nIn text independent systems both acoustics and speech analysis techniques are used.\n\nSpeaker recognition is a pattern recognition problem. The various technologies used to process and store voice prints include frequency estimation, hidden Markov models, Gaussian mixture models, pattern matching algorithms, neural networks, matrix representation, vector quantization and decision trees. For comparing utterances against voice prints, more basic methods like cosine similarity are traditionally used for their simplicity and performance. Some systems also use \"anti-speaker\" techniques such as cohort models and world models. Spectral features are predominantly used in representing speaker characteristics. Linear predictive coding (LPC) is a speech coding method used in speaker recognition and speech verification.\n\nAmbient noise levels can impede both collections of the initial and subsequent voice samples. Noise reduction algorithms can be employed to improve accuracy, but incorrect application can have the opposite effect. Performance degradation can result from changes in behavioural attributes of the voice and from enrollment using one telephone and verification on another telephone. Integration with two-factor authentication products is expected to increase. Voice changes due to ageing may impact system performance over time. Some systems adapt the speaker models after each successful verification to capture such long-term changes in the voice, though there is debate regarding the overall security impact imposed by automated adaptation.\n\nDue to the introduction of legislation like the General Data Protection Regulation in the European Union and the California Consumer Privacy Act in the United States, there has been much discussion about the use of speaker recognition in the work place. In September 2019 Irish speech recognition developer Soapbox Labs warned about the legal implications that may be involved.\n\nThe first international patent was filed in 1983, coming from the telecommunication research in CSELT (Italy) by Michele Cavazza and Alberto Ciaramella as a basis for both future telco services to final customers and to improve the noise-reduction techniques across the network.\n\nBetween 1996 and 1998, speaker recognition technology was used at the Scobey–Coronach Border Crossing to enable enrolled local residents with nothing to declare to cross the Canada–United States border when the inspection stations were closed for the night. The system was developed for the U.S. Immigration and Naturalization Service by Voice Strategies of Warren, Michigan.\n\nIn May 2013 it was announced that Barclays Wealth was to use passive speaker recognition to verify the identity of telephone customers within 30 seconds of normal conversation. The system used had been developed by voice recognition company Nuance (that in 2011 acquired the company Loquendo, the spin-off from CSELT itself for speech technology), the company behind Apple's Siri technology. A verified voiceprint was to be used to identify callers to the system and the system would in the future be rolled out across the company.\n\nThe private banking division of Barclays was the first financial services firm to deploy voice biometrics as the primary means to authenticate customers to their call centers. 93% of customer users had rated the system at \"9 out of 10\" for speed, ease of use and security.\n\nSpeaker recognition may also be used in criminal investigations, such as those of the 2014 executions of, amongst others, James Foley and Steven Sotloff.\n\nIn February 2016 UK high-street bank HSBC and its internet-based retail bank First Direct announced that it would offer 15 million customers its biometric banking software to access online and phone accounts using their fingerprint or voice.\n\n", "related": "\n- AI effect\n- Applications of artificial intelligence\n- Speaker diarisation\n- Speech recognition\n- Voice changer\n\n- Lists\n- List of emerging technologies\n- Outline of artificial intelligence\n\n- Homayoon Beigi (2011), \"Fundamentals of Speaker Recognition\", Springer-Verlag, Berlin, 2011, .\n- \"Biometrics from the movies\" –National Institute of Standards and Technology\n- Elisabeth Zetterholm (2003), \"Voice Imitation. A Phonetic Study of Perceptual Illusions and Acoustic Success\", Phd thesis, Lund University.\n- Md Sahidullah (2015), \"Enhancement of Speaker Recognition Performance Using Block Level, Relative and Temporal Information of Subband Energies\", PhD thesis, Indian Institute of Technology Kharagpur.\n\n- Circumventing Voice Authentication The PLA Radio podcast recently featured a simple way to fool rudimentary voice authentication systems.\n- Speaker recognition – Scholarpedia\n- Voice recognition benefits and challenges in access control\n\n- bob.bio.spear\n- ALIZE\n"}
{"id": "27492628", "url": "https://en.wikipedia.org/wiki?curid=27492628", "title": "Speech enhancement", "text": "Speech enhancement\n\nSpeech enhancement aims to improve speech quality by using various algorithms. \nThe objective of enhancement is improvement in intelligibility and/or overall perceptual quality of degraded speech signal using audio signal processing techniques.\n\nEnhancing of speech degraded by noise, or noise reduction, is the most important field of speech enhancement, and used for many applications such as mobile phones, VoIP, teleconferencing systems, speech recognition, and hearing aids\n\nThe algorithms of speech enhancement for noise reduction can be categorized into three fundamental classes: filtering techniques, spectral restoration, and model-based methods\n- Filtering Techniques\n- Spectral Restoration\n- Speech-Model-Based\n\n", "related": "\n- audio noise reduction\n- Speech coding\n- Speech interface guideline\n- Speech processing\n- Speech recognition\n- Voice analysis\n\n- J. Benesty, M. M. Sondhi, Y. Huang (ed). \"Springer Handbook of Speech Processing\". Springer, 2007. .\n- J. Benesty, S. Makino, J. Chen (ed). \"Speech Enhancement\". Springer, 2005. .\n- P. C. Loizou. \"Speech Enhancement: Theory and Practice\". CRC Press, 2013. .\n"}
{"id": "252008", "url": "https://en.wikipedia.org/wiki?curid=252008", "title": "Language technology", "text": "Language technology\n\nLanguage technology, often called human language technology (HLT), studies methods of how computer programs or electronic devices can analyze, produce, modify or respond to human texts and speech. It consists of natural language processing (NLP) and computational linguistics (CL) on the one hand, and speech technology on the other. It also includes many application oriented aspects of these. Working with language technology often requires broad knowledge not only about linguistics but also about computer science.\n\nThe Globalization and Localization Association (GALA), maintains a directory of language technology and software for translators and localizers. \n\nFor many of the world's lesser known languages, the foundation of language technology is providing communities with fonts and keyboard setups so their languages can be written on computers or mobile devices.\n- Johns Hopkins University Human Language Technology Center of Excellence\n- Carnegie Mellon University Language Technologies Institute\n- Institute for Applied Linguistics (IULA)at Universitat Pompeu Fabra. Barcelona, Spain\n- German Research Centre for Artificial Intelligence (DFKI) Language Technology Lab\n- CLT: Centre for Language Technology in Gothenburg, Sweden\n- Globalization and Localization Association (GALA)\n- ScriptSource, a reference to the writing systems of the world and the remaining needs for supporting them in the computing realm.\n", "related": "NONE"}
{"id": "12328438", "url": "https://en.wikipedia.org/wiki?curid=12328438", "title": "Auditory processing disorder", "text": "Auditory processing disorder\n\nAuditory processing disorder (APD), rarely known as King-Kopetzky syndrome or auditory disability with normal hearing (ADN), is an umbrella term for a variety of disorders that affect the way the brain processes auditory information. Individuals with APD usually have normal structure and function of the outer, middle, and inner ear (peripheral hearing). However, they cannot process the information they hear in the same way as others do, which leads to difficulties in recognizing and interpreting sounds, especially the sounds composing speech. It is thought that these difficulties arise from dysfunction in the central nervous system.\n\nThe American Academy of Audiology notes that APD is diagnosed by difficulties in one or more auditory processes known to reflect the function of the central auditory nervous system. It can affect both children and adults. Although the actual prevalence is currently unknown, it has been estimated to be 2–7% in children in US and UK populations. APD can continue into adulthood. Cooper and Gates (1991) estimated the prevalence of adult APD to be 10 to 20%. It has been reported that males are twice as likely to be affected by the disorder as females, and that prevalence is higher in the elderly and increases with age.\n\nMany people experience problems with learning and day-to-day tasks with difficulties over time. Adults with this disorder can experience the signs and symptoms below:\n- talk louder than necessary\n- have trouble remembering a list or sequence\n- often need words or sentences repeated\n- have poor ability to memorize information learned by listening\n- interpret words too literally\n- need assistance hearing clearly in noisy environments\n- rely on accommodation and modification strategies\n- find or request a quiet work space away from others\n- request written material when attending oral presentations\n- ask for directions to be given one step at a time\n\nIt has been discovered that APD and ADHD present overlapping symptoms. Below is a ranked order of behavioral symptoms that are most frequently observed in each disorder. Professionals evaluated the overlap of symptoms between the two disorders. The order below is of symptoms that are almost always observed. This chart proves that although the symptoms listed are different, it is easy to get confused between many of them.\n\nThere is a high rate of co-occurrence between ADHD and APD. An article published in 1994 showed that 84% of children with APD have confirmed or suspected ADHD. Co-occurrence between ADHD and APD is 41% for children with confirmed diagnosis of ADHD, and 43% for children suspected of having ADHD. \"more recently published data is needed to support or refute this statement.\"\n\nThere has been considerable debate over the relationship between APD and Specific language impairment (SLI).\n\nSLI is diagnosed when a child has difficulties with understanding or producing spoken language for no obvious cause. The problems cannot be explained in terms of peripheral hearing loss. The child is typically late in starting to talk, and may have problems in producing speech sounds clearly, and in producing or understanding complex sentences. Some theoretical accounts of SLI regard it as the result of auditory processing problems. However, this view of SLI is not universally accepted, and others regard the main difficulties in SLI as stemming from problems with higher-level aspects of language processing. Where a child has both auditory and language problems, it can be hard to sort out cause-and-effect.\n\nSimilarly with developmental dyslexia, there has been considerable interest in the idea that for some children reading problems are downstream consequences of difficulties in rapid auditory processing. Again, cause and effect can be hard to unravel. This is one reason why some experts have recommended using non-verbal auditory tests to diagnose APD. Specifically regarding the neurological factors of dyslexia, the disorder has been linked to polymicrogyria which causes cell migrational problems. This relates to APD because children that have polymicrogyri almost always present deficits on APD testing. It has also been suggested that APD may be related to cluttering, a fluency disorder marked by word and phrase repetitions.\n\nIt has been found that a higher than expected proportion of individuals diagnosed with SLI and dyslexia on the basis of language and reading tests also perform poorly on tests in which auditory processing skills are tested. APD can be assessed using tests that involve identifying, repeating or discriminating speech, and a child may do poorly because of primary language problems. In a study comparing children with a diagnosis of dyslexia and those with a diagnosis of APD, they found the two groups could not be distinguished. obtained similar findings in studies comparing children diagnosed with SLI or APD. The two groups had very similar profiles. This raises the worrying possibility that the diagnosis that a child receives may be largely a function of the specialist they see: the same child who would be diagnosed with APD by an audiologist may be diagnosed with SLI by a speech-language therapist or with dyslexia by a psychologist.\n\nAcquired APD can be caused by any damage to or dysfunction of the central auditory nervous system and can cause auditory processing problems. For an overview of neurological aspects of APD, see Griffiths.\n\nSome studies indicated an increased prevalence of a family history of hearing impairment in these patients. The pattern of results is suggestive that Auditory Processing Disorder may be related to conditions of autosomal dominant inheritance. The ability to listen to and comprehend multiple messages at the same time is a trait that is heavily influenced by our genes say federal researchers. These \"short circuits in the wiring\" sometimes run in families or result from a difficult birth, just like any learning disability. Auditory processing disorder can be associated with conditions affected by genetic traits, such as various developmental disorders. Inheritance of Auditory Processing Disorder refers to whether the condition is inherited from your parents or \"runs\" in families. Central auditory processing disorder may be hereditary neurological traits from the mother or the father.\n\nIn the majority of cases of developmental APD, the cause is unknown. An exception is acquired epileptic aphasia or Landau-Kleffner syndrome, where a child's development regresses, with language comprehension severely affected. The child is often thought to be deaf, but normal peripheral hearing is found. In other cases, suspected or known causes of APD in children include delay in myelin maturation, ectopic (misplaced) cells in the auditory cortical areas, or genetic predisposition. In a family with autosomal dominant epilepsy, seizures which affected the left temporal lobe seemed to cause problems with auditory processing. In another extended family with a high rate of APD, genetic analysis showed a haplotype in chromosome 12 that fully co-segregated with language impairment.\n\nHearing begins in utero, but the central auditory system continues to develop for at least the first decade. There is considerable interest in the idea that disruption to hearing during a sensitive period may have long-term consequences for auditory development. One study showed thalamocortical connectivity in vitro was associated with a time sensitive developmental window and required a specific cell adhesion molecule (lcam5) for proper brain plasticity to occur. This points to connectivity between the thalamus and cortex shortly after being able to hear (in vitro) as at least one critical period for auditory processing. Another study showed that rats reared in a single tone environment during critical periods of development had permanently impaired auditory processing. ‘Bad’ auditory experiences, such as temporary deafness by cochlear removal in rats leads to neuron shrinkage. In a study looking at attention in APD patients, children with one ear blocked developed a strong right-ear advantage but were not able to modulate that advantage during directed-attention tasks.\n\nIn the 1980s and 1990s, there was considerable interest in the role of chronic Otitis media (middle ear disease or 'glue ear') in causing APD and related language and literacy problems. Otitis media with effusion is a very common childhood disease that causes a fluctuating conductive hearing loss, and there was concern this may disrupt auditory development if it occurred during a sensitive period. Consistent with this, in a sample of young children with chronic ear infections recruited from a hospital otolargyngology department, increased rates of auditory difficulties were found later in childhood. However, this kind of study will suffer from sampling bias because children with otitis media will be more likely to be referred to hospital departments if they are experiencing developmental difficulties. Compared with hospital studies, epidemiological studies, which assesses a whole population for otitis media and then evaluate outcomes, have found much weaker evidence for long-term impacts of otitis media on language outcomes.\n\nIt seems that somatic anxiety (that is, physical symptoms of anxiety such as butterflies in the stomach or cotton mouth) and situations of stress may be determinants of speech-hearing disability.\n\nQuestionnaires can be used for the identification of persons with possible auditory processing disorders, as these address common problems of listening. They can help in the decision for pursuing clinical evaluation. One of the most common listening problems is speech recognition in the presence of background noise. According to the respondents who participated in a study by Neijenhuis, de Wit, and Luinge (2017), the following symptoms are characteristic in children with listening difficulties, and they are typically problematic with adolescents and adults. They include:\n\n- Difficulty hearing in noise\n- Auditory attention problems\n- Better understanding in one on one situations\n- Difficulties in noise localization\n- Difficulties in remembering oral information\n\nAccording to the New Zealand Guidelines on Auditory Processing Disorders (2017) a checklist of key symptoms of APD or comorbidities that can be used to identify individuals who should be referred for audiological and APD assessment includes, among others:\n\n- Difficulty following spoken directions unless they are brief and simple\n- Difficulty attending to and remembering spoken information\n- Slowness in processing spoken information\n- Difficulty understanding in the presence of other sounds\n- Overwhelmed by complex or “busy” auditory environments e.g. classrooms, shopping malls\n- Poor listening skills\n- Insensitivity to tone of voice or other nuances of speech\n- Acquired brain injury\n- History of frequent or persistent middle ear disease (otitis media, ‘glue ear’).\n- Difficulty with language, reading or spelling\n- Suspicion or diagnosis of dyslexia\n- Suspicion or diagnosis of language disorder or delay\n\nFinally, the New Zealand guidelines state that behavioral checklists and questionnaires should only be used to provide guidance for referrals, for information gathering (for example, prior to assessment or as outcome measures for interventions), and as measures to describe the functional impact of auditory processing disorder. They are not designed for the purpose of diagnosing auditory processing disorders. The New Zealand guidelines indicate that a number of questionnaires have been developed to identify children who might benefit from evaluation of their problems in listening. Examples of available questionnaires include the Fisher’s Auditory Problems Checklist , the Children’s Auditory Performance Scale, the Screening Instrument for Targeting Educational Risk, and the Auditory Processing Domains Questionnaire among others. All of the previous questionnaires were designed for children and none are useful for adolescents and adults.\n\nThe University of Cincinnati Auditory Processing Inventory (UCAPI) was designed for use with adolescents and adults seeking testing for evaluation of problems with listening and/or to be used following diagnosis of an auditory processing disorder to determine the subject’s status. Following a model described by Zoppo et al. (2015) a 34-item questionnaire was developed that investigates auditory processing abilities in each of the six common areas of complaint in APD (listening and concentration, understanding speech, following spoken instructions, attention, and other.) The final questionnaire was standardized on normally achieving young adults ranging from 18 to 27 years of age. Validation data was acquired from subjects with language-learning or auditory processing disorders who were either self-reported or confirmed by diagnostic testing. A UCAPI total score is calculated by combining the totals from the six listening conditions and provides an overall value to categorize listening abilities. Additionally, analysis of the scores from the six listening conditions provides an auditory profile for the subject. Each listening condition can then be utilized by the professional in making recommendation for diagnosing problem of learning through listening and treatment decisions. The UCAPI provides information on listening problems in various populations that can aid examiners in making recommendations for assessment and management.\n\nAPD has been defined anatomically in terms of the integrity of the auditory areas of the nervous system. However, children with symptoms of APD typically have no evidence of neurological disease and the diagnosis is made on the basis of performance on behavioral auditory tests. Auditory processing is \"what we do with what we hear\", and in APD there is a mismatch between peripheral hearing ability (which is typically normal) and ability to interpret or discriminate sounds. Thus in those with no signs of neurological impairment, APD is diagnosed on the basis of auditory tests. There is, however, no consensus as to which tests should be used for diagnosis, as evidenced by the succession of task force reports that have appeared in recent years. The first of these occurred in 1996. This was followed by a conference organized by the American Academy of Audiology. Experts attempting to define diagnostic criteria have to grapple with the problem that a child may do poorly on an auditory test for reasons other than poor auditory perception: for instance, failure could be due to inattention, difficulty in coping with task demands, or limited language ability. In an attempt to rule out at least some of these factors, the American Academy of Audiology conference explicitly advocated that for APD to be diagnosed, the child must have a modality-specific problem, i.e. affecting auditory but not visual processing. However, a committee of the American Speech-Language-Hearing Association subsequently rejected modality-specificity as a defining characteristic of auditory processing disorders.\n\nin 2005 The American Speech-Language-Hearing Association (ASHA) published \"Central Auditory Processing Disorders\" as an update to the 1996 \"Central Auditory Processing: Current Status of Research and Implications for Clinical Practice\". The American Academy of Audiology has released more current practice guidelines related to the disorder. ASHA formally defines APA as \"a difficulty in the efficiency and effectiveness by which the central nervous system (CNS) utilizes auditory information.\"\n\nIn 2011, the British Society of Audiology published 'best practice guidelines'.\n\nAuditory processing disorder can be developmental or acquired. It may result from ear infections, head injuries or neurodevelopmental delays that affect processing of auditory information. This can include problems with: \"...sound localization and lateralization (see also binaural fusion); auditory discrimination; auditory pattern recognition; temporal aspects of audition, including temporal integration, temporal discrimination (e.g., temporal gap detection), temporal ordering, and temporal masking; auditory performance in competing acoustic signals (including dichotic listening); and auditory performance with degraded acoustic signals\".\n\nThe Committee of UK Medical Professionals Steering the UK Auditory Processing Disorder Research Program have developed the following working definition of Auditory Processing Disorder: \"APD results from impaired neural function and is characterized by poor recognition, discrimination, separation, grouping, localization, or ordering of speech sounds. It does not solely result from a deficit in general attention, language or other cognitive processes.\"\n\n1. The SCAN-C for children and SCAN-A for adolescents and adults are the most common tools for screening and diagnosing APD in the USA. Both tests are standardized on a large number of subjects and include validation data on subjects with auditory processing disorders. The SCAN test batteries include screening tests: norm-based criterion-referenced scores; diagnostic tests: scaled scores, percentile ranks and ear advantage scores for all tests except the Gap Detection test. The four tests include four subsets on which the subject scores are derived include: discrimination of monaurally presented single words against background noise (speech in noise), acoustically degraded single words (filtered words), dichotically presented single words and sentences.\n\n2. Random Gap Detection Test (RGDT) is also a standardized test. It assesses an individual’s gap detection threshold of tones and white noise. The exam includes stimuli at four different frequencies (500, 1000, 2000, and 4000 Hz) and white noise clicks of 50 ms duration. It is a useful test because it provides an index of auditory temporal resolution. In children, an overall gap detection threshold greater than 20 ms means they have failed and may have an auditory processing disorder based on abnormal perception of sound in the time domain.\n\n3. Gaps in Noise Test (GIN) also measures temporal resolution by testing the patient's gap detection threshold in white noise.\n\n4. Pitch Patterns Sequence Test (PPT) and Duration Patterns Sequence Test (DPT) measure auditory pattern identification. The PPS has s series of three tones presented at either of two pitches (high or low). Meanwhile, the DPS has a series of three tones that vary in duration rather than pitch (long or short). Patients are then asked to describe the pattern of pitches presented.\n\n5. Masking Level Difference (MLD) at 500 Hz measures overlapping temporal processing, binaural processing, and low-redundancy by measuring the difference in threshold of an auditory stimulus when a masking noise is presented in and out of phase.\n\nThe issue of modality-specificity has led to considerable debate among experts in this field. Cacace and McFarland have argued that APD should be defined as a \"modality-specific\" perceptual dysfunction that is not due to peripheral hearing loss. They criticise more inclusive conceptualizations of APD as lacking diagnostic specificity. A requirement for modality-specificity could potentially avoid including children whose poor auditory performance is due to general factors such as poor attention or memory. Others, however, have argued that a modality-specific approach is too narrow, and that it would miss children who had genuine perceptual problems affecting both visual and auditory processing. It is also impractical, as audiologists do not have access to standardized tests that are visual analogs of auditory tests. The debate over this issue remains unresolved. It is clear, however, that a modality-specific approach will diagnose fewer children with APD than a modality-general one, and that the latter approach runs a risk of including children who fail auditory tests for reasons other than poor auditory processing. Although modality-specific testing has been advocated for well over a decade, to date no tests have been published which would allow audiologists to perform a modality-specific evaluation (i.e., no clinical versions of visual analogs to auditory processing tests exist).\n\nAnother controversy concerns the fact that most traditional tests of APD use verbal materials. The British Society of Audiology has embraced Moore's (2006) recommendation that tests for APD should assess processing of \"non-speech sounds\". The concern is that if verbal materials are used to test for APD, then children may fail because of limited language ability. An analogy may be drawn with trying to listen to sounds in a foreign language. It is much harder to distinguish between sounds or to remember a sequence of words in a language you do not know well: the problem is not an auditory one, but rather due to lack of expertise in the language. \n\nIn recent years there have been additional criticisms of some popular tests for diagnosis of APD. Tests that use tape-recorded American English have been shown to over-identify APD in speakers of other forms of English. Performance on a battery of non-verbal auditory tests devised by the Medical Research Council's Institute of Hearing Research was found to be heavily influenced by non-sensory task demands, and indices of APD had low reliability when this was controlled for. This research undermines the validity of APD as a distinct entity in its own right and suggests that the use of the term \"disorder\" itself is unwarranted. In a recent review of such diagnostic issues, it was recommended that children with suspected auditory processing impairments receive a holistic psychometric assessment including general intellectual ability, auditory memory, and attention, phonological processing, language, and literacy. The authors state that \"a clearer understanding of the relative contributions of perceptual and non-sensory, unimodal and supramodal factors to performance on psychoacoustic tests may well be the key to unravelling the clinical presentation of these individuals.\"\n\nDepending on how it is defined, APD may share common symptoms with ADD/ADHD, specific language impairment, and autism spectrum disorders. A review showed substantial evidence for atypical processing of auditory information in children with autism. Dawes and Bishop noted how specialists in audiology and speech-language pathology often adopted different approaches to child assessment, and they concluded their review as follows: \"We regard it as crucial that these different professional groups work together in carrying out assessment, treatment and management of children and undertaking cross-disciplinary research.\" In practice, this seems rare.\n\nTo ensure that APD is correctly diagnosed, the examiners must differentiate APD from other disorders with similar symptoms. Factors that should be taken into account during the diagnosis are: attention, auditory neuropathy, fatigue, hearing and sensitivity, intellectual and developmental age, medications, motivation, motor skills, native language and language experience, response strategies and decision-making style, and visual acuity.\n\nIt should also be noted that children under the age of seven cannot be evaluated correctly because their language and auditory processes are still developing. In addition, the presence of APD cannot be evaluated when a child's primary language is not English.\n\nThe National Institute on Deafness and Other Communication Disorders state that children with Auditory Processing Disorder often:\n- have trouble paying attention to and remembering information presented orally, and may cope better with visually acquired information\n- have problems carrying out multi-step directions given orally; need to hear only one direction at a time\n- have poor listening skills\n- need more time to process information\n- have low academic performance\n- have behavior problems\n- have language difficulties (e.g., they confuse syllable sequences and have problems developing vocabulary and understanding language)\n- have difficulty with reading, comprehension, spelling, and vocabulary\n\nAPD can manifest as problems determining the direction of sounds, difficulty perceiving differences between speech sounds and the sequencing of these sounds into meaningful words, confusing similar sounds such as \"hat\" with \"bat\", \"there\" with \"where\", etc. Fewer words may be perceived than were actually said, as there can be problems detecting the gaps between words, creating the sense that someone is speaking unfamiliar or nonsense words. In addition, it is common for APD to cause speech errors involving the distortion and substitution of consonant sounds. Those suffering from APD may have problems relating what has been said with its meaning, despite obvious recognition that a word has been said, as well as repetition of the word. Background noise, such as the sound of a radio, television or a noisy bar can make it difficult to impossible to understand speech, since spoken words may sound distorted either into irrelevant words or words that don't exist, depending on the severity of the auditory processing disorder. Using a telephone can be problematic for someone with auditory processing disorder, in comparison with someone with normal auditory processing, due to low quality audio, poor signal, intermittent sounds and the chopping of words. Many who have auditory processing disorder subconsciously develop visual coping strategies, such as lip reading, reading body language, and eye contact, to compensate for their auditory deficit, and these coping strategies are not available when using a telephone.\n\nAs noted above, the status of APD as a distinct disorder has been queried, especially by speech-language pathologists and psychologists, who note the overlap between clinical profiles of children diagnosed with APD and those with other forms of specific learning disability. Many audiologists, however, would dispute that APD is just an alternative label for dyslexia, SLI, or ADHD, noting that although it often co-occurs with these conditions, it can be found in isolation.\n\nBased on sensitized measures of auditory dysfunction and on psychological assessment, patients can be subdivided into seven subcategories:\n\n1. middle ear dysfunction\n2. mild cochlear pathology\n3. central/medial olivocochlear efferent system (MOCS) auditory dysfunction\n4. purely psychological problems\n5. multiple auditory pathologies\n6. combined auditory dysfunction and psychological problems\n7. unknown\n\nDifferent subgroups may represent different pathogenic and aetiological factors. Thus, subcategorization provides further understanding of the basis of Auditory Processing Disorder, and hence may guide the rehabilitative management of these patients. This was suggested by Professor Dafydd Stephens and F Zhao at the Welsh Hearing Institute, Cardiff University.\n\nTreatment of APD typically focuses on three primary areas: changing learning environment, developing higher-order skills to compensate for the disorder, and remediation of the auditory deficit itself. However, there is a lack of well-conducted evaluations of intervention using randomized controlled trial methodology. Most evidence for effectiveness adopts weaker standards of evidence, such as showing that performance improves after training. This does not control for possible influences of practice, maturation, or placebo effects. Recent research has shown that practice with basic auditory processing tasks (i.e. auditory training) may improve performance on auditory processing measures and phonemic awareness measures. Changes after auditory training have also been recorded at the physiological level. Many of these tasks are incorporated into computer-based auditory training programs such as Earobics and Fast ForWord, an adaptive software available at home and in clinics worldwide, but overall, evidence for effectiveness of these computerised interventions in improving language and literacy is not impressive. One small-scale uncontrolled study reported successful outcomes for children with APD using auditory training software.\n\nTreating additional issues related to APD can result in success. For example, treatment for phonological disorders (difficulty in speech) can result in success in terms of both the phonological disorder as well as APD. In one study, speech therapy improved auditory evoked potentials (a measure of brain activity in the auditory portions of the brain).\n\nWhile there is evidence that language training is effective for improving APD, there is no current research supporting the following APD treatments:\n- Auditory Integration Training typically involves a child attending two 30-minute sessions per day for ten days.\n- Lindamood-Bell Learning Processes (particularly, the Visualizing and Verbalizing program)\n- Physical activities that require frequent crossing of the midline (e.g., occupational therapy)\n- Sound Field Amplification\n- Neuro-Sensory Educational Therapy\n- Neurofeedback\n\nHowever, use of a FM transmitter has been shown to produce significant improvements over time with children.\n\nSamuel J. Kopetzky, who first described the condition in 1948. P. F. King, first discussed the aetiological factors behind it in 1954. Helmer Myklebust's 1954 study, \"Auditory Disorders in Children\". suggested auditory processing disorder was separate from language learning difficulties. His work sparked interest in auditory deficits after acquired brain lesions affecting the temporal lobes and led to additional work looking at the physiological basis of auditory processing, but it was not until the late seventies and early eighties that research began on APD in depth.\nIn 1977, the first conference on the topic of APD was organized by Robert W. Keith, Ph.D. at the University of Cincinnati. The proceedings of that conference was published by Grune and Stratton under the title \"Central Auditory Dysfunction\" (Keith RW Ed.) That conference started a new series of studies focusing on APD in children. Virtually all tests currently used to diagnose APD originate from this work. These early researchers also invented many of the auditory training approaches, including interhemispheric transfer training and interaural intensity difference training. This period gave us a rough understanding of the causes and possible treatment options for APD.\nMuch of the work in the late nineties and 2000s has been looking to refining testing, developing more sophisticated treatment options, and looking for genetic risk factors for APD. Scientists have worked on improving behavioral tests of auditory function, neuroimaging, electroacoustic, and electrophysiologic testing. Working with new technology has led to a number of software programs for auditory training. With global awareness of mental disorders and increasing understanding of neuroscience, auditory processing is more in the public and academic consciousness than ever before.\n\n", "related": "\n- Cocktail party effect\n- Dafydd Stephens\n- Hearing loss\n- List of eponymous diseases\n- Amblyaudia\n- Auditory verbal agnosia\n- Cortical deafness\n- Echoic memory\n- Language processing\n- Spatial hearing loss\n- Music-specific disorders\n\n- Auditory processing disorder: An overview for the clinician\n- American Speech-Language-Hearing Association (ASHA)\n"}
{"id": "16704183", "url": "https://en.wikipedia.org/wiki?curid=16704183", "title": "Speech interface guideline", "text": "Speech interface guideline\n\nSpeech interface guideline is a guideline with the aim for guiding decisions and criteria regarding designing interfaces operated by human voice. Speech interface system has many advantages such as consistent service and saving cost. However, for users, listening is a difficult task. It can become impossible when too many options are provided at once. This may mean that a user cannot intuitively reach a decision. To avoid this problem, limit options and a few clear choices the developer should consider such difficulties are usually provided. The guideline suggests the solution which is able to satisfy the users (customers). The goal of the guideline is to make an automated transaction at least as attractive and efficient as interacting with an attendant.\n\nThe following guideline is given by the Lucent Technologies (now Alcatel-Lucent USA) CONVERSANT System Version 6.0 Application Design Guidelines\n- Know Your Callers\n- Use Simple and Natural Dialogue\n- Minimize Demands on the Caller’s Memory\n- Be Consistent\n- Provide Feedback\n- Provide Easy Exits\n- Offer Shortcuts\n- Allow Time for Caller Responses\n- When Caller Errors Occur\n\n", "related": "\n- Human-computer interaction\n- Human interface guidelines\n- Speech processing\n- Speech recognition\n- Speech technology\n\n- Speech Interface Guidelines Alexander I. Rudnicky, School of Computer Science, Carnegie Mellon University\n- School of Computer Science, Carnegie Mellon University\n- Voice User Interface Design VUI\n"}
{"id": "31139924", "url": "https://en.wikipedia.org/wiki?curid=31139924", "title": "Cache language model", "text": "Cache language model\n\nA cache language model is a type of statistical language model. These occur in the natural language processing subfield of computer science and assign probabilities to given sequences of words by means of a probability distribution. Statistical language models are key components of speech recognition systems and of many machine translation systems: they tell such systems which possible output word sequences are probable and which are improbable. The particular characteristic of a cache language model is that it contains a cache component and assigns relatively high probabilities to words or word sequences that occur elsewhere in a given text. The primary, but by no means sole, use of cache language models is in speech recognition systems.\n\nTo understand why it is a good idea for a statistical language model to contain a cache component one might consider someone who is dictating a letter about elephants to a speech recognition system. Standard (non-cache) N-gram language models will assign a very low probability to the word \"elephant\" because it is a very rare word in English. If the speech recognition system does not contain a cache component the person dictating the letter may be annoyed: each time the word \"elephant\" is spoken another sequence of words with a higher probability according to the N-gram language model may be recognized (e.g., \"tell a plan\"). These erroneous sequences will have to be deleted manually and replaced in the text by \"elephant\" each time \"elephant\" is spoken. If the system has a cache language model, \"elephant\" will still probably be misrecognized the first time it is spoken and will have to be entered into the text manually; however, from this point on the system is aware that \"elephant\" is likely to occur again – the estimated probability of occurrence of \"elephant\" has been increased, making it more likely that if it is spoken it will be recognized correctly. Once \"elephant\" has occurred several times the system is likely to recognize it correctly every time it is spoken until the letter has been completely dictated. This increase in the probability assigned to the occurrence of \"elephant\" is an example of a consequence of machine learning and more specifically of pattern recognition.\n\nThere exist variants of the cache language model in which not only single words but also multi-word sequences that have occurred previously are assigned higher probabilities (e.g., if \"San Francisco\" occurred near the beginning of the text subsequent instances of it would be assigned a higher probability).\n\nThe cache language model was first proposed in a paper published in 1990, after which the IBM speech-recognition group experimented with the concept. The group found that implementation of a form of cache language model yielded a 24% drop in word-error rates once the first few hundred words of a document had been dictated. A detailed survey of language modeling techniques concluded that the cache language model was one of the few new language modeling techniques that yielded improvements over the standard N-gram approach: \"Our caching results show that caching is by far the most useful technique for perplexity reduction at small and medium training data sizes\".\n\nThe development of the cache language model has generated considerable interest among those concerned with computational linguistics in general and statistical natural language processing in particular: recently there has been interest in applying the cache language model in the field of statistical machine translation.\n\nThe success of the cache language model in improving word prediction rests on the human tendency to use words in a \"bursty\" fashion: when one is discussing a certain topic in a certain context the frequency with which one uses certain words will be quite different from their frequencies when one is discussing other topics in other contexts. The traditional N-gram language models, which rely entirely on information from a very small number (four, three, or two) of words preceding the word to which a probability is to be assigned, do not adequately model this \"burstiness\".\n\nRecently, the cache language model concept - originally conceived for the N-gram statistical language model paradigm - has been adapted for use in the neural paradigm. For instance, recent work on continuous cache language models in the recurrent neural network (RNN) setting has applied the cache concept to much larger contexts than before, yielding significant reductions in perplexity\n. Another recent line of research involves incorporating a cache component in a feed-forward neural language model (FN-LM) to achieve rapid domain adaptation \n\n", "related": "\n- Artificial intelligence\n- History of natural language processing\n- History of machine translation\n- Speech recognition\n- Statistical machine translation\n"}
{"id": "6006314", "url": "https://en.wikipedia.org/wiki?curid=6006314", "title": "TuVox", "text": "TuVox\n\nTuVox is a company that produces VXML-based telephone speech-recognition applications to replace DTMF touch-tone systems for their clients.\n\nTuVox was founded in 2001 by Steven S. Pollock and Ashok Khosla, formerly of Apple Computer Corporation and Claris Corporation. Since then, TuVox has grown to over 150 employees and has US offices in Cupertino, California and Boca Raton, Florida as well as international offices in London, Vancouver and Sydney. In 2005, TuVox acquired the customers and hosting facilities of Net-By-Tel.\n\nOn July 22, 2010, West Interactive—a subsidiary of West Corporation—announced its acquisition of TuVox.\n\nTuVox clients include 1-800-Flowers.com, AMC Entertainment, American Airlines, British Airways, M&T Bank, Canon Inc., Gateway, Inc., Motorola, Progress Energy Inc., Telecom New Zealand, Time, Inc., BECU, Virgin America and USAA.\n\n- Notes\n\n- Destination CRM, April 1, 2007: Winnowing Customer Care Woes. by Coreen Bailor, Destination CRM\n- ZDNet News, September 28, 2004: TuVox calls for the 'perfect' app. By Nadia Ilyin, ZDNet News\n- TMCnet. September 29, 2005. TuVox Acquires NetByTel's Hosted Speech. By David Sims, TMCnet CRM Alert Columnist\n- Speech Technology Magazine’s 2006 Most Innovative Solutions Awards.Canon's speech application from TuVox named “Most Innovative Solution”. Speech Technology Magazine recognizes Canon's deployment of TuVox Perfect Router as most creative in the industry.\n- Speech Technology Magazine NewsBlast May 24, 2006 - Speech Technology Magazine’s Q&A with Steve Pollock, Co-founder and EVP of TuVox\n- San Jose Mercury News, March 21, 2005 - 1E Tech Monday “They speak, thereby they brand. Speech recognition systems that converse with you give companies a new way to say who they are”\n- Red Herring (magazine) July 18, 2005 - TuVox: Anatomy of a Deal, The inside story of how TuVox raised the cash to become an international player.\n- Call Center Magazine April 1, 2006 - Is Hosting the Future of Speech?\n- The Wall Street Journal June 5, 2006 - Big Tech Companies Shop for Tiny Ventures\n- Press Release: West Interactive Acquires TuVox\n", "related": "NONE"}
{"id": "23322684", "url": "https://en.wikipedia.org/wiki?curid=23322684", "title": "Speaker diarisation", "text": "Speaker diarisation\n\nSpeaker diarisation (or diarization) is the process of partitioning an input audio stream into homogeneous segments according to the speaker identity. It can enhance the readability of an automatic speech transcription by structuring the audio stream into speaker turns and, when used together with speaker recognition systems, by providing the speaker’s true identity. It is used to answer the question \"who spoke when?\"\nSpeaker diarisation is a combination of speaker segmentation and speaker clustering. The first aims at finding speaker change points in an audio stream. The second aims at grouping together speech segments on the basis of speaker characteristics.\n\nWith the increasing number of broadcasts, meeting recordings and voice mail collected every year, speaker diarisation has received much attention by the speech community, as is manifested by the specific evaluations devoted to it under the auspices of the National Institute of Standards and Technology for telephone speech, broadcast news and meetings.\n\nIn speaker diarisation one of the most popular methods is to use a Gaussian mixture model to model each of the speakers, and assign the corresponding frames for each speaker with the help of a Hidden Markov Model. There are two main kinds of clustering scenario. The first one is by far the most popular and is called Bottom-Up. The algorithm starts in splitting the full audio content in a succession of clusters and progressively tries to merge the redundant clusters in order to reach a situation where each cluster corresponds to a real speaker. The second clustering strategy is called top-down and starts with one single cluster for all the audio data and tries to split it iteratively until reaching a number of clusters equal to the number of speakers.\nA 2010 review can be found at \n\nThere are some open source initiatives for speaker diarisation:\n\n- ALIZE Speaker Diarization (last repository update: July 2016; last release: February 2013, version: 3.0): ALIZE Diarization System, developed at the University Of Avignon, a release 2.0 is available .\n- SpkDiarization (last release: September 2013, version: 8.4.1): LIUM_SpkDiarization tool .\n- Audioseg (last repository update: May 2014; last release: January 2010, version: 1.2): AudioSeg is a toolkit dedicated to audio segmentation and classification of audio streams. .\n- SHoUT (last update: December 2010; version: 0.3): SHoUT is a software package developed at the University of Twente to aid speech recognition research. SHoUT is a Dutch acronym for \"Speech Recognition Research at the University of Twente\".\n- pyAudioAnalysis (last repository update: August 2018): Python Audio Analysis Library: Feature Extraction, Classification, Segmentation and Applications\n\n", "related": "NONE"}
{"id": "33993614", "url": "https://en.wikipedia.org/wiki?curid=33993614", "title": "Neurocomputational speech processing", "text": "Neurocomputational speech processing\n\nNeurocomputational speech processing is computer-simulation of speech production and speech perception by referring to the natural neuronal processes of speech production and speech perception, as they occur in the human nervous system (central nervous system and peripheral nervous system). This topic is based on neuroscience and computational neuroscience.\n\nNeurocomputational models of speech processing are complex. They comprise at least a cognitive part, a motor part and a sensory part.\n\nThe cognitive or linguistic part of a neurocomputational model of speech processing comprises the neural activation or generation of a phonemic representation on the side of speech production (e.g. neurocomputational and extended version of the Levelt model developed by Ardi Roelofs: WEAVER++ as well as the neural activation or generation of an intention or meaning on the side of speech perception or speech comprehension.\n\nThe motor part of a neurocomputational model of speech processing starts with a phonemic representation of a speech item, activates a motor plan and ends with the articulation of that particular speech item (see also: articulatory phonetics).\n\nThe sensory part of a neurocomputational model of speech processing starts with an acoustic signal of a speech item (acoustic speech signal), generates an auditory representation for that signal and activates a phonemic representations for that speech item.\n\nNeurocomputational speech processing is speech processing by artificial neural networks. Neural maps, mappings and pathways as described below, are model structures, i.e. important structures within artificial neural networks.\n\nAn artificial neural network can be separated in three types of neural maps, also called \"layers\":\n1. input maps (in the case of speech processing: primary auditory map within the auditory cortex, primary somatosensory map within the somatosensory cortex),\n2. output maps (primary motor map within the primary motor cortex), and\n3. higher level cortical maps (also called \"hidden layers\").\n\nThe term \"neural map\" is favoured here over the term \"neural layer\", because a cortial neural map should be modeled as a 2D-map of interconnected neurons (e.g. like a self-organizing map; see also Fig. 1). Thus, each \"model neuron\" or \"artificial neuron\" within this 2D-map is physiologically represented by a cortical column since the cerebral cortex anatomically exhibits a layered structure.\n\nA neural representation within an artificial neural network is a temporarily activated (neural) state within a specific neural map. Each neural state is represented by a specific neural activation pattern. This activation pattern changes during speech processing (e.g. from syllable to syllable).\nIn the ACT model (see below), it is assumed that an auditory state can be represented by a \"neural spectrogram\" (see Fig. 2) within an auditory state map. This auditory state map is assumed to be located in the auditory association cortex (see cerebral cortex).\n\nA somatosensory state can be divided in a tactile and proprioceptive state and can be represented by a specific neural activation pattern within the somatosensory state map. This state map is assumed to be located in the somatosensory association cortex (see cerebral cortex, somatosensory system, somatosensory cortex).\n\nA motor plan state can be assumed for representing a motor plan, i.e. the planning of speech articulation for a specific syllable or for a longer speech item (e.g. word, short phrase). This state map is assumed to be located in the premotor cortex, while the instantaneous (or lower level) activation of each speech articulator occurs within the primary motor cortex (see motor cortex).\n\nThe neural representations occurring in the sensory and motor maps (as introduced above) are distributed representations (Hinton et al. 1968): Each neuron within the sensory or motor map is more or less activated, leading to a specific activation pattern.\n\nThe neural representation for speech units occurring in the speech sound map (see below: DIVA model) is a punctual or local representation. Each speech item or speech unit is represented here by a specific neuron (model cell, see below).\n\nA neural mapping connects two cortical neural maps. Neural mappings (in contrast to neural pathways) store training information by adjusting their neural link weights (see artificial neuron, artificial neural networks). Neural mappings are capable of generating or activating a distributed representation (see above) of a sensory or motor state within a sensory or motor map from a punctual or local activation within the other map (see for example the synaptic projection from speech sound map to motor map, to auditory target region map, or to somatosensory target region map in the DIVA model, explained below; or see for example the neural mapping from phonetic map to auditory state map and motor plan state map in the ACT model, explained below and Fig. 3).\n\nNeural mapping between two neural maps are compact or dense: Each neuron of one neural map is interconnected with (nearly) each neuron of the other neural map (many-to-many-connection, see artificial neural networks). Because of this density criterion for neural mappings, neural maps which are interconnected by a neural mapping are not far apart from each other.\n\nIn contrast to neural mappings neural pathways can connect neural maps which are far apart (e.g. in different cortical lobes, see cerebral cortex). From the functional or modeling viewpoint, neural pathways mainly forward information without processing this information. A neural pathway in comparison to a neural mapping need much less neural connections. A neural pathway can be modelled by using a one-to-one connection of the neurons of both neural maps (see topographic mapping and see somatotopic arrangement).\n\nExample: In the case of two neural maps, each comprising 1,000 model neurons, a neural mapping needs up to 1,000,000 neural connections (many-to-many-connection), while only 1,000 connections are needed in the case of a neural pathway connection.\n\nFurthermore, the link weights of the connections within a neural mapping are adjusted during training, while the neural connections in the case of a neural pathway need not to be trained (each connection is maximal exhibitory).\n\nThe leading approach in neurocomputational modeling of speech production is the DIVA model developed by Frank H. Guenther and his group at Boston University. The model accounts for a wide range of phonetic and neuroimaging data but - like each neurocomputational model - remains speculative to some extent.\n\nThe organization or structure of the DIVA model is shown in Fig. 4.\n\nThe speech sound map - assumed to be located in the inferior and posterior portion of Broca's area (left frontal operculum) - represents (phonologically specified) language-specific speech units (sounds, syllables, words, short phrases). Each speech unit (mainly syllables; e.g. the syllable and word \"palm\" /pam/, the syllables /pa/, /ta/, /ka/, ...) is represented by a specific model cell within the speech sound map (i.e. punctual neural representations, see above). Each model cell (see artificial neuron) corresponds to a small population of neurons which are located at close range and which fire together.\n\nEach neuron (model cell, artificial neuron) within the speech sound map can be activated and subsequently activates a forward motor command towards the motor map, called articulatory velocity and position map. The activated neural representation on the level of that motor map determines the articulation of a speech unit, i.e. controls all articulators (lips, tongue, velum, glottis) during the time interval for producing that speech unit. Forward control also involves subcortical structures like the cerebellum, not modelled in detail here.\n\nA speech \"unit\" represents an amount of speech \"items\" which can be assigned to the same phonemic category. Thus, each speech unit is represented by one specific neuron within the speech sound map, while the realization of a speech unit may exhibit some articulatory and acoustic variability. This phonetic variability is the motivation to define sensory target \"regions\" in the DIVA model (see Guenther et al. 1998).\n\nThe activation pattern within the motor map determines the movement pattern of all model articulators (lips, tongue, velum, glottis) for a speech item. In order not to overload the model, no detailed modeling of the neuromuscular system is done. The Maeda articulatory speech synthesizer is used in order to generate articulator movements, which allows the generation of a time-varying vocal tract form and the generation of the acoustic speech signal for each particular speech item.\n\nIn terms of artificial intelligence the articulatory model can be called plant (i.e. the system, which is controlled by the brain); it represents a part of the embodiement of the neuronal speech processing system. The articulatory model generates sensory output which is the basis for generating feedback information for the DIVA model (see below: feedback control).\n\nOn the one hand the articulatory model generates sensory information, i.e. an auditory state for each speech unit which is neurally represented within the auditory state map (distributed representation), and a somatosensory state for each speech unit which is neurally represented within the somatosensory state map (distributed representation as well). The auditory state map is assumed to be located in the superior temporal cortex while the somatosensory state map is assumed to be located in the inferior parietal cortex.\n\nOn the other hand, the speech sound map, if activated for a specific speech unit (single neuron activation; punctual activation), activates sensory information by synaptic projections between speech sound map and auditory target region map and between speech sound map and somatosensory target region map. Auditory and somatosensory target regions are assumed to be located in higher-order auditory cortical regions and in higher-order somatosensory cortical regions respectively. These target region sensory activation patterns - which exist for each speech unit - are learned during speech acquisition (by imitation training; see below: learning).\n\nConsequently, two types of sensory information are available if a speech unit is activated at the level of the speech sound map: (i) learned sensory target regions (i.e. \"intended\" sensory state for a speech unit) and (ii) sensory state activation patterns resulting from a possibly imperfect execution (articulation) of a specific speech unit (i.e. \"current\" sensory state, reflecting the current production and articulation of that particular speech unit). Both types of sensory information is projected to sensory error maps, i.e. to an auditory error map which is assumed to be located in the superior temporal cortex (like the auditory state map) and to a somatosensosry error map which is assumed to be located in the inferior parietal cortex (like the somatosensory state map) (see Fig. 4).\n\nIf the current sensory state deviates from the intended sensory state, both error maps are generating feedback commands which are projected towards the motor map and which are capable to correct the motor activation pattern and subsequently the articulation of a speech unit under production. Thus, in total, the activation pattern of the motor map is not only influenced by a specific feedforward command learned for a speech unit (and generated by the synaptic projection from the speech sound map) but also by a feedback command generated at the level of the sensory error maps (see Fig. 4).\n\nWhile the \"structure\" of a neuroscientific model of speech processing (given in Fig. 4 for the DIVA model) is mainly determined by evolutionary processes, the (language-specific) \"knowledge\" as well as the (language-specific) \"speaking skills\" are learned and trained during speech acquisition. In the case of the DIVA model it is assumed that the newborn has not available an already structured (language-specific) speech sound map; i.e. no neuron within the speech sound map is related to any speech unit. Rather the organization of the speech sound map as well as the tuning of the projections to the motor map and to the sensory target region maps is learned or trained during speech acquisition. Two important phases of early speech acquisition are modeled in the DIVA approach: Learning by babbling and by imitation.\n\nDuring babbling the synaptic projections between sensory error maps and motor map are tuned. This training is done by generating an amount of semi-random feedforward commands, i.e. the DIVA model \"babbles\". Each of these babbling commands leads to the production of an \"articulatory item\", also labeled as \"pre-linguistic (i.e. non language-specific) speech item\" (i.e. the articulatory model generates an articulatory movement pattern on the basis of the babbling motor command). Subsequently, an acoustic signal is generated.\n\nOn the basis of the articulatory and acoustic signal, a specific auditory and somatosensory state pattern is activated at the level of the sensory state maps (see Fig. 4) for each (pre-linguistic) speech item. At this point the DIVA model has available the sensory and associated motor activation pattern for different speech items, which enables the model to tune the synaptic projections between sensory error maps and motor map. Thus, during babbling the DIVA model learns feedback commands (i.e. how to produce a proper (feedback) motor command for a specific sensory input).\n\nDuring imitation the DIVA model organizes its speech sound map and tunes the synaptic projections between speech sound map and motor map - i.e. tuning of forward motor commands - as well as the synaptic projections between speech sound map and sensory target regions (see Fig. 4). Imitation training is done by exposing the model to an amount of acoustic speech signals representing realizations of language-specific speech units (e.g. isolated speech sounds, syllables, words, short phrases).\n\nThe tuning of the synaptic projections between speech sound map and auditory target region map is accomplished by assigning one neuron of the speech sound map to the phonemic representation of that speech item and by associating it with the auditory representation of that speech item, which is activated at the auditory target region map. Auditory \"regions\" (i.e. a specification of the auditory variability of a speech unit) occur, because one specific speech unit (i.e. one specific phonemic representation) can be realized by several (slightly) different acoustic (auditory) realizations (for the difference between speech \"item\" and speech \"unit\" see above: feedforward control) .\n\nThe tuning of the synaptic projections between speech sound map and motor map (i.e. tuning of forward motor commands) is accomplished with the aid of feedback commands, since the projections between sensory error maps and motor map were already tuned during babbling training (see above). Thus the DIVA model tries to \"imitate\" an auditory speech item by attempting to find a proper feedforward motor command. Subsequently, the model compares the resulting sensory output (\"current\" sensory state following the articulation of that attempt) with the already learned auditory target region (\"intended\" sensory state) for that speech item. Then the model updates the current feedforward motor command by the current feedback motor command generated from the auditory error map of the auditory feedback system. This process may be repeated several times (several attempts). The DIVA model is capable of producing the speech item with a decreasing auditory difference between current and intended auditory state from attempt to attempt.\n\nDuring imitation the DIVA model is also capable of tuning the synaptic projections from speech sound map to somatosensory target region map, since each new imitation attempt produces a new articulation of the speech item and thus produces a somatosensory state pattern which is associated with the phonemic representation of that speech item.\n\nWhile auditory feedback is most important during speech acquisition, it may be activated less if the model has learned a proper feedforward motor command for each speech unit. But it has been shown that auditory feedback needs to be strongly coactivated in the case of auditory perturbation (e.g. shifting a formant frequency, Tourville et al. 2005). This is comparable to the strong influence of visual feedback on reaching movements during visual perturbation (e.g. shifting the location of objects by viewing through a prism).\n\nIn a comparable way to auditory feedback, also somatosensory feedback can be strongly coactivated during speech production, e.g. in the case of unexpected blocking of the jaw (Tourville et al. 2005).\n\nA further approach in neurocomputational modeling of speech processing is the ACT model developed by Bernd J. Kröger and his group at RWTH Aachen University, Germany (Kröger et al. 2014, Kröger et al. 2009, Kröger et al. 2011). The ACT model is in accord with the DIVA model in large parts. The ACT model focuses on the \"action repository\" (i.e. repository for sensorimotor speaking skills, comparable to the mental syllablary, see Levelt and Wheeldon 1994), which is not spelled out in detail in the DIVA model. Moreover, the ACT model explicitly introduces a level of motor plans, i.e. a high-level motor description for the production of speech items (see motor goals, motor cortex). The ACT model - like any neurocomputational model - remains speculative to some extent.\n\nThe organization or structure of the ACT model is given in Fig. 5.\n\nFor speech production, the ACT model starts with the activation of a phonemic representation of a speech item (phonemic map). In the case of a \"frequent syllable\", a co-activation occurs at the level of the phonetic map, leading to a further co-activation of the intended sensory state at the level of the sensory state maps and to a co-activation of a motor plan state at the level of the motor plan map. In the case of an \"infrequent syllable\", an attempt for a motor plan is generated by the motor planning module for that speech item by activating motor plans for phonetic similar speech items via the phonetic map (see Kröger et al. 2011). The motor plan or vocal tract action score comprises temporally overlapping vocal tract actions, which are programmed and subsequently executed by the motor programming, execution, and control module. This module gets real-time somatosensory feedback information for controlling the correct execution of the (intended) motor plan. Motor programing leads to activation pattern at the level lof the primary motor map and subsequently activates neuromuscular processing. Motoneuron activation patterns generate muscle forces and subsequently movement patterns of all model articulators (lips, tongue, velum, glottis). The Birkholz 3D articulatory synthesizer is used in order to generate the acoustic speech signal.\n\nArticulatory and acoustic feedback signals are used for generating somatosensory and auditory feedback information via the sensory preprocessing modules, which is forwarded towards the auditory and somatosensory map. At the level of the sensory-phonetic processing modules, auditory and somatosensory information is stored in short-term memory and the external sensory signal (ES, Fig. 5, which are activated via the sensory feedback loop) can be compared with the already trained sensory signals (TS, Fig. 5, which are activated via the phonetic map). Auditory and somatosensory error signals can be generated if external and intended (trained) sensory signals are noticeably different (cf. DIVA model).\n\nThe light green area in Fig. 5 indicates those neural maps and processing modules, which process a syllable as a whole unit (specific processing time window around 100 ms and more). This processing comprises the phonetic map and the directly connected sensory state maps within the sensory-phonetic processing modules and the directly connected motor plan state map, while the primary motor map as well as the (primary) auditory and (primary) somatosensory map process smaller time windows (around 10 ms in the ACT model).\nThe hypothetical cortical location of neural maps within the ACT model is shown in Fig. 6. The hypothetical locations of primary motor and primary sensory maps are given in magenta, the hypothetical locations of motor plan state map and sensory state maps (within sensory-phonetic processing module, comparable to the error maps in DIVA) are given in orange, and the hypothetical locations for the mirrored phonetic map is given in red. Double arrows indicate neuronal mappings. Neural mappings connect neural maps, which are not far apart from each other (see above). The two mirrored locations of the phonetic map are connected via a neural pathway (see above), leading to a (simple) one-to-one mirroring of the current activation pattern for both realizations of the phonetic map. This neural pathway between the two locations of the phonetic map is assumed to be a part of the fasciculus arcuatus (AF, see Fig. 5 and Fig. 6).\n\nFor speech perception, the model starts with an external acoustic signal (e.g. produced by an external speaker). This signal is preprocessed, passes the auditory map, and leads to an activation pattern for each syllable or word on the level of the auditory-phonetic processing module (ES: external signal, see Fig. 5). The ventral path of speech perception (see Hickok and Poeppel 2007) would directly activate a lexical item, but is not implemented in ACT. Rather, in ACT the activation of a phonemic state occurs via the phonemic map and thus may lead to a coactivation of motor representations for that speech item (i.e. dorsal pathway of speech perception; ibid.).\n\nThe phonetic map together with the motor plan state map, sensory state maps (occurring within the sensory-phonetic processing modules), and phonemic (state) map form the action repository. The phonetic map is implemented in ACT as a self-organizing neural map and different speech items are represented by different neurons within this map (punctual or local representation, see above: neural representations). The phonetic map exhibits three major characteristics: \n- More than one phonetic realization may occur within the phonetic map for one phonemic state (see phonemic link weights in Fig. 7: e.g. the syllable /de:m/ is represented by three neurons within the phonetic map)\n- Phonetotopy: The phonetic map exhibits an ordering of speech items with respect to different phonetic features (see phonemic link weights in Fig. 7. Three examples: (i) the syllables /p@/, /t@/, and /k@/ occur in an upward ordering at the left side within the phonetic map; (ii) syllable-initial plosives occur in the upper left part of the phonetic map while syllable initial fricatives occur in the lower right half; (iii) CV syllables and CVC syllables as well occur in different areas of the phonetic map.).\n- The phonetic map is hypermodal or multimodal: The activation of a phonetic item at the level of the phonetic map coactivates (i) a phonemic state (see phonemic link weights in Fig. 7), (ii) a motor plan state (see motor plan link weights in Fig. 7), (iii) an auditory state (see auditory link weights in Fig. 7), and (iv) a somatosensory state (not shown in Fig. 7). All these states are learned or trained during speech acquisition by tuning the synaptic link weights between each neuron within the phonetic map, representing a particular phonetic state and all neurons within the associated motor plan and sensory state maps (see also Fig. 3).\nThe phonetic map implements the action-perception-link within the ACT model (see also Fig. 5 and Fig. 6: the dual neural representation of the phonetic map in the frontal lobe and at the intersection of temporal lobe and parietal lobe).\n\nA motor plan is a high level motor description for the production and articulation of a speech items (see motor goals, motor skills, articulatory phonetics, articulatory phonology). In our neurocomputational model ACT a motor plan is quantified as a vocal tract action score. Vocal tract action scores quantitatively determine the number of vocal tract actions (also called articulatory gestures), which need to be activated in order to produce a speech item, their degree of realization and duration, and the temporal organization of all vocal tract actions building up a speech item (for a detailed description of vocal tract actions scores see e.g. Kröger & Birkholz 2007). The detailed realization of each vocal tract action (articulatory gesture) depends on the temporal organization of all vocal tract actions building up a speech item and especially on their temporal overlap. Thus the detailed realization of each vocal tract action within an speech item is specified below the motor plan level in our neurocomputational model ACT (see Kröger et al. 2011).\n\nA severe problem of phonetic or sensorimotor models of speech processing (like DIVA or ACT) is that the development of the phonemic map during speech acquisition is not modeled. A possible solution of this problem could be a direct coupling of action repository and mental lexicon without explicitly introducing a phonemic map at the beginning of speech acquisition (even at the beginning of imitation training; see Kröger et al. 2011 PALADYN Journal of Behavioral Robotics).\n\nA very important issue for all neuroscientific or neurocomputational approaches is to separate structure and knowledge. While the structure of the model (i.e. of the human neuronal network, which is needed for processing speech) is mainly determined by evolutionary processes, the knowledge is gathered mainly during speech acquisition by processes of learning. Different learning experiments were carried out with the model ACT in order to learn (i) a five-vowel system /i, e, a, o, u/ (see Kröger et al. 2009), (ii) a small consonant system (voiced plosives /b, d, g/ in combination with all five vowels acquired earlier as CV syllables (ibid.), (iii) a small model language comprising the five-vowel system, voiced and unvoiced plosives /b, d, g, p, t, k/, nasals /m, n/ and the lateral /l/ and three syllable types (V, CV, and CCV) (see Kröger et al. 2011) and (iv) the 200 most frequent syllables of Standard German for a 6-year-old child (see Kröger et al. 2011). In all cases, an ordering of phonetic items with respect to different phonetic features can be observed.\n\nDespite the fact that the ACT model in its earlier versions was designed as a pure speech production model (including speech acquisition), the model is capable of exhibiting important basic phenomena of speech perception, i.e. categorical perception and the McGurk effect. In the case of categorical perception, the model is able to exhibit that categorical perception is stronger in the case of plosives than in the case of vowels (see Kröger et al. 2009). Furthermore, the model ACT was able to exhibit the McGurk effect, if a specific mechanism of inhibition of neurons of the level of the phonetic map was implemented (see Kröger and Kannampuzha 2008).\n\n", "related": "\n- Speech production\n- Speech perception\n- Computational neuroscience\n- Articulatory synthesis\n- Auditory feedback\n\n- Iaroslav Blagouchine and Eric Moreau. \"Control of a Speech Robot via an Optimum Neural-Network-Based Internal Model with Constraints.\" IEEE Transactions on Robotics, vol. 26, no. 1, pp. 142—159, February 2010.\n"}
{"id": "39313229", "url": "https://en.wikipedia.org/wiki?curid=39313229", "title": "Fujisaki model", "text": "Fujisaki model\n\nThe Fujisaki model is a superpositional model for representing F contour of speech. According to the model, F contour is generated as a result of the superposition of the outputs of two second order linear filters with a base frequency value. The second order linear filters are for generating the phrase and accent components of speech. The base frequency is the minimum frequency value of the speaker.In other words, F contour is obtained by adding base frequency, phrase components and accent components. The model was proposed by Hiroya Fujisaki.\n\nformula_1\n\nwhere\n\nformula_2\n\nformula_3\n\n- An Introduction to Text-to-Speech Synthesis\n", "related": "NONE"}
{"id": "39306910", "url": "https://en.wikipedia.org/wiki?curid=39306910", "title": "Compressed sensing in speech signals", "text": "Compressed sensing in speech signals\n\nIn communications technology, the technique of \"compressed sensing\" (CS) may be applied to the processing of speech signals under certain conditions. In particular, compressed sensing can be used to reconstruct a sparse vector from a smaller number of measurements, provided the signal can be represented in sparse domain. \"Sparse domain\" refers to a domain in which only a few measurements have non-zero values. \n\nSuppose a signal formula_1 can be represented in a domain where only formula_2 coefficients out of formula_3 (where formula_4) are non-zero, then the signal is said to be sparse in that domain.\nThis reconstructed sparse vector can be used to construct back the original signal if the sparse domain of signal is known. CS can be applied to speech signal only if sparse domain of speech signal is known.\n\nConsider a speech signal formula_5, which can be represented in a domain formula_6 such that formula_7, where speech signal formula_8, dictionary matrix formula_9 and the sparse coefficient vector formula_10. This speech signal is said to be sparse in domain formula_6, if the number of significant (non zero) coefficients in sparse vector formula_12 is formula_13, where formula_14.\n\nThe observed signal formula_15 is of dimension formula_16. To reduce the complexity for solving formula_12 using CS speech signal is observed using a measurement matrix formula_18 such that\nwhere formula_19, and measurement matrix formula_20 such that formula_21.\nSparse decomposition problem for eq. 1 can be solved as standard formula_22 minimization as\n\nIf measurement matrix formula_18 satisfies the restricted isometric property (RIP) and is incoherent with dictionary matrix formula_6. then the reconstructed signal is much closer to the original speech signal.\nDifferent types of measurement matrices like random matrices can be used for speech signals.\nEstimating the sparsity of a speech signal is a problem since the speech signal varies greatly over time and thus sparsity of speech signal also varies highly over time. If sparsity of speech signal can be calculated over time without much complexity that will be best. If this is not possible then worst-case scenario for sparsity can be considered for a given speech signal.\nSparse vector (formula_25) for a given speech signal is reconstructed from as small as possible a number of measurements (formula_26) using formula_22 minimization. Then original speech signal is reconstructed form the calculated sparse vector formula_25 using the fixed dictionary matrix as formula_6 as formula_30 = formula_6formula_25. \nEstimation of both the dictionary matrix and sparse vector from random measurements only has been done iteratively.\nThe speech signal reconstructed from estimated sparse vector and dictionary matrix is much closer to the original signal.\nSome more iterative approaches to calculate both dictionary matrix and speech signal from just random measurements of speech signal have been developed.\n\nThe application of structured sparsity for joint speech localization-separation in reverberant acoustics has been investigated for multiparty speech recognition. Further applications of the concept of sparsity are yet to be studied in the field of speech processing. The idea behind applying CS to speech signals is to formulate algorithms or methods that use only those random measurements (formula_33) to carry out various forms of application-based processing such as speaker recognition and speech enhancement.\n", "related": "NONE"}
{"id": "28804", "url": "https://en.wikipedia.org/wiki?curid=28804", "title": "Signal separation", "text": "Signal separation\n\nSource separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing.\n\nThis problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio. However, blind signal separation is now routinely performed on multidimensional data, such as images and tensors, which may involve no time dimension whatsoever.\n\nSeveral approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.\n\nThe human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.\n\nAt a cocktail party, there is a group of people talking at the same time. You have multiple microphones picking up mixed signals, but you want to isolate the speech of a single person. BSS can be used to separate the individual sources by using mixed signals. In the presence of noise, dedicated optimization criteria need to be used\n\nFigure 2 shows the basic concept of BSS. The individual source signals are shown as well as the mixed signals which are received signals. BSS is used to separate the mixed signals with only knowing mixed signals and nothing about original signal or how they were mixed. The separated signals are only approximations of the source signals. The separated images, were separated using Python and the Shogun toolbox using Joint Approximation Diagonalization of Eigen-matrices (JADE) algorithm which is based off independent component analysis, ICA. This toolbox method can be used with multi-dimensions but for an easy visual aspect images(2-D) were used.\n\nOne of the practical applications being researched in this area is medical imaging of the brain with magnetoencephalography (MEG). This kind of imaging involves careful measurements of magnetic fields outside the head which yield an accurate 3D-picture of the interior of the head. However, external sources of electromagnetic fields, such as a wristwatch on the subject's arm, will significantly degrade the accuracy of the measurement. Applying source separation techniques on the measured signals can help remove undesired artifacts from the signal.\n\nIn electroencephalogram (EEG) and magnetoencephalography (MEG), the interference from muscle activity masks the desired signal from brain activity. BSS, however, can be used to separate the two so an accurate representation of brain activity may be achieved.\n\nAnother application is the separation of musical signals. For a stereo mix of relatively simple signals it is now possible to make a fairly accurate separation, although some artifacts remain.\n\nOther applications:\n- Communications\n- Stock Prediction\n- Seismic Monitoring\n- Text Document Analysis\n\nThe set of individual source signals, formula_1, is 'mixed' using a matrix, formula_2, to produce a set of 'mixed' signals, formula_3, as follows. Usually, formula_4 is equal to formula_5. If formula_6, then the system of equations is overdetermined and thus can be unmixed using a conventional linear method. If formula_7, the system is underdetermined and a non-linear method must be employed to recover the unmixed signals. The signals themselves can be multidimensional.\n\nformula_8\n\nThe above equation is effectively 'inverted' as follows. Blind source separation separates the set of mixed signals, formula_9, through the determination of an 'unmixing' matrix, formula_10, to 'recover' an approximation of the original signals, formula_11.\n\nformula_12\n\nSince the chief difficulty of the problem is its underdetermination, methods for blind source separation generally seek to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution. In one approach, exemplified by principal and independent component analysis, one seeks source signals that are minimally correlated or maximally independent in a probabilistic or information-theoretic sense. A second approach, exemplified by nonnegative matrix factorization, is to impose structural constraints on the source signals. These structural constraints may be derived from a generative model of the signal, but are more commonly heuristics justified by good empirical performance. A common theme in the second approach is to impose some kind of low-complexity constraint on the signal, such as sparsity in some basis for the signal space. This approach can be particularly effective if one requires not the whole signal, but merely its most salient features.\n\nThere are different methods of blind signal separation:\n\n- Principal components analysis\n- Singular value decomposition\n- Independent component analysis\n- Dependent component analysis\n- Non-negative matrix factorization\n- Low-complexity coding and decoding\n- Stationary subspace analysis\n- Common spatial pattern\n\n", "related": "\n- Adaptive filtering\n- Celemony Software#DNA Direct Note Access\n- Colin Cherry\n- Deconvolution\n- Factorial codes\n- Infomax principle\n- Segmentation (image processing)\n- Speech segmentation\n\n- Explanation of Independent Component Analysis (ICA)\n- A tutorial-style dissertation by Volker Koch that introduces message-passing on factor graphs to decompose EMG signals\n- Blind source separation flash presentation\n- Removing electroencephalographic artifacts by blind source separation\n"}
{"id": "5748975", "url": "https://en.wikipedia.org/wiki?curid=5748975", "title": "Time-compressed speech", "text": "Time-compressed speech\n\nTime-compressed speech refers to an audio recording of verbal text in which the text is presented in a much shorter time interval than it would through normally-paced real time speech. The basic purpose is to make recorded speech contain more words in a given time, yet still be understandable. For example: a paragraph that might normally be expected to take 20 seconds to read, might instead be presented in 15 seconds, which would represent a time-compression of 25% (5 seconds out of 20).\n\nThe term \"time-compressed speech\" should not be confused with \"speech compression\", which controls the volume range of a sound, but does not alter its time envelope.\n\nWhile some voice talents are capable of speaking at rates significantly in excess of general norms, the term \"time-compressed speech\" most usually refers to examples in which the time-reduction has been accomplished through some form of electronic processing of the recorded speech.\n\nIn general, recorded speech can be electronically time-compressed by: increasing its speed (linear compression); removing silences (selective editing); a combination of the two (non-linear compression). The speed of a recording can be increased, which will cause the material to be presented at a faster rate (and hence in a shorter amount of time), but this has the undesirable side-effect of increasing the frequency of the whole passage, raising the pitch of the voices, which can reduce intelligibility.\n\nThere are normally silences between words and sentences, and even small silences within certain words, both of which can be reduced or removed (\"edited-out\") which will also reduce the amount of time occupied by the full speech recording. However, this can also have the effect of removing verbal \"punctuation\" from the speech, causing words and sentences to run together unnaturally, again reducing intelligibility.\n\nThe current preferred method of time-compression is called \"non-linear compression\", which employs a combination of selectively removing silences; speeding up the speech to make the reduced silences sound normally-proportioned to the text; and finally applying various data algorithms to bring the speech back down to the proper pitch. This produces a more acceptable result than either of the two earlier techniques; however, if unrestrained, removing the silences and increasing the speed can make a selection of speech sound more insistent, possibly to the point of unpleasantness.\n\nTime-compressed speech is frequently used in television and radio advertising. The advantage of time-compressed speech is that the same number of words can be compressed into a smaller amount of time, reducing advertising costs, and/or allowing more information to be included in a given radio or TV advertisement. It is usually most noticeable in the information-dense caveats and disclaimers presented (usually by legal requirement) at the end of commercials—the aural equivalent of the \"fine print\" in a printed contract. This practice, however, is not new: before electronic methods were developed, spokespeople who could talk extremely quickly and still be understood were widely used as voice talents for radio and TV advertisements, and especially for recording such disclaimers.\n\nTime-compressed speech has educational applications such as increasing the information density of trainings, and as a study aid. A number of studies have demonstrated that the average person is capable of relatively easily comprehending speech delivered at higher-than-normal rates, with the peak occurring at around 25% compression (that is, 25% faster than normal); this facility has been demonstrated in several languages. Conversational speech (in English) takes place at a rate of around 150 wpm (words per minute), but the average person is able to comprehend speech presented at rates of up to 200-250 wpm without undue difficulty. Blind and severely visually impaired subjects scored similar comprehension levels at even higher rates, up to 300-350 wpm. Blind people have been found to use time-compressed speech extensively, for example, when reviewing recorded lectures from high school and college classes, or professional trainings. Comprehension rates in older blind subjects have been found to be as good, or in some cases better than those found in younger sighted subjects.\n\nOther studies have determined that the ability to comprehend highly time-compressed speech tends to fall off with increased age, and is also reduced when the language of the time-compressed speech is not the listener's native language. Non-native speakers can, however, improve their comprehension level of time-compressed speech with multiday training.\n\nVoice mail systems have employed time-compressed speech since as far back as the 1970s. IN this application, the technology enables the rapid review of messages in high-traffic systems, by a relatively small number of people.\n\nTime-compressed speech has been explored as one of a variety of interrelated factors which may be manipulated to increase the efficiency of streaming multimedia presentations, by significantly reducing the latency times involved in the transfer of large digitally encoded media files.\n\n", "related": "\n- Audio timescale-pitch modification\n- John Moschitta Jr., a spokesman capable of very fast speech\n"}
{"id": "433584", "url": "https://en.wikipedia.org/wiki?curid=433584", "title": "McGurk effect", "text": "McGurk effect\n\nThe McGurk effect is a perceptual phenomenon that demonstrates an interaction between hearing and vision in speech perception. The illusion occurs when the auditory component of one sound is paired with the visual component of another sound, leading to the perception of a third sound. The visual information a person gets from seeing a person speak changes the way they hear the sound. If a person is getting poor quality auditory information but good quality visual information, they may be more likely to experience the McGurk effect. Integration abilities for audio and visual information may also influence whether a person will experience the effect. People who are better at sensory integration have been shown to be more susceptible to the effect. Many people are affected differently by the McGurk effect based on many factors, including brain damage and other disorders.\nIt was first described in 1976 in a paper by Harry McGurk and John MacDonald, titled \"Hearing Lips and Seeing Voices\" in \"Nature\" (23 Dec 1976). This effect was discovered by accident when McGurk and his research assistant, MacDonald, asked a technician to dub a video with a different phoneme from the one spoken while conducting a study on how infants perceive language at different developmental stages. When the video was played back, both researchers heard a third phoneme rather than the one spoken or mouthed in the video.\n\nThis effect may be experienced when a video of one phoneme's production is dubbed with a sound-recording of a different phoneme being spoken. Often, the perceived phoneme is a third, intermediate phoneme. As an example, the syllables /ba-ba/ are spoken over the lip movements of /ga-ga/, and the perception is of /da-da/. McGurk and MacDonald originally believed that this resulted from the common phonetic and visual properties of /b/ and /g/. Two types of illusion in response to incongruent audiovisual stimuli have been observed: fusions ('ba' auditory and 'ga' visual produce 'da') and combinations ('ga' auditory and 'ba' visual produce 'bga'). This is the brain's effort to provide the consciousness with its best guess about the incoming information. The information coming from the eyes and ears is contradictory, and in this instance, the eyes (visual information) have had a greater effect on the brain and thus the fusion and combination responses have been created.\n\nVision is the primary sense for humans, but speech perception is multimodal, which means that it involves information from more than one sensory modality, in particular, audition and vision. The McGurk effect arises during phonetic processing because the integration of audio and visual information happens early in speech perception. The McGurk effect is very robust; that is, knowledge about it seems to have little effect on one's perception of it. This is different from certain optical illusions, which break down once one 'sees through' them. Some people, including those that have been researching the phenomenon for more than twenty years, experience the effect even when they are aware that it is taking place. With the exception of people who can identify most of what is being said from speech-reading alone, most people are quite limited in their ability to identify speech from visual-only signals. A more extensive phenomenon is the ability of visual speech to increase the intelligibility of heard speech in a noisy environment. Visible speech can also alter the perception of perfectly audible speech sounds when the visual speech stimuli are mismatched with the auditory speech. Normally, speech perception is thought to be an auditory process; however, our use of information is immediate, automatic, and, to a large degree, unconscious and therefore, despite what is widely accepted as true, speech is not only something we hear. Speech is perceived by all of the senses working together (seeing, touching, and listening to a face move). The brain is often unaware of the separate sensory contributions of what it perceives. Therefore, when it comes to recognizing speech the brain cannot differentiate whether it is seeing or hearing the incoming information.\n\nIt has also been examined in relation to witness testimony. Wareham and Wright's 2005 study showed that inconsistent visual information can change the perception of spoken utterances, suggesting that the McGurk effect may have many influences in everyday perception. Not limited to syllables, the effect can occur in whole words and have an effect on daily interactions that people are unaware of. Research into this area can provide information on not only theoretical questions, but also it can provide therapeutic and diagnostic relevance for those with disorders relating to audio and visual integration of speech cues.\n\nBoth hemispheres of the brain make a contribution to the McGurk effect. They work together to integrate speech information that is received through the auditory and visual senses. A McGurk response is more likely to occur in right-handed individuals for whom the face has privileged access to the right hemisphere and words to the left hemisphere. In people that have had callosotomies done, the McGurk effect is still present but significantly slower. In people with lesions to the left hemisphere of the brain, visual features often play a critical role in speech and language therapy. People with lesions in the left hemisphere of the brain show a greater McGurk effect than normal controls. Visual information strongly influences speech perception in these people. There is a lack of susceptibility to the McGurk illusion if left hemisphere damage resulted in a deficit to visual segmental speech perception. In people with right hemisphere damage, impairment on both visual-only and audio-visual integration tasks is exhibited, although they are still able to integrate the information to produce a McGurk effect. Integration only appears if visual stimuli is used to improve performance when the auditory signal is impoverished but audible. Therefore, there is a McGurk effect exhibited in people with damage to the right hemisphere of the brain but the effect is not as strong as a normal group.\n\nDyslexic individuals exhibit a smaller McGurk effect than normal readers of the same chronological age, but they showed the same effect as reading-level age-matched readers. Dyslexics particularly differed for combination responses, not fusion responses. The smaller McGurk effect may be due to the difficulties dyslexics have in perceiving and producing consonant clusters.\n\nChildren with specific language impairment show a significantly lower McGurk effect than the average child. They use less visual information in speech perception, or have a reduced attention to articulatory gestures, but have no trouble perceiving auditory-only cues.\n\nChildren with autism spectrum disorders (ASD) showed a significantly reduced McGurk effect than children without. However, if the stimulus was nonhuman (for example bouncing a tennis ball to the sound of a bouncing beach ball) then they scored similarly to children without ASD. Younger children with ASD show a very reduced McGurk effect; however, this diminishes with age. As the individuals grow up, the effect they show becomes closer to those that did not have ASD. It has been suggested that the weakened McGurk effect seen in people with ASD is due to deficits in identifying both the auditory and visual components of speech rather than in the integration of said components (although distinguishing speech components as speech components may be isomorphic to integrating them).\n\nAdults with language-learning disabilities exhibit a much smaller McGurk effect than other adults. These people are not as influenced by visual input as most people. Therefore, people with poor language skills will produce a smaller McGurk effect. A reason for the smaller effect in this population is that there may be uncoupled activity between anterior and posterior regions of the brain, or left and right hemispheres. Cerebellar or basal ganglia etiology is also possible.\n\nIn patients with Alzheimer's disease (AD), there is a smaller McGurk effect exhibited than in those without. Often a reduced size of the corpus callosum produces a hemisphere disconnection process. Less influence on visual stimulus is seen in patients with AD, which is a reason for the lowered McGurk effect.\n\nThe McGurk effect is not as pronounced in schizophrenic individuals as in normal individuals. However, it is not significantly different in adults. Schizophrenia slows down the development of audiovisual integration and does not allow it to reach its developmental peak. However, no degradation is observed. Schizophrenics are more likely to rely on auditory cues than visual cues in speech perception.\n\nPeople with aphasia show impaired perception of speech in all conditions (visual-only, auditory-only, and audio-visual), and therefore exhibited a small McGurk effect. The greatest difficulty for aphasics is in the visual-only condition showing that they use more auditory stimuli in speech perception.\n\nDiscrepancy in vowel category significantly reduced the magnitude of the McGurk effect for fusion responses. Auditory /a/ tokens dubbed onto visual /i/ articulations were more compatible than the reverse. This could be because /a/ has a wide range of articulatory configurations whereas /i/ is more limited, which makes it much easier for subjects to detect discrepancies in the stimuli. /i/ vowel contexts produce the strongest effect, while /a/ produces a moderate effect, and /u/ has almost no effect.\n\nThe McGurk effect is stronger when the right side of the speaker's mouth (on the viewer's left) is visible. People tend to get more visual information from the right side of a speaker's mouth than the left or even the whole mouth. This relates to the hemispheric attention factors discussed in the brain hemispheres section above.\n\nThe McGurk effect is weaker when there is a visual distractor present that the listener is attending to. Visual attention modulates audiovisual speech perception. Another form of distraction is movement of the speaker. A stronger McGurk effect is elicited if the speaker's face/head is motionless, rather than moving.\n\nA strong McGurk effect can be seen for click-vowel syllables compared to weak effects for isolated clicks. This shows that the McGurk effect can happen in a non-speech environment. Phonological significance is not a necessary condition for a McGurk effect to occur; however, it does increase the strength of the effect.\n\nFemales show a stronger McGurk effect than males. Women show significantly greater visual influence on auditory speech than men did for brief visual stimuli, but no difference is apparent for full stimuli. Another aspect regarding gender is the issue of male faces and voices as stimuli in comparison to female faces and voices as stimuli. Although, there is no difference in the strength of the McGurk effect for either situation. If a male face is dubbed with a female voice, or vice versa, there is still no difference in strength of the McGurk effect. Knowing that the voice you hear is different from the face you see – even if different genders – doesn’t eliminate the McGurk effect.\n\nSubjects who are familiar with the faces of the speakers are less susceptible to the McGurk effect than those who are unfamiliar with the faces of the speakers. On the other hand, there was no difference regarding voice familiarity.\n\nSemantic congruency had a significant impact on the McGurk illusion. The effect is experienced more often and rated as clearer in the semantically congruent condition relative to the incongruent condition. When a person was expecting a certain visual or auditory appearance based on the semantic information leading up to it, the McGurk effect was greatly increased.\n\nThe McGurk effect can be observed when the listener is also the speaker or articulator. While looking at oneself in the mirror and articulating visual stimuli while listening to another auditory stimulus, a strong McGurk effect can be observed. In the other condition, where the listener speaks auditory stimuli softly while watching another person articulate the conflicting visual gestures, a McGurk effect can still be seen, although it is weaker.\n\nTemporal synchrony is not necessary for the McGurk effect to be present. Subjects are still strongly influenced by auditory stimuli even when it lagged the visual stimuli by 180 milliseconds (point at which McGurk effect begins to weaken). There was less tolerance for the lack of synchrony if the auditory stimuli preceded the visual stimuli. In order to produce a significant weakening of the McGurk effect, the auditory stimuli had to precede the visual stimuli by 60 milliseconds, or lag by 240 milliseconds.\n\nThe McGurk effect was greatly reduced when attention was diverted to a tactile task (touching something). Touch is a sensory perception like vision and audition, therefore increasing attention to touch decreases the attention to auditory and visual senses.\n\nThe eyes do not need to fixate in order to integrate audio and visual information in speech perception. There was no difference in the McGurk effect when the listener was focusing anywhere on the speaker's face. The effect does not appear if the listener focuses beyond the speaker's face. In order for the McGurk effect to become insignificant, the listener's gaze must deviate from the speaker's mouth by at least 60 degrees.\n\nPeople of all languages rely to some extent on visual information in speech perception, but the intensity of the McGurk effect can change between languages. Dutch, English, Spanish, German, Italian and Turkish language listeners experience a robust McGurk effect, while it is weaker for Japanese and Chinese listeners. Most research on the McGurk effect between languages has been conducted between English and Japanese. There is a smaller McGurk effect in Japanese listeners than in English listeners. The cultural practice of face avoidance in Japanese people may have an effect on the McGurk effect, as well as tone and syllabic structures of the language. This could also be why Chinese listeners are less susceptible to visual cues, and similar to Japanese, produce a smaller effect than English listeners. Studies have also shown that Japanese listeners do not show a developmental increase in visual influence after the age of six, as English children do. Japanese listeners are more able to identify an incompatibility between the visual and auditory stimulus than English listeners are. This result could be in relation to the fact that in Japanese, consonant clusters do not exist. In noisy environments where speech is unintelligible, however, people of all languages resort to using visual stimuli and are then equally subject to the McGurk effect. The McGurk effect works with speech perceivers of every language for which it has been tested.\n\nExperiments have been conducted involving hard of hearing individuals as well as individuals that have had cochlear implants. These individuals tend to weigh visual information from speech more heavily than auditory information. In comparison to normal hearing individuals, this is not different unless there is more than one syllable, such as a word. Regarding the McGurk experiment, responses from cochlear implanted users produced the same responses as normal hearing individuals when an auditory bilabial stimulus is dubbed onto a visual velar stimulus. However, when an auditory dental stimulus is dubbed onto a visual bilabial stimulus, the responses are quite different. The McGurk effect is still present in individuals with impaired hearing or using cochlear implants, although it is quite different in some aspects.\n\nBy measuring an infant's attention to certain audiovisual stimuli, a response that is consistent with the McGurk effect can be recorded. From just minutes to a couple of days old, infants can imitate adult facial movements, and within weeks of birth, infants can recognize lip movements and speech sounds. At this point, the integration of audio and visual information can happen, but not at a proficient level. The first evidence of the McGurk effect can be seen at four months of age; however, more evidence is found for 5-month-olds. Through the process of habituating an infant to a certain stimulus and then changing the stimulus (or part of it, such as ba-voiced/va-visual to da-voiced/va-visual), a response that simulates the McGurk effect becomes apparent. The strength of the McGurk effect displays a developmental pattern that increases throughout childhood and extends into adulthood.\n\n", "related": "\n- Duplex perception\n- Ideasthesia\n- Lip reading\n- Motor theory of speech perception\n- Multisensory integration\n- Speech perception\n- Viseme\n- Yanny or Laurel\n\n\n- A constraint-based explanation of the McGurk effect a write up of the McGurk effect by Paul Boersma of University of Amsterdam. PDF available from academic webpage of author.\n- Try The McGurk Effect! – Horizon: Is Seeing Believing? – BBC Two\n- McGurk Effect (with explanation)\n"}
{"id": "52222085", "url": "https://en.wikipedia.org/wiki?curid=52222085", "title": "Arabic Speech Corpus", "text": "Arabic Speech Corpus\n\nThe Arabic Speech Corpus is a Modern Standard Arabic (MSA) speech corpus for speech synthesis. The corpus contains phonetic and orthographic transcriptions of more than 3.7 hours of MSA speech aligned with recorded speech on the phoneme level. The annotations include word stress marks on the individual phonemes.\n\nThe Arabic Speech Corpus was built as part of a doctoral project by Nawar Halabi at the University of Southampton funded by MicroLinkPC who own an exclusive license to commercialise the corpus, but the corpus is available for strictly non-commercial purposes through the official Arabic Speech Corpus website. It is distributed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\nThe corpus was mainly built for speech synthesis purposes, specifically Speech Synthesis, but the corpus has been used for building HMM based voices in Arabic. It was also used to automatically align other speech corpora with their phonetic transcript and could be used as part of a larger corpus for training speech recognition systems.\n\nThe package contains the following:\n- 1813 .wav files containing spoken utterances.\n- 1813 .lab files containing text utterances.\n- 1813 .TextGrid files containing the phoneme labels with time stamps of the boundaries where these occur in the .wav files. These files can be opened using Praat software.\n- phonetic-transcript.txt which has the form \"[wav_filename]\" \"[Phoneme Sequence]\" in every line.\n- orthographic-transcript.txt which has the form \"[wav_filename]\" \"[Orthographic Transcript]\" in every line. Orthography is in Buckwalter Format which is friendlier where there is software that does not read Arabic script. It can be easily converted back to Arabic.\n- There is an extra 18 minutes of fully annotated corpus (separate from above but with the same structure as above) which was used to evaluated the corpus (see PhD thesis).\n\nThe corpus was also used to prove that using automatically extracted, orthography-based stress marks improve the quality of speech synthesis in MSA.\n\n", "related": "\n- Comparison of datasets in machine learning\n\n- The Arabic Speech Corpus official website\n- The Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\n"}
{"id": "1326932", "url": "https://en.wikipedia.org/wiki?curid=1326932", "title": "Beamforming", "text": "Beamforming\n\nBeamforming or spatial filtering is a signal processing technique used in sensor arrays for directional signal transmission or reception. This is achieved by combining elements in an antenna array in such a way that signals at particular angles experience constructive interference while others experience destructive interference. Beamforming can be used at both the transmitting and receiving ends in order to achieve spatial selectivity. The improvement compared with omnidirectional reception/transmission is known as the directivity of the array.\n\nBeamforming can be used for radio or sound waves. It has found numerous applications in radar, sonar, seismology, wireless communications, radio astronomy, acoustics and biomedicine. Adaptive beamforming is used to detect and estimate the signal of interest at the output of a sensor array by means of optimal (e.g. least-squares) spatial filtering and interference rejection.\n\nTo change the directionality of the array when transmitting, a beamformer controls the phase and relative amplitude of the signal at each transmitter, in order to create a pattern of constructive and destructive interference in the wavefront. When receiving, information from different sensors is combined in a way where the expected pattern of radiation is preferentially observed.\n\nFor example, in sonar, to send a sharp pulse of underwater sound towards a ship in the distance, simply simultaneously transmitting that sharp pulse from every sonar projector in an array fails because the ship will first hear the pulse from the speaker that happens to be nearest the ship, then later pulses from speakers that happen to be further from the ship. The beamforming technique involves sending the pulse from each projector at slightly different times (the projector closest to the ship last), so that every pulse hits the ship at exactly the same time, producing the effect of a single strong pulse from a single powerful projector. The same technique can be carried out in air using loudspeakers, or in radar/radio using antennas.\n\nIn passive sonar, and in reception in active sonar, the beamforming technique involves combining delayed signals from each hydrophone at slightly different times (the hydrophone closest to the target will be combined after the longest delay), so that every signal reaches the output at exactly the same time, making one loud signal, as if the signal came from a single, very sensitive hydrophone. Receive beamforming can also be used with microphones or radar antennas.\n\nWith narrow-band systems the time delay is equivalent to a \"phase shift\", so in this case the array of antennas, each one shifted a slightly different amount, is called a phased array. A narrow band system, typical of radars, is one where the bandwidth is only a small fraction of the center frequency. With wide band systems this approximation no longer holds, which is typical in sonars.\n\nIn the receive beamformer the signal from each antenna may be amplified by a different \"weight.\" Different weighting patterns (e.g., Dolph-Chebyshev) can be used to achieve the desired sensitivity patterns. A main lobe is produced together with nulls and sidelobes. As well as controlling the main lobe width (beamwidth) and the sidelobe levels, the position of a null can be controlled. This is useful to ignore noise or jammers in one particular direction, while listening for events in other directions. A similar result can be obtained on transmission.\n\nFor the full mathematics on directing beams using amplitude and phase shifts, see the mathematical section in phased array.\n\nBeamforming techniques can be broadly divided into two categories:\n- conventional (fixed or switched beam) beamformers\n- adaptive beamformers or phased array\n- Desired signal maximization mode\n- Interference signal minimization or cancellation mode\n\nConventional beamformers, such as the Butler matrix, use a fixed set of weightings and time-delays (or phasings) to combine the signals from the sensors in the array, primarily using only information about the location of the sensors in space and the wave directions of interest. In contrast, adaptive beamforming techniques (e.g., MUSIC, SAMV) generally combine this information with properties of the signals actually received by the array, typically to improve rejection of unwanted signals from other directions. This process may be carried out in either the time or the frequency domain.\n\nAs the name indicates, an adaptive beamformer is able to automatically adapt its response to different situations. Some criterion has to be set up to allow the adaptation to proceed such as minimizing the total noise output. Because of the variation of noise with frequency, in wide band systems it may be desirable to carry out the process in the frequency domain.\n\nBeamforming can be computationally intensive. Sonar phased array has a data rate low enough that it can be processed in real-time in software, which is flexible enough to transmit or receive in several directions at once. In contrast, radar phased array has a data rate so high that it usually requires dedicated hardware processing, which is hard-wired to transmit or receive in only one direction at a time. However, newer field programmable gate arrays are fast enough to handle radar data in real-time, and can be quickly re-programmed like software, blurring the hardware/software distinction.\n\nSonar beamforming utilizes a similar technique to electromagnetic beamforming, but varies considerably in implementation details. Sonar applications vary from 1 Hz to as high as 2 MHz, and array elements may be few and large, or number in the hundreds yet very small. This will shift sonar beamforming design efforts significantly between demands of such system components as the \"front end\" (transducers, pre-amplifiers and digitizers) and the actual beamformer computational hardware downstream. High frequency, focused beam, multi-element imaging-search sonars and acoustic cameras often implement fifth-order spatial processing that places strains equivalent to Aegis radar demands on the processors.\n\nMany sonar systems, such as on torpedoes, are made up of arrays of up to 100 elements that must accomplish beam steering over a 100 degree field of view and work in both active and passive modes.\n\nSonar arrays are used both actively and passively in 1-, 2-, and 3-dimensional arrays.\n\n- 1-dimensional \"line\" arrays are usually in multi-element passive systems towed behind ships and in single- or multi-element side-scan sonar.\n- 2-dimensional \"planar\" arrays are common in active/passive ship hull mounted sonars and some side-scan sonar.\n- 3-dimensional spherical and cylindrical arrays are used in 'sonar domes' in the modern submarine and ships.\n\nSonar differs from radar in that in some applications such as wide-area-search all directions often need to be listened to, and in some applications broadcast to, simultaneously. Thus a multibeam system is needed. In a narrowband sonar receiver the phases for each beam can be manipulated entirely by signal processing software, as compared to present radar systems that use hardware to 'listen' in a single direction at a time.\n\nSonar also uses beamforming to compensate for the significant problem of the slower propagation speed of sound as compared to that of electromagnetic radiation. In side-look-sonars, the speed of the towing system or vehicle carrying the sonar is moving at sufficient speed to move the sonar out of the field of the returning sound \"ping\". In addition to focusing algorithms intended to improve reception, many side scan sonars also employ beam steering to look forward and backward to \"catch\" incoming pulses that would have been missed by a single sidelooking beam.\n\n- A conventional beamformer can be a simple beamformer also known as delay-and-sum beamformer. All the weights of the antenna elements can have equal magnitudes. The beamformer is steered to a specified direction only by selecting appropriate phases for each antenna. If the noise is uncorrelated and there are no directional interferences, the signal-to-noise ratio of a beamformer with formula_1 antennas receiving a signal of power formula_2, (where formula_3 is Noise variance or Noise power), is: formula_4\n- Null-steering beamformer\n- Frequency domain beamformer\n\nBeamforming techniques used in cellular phone standards have advanced through the generations to make use of more complex systems to achieve higher density cells, with higher throughput.\n- Passive mode: (almost) non-standardized solutions\n- Wideband Code Division Multiple Access (WCDMA) supports direction of arrival (DOA) based beamforming\n\n- Active mode: mandatory standardized solutions\n- 2G — Transmit antenna selection as an elementary beamforming\n- 3G — WCDMA: Transmit antenna array (TxAA) beamforming\n- 3G evolution — LTE/UMB: Multiple-input multiple-output (MIMO) precoding based beamforming with partial Space-Division Multiple Access (SDMA)\n- Beyond 3G (4G, 5G, …) — More advanced beamforming solutions to support Space-division multiple access (SDMA) such as closed loop beamforming and multi-dimensional beamforming are expected\n\nAn increasing number of consumer 802.11ac Wi-Fi devices with MIMO capability can support beamforming to boost data communication rates.\n\nFor receive (but not transmit), there is a distinction between analog and digital beamforming. For example, if there are 100 sensor elements, the \"digital beamforming\" approach entails that each of the 100 signals passes through an analog-to-digital converter to create 100 digital data streams. Then these data streams are added up digitally, with appropriate scale-factors or phase-shifts, to get the composite signals. By contrast, the \"analog beamforming\" approach entails taking the 100 analog signals, scaling or phase-shifting them using analog methods, summing them, and then usually digitizing the \"single\" output data stream.\n\nDigital beamforming has the advantage that the digital data streams (100 in this example) can be manipulated and combined in many possible ways in parallel, to get many different output signals in parallel. The signals from every direction can be measured simultaneously, and the signals can be integrated for a longer time when studying far-off objects and simultaneously integrated for a shorter time to study fast-moving close objects, and so on. This cannot be done as effectively for analog beamforming, not only because each parallel signal combination requires its own circuitry, but more fundamentally because digital data can be copied perfectly but analog data cannot. (There is only so much analog power available, and amplification adds noise.) Therefore, if the received analog signal is split up and sent into a large number of different signal combination circuits, it can reduce the signal-to-noise ratio of each.\n\nIn MIMO communication systems with large number of antennas, so called massive MIMO systems, the beamforming algorithms executed at the digital baseband can get very complex.\nIn addition, if all beamforming is done at baseband, each antenna needs its own RF feed. At high frequencies and with large number of antenna elements, this can be very costly, and increase loss and complexity in the system. To remedy these issues, hybrid beamforming has been suggested where some of the beamforming is done using analog components and not digital.\n\nThere are many possible different functions that can be performed using analog components instead of at the digital baseband.\n\nBeamforming can be used to try to extract sound sources in a room, such as multiple speakers in the cocktail party problem. This requires the locations of the speakers to be known in advance, for example by using the time of arrival from the sources to mics in the array, and inferring the locations from the distances.\n\nCompared to carrier-wave telecommunications, natural audio contains a variety of frequencies. It is advantageous to separate frequency bands prior to beamforming because different frequencies have different optimal beamform filters (and hence can be treated as separate problems, in parallel, and then recombined afterward). Properly isolating these bands involves specialized non-standard filter banks. In contrast, for example, the standard fast Fourier transform (FFT) band-filters implicitly assume that the only frequencies present in the signal are exact harmonics; frequencies which lie between these harmonics will typically activate all of the FFT channels (which is not what is wanted in a beamform analysis). Instead, filters can be designed in which only local frequencies are detected by each channel (while retaining the recombination property to be able to reconstruct the original signal), and these are typically non-orthogonal unlike the FFT basis.\n\n", "related": "\n- Three-dimensional beamforming\n- Aperture synthesis\n- Inverse synthetic aperture radar (ISAR)\n- Synthetic aperture radar\n- Synthetic aperture sonar\n- Thinned array curse\n- Window function\n- Synthetic-aperture magnetometry (SAM)\n- Microphone array\n- Zero-forcing precoding\n- Multibeam echosounder\n- Pencil (optics)\n- Periodogram\n- MUSIC\n- SAMV\n- Spatial multiplexing\n- Antenna diversity\n- Channel state information\n- Space–time code\n- Space–time block code\n- Dirty paper coding (DPC)\n- Smart antenna\n- WSDMA (Wideband Space Division Multiple Access)\n- Golomb ruler\n- Audio Surveillance\n- Reconfigurable antenna\n- Sensor array\n\n- Louay M. A. Jalloul and Sam. P. Alex, \"Evaluation Methodology and Performance of an IEEE 802.16e System\", Presented to the IEEE Communications and Signal Processing Society, Orange County Joint Chapter (ComSig), December 7, 2006. Available at: https://web.archive.org/web/20110414143801/http://chapters.comsoc.org/comsig/meet.html\n- H. L. Van Trees, Optimum Array Processing, Wiley, NY, 2002.\n- Jian Li, and Petre Stoica, eds. Robust adaptive beamforming. New Jersey: John Wiley, 2006.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n- \"A Primer on Digital Beamforming\" by Toby Haynes, March 26, 1998\n- \"What Is Beamforming?\", an introduction to sonar beamforming by Greg Allen.\n- \"Dolph–Chebyshev Weights\" \"antenna-theory.com\"\n- A collection of pages providing a simple introduction to microphone array beamforming\n- MU-MIMO Beamforming by Constructive Interference, Wolfram Demonstrations Project\n"}
{"id": "23228963", "url": "https://en.wikipedia.org/wiki?curid=23228963", "title": "IEEE James L. Flanagan Speech and Audio Processing Award", "text": "IEEE James L. Flanagan Speech and Audio Processing Award\n\nThe IEEE James L. Flanagan Speech and Audio Processing Award is a Technical Field Award presented by the IEEE for an outstanding contribution to the advancement of speech and/or audio signal processing. It may be presented to an individual or a team of up to three people. The award was established by the IEEE Board of Directors in 2002. The award is named after James L. Flanagan, who was a scientist from Bell Labs where he worked on acoustics for many years.\n\nRecipients of this award receive a bronze medal, certificate and honorarium.\n\nSource\n- 2020: Hynek Hermansky\n- 2019: Hermann Ney\n- 2018: Mari Ostendorf\n- 2017: Mark Y. Liberman\n- 2016: Takehiro Moriya\n- 2015: Steve Young\n- 2014: Biing-Hwang Juang\n- 2013: Victor Zue\n- 2012: James Baker and Janet M. Baker\n- 2011: Julia Hirschberg\n- 2010: Sadaoki Furui\n- 2009: John Makhoul\n- 2008: Raj Reddy\n- 2007: Allen Gersho\n- 2006: James D. Johnston\n- 2005: Frederick Jelinek\n- 2004: Kenneth N. Stevens\n- 2004: Gunnar Fant\n\n- Information on the award at IEEE\n", "related": "NONE"}
{"id": "28448", "url": "https://en.wikipedia.org/wiki?curid=28448", "title": "Speech processing", "text": "Speech processing\n\nSpeech processing is the study of speech signals and the processing methods of signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. The input is called speech recognition and the output is called speech synthesis.\n\nEarly attempts at speech processing and recognition were primarily focused on understanding a handful of simple phonetic elements such as vowels. In 1952, three researchers at Bell Labs, Stephen. Balashek, R. Biddulph, and K. H. Davis, developed a system that could recognize digits spoken by a single speaker. \n\nLinear predictive coding (LPC), a speech processing algorithm, was first proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966. Further developments in LPC technology were made by Bishnu S. Atal and Manfred R. Schroeder at Bell Labs during the 1970s. LPC was the basis for voice-over-IP (VoIP) technology, as well as speech synthesizer chips, such as the Texas Instruments LPC Speech Chips used in the Speak & Spell toys from 1978.\n\nOne of the first commercially available speech recognition products was Dragon Dictate, released in 1990. In 1992, technology developed by Lawrence Rabiner and others at Bell Labs was used by AT&T in their Voice Recognition Call Processing service to route calls without a human operator. By this point, the vocabulary of these systems was larger than the average human vocabulary. \n\nBy the early 2000s, the dominant speech processing strategy started to shift away from Hidden Markov Models towards more modern neural networks and deep learning.\n\nDynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed. In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules. The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.\n\nA hidden Markov model can be represented as the simplest dynamic Bayesian network. The goal of the algorithm is to estimate a hidden variable x(t) given a list of observations y(t). By applying the Markov property, the conditional probability distribution of the hidden variable \"x\"(\"t\") at time \"t\", given the values of the hidden variable \"x\" at all times, depends \"only\" on the value of the hidden variable \"x\"(\"t\" − 1). Similarly, the value of the observed variable \"y\"(\"t\") only depends on the value of the hidden variable \"x\"(\"t\") (both at time \"t\").\n\nAn artificial neural network (ANN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\n\n- Interactive Voice Systems\n- Virtual Assistants\n- Voice Identification\n- Emotion Recognition\n- Call Center Automation\n- Robotics\n\n", "related": "\n- Neurocomputational speech processing\n- Speech coding\n- Speech technology\n- Natural Language Processing\n"}
{"id": "12796439", "url": "https://en.wikipedia.org/wiki?curid=12796439", "title": "Windows Speech Recognition", "text": "Windows Speech Recognition\n\nWindows Speech Recognition (WSR) is a speech recognition component developed by Microsoft for Windows Vista that enables voice commands to control the desktop user interface; dictate text in electronic documents and email; navigate websites; perform keyboard shortcuts; and to operate the mouse cursor. It also supports the creation of custom macros to perform additional or supplementary tasks.\n\nWSR is a locally processed speech recognition platform; it does not rely on cloud computing for accuracy, dictation, or recognition, but adapts based on contexts, grammars, speech samples, training sessions, and vocabularies. It provides a personal dictionary that allows users to include or exclude words or expressions from dictation and to optionally record pronunciations to increase recognition accuracy. With Windows Search, it can optionally analyze and collect text in documents, email, as well as handwritten tablet PC input to contextualize and disambiguate terms to further adapt the recognizer. Custom language models that adapt the recognizer to the specific contexts, phonetics, and terminologies of users in particular occupational fields such as legal or medical are also supported.\n\nWSR was developed to be integrated into Windows Vista, as Windows previously only supported speech recognition features exclusive to applications such as Windows Media Player. Microsoft Office XP introduced speech recognition, but it was mainly limited to Internet Explorer and Office. With the release of Windows Vista, Office 2007 and later versions of Office rely on WSR, replacing the separate Office speech recognition. The majority of integrated applications in Windows Vista can be controlled through speech. WSR is present in Windows 7, Windows 8, Windows 8.1, Windows RT, and Windows 10.\n\nMicrosoft was involved in speech recognition and speech synthesis research for many years before WSR. In 1993, Microsoft hired Xuedong Huang from Carnegie Mellon University to lead its speech development efforts; the company's research led to the development of the Speech API introduced in 1994. Speech recognition had also been used in previous Microsoft products. Office XP and Office 2003 provided speech recognition capabilities among Internet Explorer and Office applications; it also enabled limited speech functionality in Windows 98, Windows ME, Windows NT 4.0, and Windows 2000. Windows XP Tablet PC Edition 2002 included speech recognition capabilities with the Tablet PC Input Panel, and the Microsoft Plus! for Windows XP expansion package enabled voice commands to be used in Windows Media Player. However, this required installation of speech recognition as an additional component (with support primarily limited to individual applications); before Windows Vista, Windows did not include extensive or integrated speech recognition capabilities.\n\nAt the 2002 Windows Hardware Engineering Conference (WinHEC 2002) Microsoft announced that Windows Vista (then codenamed \"Longhorn\") would include advances in speech recognition and in features such as microphone array support; these features were part of the company's goal to \"provide a consistent quality audio infrastructure for natural (continuous) speech recognition and (discrete) command and control.\" Bill Gates stated during the 2003 Professional Developers Conference (PDC 2003) that Microsoft would \"build speech capabilities into the system -- a big advance for that in 'Longhorn,' in both recognition and synthesis, real-time\"; and pre-release builds throughout the development of Windows Vista included a speech engine with training features. A PDC 2003 developer presentation stated that Windows Vista would also include a user interface for microphone feedback and control, and user configuration and training features. Microsoft later clarified the extent to which speech recognition would be integrated when it stated in a pre-release software development kit that \"the common speech scenarios, like speech-enabling menus and buttons, will be enabled system-wide.\"\n\nDuring WinHEC 2004, Microsoft listed WSR as part of its \"Longhorn\" mobile PC strategy to improve productivity. At WinHEC 2005, Microsoft emphasized accessibility, new mobility scenarios, and improvements to the speech user experience. Unlike the speech support included in Windows XP, which was integrated with the Tablet PC Input Panel and required switching between separate Commanding and Dictation modes, Windows Vista would introduce a dedicated interface for speech input on the desktop and unify the separate speech modes; users previously could not speak a command after dictating or vice versa without first switching between these two modes. Microsoft also stated that Windows Vista would improve dictation accuracy and support additional language; a demonstration emphasized email dictation, and a presentation about microphone arrays was also shown. Windows Vista Beta 1 included an integrated speech recognition application. To incentivize company employees to analyze WSR for software glitches and provide feedback during its development, Microsoft offered an opportunity for testers to win a Premium model of the Xbox 360.\n\nDuring a demonstration by Micorosoft on July 27, 2006, before Windows Vista's release to manufacturing (RTM), a notable incident involving WSR occurred that resulted in an unintended output of \"Dear aunt, let's set so double the killer delete select all\" when several attempts to dictate led to consecutive output errors; the incident was a subject of significant derision among analysts and journalists in the audience. Microsoft later revealed that these issues were due to an audio gain glitch that caused the speech recognizer to distort the dictated words; the glitch was fixed before Windows Vista's release.\n\nIn early 2007, reports surfaced that WSR might be vulnerable to an attack that could allow attackers to play audio through a computer's speakers, thereby using speech recognition to perform undesired user operations on a target computer; it was the first vulnerability discovered after Windows Vista's general availability. While Microsoft stated that such an attack is theoretically possible, it would have to meet a number of prerequisites to be successful: the target system would have to have the speech recognition feature properly configured and activated; speakers and microphone(s) connected to the targeted system would need to be turned on; and the exploit would require the software to interpret commands without a user noticing—an unlikely scenario as the affected system would perform visible interface operations and produce audible feedback. Mitigating factors include dictation clarity and microphone feedback and placement. Because of User Account Control, an exploit of this nature also would not be able to perform privileged operations for users or protected administrators without explicit consent.\n\nWith Windows 7 Microsoft introduced several changes to improve the user experience. The recognizer was updated to use Microsoft UI Automation—substantially enhancing its performance—and the recognition engine now uses the WASAPI audio stack, which enables support for echo cancellation. The document harvester, which optionally analyzes and collects text in email and documents to contextualize and disambiguate user terms has improved performance, and has been updated to run periodically in the background instead of only after recognizer startup. Sleep mode has also seen performance improvements and, to address security issues, Windows 7 introduces a new \"voice activation\" option—enabled by default—that turns the recognizer off after users speak \"stop listening\" instead of putting the recognizer to sleep. Windows 7 also introduces an option to submit speech training data to Microsoft to improve future recognizer versions.\n\nWindows 7 introduced an optional dictation scratchpad interface that functions as a temporary document into which users can dictate or type text for insertion into applications that are not compatible with the Text Services Framework. WSR previously provided an \"enable dictation everywhere option\" in Windows Vista.\n\nWSR can be used to control the Metro user interface in Windows 8, Windows 8.1, and Windows RT with commands to open the Charms bar (\"Press Windows C\"); to dictate or display commands in Metro-style apps (\"Press Windows Z\"); to perform tasks in apps (e.g., \"Change to Celsius\" in MSN Weather); and to display all installed apps listed by the Start screen (\"Apps\").\n\nWSR is featured in the Settings application starting with the Windows 10 April 2018 Update (Version 1803); the change first appeared in Insider Preview Build 17083. The April 2018 Update also introduces a new ++ keyboard shortcut to activate WSR.\n\nWSR allows a user to control a computer, including the operating system desktop user interface, through voice commands. Applications, including most of those bundled with Windows, can also be controlled through voice commands. By using speech recognition, users can dictate text within documents, email, and forms; control the operating system user interface; perform keyboard shortcuts; and move the mouse cursor.\n\nWSR uses a local speech profile to store information about a user's voice. Accuracy of speech recognition increases through use, which helps the feature adapt to a user's grammar, speech patterns, vocabulary, and word usage. Speech recognition also includes a tutorial to improve accuracy, and can optionally review a user's personal documents—including email—to improve its command and dictation accuracy. Individual speech profiles can be created on a per-user basis, and backups of profiles can be performed via Windows Easy Transfer. WSR supports the following languages: Chinese (Traditional), Chinese (Simplified), English (U.S.), English (U.K.), French, German, Japanese, and Spanish. WSR relies on the Speech API developed by Microsoft, and third-party applications must support the Text Services Framework.\n\nThe WSR interface consists of a status area that displays instructions, information about commands (e.g., if a command is not heard by the recognizer), and the status of the recognizer; a voice meter displays visual feedback about volume levels. The status area represents the current state of WSR in a total of three modes, listed below with their respective meanings:\n- Listening: The recognizer is active and waiting for user input\n- Sleeping: The recognizer will not listen for or respond to commands other than \"Start listening\"\n- Off: The recognizer will not listen or respond to any commands; this mode can be enabled by speaking \"Stop listening\"\nColors of the recognizer listening mode button denote its various modes of operation: blue when listening; blue-gray when sleeping; gray when turned off; and yellow when the user switches context (e.g., from the desktop to the taskbar) or when a voice command is misinterpreted. The status area can also display custom user information as part of Windows Speech Recognition Macros.\n\nAn \"alternates panel\" disambiguation interface displays a list of items interpreted as being relevant to a user's spoken word(s); if the word or phrase that a user desired to insert into an application is listed among results, a user can speak the corresponding number of the word or phrase in the results and confirm this choice by speaking \"OK\" to insert it within the application. The alternates panel will also appear when launching applications or speaking commands that refer to more than one item (e.g., speaking \"Start Internet Explorer\" may list the web browser and a version of it with browser add-ons disabled). However, an \"ExactMatchOverPartialMatch\" Windows Registry entry can limit commands to items with exact names if there is more than one instance included in results.\n\nListed below are common WSR commands. Words in \"italics\" indicate a word that can be substituted for a desired item (e.g., the word \"direction\" in the \"scroll \"direction\" command can be substituted with the word \"down\"\"). A \"start typing\" command enables WSR to interpret all dictation commands as keyboard shortcuts.\n\nA \"mousegrid\" command enables users to control the mouse cursor by overlaying numbers across nine regions on the screen; these regions gradually narrow as a user speaks the number(s) of the region on which to focus until the desired interface element is reached. The regions with which a user can interact are based on commands including \"Click \"number of region\",\" which moves the mouse cursor to the desired region and then clicks it; and \"Mark \"number of region\"\", which allows an item (such as a computer icon) in a region to be selected, which can then be clicked with the previous \"click\" command. A user can also simultaneously interact with multiple regions of the mousegrid.\n\nApplications and interface elements that do not present identifiable commands can still be controlled by asking the system to overlay numbers on top of them through a \"show numbers\" command. Once active, speaking the overlaid number selects that item so a user can open it or perform other operations. \"Show numbers\" was designed so that users could interact with items that are not readily identifiable.\n\nWSR enables dictation of text in the operating system and applications. If a dictation mistake occurs it can be corrected by speaking \"Correct \"word\"\" or \"Correct that\" and the alternates panel will appear and provide suggestions for correction; these suggestions can be selected by speaking the number corresponding to the number of the suggestion in the list and by speaking \"OK.\" If the desired item is not listed among suggestions, a user can speak it so that it might appear. Alternatively, users can speak \"Spell it\" or \"I'll spell it myself\" to speak the desired item on a per-letter basis; users can use their personal alphabet or the NATO phonetic alphabet when spelling. Multiple words in a sentence can be corrected simultaneously (for example, if a user speaks \"dictating\" but the recognizer interprets this word as \"the thing,\" a user can state \"correct the thing\" to correct both words). In the English language over 100,000 words are recognized by default.\n\nWSR includes a personal dictionary that allows users to include or exclude certain words or expressions from dictation. When a user adds a word beginning with a capital letter to the dictionary, a user can specify whether it should always be capitalized or if capitalization depends on the context in which the word is spoken. Users can also record pronunciations for words added to the dictionary to increase recognition accuracy; words written via a stylus on a tablet PC for the Windows handwriting recognition feature are also stored. Most of the information stored within a dictionary is included as part of a user's speech profile.\n\nWSR supports custom macros through a supplementary application by Microsoft that enables additional natural language commands. As an example of this functionality, an email macro released by Microsoft enables a natural language command where a user can state \"send email to \"contact\" about \"subject\",\" which opens Microsoft Outlook to compose a new message with the designated contact and subject automatically inserted. Microsoft has also released sample macros for the speech dictionary, for Windows Media Player, for Microsoft PowerPoint, for speech synthesis, to switch between multiple microphones, to customize various aspects of audio device configuration such as volume levels, and for general natural language queries such as \"What is the weather forecast?\" \"What time is it?\" and \"What's the date?\" Answers to these queries are spoken via a speech synthesizer.\n\nUsers and developers can create their own macros that can be based on text transcription and substitution; application execution (with support for command-line arguments); keyboard shortcuts; emulation of existing voice commands; or a combination of these items. XML, JScript and VBScript are supported. Macros can be limited to individual applications if desired and rules for macros can be defined programmatically.\nFor a macro to load, it must be stored in a \"Speech Macros\" folder within the current user's \"Documents\" directory. All macros are digitally signed by default if a user certificate is available, to ensure that commands are not corrupted or loaded by third-parties; if one is not available, an administrator can create a certificate for use. The macros utility also includes security levels to prohibit unsigned macros from being loaded; to prompt users to sign macros; and to load unsigned macros.\n\n WSR uses Microsoft Speech Recognizer 8.0, which has not been changed since Windows Vista. For dictation it was found to be 93.6% accurate without training by Mark Hachman, a Senior Editor of \"PC World\"—a rate that is not as accurate as competing software. According to Microsoft, the rate of accuracy when trained is 99%. Hachman commented that Microsoft does not publicly discuss WSR, attributing this to the 2006 incident during development of Windows Vista, with few users knowing that documents could be dictated within Windows before the introduction of Cortana.\n\n", "related": "\n- List of speech recognition software\n- Microsoft Narrator\n- Microsoft Voice Command\n- Technical features new to Windows Vista\n- Windows HotStart\n- Windows Mobility Center\n- Windows SideShow\n\n- Windows Vista Speech Recognition demonstration at Microsoft Financial Analyst Meeting\n"}
