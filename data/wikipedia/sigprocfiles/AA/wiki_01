{"id": "38878679", "url": "https://en.wikipedia.org/wiki?curid=38878679", "title": "Error concealment", "text": "Error concealment\n\nError concealment is a technique used in signal processing that aims to minimize the deterioration of signals caused by missing data, called packet loss. A signal is a message sent from a transmitter to a receiver in multiple small packets. Packet loss occurs when these packets are misdirected, delayed, resequenced, or corrupted.\n\nWhen error recovery occurs at the receiving end of the signal, it is receiver-based. These techniques focus on correcting corrupted or missing data.\n\nPreliminary attempts at receiver-based error concealment involved packet repetition, replacing lost packets with copies of previously received packets. This function is computationally simple and is performed by a device on the receiver end called a \"drop-out compensator\".\n\nWhen this technique is used, if a packet is lost, its entries are replaced with 0s.\n\nInterpolation involves making educated guesses about the nature of a missing packet. For example, by following speech patterns in audio or faces in video.\n\nData buffers are used for temporarily storing data while waiting for delayed packets to arrive. They are common in internet browser loading bars and video applications, like YouTube.\n\nRather than attempting to recover lost packets, other techniques involve anticipating data loss, manipulating the data prior to transmission.\n\nThe simplest transmitter-based technique is retransmission, sending the message multiple times. Although this idea is simple, because of the extra time required to send multiple signals, this technique is incapable of supporting real-time applications.\n\nPacket repetition, also called forward error correction (FEC), adds redundant data, which the receiver can use to recover lost packets. This minimizes loss, but increases the size of the packet.\n\nInterleaving involves scrambling the data before transmission. When a packet is lost, rather than losing an entire set of data, small portions of several sets will be gone. At the receiving end, the message is then deinterleaved to reveal the original message with minimal loss.\n\nTransmission without interleaving:\n\nThe term \"AnExample\" ends up mostly unintelligible and difficult to correct.\n\nWith interleaving:\n\nNo word is completely lost and the missing letters can be recovered with minimal guesswork.\n\nDepending on the method of transmission (analog or digital), there are a variety of ways for errors to propagate in the message.\n\nSince its invention in the 1950s, the magnetic coating used in analog video tape has experienced radio frequency (RF) signal drop-outs. Some of the techniques that were used for resolving these issues are analogous to those used for concealing errors in modern compressed video signals.\n\nThe process of click removal in audio restoration is another example of error concealment. A closely analogous example in the domain of image processing is the use of digital dust and scratch removal processing in film restoration.\n\nError concealment has many digital applications, including web browsing, video conferencing, Skype, and YouTube\n\n", "related": "\n- Packet loss concealment\n"}
{"id": "405746", "url": "https://en.wikipedia.org/wiki?curid=405746", "title": "Analog signal processing", "text": "Analog signal processing\n\nAnalog signal processing is a type of signal processing conducted on continuous analog signals by some analog means (as opposed to the discrete digital signal processing where the signal processing is carried out by a digital process). \"Analog\" indicates something that is mathematically represented as a set of continuous values. This differs from \"digital\" which uses a series of discrete quantities to represent signal. Analog values are typically represented as a voltage, electric current, or electric charge around components in the electronic devices. An error or noise affecting such physical quantities will result in a corresponding error in the signals represented by such physical quantities.\n\nExamples of \"analog signal processing\" include crossover filters in loudspeakers, \"bass\", \"treble\" and \"volume\" controls on stereos, and \"tint\" controls on TVs. Common analog processing elements include capacitors, resistors and inductors (as the passive elements) and transistors or opamps (as the active elements).\n\nA system's behavior can be mathematically modeled and is represented in the time domain as h(t) and in the frequency domain as H(s), where s is a complex number in the form of s=a+ib, or s=a+jb in electrical engineering terms (electrical engineers use \"j\" instead of \"i\" because current is represented by the variable i). Input signals are usually called x(t) or X(s) and output signals are usually called y(t) or Y(s).\n\nConvolution is the basic concept in signal processing that states an input signal can be combined with the system's function to find the output signal. It is the integral of the product of two waveforms after one has reversed and shifted; the symbol for convolution is *. \nThat is the convolution integral and is used to find the convolution of a signal and a system; typically a = -∞ and b = +∞.\n\nConsider two waveforms f and g. By calculating the convolution, we determine how much a reversed function g must be shifted along the x-axis to become identical to function f. The convolution function essentially reverses and slides function g along the axis, and calculates the integral of their (f and the reversed and shifted g) product for each possible amount of sliding. When the functions match, the value of (f*g) is maximized. This occurs because when positive areas (peaks) or negative areas (troughs) are multiplied, they contribute to the integral.\n\nThe Fourier transform is a function that transforms a signal or system in the time domain into the frequency domain, but it only works for certain functions. The constraint on which systems or signals can be transformed by the Fourier Transform is that: \nThis is the Fourier transform integral:\nUsually the Fourier transform integral isn't used to determine the transform; instead, a table of transform pairs is used to find the Fourier transform of a signal or system. The inverse Fourier transform is used to go from frequency domain to time domain: \nEach signal or system that can be transformed has a unique Fourier transform. There is only one time signal for any frequency signal, and vice versa.\n\nThe Laplace transform is a generalized Fourier transform. It allows a transform of any system or signal because it is a transform into the complex plane instead of just the jω line like the Fourier transform. The major difference is that the Laplace transform has a region of convergence for which the transform is valid. This implies that a signal in frequency may have more than one signal in time; the correct time signal for the transform is determined by the region of convergence. If the region of convergence includes the jω axis, jω can be substituted into the Laplace transform for s and it's the same as the Fourier transform. The Laplace transform is:\nand the inverse Laplace transform, if all the singularities of X(s) are in the left half of the complex plane, is:\n\nBode plots are plots of magnitude vs. frequency and phase vs. frequency for a system. The magnitude axis is in [Decibel] (dB). The phase axis is in either degrees or radians. The frequency axes are in a [logarithmic scale]. These are useful because for sinusoidal inputs, the output is the input multiplied by the value of the magnitude plot at the frequency and shifted by the value of the phase plot at the frequency.\n\nThis is the domain that most people are familiar with. A plot in the time domain shows the amplitude of the signal with respect to time.\n\nA plot in the frequency domain shows either the phase shift or magnitude of a signal at each frequency that it exists at. These can be found by taking the Fourier transform of a time signal and are plotted similarly to a bode plot.\n\nWhile any signal can be used in analog signal processing, there are many types of signals that are used very frequently.\n\nSinusoids are the building block of analog signal processing. All real world signals can be represented as an infinite sum of sinusoidal functions via a Fourier series. A sinusoidal function can be represented in terms of an exponential by the application of Euler's Formula.\n\nAn impulse (Dirac delta function) is defined as a signal that has an infinite magnitude and an infinitesimally narrow width with an area under it of one, centered at zero. An impulse can be represented as an infinite sum of sinusoids that includes all possible frequencies. It is not, in reality, possible to generate such a signal, but it can be sufficiently approximated with a large amplitude, narrow pulse, to produce the theoretical impulse response in a network to a high degree of accuracy. The symbol for an impulse is δ(t). If an impulse is used as an input to a system, the output is known as the impulse response. The impulse response defines the system because all possible frequencies are represented in the input\n\nA unit step function, also called the Heaviside step function, is a signal that has a magnitude of zero before zero and a magnitude of one after zero. The symbol for a unit step is u(t). If a step is used as the input to a system, the output is called the step response. The step response shows how a system responds to a sudden input, similar to turning on a switch. The period before the output stabilizes is called the transient part of a signal. The step response can be multiplied with other signals to show how the system responds when an input is suddenly turned on.\n\nThe unit step function is related to the Dirac delta function by;\n\nLinearity means that if you have two inputs and two corresponding outputs, if you take a linear combination of those two inputs you will get a linear combination of the outputs. An example of a linear system is a first order low-pass or high-pass filter. Linear systems are made out of analog devices that demonstrate linear properties. These devices don't have to be entirely linear, but must have a region of operation that is linear. An operational amplifier is a non-linear device, but has a region of operation that is linear, so it can be modeled as linear within that region of operation. Time-invariance means it doesn't matter when you start a system, the same output will result. For example, if you have a system and put an input into it today, you would get the same output if you started the system tomorrow instead. There aren't any real systems that are LTI, but many systems can be modeled as LTI for simplicity in determining what their output will be. All systems have some dependence on things like temperature, signal level or other factors that cause them to be non-linear or non-time-invariant, but most are stable enough to model as LTI. Linearity and time-invariance are important because they are the only types of systems that can be easily solved using conventional analog signal processing methods. Once a system becomes non-linear or non-time-invariant, it becomes a non-linear differential equations problem, and there are very few of those that can actually be solved. (Haykin & Van Veen 2003)\n\n", "related": "\n- Signal processing\n- Digital signal processing\n- Signal\n- Analog electronics\n- Analog recording vs. digital recording\n- Electronics\n- Electrical engineering\n- Capacitor\n- Inductor\n- Resistor\n- Transistor\n\n- RC circuit\n- LC circuit\n- RLC circuit\n- Series and parallel circuits\n\n- Band-pass filter\n- Band-stop filter\n- High-pass filter\n- Low-pass filter\n\n- Haykin, Simon, and Barry Van Veen. Signals and Systems. 2nd ed. Hoboken, NJ: John Wiley and Sons, Inc., 2003.\n- McClellan, James H., Ronald W. Schafer, and Mark A. Yoder. Signal Processing First. Upper Saddle River, NJ: Pearson Education, Inc., 2003.\n"}
{"id": "105499", "url": "https://en.wikipedia.org/wiki?curid=105499", "title": "Whittaker–Shannon interpolation formula", "text": "Whittaker–Shannon interpolation formula\n\nThe Whittaker–Shannon interpolation formula or sinc interpolation is a method to construct a continuous-time bandlimited function from a sequence of real numbers. The formula dates back to the works of E. Borel in 1898, and E. T. Whittaker in 1915, and was cited from works of J. M. Whittaker in 1935, and in the formulation of the Nyquist–Shannon sampling theorem by Claude Shannon in 1949. It is also commonly called Shannon's interpolation formula and Whittaker's interpolation formula. E. T. Whittaker, who published it in 1915, called it the Cardinal series.\n\nGiven a sequence of real numbers, \"x\"[\"n\"], the continuous function\n\nThe interpolation formula is derived in the Nyquist–Shannon sampling theorem article, which points out that it can also be expressed as the convolution of an infinite impulse train with a sinc function:\n\nThis is equivalent to filtering the impulse train with an ideal (\"brick-wall\") low-pass filter.\n\nThe interpolation formula always converges absolutely and locally uniformly as long as\n\nBy the Hölder inequality this is satisfied if the sequence formula_4 belongs to any of the formula_5 spaces with 1 ≤ \"p\" < ∞, that is\n\nThis condition is sufficient, but not necessary. For example, the sum will generally converge if the sample sequence comes from sampling almost any stationary process, in which case the sample sequence is not square summable, and is not in any formula_5 space.\n\nIf \"x\"[\"n\"] is an infinite sequence of samples of a sample function of a wide-sense stationary process, then it is not a member of any formula_8 or L space, with probability 1; that is, the infinite sum of samples raised to a power \"p\" does not have a finite expected value. Nevertheless, the interpolation formula converges with probability 1. Convergence can readily be shown by computing the variances of truncated terms of the summation, and showing that the variance can be made arbitrarily small by choosing a sufficient number of terms. If the process mean is nonzero, then pairs of terms need to be considered to also show that the expected value of the truncated terms converges to zero.\n\nSince a random process does not have a Fourier transform, the condition under which the sum converges to the original function must also be different. A stationary random process does have an autocorrelation function and hence a spectral density according to the Wiener–Khinchin theorem. A suitable condition for convergence to a sample function from the process is that the spectral density of the process be zero at all frequencies equal to and above half the sample rate.\n\n", "related": "\n- Aliasing, Anti-aliasing filter, Spatial anti-aliasing\n- Rectangular function\n- Sampling (signal processing)\n- Signal (electronics)\n- Sinc function, Sinc filter\n- Lanczos resampling\n"}
{"id": "1705432", "url": "https://en.wikipedia.org/wiki?curid=1705432", "title": "Routh–Hurwitz stability criterion", "text": "Routh–Hurwitz stability criterion\n\nIn control system theory, the Routh–Hurwitz stability criterion is a mathematical test that is a necessary and sufficient condition for the stability of a linear time invariant (LTI) control system. The Routh test is an efficient recursive algorithm that English mathematician Edward John Routh proposed in 1876 to determine whether all the roots of the characteristic polynomial of a linear system have negative real parts. German mathematician Adolf Hurwitz independently proposed in 1895 to arrange the coefficients of the polynomial into a square matrix, called the Hurwitz matrix, and showed that the polynomial is stable if and only if the sequence of determinants of its principal submatrices are all positive. The two procedures are equivalent, with the Routh test providing a more efficient way to compute the Hurwitz determinants than computing them directly. A polynomial satisfying the Routh–Hurwitz criterion is called a Hurwitz polynomial.\n\nThe importance of the criterion is that the roots p of the characteristic equation of a linear system with negative real parts represent solutions e of the system that are stable (bounded). Thus the criterion provides a way to determine if the equations of motion of a linear system have only stable solutions, without solving the system directly. For discrete systems, the corresponding stability test can be handled by the Schur–Cohn criterion, the Jury test and the Bistritz test. With the advent of computers, the criterion has become less widely used, as an alternative is to solve the polynomial numerically, obtaining approximations to the roots directly.\n\nThe Routh test can be derived through the use of the Euclidean algorithm and Sturm's theorem in evaluating Cauchy indices. Hurwitz derived his conditions differently.\n\nThe criterion is related to Routh–Hurwitz theorem. From the statement of that theorem, we have formula_1 where:\n- formula_2 is the number of roots of the polynomial formula_3 with negative real part;\n- formula_4 is the number of roots of the polynomial formula_3 with positive real part (according to the theorem, formula_6 is supposed to have no roots lying on the imaginary line);\n- \"w\"(\"x\") is the number of variations of the generalized Sturm chain obtained from formula_7 and formula_8 (by successive Euclidean divisions) where formula_9 for a real \"y\".\nBy the fundamental theorem of algebra, each polynomial of degree \"n\" must have \"n\" roots in the complex plane (i.e., for an \"ƒ\" with no roots on the imaginary line, \"p\" + \"q\" = \"n\"). Thus, we have the condition that \"ƒ\" is a (Hurwitz) stable polynomial if and only if \"p\" − \"q\" = \"n\" (the proof is given below). Using the Routh–Hurwitz theorem, we can replace the condition on \"p\" and \"q\" by a condition on the generalized Sturm chain, which will give in turn a condition on the coefficients of \"ƒ\".\n\nLet \"f\"(\"z\") be a complex polynomial. The process is as follows:\n1. Compute the polynomials formula_7 and formula_8 such that formula_9 where \"y\" is a real number.\n2. Compute the Sylvester matrix associated to formula_7 and formula_8.\n3. Rearrange each row in such a way that an odd row and the following one have the same number of leading zeros.\n4. Compute each principal minor of that matrix.\n5. If at least one of the minors is negative (or zero), then the polynomial \"f\" is not stable.\n\n- Let formula_15 (for the sake of simplicity we take real coefficients) where formula_16 (to avoid a root in zero so that we can use the Routh–Hurwitz theorem). First, we have to calculate the real polynomials formula_7 and formula_8:\nNotice that we had to suppose \"b\" different from zero in the first division. The generalized Sturm chain is in this case formula_24. Putting formula_25, the sign of formula_26 is the opposite sign of \"a\" and the sign of \"by\" is the sign of \"b\". When we put formula_27, the sign of the first element of the chain is again the opposite sign of \"a\" and the sign of \"by\" is the opposite sign of \"b\". Finally, -\"c\" has always the opposite sign of \"c\".\n\n- The second-degree polynomial, formula_31has both roots in the open left half plane (and the system with characteristic equation formula_32 is stable) if and only if both coefficients satisfy formula_33.\n- The third-order polynomial formula_34has all roots in the open left half plane if and only if formula_35, formula_36 are positive and formula_37\n- In general the Routh stability criterion states a polynomial has all roots in the open left half plane if and only if all first-column elements of the Routh array have the same sign.\n\nA tabular method can be used to determine the stability when the roots of a higher order characteristic polynomial are difficult to obtain. For an \"n\"th-degree polynomial\n- formula_38\nthe table has \"n\" + 1 rows and the following structure:\n\nwhere the elements formula_39 and formula_40 can be computed as follows:\n- formula_41\n- formula_42\nWhen completed, the number of sign changes in the first column will be the number of non-negative roots.\n\nIn the first column, there are two sign changes (0.75 → −3, and −3 → 3), thus there are two non-negative roots where the system is unstable.\n\nThe characteristic equation of a servo system is given by :\n\n- formula_43\nfor stability, all the elements in the first column of the Routh array must be positive. So the conditions that must be satisfied for stability of the given system as follows :\n\nformula_44\n\nWe see that if\n\nformula_45\n\nthen \n\nformula_46\n\nIs satisfied.\n\n- formula_47\n\nWe have the following table :\n\nthere are two sign changes. The system is unstable, since it has two right-half-plane poles and two left-half-plane poles. The system cannot have jω poles since a row of zeros did not appear in the Routh table.\n\nSometimes the presence of poles on the imaginary axis creates a situation of marginal stability. In that case the coefficients of the \"Routh array\" in a whole row become zero and thus further solution of the polynomial for finding changes in sign is not possible. Then another approach comes into play. The row of polynomial which is just above the row containing the zeroes is called the \"auxiliary polynomial\".\n\n- formula_48\nWe have the following table:\n\nIn such a case the auxiliary polynomial is formula_49 which is again equal to zero. The next step is to differentiate the above equation which yields the following polynomial. formula_50. The coefficients of the row containing zero now become\n\"8\" and \"24\". The process of Routh array is proceeded using these values which yield two points on the imaginary axis. These two points on the imaginary axis are the prime cause of marginal stability.\n\n", "related": "\n- Control engineering\n- Derivation of the Routh array\n- Nyquist stability criterion\n- Routh–Hurwitz theorem\n- Root locus\n- Transfer function\n- Jury stability criterion\n- Bistritz stability criterion\n- Kharitonov's theorem\n- Liénard–Chipart criterion\n- Felix Gantmacher (J.L. Brenner translator) (1959) \"Applications of the Theory of Matrices\", pp 177–80, New York: Interscience.\n\n- A MATLAB script implementing the Routh-Hurwitz test\n- Online implementation of the Routh-Hurwitz-Criterion\n"}
{"id": "3350111", "url": "https://en.wikipedia.org/wiki?curid=3350111", "title": "Wiener–Khinchin theorem", "text": "Wiener–Khinchin theorem\n\nIn applied mathematics, the Wiener–Khinchin theorem, also known as the Wiener–Khintchine theorem and sometimes as the Wiener–Khinchin–Einstein theorem or the Khinchin–Kolmogorov theorem, states that the autocorrelation function of a wide-sense-stationary random process has a spectral decomposition given by the power spectrum of that process.\n\nNorbert Wiener proved this theorem for the case of a deterministic function in 1930; Aleksandr Khinchin later formulated an analogous result for stationary stochastic processes and published that probabilistic analogue in 1934. Albert Einstein explained, without proofs, the idea in a brief two-page memo in 1914.\n\nFor continuous time, the Wiener–Khinchin theorem says that if formula_1 is a wide-sense stationary process such that its autocorrelation function (sometimes called autocovariance) defined in terms of statistical expected value, formula_2 (the asterisk denotes complex conjugate, and of course it can be omitted if the random process is real-valued), exists and is finite at every lag formula_3, then there exists a monotone function formula_4 in the frequency domain formula_5 such that\n\nwhere the integral is a Riemann–Stieltjes integral. This is a kind of spectral decomposition of the auto-correlation function. \"F\" is called the power spectral distribution function and is a statistical distribution function. It is sometimes called the integrated spectrum.\n\nThe Fourier transform of formula_7 does not exist in general, because stationary random functions are not generally either square-integrable or absolutely integrable. Nor is formula_8 assumed to be absolutely integrable, so it need not have a Fourier transform either.\n\nBut if formula_4 is absolutely continuous, for example, if the process is purely indeterministic, then formula_10 is differentiable almost everywhere. In this case, one can define formula_11, the power spectral density of formula_7, by taking the averaged derivative of formula_10. Because the left and right derivatives of formula_10 exist everywhere, we can put formula_15 everywhere, (obtaining that \"F\" is the integral of its averaged derivative), and the theorem simplifies to\n\nIf now one assumes that \"r\" and \"S\" satisfy the necessary conditions for Fourier inversion to be valid, the Wiener–Khinchin theorem takes the simple form of saying that \"r\" and \"S\" are a Fourier-transform pair, and\n\nFor the discrete-time case, the power spectral density of the function with discrete values formula_18 is\n\nwhere\n\nis the discrete autocorrelation function of formula_18, provided this is absolutely integrable. Being a sampled and discrete-time sequence, the spectral density is periodic in the frequency domain. This is due to the problem of aliasing: the contribution of any frequency higher than the Nyquist frequency seems to be equal to that of its alias between 0 and 1. For this reason, the domain of the function formula_22 is usually restricted to lie between 0 and 1 or between -0.5 and 0.5.\n\nThe theorem is useful for analyzing linear time-invariant systems (LTI systems) when the inputs and outputs are not square-integrable, so their Fourier transforms do not exist. A corollary is that the Fourier transform of the autocorrelation function of the output of an LTI system is equal to the product of the Fourier transform of the autocorrelation function of the input of the system times the squared magnitude of the Fourier transform of the system impulse response. This works even when the Fourier transforms of the input and output signals do not exist because these signals are not square-integrable, so the system inputs and outputs cannot be directly related by the Fourier transform of the impulse response.\n\nSince the Fourier transform of the autocorrelation function of a signal is the power spectrum of the signal, this corollary is equivalent to saying that the power spectrum of the output is equal to the power spectrum of the input times the energy transfer function.\n\nThis corollary is used in the parametric method for power spectrum estimation.\n\nIn many textbooks and in much of the technical literature it is tacitly assumed that Fourier inversion of the autocorrelation function and the power spectral density is valid, and the Wiener–Khinchin theorem is stated, very simply, as if it said that the Fourier transform of the autocorrelation function was equal to the power spectral density, ignoring all questions of convergence (Einstein is an example).\nBut the theorem (as stated here) was applied by Norbert Wiener and Aleksandr Khinchin to the sample functions (signals) of wide-sense-stationary random processes, signals whose Fourier transforms do not exist.\nThe whole point of Wiener's contribution was to make sense of the spectral decomposition of the autocorrelation function of a sample function of a wide-sense-stationary random process even when the integrals for the Fourier transform and Fourier inversion do not make sense.\n\nFurther complicating the issue is that the discrete Fourier transform always exists for digital, finite-length sequences, meaning that the theorem can be blindly applied to calculate auto-correlations of numerical sequences. As mentioned earlier, the relation of this discrete sampled data to a mathematical model is often misleading, and related errors can show up as a divergence when the sequence length is modified.\n\nSome authors refer to formula_23 as the autocovariance function. They then proceed to normalise it, by dividing by formula_24, to obtain what they refer to as the autocorrelation function.\n\n- (a classified document written for the Dept. of War in 1943).\n", "related": "NONE"}
{"id": "14430199", "url": "https://en.wikipedia.org/wiki?curid=14430199", "title": "Hilbert–Huang transform", "text": "Hilbert–Huang transform\n\nThe Hilbert–Huang transform (HHT) is a way to decompose a signal into so-called intrinsic mode functions (IMF) along with a trend, and obtain instantaneous frequency data. It is designed to work well for data that is nonstationary and nonlinear. In contrast to other common transforms like the Fourier transform, the HHT is more like an algorithm (an empirical approach) that can be applied to a data set, rather than a theoretical tool.\n\nThe Hilbert–Huang transform (HHT), a NASA designated name, was proposed by Norden E. Huang et al. (1996, 1998, 1999, 2003, 2012). It is the result of the empirical mode decomposition (EMD) and the Hilbert spectral analysis (HSA). The HHT uses the EMD method to decompose a signal into so-called intrinsic mode functions (IMF) with a trend, and applies the HSA method to the IMFs to obtain instantaneous frequency data. Since the signal is decomposed in time domain and the length of the IMFs is the same as the original signal, HHT preserves the characteristics of the varying frequency. This is an important advantage of HHT since real-world signal usually has multiple causes happening in different time intervals. The HHT provides a new method of analyzing nonstationary and nonlinear time series data.\n\nThe fundamental part of the HHT is the empirical mode decomposition (EMD) method. Breaking down signals into various components, EMD can be compared with other analysis methods such as Fourier transform and Wavelet transform. Using the EMD method, any complicated data set can be decomposed into a finite and often small number of components. These components form a complete and nearly orthogonal basis for the original signal. In addition, they can be described as intrinsic mode functions (IMF).\n\nBecause the first IMF usually carries the most oscillating (high-frequency) components, it can be rejected to remove high-frequency components (e.g., random noise). EMD based smoothing algorithms have been widely used in seismic data processing, where high-quality seismic records are highly demanded.\n\nWithout leaving the time domain, EMD is adaptive and highly efficient. Since the decomposition is based on the local characteristic time scale of the data, it can be applied to nonlinear and nonstationary processes.\n\nAn IMF is defined as a function that satisfies the following requirements:\n1. In the whole data set, the number of extrema and the number of zero-crossings must either be equal or differ at most by one.\n2. At any point, the mean value of the envelope defined by the local maxima and the envelope defined by the local minima is zero.\n\nIt represents a generally simple oscillatory mode as a counterpart to the simple harmonic function. By definition, an IMF is any function with the same number of extrema and zero crossings, whose envelopes are symmetric with respect to zero. This definition guarantees a well-behaved Hilbert transform of the IMF.\n\nHilbert spectral analysis (HSA) is a method for examining each IMF's instantaneous frequency as functions of time. The final result is a frequency-time distribution of signal amplitude (or energy), designated as the Hilbert spectrum, which permits the identification of localized features.\n\nThe EMD method is a necessary step to reduce any given data into a collection of intrinsic mode functions (IMF) to which the Hilbert spectral analysis can be applied.\n\nIMF represents a simple oscillatory mode as a counterpart to the simple harmonic function, but it is much more general: instead of constant amplitude and frequency in a simple harmonic component, an IMF can have variable amplitude and frequency along the time axis. \n\nThe procedure of extracting an IMF is called sifting. The sifting process is as follows:\n\n1. Identify all the local extrema in the test data.\n2. Connect all the local maxima by a cubic spline line as the upper envelope.\n3. Repeat the procedure for the local minima to produce the lower envelope.\n\nThe upper and lower envelopes should cover all the data between them. Their mean is \"m\". The difference between the data and \"m\" is the first component \"h\":\n\nIdeally, \"h\" should satisfy the definition of an IMF, since the construction of h described above should have made it symmetric and having all maxima positive and all minima negative. After the first round of sifting, a crest may become a local maximum. New extrema generated in this way actually reveal the proper modes lost in the initial examination. In the subsequent sifting process, h can only be treated as a proto-IMF. In the next step, \"h\" is treated as data:\n\nAfter repeated sifting up to \"k\" times, h becomes an IMF, that is\n\nThen, \"h\" is designated as the first IMF component of the data:\n\nThe stoppage criterion determines the number of sifting steps to produce an IMF. Following are the four existing stoppage criterion:\n- Standard Deviation\nThis criterion is proposed by Huang et al. (1998). It is similar to the Cauchy convergence test, and we define a sum of the difference, SD, as\n\n", "related": "\n- Hilbert transform\n- Hilbert spectral analysis\n- Hilbert spectrum\n- Instantaneous frequency\n- Multidimensional empirical mode decomposition\n- Nonlinear\n- Wavelet transform\n- Fourier transform\n- Signal envelope\n\n"}
{"id": "15249674", "url": "https://en.wikipedia.org/wiki?curid=15249674", "title": "Bilinear time–frequency distribution", "text": "Bilinear time–frequency distribution\n\nBilinear time–frequency distributions, or quadratic time–frequency distributions, arise in a sub-field of signal analysis and signal processing called time–frequency signal processing, and, in the statistical analysis of time series data. Such methods are used where one needs to deal with a situation where the frequency composition of a signal may be changing over time; this sub-field used to be called time–frequency signal analysis, and is now more often called time–frequency signal processing due to the progress in using these methods to a wide range of signal-processing problems.\n\nMethods for analysing time series, in both signal analysis and time series analysis, have been developed as essentially separate methodologies applicable to, and based in, either the time or the frequency domain. A mixed approach is required in time–frequency analysis techniques which are especially effective in analyzing non-stationary signals, whose frequency distribution and magnitude vary with time. Examples of these are acoustic signals. Classes of \"quadratic time-frequency distributions\" (or bilinear time–frequency distributions\") are used for time–frequency signal analysis. This class is similar in formulation to Cohen's class distribution function that was used in 1966 in the context of quantum mechanics. This distribution function is mathematically similar to a generalized time–frequency representation which utilizes bilinear transformations. Compared with other time–frequency analysis techniques, such as short-time Fourier transform (STFT), the bilinear-transformation (or quadratic time–frequency distributions) may not have higher clarity for most practical signals, but it provides an alternative framework to investigate new definitions and new methods. While it does suffer from an inherent cross-term contamination when analyzing multi-component signals, by using a carefully chosen window function(s), the interference can be significantly mitigated, at the expense of resolution. All these bilinear distributions are inter-convertible to each other, cf. transformation between distributions in time–frequency analysis.\n\nThe Wigner–Ville distribution is a quadratic form that measures a local time-frequency energy given by:\n\nThe Wigner–Ville distribution remains real as it is the fourier transform of \"f\"(\"u\" + \"τ\"/2)·\"f\"*(\"u\" − \"τ\"/2), which has Hermitian symmetry in \"τ\". It can also be written as a frequency integration by applying the Parseval formula:\n\nLet formula_12 be a composite signal. We can then write,\nwhere \nis the cross Wigner–Ville distribution of two signals. The interference term\nis a real function that creates non-zero values at unexpected locations (close to the origin) in the formula_16 plane. Interference terms present in a real signal can be avoided by computing the analytic part formula_17.\n\nThe interference terms are oscillatory since the marginal integrals vanish and can be partially removed by smoothing formula_18 with a kernel \"θ\"\n\nThe time-frequency resolution of this distribution depends on the spread of kernel \"θ\" in the neighborhood of formula_16. Since the interferences take negative values, one can guarantee that all interferences are removed by imposing that\n\nThe spectrogram and scalogram are examples of positive time-frequency energy distributions. Let a linear transform formula_22 be defined over a family of time-frequency atoms formula_23. For any formula_16 there exists a unique atom formula_25 centered in time-frequency at formula_16. The resulting time-frequency energy density is \n\nFrom the Moyal formula,\n\nwhich is the time frequency averaging of a Wigner–Ville distribution. The smoothing kernel thus can be written as\n\nThe loss of time-frequency resolution depends on the spread of the distribution formula_30 in the neighborhood of formula_16.\n\nA spectrogram computed with windowed fourier atoms,\n\nFor a spectrogram, the Wigner–Ville averaging is therefore a 2-dimensional convolution with formula_34. If g is a Gaussian window,formula_34 is a 2-dimensional Gaussian. This proves that averaging formula_18 with a sufficiently wide Gaussian defines positive energy density. The general class of time-frequency distributions obtained by convolving formula_18 with an arbitrary kernel \"θ\" is called a Cohen's class, discussed below.\n\nWigner Theorem. There is no positive quadratic energy distribution \"Pf\" that satisfies the following time and frequency marginal integrals:\n\nThe definition of Cohen's class of bilinear (or quadratic) time–frequency distributions is as follows:\n\nwhere formula_41 is the ambiguity function (AF), which will be discussed later; and formula_42 is Cohen's kernel function, which is often a low-pass function, and normally serves to mask out the interference. In the original Wigner representation, formula_43.\n\nAn equivalent definition relies on a convolution of the Wigner distribution function (WD) instead of the AF :\n\nwhere the kernel function formula_45 is defined in the time-frequency domain instead of the ambiguity one. In the original Wigner representation, formula_46. The relationship between the two kernels is the same as the one between the WD and the AF, namely two successive Fourier transforms (cf. diagram).\n\ni.e.\n\nor equivalently\n\nThe class of bilinear (or quadratic) time–frequency distributions can be most easily understood in terms of the ambiguity function, an explanation of which follows.\n\nConsider the well known power spectral density formula_50 and the signal auto-correlation function formula_51 in the case of a stationary process. The relationship between these functions is as follows:\n\nFor a non-stationary signal formula_54, these relations can be generalized using a time-dependent power spectral density or equivalently the famous Wigner distribution function of formula_54 as follows:\n\nIf the Fourier transform of the auto-correlation function is taken with respect to \"t\" instead of \"τ\", we get the ambiguity function as follows:\n\nThe relationship between the Wigner distribution function, the auto-correlation function and the ambiguity function can then be illustrated by the following figure.\n\nBy comparing the definition of bilinear (or quadratic) time–frequency distributions with that of the Wigner distribution function, it is easily found that the latter is a special case of the former with formula_59. Alternatively, bilinear (or quadratic) time–frequency distributions can be regarded as a masked version of the Wigner distribution function if a kernel function formula_60 is chosen. A properly chosen kernel function can significantly reduce the undesirable cross-term of the Wigner distribution function.\n\nWhat is the benefit of the additional kernel function? The following figure shows the distribution of the auto-term and the cross-term of a multi-component signal in both the ambiguity and the Wigner distribution function.\n\nFor multi-component signals in general, the distribution of its auto-term and cross-term within its Wigner distribution function is generally not predictable, and hence the cross-term cannot be removed easily. However, as shown in the figure, for the ambiguity function, the auto-term of the multi-component signal will inherently tend to close the origin in the \"ητ\"-plane, and the cross-term will tend to be away from the origin. With this property, the cross-term in can be filtered out effortlessly if a proper low-pass kernel function is applied in \"ητ\"-domain. The following is an example that demonstrates how the cross-term is filtered out.\n\nThe Fourier transform of formula_61 is\n\nThe following proposition gives necessary and sufficient conditions to ensure that formula_63 satisfies marginal energy properties like those of the Wigner–Ville distribution.\n\nAforementioned, the Wigner distribution function is a member of the class of quadratic time-frequency distributions (QTFDs) with the kernel function formula_68. The definition of Wigner distribution is as follows:\n\nWe can design time-frequency energy distributions that satisfy the scaling property\n\nas does the Wigner–Ville distribution. If \n\nthen\n\nThis is equivalent to imposing that\n\nand hence\n\nThe Rihaczek and Choi–Williams distributions are examples of affine invariant Cohen's class distributions.\n\nThe kernel of Choi–Williams distribution is defined as follows:\n\nwhere \"α\" is an adjustable parameter.\n\nThe kernel of Rihaczek distribution is defined as follows:\n\nWith this particular kernel a simple calculation proves that\n\nThe kernel of cone-shape distribution function is defined as follows:\n\nwhere \"α\" is an adjustable parameter. See Transformation between distributions in time-frequency analysis. More such QTFDs and a full list can be found in, e.g., Cohen's text cited.\n\nA time-varying spectrum for non-stationary processes is defined from the expected Wigner–Ville distribution. Locally stationary processes appear in many physical systems where random fluctuations are produced by a mechanism that changes slowly in time. Such processes can be approximated locally by a stationary process. Let formula_79 be a real valued zero-mean process with covariance\n\nThe covariance operator \"K\" is defined for any deterministic signal formula_66 by\n\nFor locally stationary processes, the eigenvectors of \"K\" are well approximated by the Wigner–Ville spectrum.\n\nThe properties of the covariance formula_83 are studied as a function of formula_84 and formula_85:\n\nThe process is \"wide-sense stationary\" if the covariance depends only on formula_84:\n\nThe eigenvectors are the complex exponentials formula_89 and the corresponding eigenvalues are given by the power spectrum\n\nFor non-stationary processes, Martin and Flandrin have introduced a \"time-varying spectrum\"\n\nTo avoid convergence issues we suppose that \"X\" has compact support so that formula_92 has compact support in formula_93. From above we can write\n\nwhich proves that the time varying spectrum is the expected value of the Wigner–Ville transform of the process \"X\". Here, the Wigner–Ville stochastic integral is interpreted as a mean-square integral:\n\n- L. Cohen, Time-Frequency Analysis, Prentice-Hall, New York, 1995.\n- B. Boashash, editor, “Time-Frequency Signal Analysis and Processing – A Comprehensive Reference”, Elsevier Science, Oxford, 2003.\n- L. Cohen, “Time-Frequency Distributions—A Review,” Proceedings of the IEEE, vol. 77, no. 7, pp. 941–981, 1989.\n- S. Qian and D. Chen, Joint Time-Frequency Analysis: Methods and Applications, Chap. 5, Prentice Hall, N.J., 1996.\n- H. Choi and W. J. Williams, “Improved time-frequency representation of multicomponent signals using exponential kernels,” IEEE. Trans. Acoustics, Speech, Signal Processing, vol. 37, no. 6, pp. 862–871, June 1989.\n- Y. Zhao, L. E. Atlas, and R. J. Marks, “The use of cone-shape kernels for generalized time-frequency representations of nonstationary signals,” IEEE Trans. Acoustics, Speech, Signal Processing, vol. 38, no. 7, pp. 1084–1091, July 1990.\n- B. Boashash, “Heuristic Formulation of Time-Frequency Distributions”, Chapter 2, pp. 29–58, in B. Boashash, editor, Time-Frequency Signal Analysis and Processing: A Comprehensive Reference, Elsevier Science, Oxford, 2003.\n- B. Boashash, “Theory of Quadratic TFDs”, Chapter 3, pp. 59–82, in B. Boashash, editor, Time-Frequency Signal Analysis & Processing: A Comprehensive Reference, Elsevier, Oxford, 2003.\n", "related": "NONE"}
{"id": "15585953", "url": "https://en.wikipedia.org/wiki?curid=15585953", "title": "Itakura–Saito distance", "text": "Itakura–Saito distance\n\nThe Itakura–Saito distance (or Itakura–Saito divergence) is a measure of the difference between an original spectrum formula_1 and an approximation formula_2 of that spectrum. Although it is not a perceptual measure, it is intended to reflect perceptual (dis)similarity. It was proposed by Fumitada Itakura and Shuzo Saito in the 1960s while they were with NTT.\n\nThe distance is defined as:\n\nThe Itakura–Saito distance is a Bregman divergence generated by minus the logarithmic function, but is not a true metric since it is not symmetric and it does not fulfil triangle inequality.\n\nIn Non-negative matrix factorization, the Itakura-Saito divergence can be used as a measure of the quality of the factorization: this implies a meaningful statistical model of the components and can be solved through an iterative method.\n\nThe Itakura-Saito distance is the Bregman divergence associated with the Gamma exponential family where the information divergence of one distribution in the family from another element in the family is given by the Itakura-Saito divergence of the mean value of the first distribution from the mean value of the second distribution.\n\n", "related": "\n- Log-spectral distance\n"}
{"id": "17160278", "url": "https://en.wikipedia.org/wiki?curid=17160278", "title": "Overlap–save method", "text": "Overlap–save method\n\nIn signal processing, Overlap–save is the traditional name for an efficient way to evaluate the discrete convolution between a very long signal formula_1 and a finite impulse response (FIR) filter formula_2:\n\nwhere for \"m\" outside the region .\n\nThe concept is to compute short segments of \"y\"[\"n\"] of an arbitrary length \"L\", and concatenate the segments together. Consider a segment that begins at \"n\" = \"kL\" + \"M\", for any integer \"k\", and define:\n\nThen, for \"kL\" + \"M\"  ≤  \"n\"  ≤  \"kL\" + \"L\" + \"M\" − 1, and equivalently M  ≤  \"n\" − \"kL\"  ≤  \"L\" + \"M\" − 1, we can write:\n\nWith the substitution  ,  the task is reduced to computing , for M  ≤   ≤  L\" +\" M\" − 1\"'. These steps are illustrated in the first 3 traces of Figure 1, except that the desired portion of the output (third trace) corresponds to 1  ≤   ≤  L\".\n\nIf we periodically extend \"x\"[\"n\"] with period \"N\"  ≥  \"L\" + \"M\" − 1, according to:\n\nthe convolutions  formula_7  and  formula_8  are equivalent in the region \"M\"  ≤  \"n\"  ≤  \"L\" + \"M\" − 1.  It is therefore sufficient to compute the N-point circular (or cyclic) convolution of formula_9 with formula_10  in the region [1, \"N\"].  The subregion [\"M\", \"L\" + \"M\" − 1] is appended to the output stream, and the other values are discarded.  The advantage is that the circular convolution can be computed more efficiently than linear convolution, according to the circular convolution theorem:\n\nwhere:\n- DFT and IDFT refer to the Discrete Fourier transform and its inverse, evaluated over \"N\" discrete points, and\n- is customarily chosen such that is an integer power-of-2, and the transforms are implemented with the FFT algorithm, for efficiency.\n- The leading and trailing edge-effects of circular convolution are overlapped and added, and subsequently discarded.\n\n (\"Overlap-save algorithm for linear convolution\")\n\nWhen the DFT and IDFT are implemented by the FFT algorithm, the pseudocode above requires about complex multiplications for the FFT, product of arrays, and IFFT. Each iteration produces output samples, so the number of complex multiplications per output sample is about:\n\nFor example, when M=201 and N=1024, equals 13.67, whereas direct evaluation of would require up to 201 complex multiplications per output sample, the worst case being when both x and h are complex-valued. Also note that for any given M, has a minimum with respect to N. Figure 2 is a graph of the values of N that minimize for a range of filter lengths (M).\n\nInstead of , we can also consider applying to a long sequence of length formula_11 samples. The total number of complex multiplications would be:\n\nComparatively, the number of complex multiplications required by the pseudocode algorithm is:\n\nHence the \"cost\" of the overlap–save method scales almost as formula_14 while the cost of a single, large circular convolution is almost formula_15. \n\n\"Overlap–discard\" and \"Overlap–scrap\" are less commonly used labels for the same method described here. However, these labels are actually better (than \"overlap–save\") to distinguish from overlap–add, because both methods \"save\", but only one discards. \"Save\" merely refers to the fact that \"M\" − 1 input (or output) samples from segment \"k\" are needed to process segment \"k\" + 1.\n\nThe overlap–save algorithm can be extended to include other common operations of a system:\n\n- additional IFFT channels can be processed more cheaply than the first by reusing the forward FFT\n- sampling rates can be changed by using different sized forward and inverse FFTs\n- frequency translation (mixing) can be accomplished by rearranging frequency bins\n\n", "related": "\n- Overlap–add method\n\n"}
{"id": "317018", "url": "https://en.wikipedia.org/wiki?curid=317018", "title": "Quantization (signal processing)", "text": "Quantization (signal processing)\n\nQuantization, in mathematics and digital signal processing, is the process of mapping input values from a large set (often a continuous set) to output values in a (countable) smaller set, often with a finite number of elements. Rounding and truncation are typical examples of quantization processes. Quantization is involved to some degree in nearly all digital signal processing, as the process of representing a signal in digital form ordinarily involves rounding. Quantization also forms the core of essentially all lossy compression algorithms.\n\nThe difference between an input value and its quantized value (such as round-off error) is referred to as quantization error. A device or algorithmic function that performs quantization is called a quantizer. An analog-to-digital converter is an example of a quantizer.\n\nBecause quantization is a many-to-few mapping, it is an inherently non-linear and irreversible process (i.e., because the same output value is shared by multiple input values, it is impossible, in general, to recover the exact input value when given only the output value).\n\nThe set of possible input values may be infinitely large, and may possibly be continuous and therefore uncountable (such as the set of all real numbers, or all real numbers within some limited range). The set of possible output values may be finite or countably infinite. The input and output sets involved in quantization can be defined in a rather general way. For example, vector quantization is the application of quantization to multi-dimensional (vector-valued) input data.\n\nAn analog-to-digital converter (ADC) can be modeled as two processes: sampling and quantization. Sampling converts a time-varying voltage signal into a discrete-time signal, a sequence of real numbers. Quantization replaces each real number with an approximation from a finite set of discrete values. Most commonly, these discrete values are represented as fixed-point words. Though any number of quantization levels is possible, common word-lengths are 8-bit (256 levels), 16-bit (65,536 levels) and 24-bit (16.8 million levels). Quantizing a sequence of numbers produces a sequence of quantization errors which is sometimes modeled as an additive random signal called quantization noise because of its stochastic behavior. The more levels a quantizer uses, the lower is its quantization noise power.\n\n\"Rate–distortion optimized\" quantization is encountered in source coding for lossy data compression algorithms, where the purpose is to manage distortion within the limits of the bit rate supported by a communication channel or storage medium. The analysis of quantization in this context involves studying the amount of data (typically measured in digits or bits or bit \"rate\") that is used to represent the output of the quantizer, and studying the loss of precision that is introduced by the quantization process (which is referred to as the \"distortion\").\n\nAs an example, rounding a real number formula_1 to the nearest integer value forms a very basic type of quantizer – a \"uniform\" one. A typical (\"mid-tread\") uniform quantizer with a quantization \"step size\" equal to some value formula_2 can be expressed as\n\nwhere the notation formula_4 or formula_5 depicts the floor function.\n\nThe essential property of a quantizer is that it has a countable set of possible output values that has fewer members than the set of possible input values. The members of the set of output values may have integer, rational, or real values. For simple rounding to the nearest integer, the step size formula_2 is equal to 1. With formula_7 or with formula_2 equal to any other integer value, this quantizer has real-valued inputs and integer-valued outputs.\n\nWhen the quantization step size (Δ) is small relative to the variation in the signal being quantized, it is relatively simple to show that the mean squared error produced by such a rounding operation will be approximately formula_9. Mean squared error is also called the quantization \"noise power\". Adding one bit to the quantizer halves the value of Δ, which reduces the noise power by the factor ¼. In terms of decibels, the noise power change is formula_10\n\nBecause the set of possible output values of a quantizer is countable, any quantizer can be decomposed into two distinct stages, which can be referred to as the \"classification\" stage (or \"forward quantization\" stage) and the \"reconstruction\" stage (or \"inverse quantization\" stage), where the classification stage maps the input value to an integer \"quantization index\" formula_11 and the reconstruction stage maps the index formula_11 to the \"reconstruction value\" formula_13 that is the output approximation of the input value. For the example uniform quantizer described above, the forward quantization stage can be expressed as\nand the reconstruction stage for this example quantizer is simply \n\nThis decomposition is useful for the design and analysis of quantization behavior, and it illustrates how the quantized data can be communicated over a communication channel – a \"source encoder\" can perform the forward quantization stage and send the index information through a communication channel, and a \"decoder\" can perform the reconstruction stage to produce the output approximation of the original input data. In general, the forward quantization stage may use any function that maps the input data to the integer space of the quantization index data, and the inverse quantization stage can conceptually (or literally) be a table look-up operation to map each quantization index to a corresponding reconstruction value. This two-stage decomposition applies equally well to vector as well as scalar quantizers.\n\nMost uniform quantizers for signed input data can be classified as being of one of two types: \"mid-riser\" and \"mid-tread\". The terminology is based on what happens in the region around the value 0, and uses the analogy of viewing the input-output function of the quantizer as a stairway. Mid-tread quantizers have a zero-valued reconstruction level (corresponding to a \"tread\" of a stairway), while mid-riser quantizers have a zero-valued classification threshold (corresponding to a \"riser\" of a stairway).\n\nMid-tread quantization involves rounding. The formulas for mid-tread uniform quantization are provided in the previous section.\n\nMid-riser quantization involves truncation. The input-output formula for a mid-riser uniform quantizer is given by:\nwhere the classification rule is given by\nand the reconstruction rule is\n\nNote that mid-riser uniform quantizers do not have a zero output value – their minimum output magnitude is half the step size. In contrast, mid-tread quantizers do have a zero output level. For some applications, having a zero output signal representation may be a necessity.\n\nIn general, a mid-riser or mid-tread quantizer may not actually be a \"uniform\" quantizer – i.e., the size of the quantizer's classification intervals may not all be the same, or the spacing between its possible output values may not all be the same. The distinguishing characteristic of a mid-riser quantizer is that it has a classification threshold value that is exactly zero, and the distinguishing characteristic of a mid-tread quantizer is that is it has a reconstruction value that is exactly zero.\n\nA dead-zone quantizer is a type of mid-tread quantizer with symmetric behavior around 0. The region around the zero output value of such a quantizer is referred to as the \"dead zone\" or \"deadband\". The dead zone can sometimes serve the same purpose as a noise gate or squelch function. Especially for compression applications, the dead-zone may be given a different width than that for the other steps. For an otherwise-uniform quantizer, the dead-zone width can be set to any value formula_19 by using the forward quantization rule\nwhere the function is the sign function (also known as the \"signum\" function). The general reconstruction rule for such a dead-zone quantizer is given by\nwhere formula_22 is a reconstruction offset value in the range of 0 to 1 as a fraction of the step size. Ordinarily, formula_23 when quantizing input data with a typical pdf that is symmetric around zero and reaches its peak value at zero (such as a Gaussian, Laplacian, or generalized Gaussian pdf). Although formula_22 may depend on formula_11 in general, and can be chosen to fulfill the optimality condition described below, it is often simply set to a constant, such as formula_26. (Note that in this definition, formula_27 due to the definition of the function, so formula_28 has no effect.)\n\nA very commonly used special case (e.g., the scheme typically used in financial accounting and elementary mathematics) is to set formula_29 and formula_30 for all formula_11. In this case, the dead-zone quantizer is also a uniform quantizer, since the central dead-zone of this quantizer has the same width as all of its other steps, and all of its reconstruction values are equally spaced as well.\n\nOften the design of a quantizer involves supporting only a limited range of possible output values and performing clipping to limit the output to this range whenever the input exceeds the supported range. The error introduced by this clipping is referred to as \"overload\" distortion. Within the extreme limits of the supported range, the amount of spacing between the selectable output values of a quantizer is referred to as its \"granularity\", and the error introduced by this spacing is referred to as \"granular\" distortion. It is common for the design of a quantizer to involve determining the proper balance between granular distortion and overload distortion. For a given supported number of possible output values, reducing the average granular distortion may involve increasing the average overload distortion, and vice versa. A technique for controlling the amplitude of the signal (or, equivalently, the quantization step size formula_2) to achieve the appropriate balance is the use of \"automatic gain control\" (AGC). However, in some quantizer designs, the concepts of granular error and overload error may not apply (e.g., for a quantizer with a limited range of input data or with a countably infinite set of selectable output values).\n\nA common assumption for the analysis of quantization error is that it affects a signal processing system in a similar manner to that of additive white noise – having negligible correlation with the signal and an approximately flat power spectral density. The additive noise model is commonly used for the analysis of quantization error effects in digital filtering systems, and it can be very useful in such analysis. It has been shown to be a valid model in cases of high resolution quantization (small formula_2 relative to the signal strength) with smooth probability density functions. \n\nAdditive noise behavior is not always a valid assumption. Quantization error (for quantizers defined as described here) is deterministically related to the signal and not entirely independent of it. Thus, periodic signals can create periodic quantization noise. And in some cases it can even cause limit cycles to appear in digital signal processing systems. One way to ensure effective independence of the quantization error from the source signal is to perform \"dithered quantization\" (sometimes with \"noise shaping\"), which involves adding random (or pseudo-random) noise to the signal prior to quantization.\n\nIn the typical case, the original signal is much larger than one least significant bit (LSB). When this is the case, the quantization error is not significantly correlated with the signal, and has an approximately uniform distribution. When rounding is used to quantize, the quantization error has a mean of zero and the root mean square (RMS) value is the standard deviation of this distribution, given by formula_34. When truncation is used, the error has a non-zero mean of formula_35 and the RMS value is formula_36. In either case, the standard deviation, as a percentage of the full signal range, changes by a factor of 2 for each 1-bit change in the number of quantization bits. The potential signal-to-quantization-noise power ratio therefore changes by 4, or formula_37, approximately 6 dB per bit.\n\nAt lower amplitudes the quantization error becomes dependent on the input signal, resulting in distortion. This distortion is created after the anti-aliasing filter, and if these distortions are above 1/2 the sample rate they will alias back into the band of interest. In order to make the quantization error independent of the input signal, the signal is dithered by adding noise to the signal. This slightly reduces signal to noise ratio, but can completely eliminates the distortion.\n\nQuantization noise is a model of quantization error introduced by quantization in the analog-to-digital conversion (ADC). It is a rounding error between the analog input voltage to the ADC and the output digitized value. The noise is non-linear and signal-dependent. It can be modelled in several different ways.\n\nIn an ideal analog-to-digital converter, where the quantization error is uniformly distributed between −1/2 LSB and +1/2 LSB, and the signal has a uniform distribution covering all quantization levels, the Signal-to-quantization-noise ratio (SQNR) can be calculated from\n\nWhere Q is the number of quantization bits.\n\nThe most common test signals that fulfill this are full amplitude triangle waves and sawtooth waves.\n\nFor example, a 16-bit ADC has a maximum signal-to-quantization-noise ratio of 6.02 × 16 = 96.3 dB.\n\nWhen the input signal is a full-amplitude sine wave the distribution of the signal is no longer uniform, and the corresponding equation is instead\n\nHere, the quantization noise is once again \"assumed\" to be uniformly distributed. When the input signal has a high amplitude and a wide frequency spectrum this is the case. In this case a 16-bit ADC has a maximum signal-to-noise ratio of 98.09 dB. The 1.761 difference in signal-to-noise only occurs due to the signal being a full-scale sine wave instead of a triangle or sawtooth.\n\nFor complex signals in high-resolution ADCs this is an accurate model. For low-resolution ADCs, low-level signals in high-resolution ADCs, and for simple waveforms the quantization noise is not uniformly distributed, making this model inaccurate. In these cases the quantization noise distribution is strongly affected by the exact amplitude of the signal.\n\nThe calculations are relative to full-scale input. For smaller signals, the relative quantization distortion can be very large. To circumvent this issue, analog companding can be used, but this can introduce distortion.\n\nA scalar quantizer, which performs a quantization operation, can ordinarily be decomposed into two stages:\n- Classification: A process that classifies the input signal range into formula_40 non-overlapping intervals formula_41, by defining formula_42 boundary (decision) values formula_43, such that formula_44 for formula_45, with the extreme limits defined by formula_46 and formula_47. All the inputs formula_1 that fall in a given interval range formula_49 are associated with the same quantization index formula_11.\n- Reconstruction: Each interval formula_51 is represented by a reconstruction value formula_52 which implements the mapping formula_53.\n\nThese two stages together comprise the mathematical operation of formula_54.\n\nEntropy coding techniques can be applied to communicate the quantization indices from a source encoder that performs the classification stage to a decoder that performs the reconstruction stage. One way to do this is to associate each quantization index formula_11 with a binary codeword formula_56. An important consideration is the number of bits used for each codeword, denoted here by formula_57.\n\nAs a result, the design of an formula_40-level quantizer and an associated set of codewords for communicating its index values requires finding the values of formula_43, formula_60 and formula_61 which optimally satisfy a selected set of design constraints such as the bit rate formula_62 and distortion formula_63.\n\nAssuming that an information source formula_64 produces random variables formula_65 with an associated probability density function formula_66, the probability formula_67 that the random variable falls within a particular quantization interval formula_49 is given by\n\nThe resulting bit rate formula_62, in units of average bits per quantized value, for this quantizer can be derived as follows:\n\nIf it is assumed that distortion is measured by mean squared error, the distortion D, is given by:\n\nNote that other distortion measures can also be considered, although mean squared error is a popular one.\n\nA key observation is that rate formula_62 depends on the decision boundaries formula_74 and the codeword lengths formula_75, whereas the distortion formula_63 depends on the decision boundaries formula_74 and the reconstruction levels formula_78.\n\nAfter defining these two performance metrics for the quantizer, a typical Rate–Distortion formulation for a quantizer design problem can be expressed in one of two ways:\n1. Given a maximum distortion constraint formula_79, minimize the bit rate formula_62\n2. Given a maximum bit rate constraint formula_81, minimize the distortion formula_63\n\nOften the solution to these problems can be equivalently (or approximately) expressed and solved by converting the formulation to the unconstrained problem formula_83 where the Lagrange multiplier formula_84 is a non-negative constant that establishes the appropriate balance between rate and distortion. Solving the unconstrained problem is equivalent to finding a point on the convex hull of the family of solutions to an equivalent constrained formulation of the problem. However, finding a solution – especially a closed-form solution – to any of these three problem formulations can be difficult. Solutions that do not require multi-dimensional iterative optimization techniques have been published for only three probability distribution functions: the uniform, exponential, and Laplacian distributions. Iterative optimization approaches can be used to find solutions in other cases.\n\nNote that the reconstruction values formula_78 affect only the distortion – they do not affect the bit rate – and that each individual formula_13 makes a separate contribution formula_87 to the total distortion as shown below:\nwhere\nThis observation can be used to ease the analysis – given the set of formula_74 values, the value of each formula_13 can be optimized separately to minimize its contribution to the distortion formula_63.\n\nFor the mean-square error distortion criterion, it can be easily shown that the optimal set of reconstruction values formula_93 is given by setting the reconstruction value formula_13 within each interval formula_49 to the conditional expected value (also referred to as the \"centroid\") within the interval, as given by:\n\nThe use of sufficiently well-designed entropy coding techniques can result in the use of a bit rate that is close to the true information content of the indices formula_97, such that effectively\nand therefore\n\nThe use of this approximation can allow the entropy coding design problem to be separated from the design of the quantizer itself. Modern entropy coding techniques such as arithmetic coding can achieve bit rates that are very close to the true entropy of a source, given a set of known (or adaptively estimated) probabilities formula_100.\n\nIn some designs, rather than optimizing for a particular number of classification regions formula_40, the quantizer design problem may include optimization of the value of formula_40 as well. For some probabilistic source models, the best performance may be achieved when formula_40 approaches infinity.\n\nIn the above formulation, if the bit rate constraint is neglected by setting formula_84 equal to 0, or equivalently if it is assumed that a fixed-length code (FLC) will be used to represent the quantized data instead of a variable-length code (or some other entropy coding technology such as arithmetic coding that is better than an FLC in the rate–distortion sense), the optimization problem reduces to minimization of distortion formula_63 alone.\n\nThe indices produced by an formula_40-level quantizer can be coded using a fixed-length code using formula_107 bits/symbol. For example, when formula_108256 levels, the FLC bit rate formula_62 is 8 bits/symbol. For this reason, such a quantizer has sometimes been called an 8-bit quantizer. However using an FLC eliminates the compression improvement that can be obtained by use of better entropy coding.\n\nAssuming an FLC with formula_40 levels, the Rate–Distortion minimization problem can be reduced to distortion minimization alone.\nThe reduced problem can be stated as follows: given a source formula_65 with pdf formula_66 and the constraint that the quantizer must use only formula_40 classification regions, find the decision boundaries formula_114 and reconstruction levels formula_115 to minimize the resulting distortion \n\nFinding an optimal solution to the above problem results in a quantizer sometimes called a MMSQE (minimum mean-square quantization error) solution, and the resulting pdf-optimized (non-uniform) quantizer is referred to as a \"Lloyd–Max\" quantizer, named after two people who independently developed iterative methods to solve the two sets of simultaneous equations resulting from formula_117 and formula_118, as follows:\nwhich places each threshold at the midpoint between each pair of reconstruction values, and\nwhich places each reconstruction value at the centroid (conditional expected value) of its associated classification interval.\n\nLloyd's Method I algorithm, originally described in 1957, can be generalized in a straightforward way for application to vector data. This generalization results in the Linde–Buzo–Gray (LBG) or k-means classifier optimization methods. Moreover, the technique can be further generalized in a straightforward way to also include an entropy constraint for vector data.\n\nThe Lloyd–Max quantizer is actually a uniform quantizer when the input pdf is uniformly distributed over the range formula_121. However, for a source that does not have a uniform distribution, the minimum-distortion quantizer may not be a uniform quantizer.\n\nThe analysis of a uniform quantizer applied to a uniformly distributed source can be summarized in what follows:\n\nA symmetric source X can be modelled with formula_122, for formula_123 and 0 elsewhere.\nThe step size formula_124 and the \"signal to quantization noise ratio\" (SQNR) of the quantizer is\n\nFor a fixed-length code using formula_126 bits, formula_127, resulting in\nformula_128,\n\nor approximately 6 dB per bit. For example, for formula_126=8 bits, formula_40=256 levels and SQNR = 8*6 = 48 dB; and for formula_126=16 bits, formula_40=65536 and SQNR = 16*6 = 96 dB. The property of 6 dB improvement in SQNR for each extra bit used in quantization is a well-known figure of merit. However, it must be used with care: this derivation is only for a uniform quantizer applied to a uniform source.\n\nFor other source pdfs and other quantizer designs, the SQNR may be somewhat different from that predicted by 6 dB/bit, depending on the type of pdf, the type of source, the type of quantizer, and the bit rate range of operation.\n\nHowever, it is common to assume that for many sources, the slope of a quantizer SQNR function can be approximated as 6 dB/bit when operating at a sufficiently high bit rate. At asymptotically high bit rates, cutting the step size in half increases the bit rate by approximately 1 bit per sample (because 1 bit is needed to indicate whether the value is in the left or right half of the prior double-sized interval) and reduces the mean squared error by a factor of 4 (i.e., 6 dB) based on the formula_133 approximation.\n\nAt asymptotically high bit rates, the 6 dB/bit approximation is supported for many source pdfs by rigorous theoretical analysis. Moreover, the structure of the optimal scalar quantizer (in the rate–distortion sense) approaches that of a uniform quantizer under these conditions.\nMany physical quantities are actually quantized by physical entities. Examples of fields where this limitation applies include electronics (due to electrons), optics (due to photons), biology (due to DNA), physics (due to Planck limits) and chemistry (due to molecules). This limitation is sometimes known in these fields as the \"quantum noise limit\".\n\n", "related": "\n- Analog-to-digital converter\n- Beta encoder\n- Data binning\n- Discretization\n- Discretization error\n- Posterization\n- Pulse code modulation\n- Quantile\n- Regression dilution – a bias in parameter estimates caused by errors such as quantization in the explanatory or independent variable\n\n\n- Quantization noise in Digital Computation, Signal Processing, and Control, Bernard Widrow and István Kollár, 2007.\n- The Relationship of Dynamic Range to Data Word Size in Digital Audio Processing\n- Round-Off Error Variance – derivation of noise power of formula_9 for round-off error\n- Dynamic Evaluation of High-Speed, High Resolution D/A Converters Outlines HD, IMD and NPR measurements, also includes a derivation of quantization noise\n- Signal to quantization noise in quantized sinusoidal\n"}
{"id": "3265205", "url": "https://en.wikipedia.org/wiki?curid=3265205", "title": "Directional symmetry (time series)", "text": "Directional symmetry (time series)\n\nIn statistical analysis of time series and in signal processing, directional symmetry is a statistical measure of a model's performance in predicting the direction of change, positive or negative, of a time series from one time period to the next.\n\nGiven a time series formula_1 with values formula_2 at times formula_3 and a model that makes predictions for those values formula_4, then the directional symmetry (DS) statistic is defined as\n\nThe DS statistic gives the percentage of occurrences in which the sign of the change in value from one time period to the next is the same for both the actual and predicted time series. The DS statistic is a measure of the performance of a model in predicting the direction of value changes. The case formula_7 would indicate that a model perfectly predicts the direction of change of a time series from one time period to the next.\n\n", "related": "\n- Statistical finance\n\n- Drossu, Radu, and Zoran Obradovic. \"INFFC data analysis: lower bounds and testbed design recommendations.\" \"Computational Intelligence for Financial Engineering (CIFEr)\", 1997., Proceedings of the IEEE/IAFE 1997. IEEE, 1997.\n- Lawrance, A. J., \"Directionality and Reversibility in Time Series\", \"International Statistical Review\", 59 (1991), 67–79.\n- Tay, Francis EH, and Lijuan Cao. \"Application of support vector machines in financial time series forecasting.\" \"Omega\" 29.4 (2001): 309–317.\n- Xiong, Tao, Yukun Bao, and Zhongyi Hu. \"Beyond one-step-ahead forecasting: Evaluation of alternative multi-step-ahead forecasting models for crude oil prices.\" \"Energy Economics\" 40 (2013): 405–415.\n"}
{"id": "39359866", "url": "https://en.wikipedia.org/wiki?curid=39359866", "title": "Order tracking (signal processing)", "text": "Order tracking (signal processing)\n\nIn rotordynamics, order tracking is a family of signal processing tools aimed at transforming a measured signal from time domain to angular (or order) domain. These techniques are applied to asynchronously sampled signals (i.e. with a constant sample rate in Hertz) to obtain the same signal sampled at constant angular increments of a reference shaft. In some cases the outcome of the Order Tracking is directly the Fourier transform of such angular domain signal, whose frequency counterpart is defined as \"order\". Each order represents a fraction of the angular velocity of the reference shaft.\n\nOrder tracking is based on a velocity measurement, generally obtained by means of a tachometer or encoder, needed to estimate the instantaneous velocity and/or the angular position of the shaft.\n\nThree main families of computed order tracking techniques have been developed in the past: Computed Order Tracking (COT), Vold-Kalman Filter (VKF) and Order Tracking Transforms.\n\nComputed order tracking is a resampling technique based on interpolation.\nThe procedure begins by estimating the time instants formula_1 formula_2 corresponding to integer rotations of the shaft (i.e. angle equal to formula_3). Then an angular rotation vector is defined:\n\nformula_4\n\naccordingly to the desired angular resolution:\n\nformula_5\n\nA corresponding vector of time instants is obtained by means of a first interpolation step\n\nformula_6\n\nA second interpolation step is then applied to obtain the angular resampled signal formula_7 from the original time domain signal formula_8:\n\nformula_9\n\nVold-Kalman filter is a particular formulation of Kalman filter, able to estimate both instantaneous speed and amplitude of a series of harmonics of the shaft rotational velocity.\n\nOrder tracking transforms are mathematical transforms which perform in a single step both the order tracking (synchronization of the signal domain with the reference shaft) and the Fourier transform to assess amplitude and phase of each order of the so obtained spectrum. With such transforms it is possible to directly assess the amplitude of a synchronous, sub-synchronous or super-synchronous shaft-locked harmonics, without an additional resampling step.\n\nThe most recent formulation of such transforms is the Velocity Synchronous Discrete Fourier Transform, defined as follows:\n\nformula_10\n\nwhere formula_11 is the order of the harmonics to be estimated, formula_12 is the total angular rotation of the shaft in the acquisition window, formula_13 and formula_14 are respectively the instantaneous angular rotation and velocity of the reference shaft.\n", "related": "NONE"}
{"id": "30485319", "url": "https://en.wikipedia.org/wiki?curid=30485319", "title": "Biot–Tolstoy–Medwin diffraction model", "text": "Biot–Tolstoy–Medwin diffraction model\n\nIn applied mathematics, the Biot–Tolstoy–Medwin (BTM) diffraction model describes edge diffraction. Unlike the uniform theory of diffraction (UTD), BTM does not make the high frequency assumption (in which edge lengths and distances from source and receiver are much larger than the wavelength). BTM sees use in acoustic simulations.\n\nThe impulse response according to BTM is given as follows:\n\nThe general expression for sound pressure is given by the convolution integral\n\nwhere formula_2 represents the source signal, and formula_3 represents the impulse response at the receiver position. The BTM gives the latter in terms of\n\n- the source position in cylindrical coordinates formula_4 where the formula_5-axis is considered to lie on the edge and formula_6 is measured from one of the faces of the wedge.\n- the receiver position formula_7\n- the (outer) wedge angle formula_8 and from this the wedge index formula_9\n- the speed of sound formula_10\n\nas an integral over edge positions formula_5\n\nwhere the summation is over the four possible choices of the two signs, formula_13 and formula_14 are the distances from the point formula_5 to the source and receiver respectively, and formula_16 is the Dirac delta function.\n\nwhere\n\n", "related": "\n- Uniform theory of diffraction\n\n- Calamia, Paul T. and Svensson, U. Peter, \"Fast time-domain edge-diffraction calculations for interactive acoustic simulations,\" EURASIP Journal on Advances in Signal Processing, Volume 2007, Article ID 63560.\n"}
{"id": "35164689", "url": "https://en.wikipedia.org/wiki?curid=35164689", "title": "Mojette Transform", "text": "Mojette Transform\n\nThe Mojette Transform is an application of discrete geometry. More specifically, it is a discrete and exact version of the Radon transform, thus a projection operator.\n\nThe IRCCyN laboratory - UMR CNRS 6597 in Nantes, France has been developing it since 1994.\n\nThe first characteristic of the Mojette Transform is using only additions and subtractions. The second characteristic is that the Mojette Transform is redundant, spreading the initial geometrical information into several projections.\n\nThis transform uses discrete geometry in order to dispatch information onto a discrete geometrical support. This support is then projected by the Mojette operator along discrete directions. When enough projections are available, the initial information can be reconstructed.\n\nThe Mojette transform has been already used in numerous applications domains:\n- Medical tomography\n- Network packet transfer\n- Distributed storage on disks or networks\n- Image fingerprinting and image cryptography schemes\n\nAfter one year of research, the first communication introducing the Mojette Transform was held in May 1995 in the first edition of CORESA National Congress CCITT Rennes. Many others will follow it for 18 years of existence. In 2011, the book \"The Mojette Transform: Theory and Applications\" at ISTE-Wiley was well received by the scientific community. All this support has encouraged the IRCCyN research team to continue the research on this topic.\n\nJeanpierre Guédon, professor and inventor of the transform called it: \"Mojette Transform\". The word \"Mojette\" comes from the name of white beans in Vendee, originally written \"Moghette\" or \"Mojhette\". In many countries, bean is a basic educational tool representing an exact unit that teaches visually additions and subtractions. Therefore, the choice of the name \"Mojette\" serves to emphasize the fact that the transform uses only exact unit in additions and subtractions.\n\nThere is an old French saying in Vendee: \"counting his mojettes\", meaning to know how to count his money. It is quite amazing that in the English-speaking world, the words \"bean counter\" refers to a non-zealous official making additions. An old English expression says \"he knows how many beans make five\", which means: \"He knows his stuff\".\n\nThe original purpose of the Mojette Transform was to create a discrete tool to divide the Fourier plane into angular and radial sectors. The first attempt of application was the psychovisual encoding of image, reproducing the human vision channel. However, it was never realized.\n\nThe \"raw\" transform Mojette definition is this:\n\nformula_1\nThe following figure 1 helps to explain the “raw” transform Mojette.\n\nWe start with the function f represented by 16 pixels from p1 to p16. The possible values of the function at the point (k, l) are different according to the applications. This can be a binary value of 0 or 1 that it often used to differentiate the object and the background. This can be a ternary value as in the Mojette game. This can be also a finite set of integers value from 0 to (n-1), or more often we take a set of cardinality equal to a power of 2 or a prime number. But it can be integers and real numbers with an infinite number of possibilities, even though this idea has never been used.\n\nWith the index \"k\" as \"kolumn\" and “l” as a “line”, we define a Cartesian coordinate system. But here we will only need the integer coordinates. On Figure 2, we have arbitrarily chosen the left bottom point as the origin (0,0) and the direction of the two axes. The coordinates of each pixel are denoted in red on Figure 2.\n\nFor the projections, the coordinate system is derived from the one of the grid. Indeed, it meets two requirements:\n1) The pixel (0,0) is always projected on the point 0 of the projection (this is a consequence of linearity of the Mojette operator)\n2) The direction of the projection is fixed \"counterclockwise\" as in trigonometry when going from 0 ° to 180 °.\n\nAltogether, it necessarily gives the positions of the bins like the ones in blue color on the Figure 2.\nLet’s head back to the formula (1): the red dots correspond to the index (k, l) and the blue dots to the index b. The only elements remaining to clarify are the (p, q) values.\n\nThese two values (p, q) are precisely those characterizing the Mojette Transform. They define the projection angle. Figure 3 shows colored arrows corresponding with the color code to the projection indexed by (p, q). For the 90° angle, the projection is shown below the grid for convenience but the direction is upward. Table 1 shows the correspondence between the angles in degrees and the values of p and q.\n\n\"Table 1 : The correspondence of the angles projections with direction equation b + qk - pl = 0\"\nThe only valid Mojette angles are given by the following rules:\n1. An angle is given by the direction of projection in line and column\n2. A direction is composed of two integers (p, q) with gcd (p, q) = 1\n3. An angle is always between 0 and 180 °, which means that q is never negative\n\nThese rules ensure the uniqueness in the correspondence of an angle and (p, q) values. For example, the 45 ° angle, the Rule 2 forbid to define the angle pairs (2,2) or (3,3) and Rule 3 prohibits to use (-2, -2) and (-1, -1). Only the angle (p = 1, q = 1) satisfies the three rules.\n\nThe most important area of application using the \"Mojette Transform\" is distributed storage. Particularly, this method is used in RozoFS, an open-source distributed file system. In this application, the \"Mojette Transform\" is used as an erasure code in order to provide reliability, while significantly reducing the total amount of stored data when compared to classical techniques like replication (typically by a factor of 2). Thus, it significantly reduces the cost of the storage cluster in terms of hardware, maintenance or energy consumption for example.\n\nIn 2010, Pierre Evenou, research engineer of the IVC team IRCCyN laboratory, decided to create the start-up Fizians (currently known as Rozo Systems) using this application. The start-up offers storage solutions in cloud computing, virtualization, storage servers, file servers, backup and archiving.\n\nThanks to the redundancy of the transform, sent packets can be fragmented without loss. Additionally, the fact of using only additions and subtractions increases the speed of information transmission. Finally, the information cannot be reconstructed without having the initial angle of the projections, so it also provides data security.\n\nThis application has been selected by Thales Cholet for its ad hoc network (using wireless network and terminals to transmit messages between them) in order to secure the information and has multiple paths between the source and destination. In 2002, the start-up PIBI has used this technology to provide secure Internet payment services.\n\nIn the field of medical imaging, the properties of the “Transform Mojette” create a direct mapping and solve the missing wedge problem. However, the image acquisition using the Mojette transform has not been yet developed. The problem of obtaining exact “Mojette” values while using approximated data acquisition has been studied but has to be continued. Besides, the post-processing of medical images is doing well since data acquisition is already done.\n\nThese results are used by the company Keosys in 2001 with Jerome Fortineau and the company Qualiformed created in 2006 by Stephen Beaumont. Prof. Guédon and the IRCCyN laboratory were heavily involved in the creation of these two companies. The companies have already financed several PhD students and participated in research projects in order to continue the development of the application in medical tomography. The results have led to apply patents and implementation on their equipment of image processing.\n\nCryptography and watermarking were also part of the research conducted in the IRCCyN laboratory. It provides solutions for security and authentication.\n\nIn cryptography, the instability of the transformed Mojette secures data. The fact that the transform is exact encrypts information and allows no deviation even minimal. For watermarking, the transform is very effective in fingerprinting. By inserting \"Mojette Transform\" marks in images, one can authenticate documents using the same properties as in cryptography.\n\n- Jeanpierre Guédon, N. Normand, B. Parrein, and C. Pouliquen, “Distributed image transmission and storage on Internet system,” in ACIDCA, 2000, pp. 164–169.\n- B. Parrein, N. Normand, and J. Guédon, “Multiple description coding using exact discrete Radon transform,” in IEEE Data Compression Conference, 2001, p. 508.\n- J. Guédon, N. Normand, P. Verbert, B. Parrein, F. Autrusseau, “Load-balancing and scalable multimedia distribution using the Mojette transform,” in Internet Multimedia Management Systems II, ITCOM, 2001, pp. 226–234.\n- J. Guédon, B. Parrein, N. Normand, “Internet Distributed Image Information System,” Integrated Computer-Aided Engineering, vol. 8, no. 3, pp. 205–214, Sep. 2008.\n- B. Parrein, “Description multiple de l’information par transformation Mojette,” Université de Nantes, 2008.\n- F. Autrusseau and J. Guédon, “Image watermarking for copyright protection and data hiding via the Mojette transform,” in Security and Watermarking of Multimedia Contents IV, 2002, pp. 378–386.\n- F. Autrusseau and J. Guédon, “Image Watermarking in the Fourier Domain Using the Mojette Transform,” in Digital Signal Processing, 2002, pp. 725–728.\n- F. Autrusseau, “Modélisation Psychovisuelle pour le tatouage des images,” Université de Nantes, 2011.\n- F. Autrusseau and J. Guédon, “A joint multiple description-encryption image algorithm,” in International Conference on Image Processing, 2003, pp. 269–272.\n- J. Guédon, N. Normand, and B. Parrein, “Multimedia packet transport: multiple layers or descriptions?,” in IEEE Packet Video workshop, 2003, p. 7 p.\n- B. Parrein, N. Normand, and J. Guédon, “Multimedia forward error correcting codes for wireless LAN,” Annales des Télécommunications, vol. 58, no. 3–4, pp. 448–463, Jul. 2008.\n- F. Autrusseau and J. Guédon, “Chiffrement Mojette d’images médicales,” Ingénierie des Systèmes d’Information (ISI), vol. 8, no. 1, pp. 113–134, Feb. 2008.\n- O. Déforges, M. Babel, N. Normand, B. Parrein, J. Ronsin, J. Guédon, and L. Bédat, “Le LAR aux Mojettes,” in CORESA 04 - COmpression et REprésentation des Signaux Audiovisuels, 2004, pp. 165–168.\n- P. Verbert, V. Ricordel, J. Guédon, and P. Verbert, “Analysis of mojette transform projections for an efficient coding,” in Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS, 2004, p. -.\n- M. Babel, B. Parrein, O. Déforges, N. Normand, J. Guédon, and J. Ronsin, “Secured and progressive transmission of compressed images on the Internet: application to telemedicine,” in SPIE 17th Annual Symposium / Electronic Imaging - Internet Imaging, 2005, pp. 126–136.\n- J. Guédon and N. Normand, “The Mojette Transform: The First Ten Years,” in Discrete Geometry for Computer Imagery, 2005, vol. 3429, pp. 79–91.\n- M. Servières, N. Normand, J. Guédon, and Y. Bizais, “The Mojette Transform: Discrete Angles for Tomography,” in Discrete Tomography and its Applications, 2005, vol. 20, pp. 587–606.\n- M. Servieres, “Reconstruction Tomographique Mojette,” Université de Nantes; Ecole centrale de nantes - ECN, 2009.\n- F. Autrusseau, P. Evenou, and T. Hamon, “Secure Distributed Storage based on the Mojette transform,” in Nouvelles technologies de la répartition, 2006, pp. 161–170.\n- F. Autrusseau, B. Parrein, and M. Servieres, “Lossless Compression Based on a Discrete and Exact Radon Transform: A Preliminary Study,” in International Conference on Acoustics, Speech and Signal Processing, 2006, pp. 425–428.\n- M. Kalantari, F. Jung, G. Moreau, and J. Guédon, “Détection entièrement automatique de points de fuite dans des scènes architecturales urbaines,” in CORESA 2006 COmpression et REprésentation des Signaux Audiovisuels, 2006, pp. 41–46.\n- E. Denis, J. Guédon, S. Beaumont, and N. Normand, “Discrete and continuous description of a three dimensional scene for quality control of radiotherapy treatment planning systems,” in Medical Imaging, 2006, vol. 6142, p. 187.\n- M. Servières, N. Normand, and J. Guédon, “Interpolation method for the Mojette Transform,” in Medical Imaging 2006: Physics of Medical Imaging, 2006, vol. 6142, p. 61424I.\n- N. Normand, A. Kingston, and P. Évenou, “A Geometry Driven Reconstruction Algorithm for the Mojette Transform,” in Discrete Geometry for Computer Imagery, 2006, vol. 4245, pp. 122–133.\n- S. Hamma, E. Cizeron, H. Issaka, and J. Guédon, “Performance evaluation of reactive and proactive routing protocol in IEEE 802.11 ad hoc network,” in ITCom 06 - next generation and sensor networks, 2008, p. 638709.\n- M. Kalantari and M. Kasser, “Implementation of a low-cost photogrammetric methodology for 3d modelling of ceramic fragments,” in XXI International CIPA Symposium, 01-6 October, Athens, Greece, 2007, p. FP079.\n- A. Kingston, S. Colosimo, P. Campisi, and F. Autrusseau, “Lossless Image Compression and Selective Encryption Using a Discrete Radon Transform,” in International Conference on Image Processing, 2007, pp. 465–468.\n- E. Denis, S. Beaumont, J. Guédon, N. Normand, and T. Torfeh, “Automatic quality control of digitally reconstructed radiograph computation and comparison with standard methods,” in Medical Imaging 2007: Physics of Medical Imaging, 2007, vol. 6510, p. 65104J.\n- A. Daurat and N. Normand, “Transformation et reconstruction par projections,” in Géométrie discrète et images numériques, A. M. David Coeurjolly, Ed. Hermès, 2008, pp. 239–251.\n- N. Normand and J. Guédon, “Applications de la transformation Mojette,” in Géométrie discrète et images numériques, A. M. David Coeurjolly, Ed. Hermès, 2008, pp. 337–347.\n- B. Parrein, F. Boulos, P. Le Callet, and J. Guédon, “Priority image and video encoding transmission based on a discrete Radon transform,” in IEEE Packet Video 2007, 2007, p. 6 pages.\n- S. Chandra, I. Svalbe, and J. Guédon, “An exact, non-iterative Mojette inversion technique utilising ghosts,” in 14th IAPR international conference on Discrete geometry for computer imagery, 2008, p. .\n- H. Fayad, J. Guédon, I. Svalbe, N. Normand, and Y. Bizais, “Mojette and FRT tomographs,” in Medical Imaging 2008, 2008, vol. 6913, p. -.\n- M. Kalantari, F. Jung, J. Guédon, and N. Paparoditis, “Détection automatique des points de fuite et calcul de leur incertitude à l’aide de la géométrie projective,” in RFIA 2008, 2008, pp. 703–712.\n- M. Kalantari, F. Jung, N. Paparoditis, and J. Guédon, “Robust and automatic vanishing points detection with their uncertainties from a single uncalibrated image, by planes extraction on the unit SPHERE,” in ISPRS2008, 2008, pp. 203–208.\n- H. Fayad, J. Guédon, I. Svalbe, Y. Bizais, and N. Normand, “Applying Mojette discrete Radon transforms to classical tomographic data,” in Medical Imaging, 2008, vol. 6913, p. 69132S.\n- A. Kingston and F. Autrusseau, “Lossless Image Compression via Predictive Coding of Discrete Radon Projections,” Signal Processing Image Communication, vol. 23, no. 4, pp. 313–324, Jun. 2008.\n- E. Denis, S. Beaumont, J. Guédon, T. Torfeh, N. Normand, and A. Norbert, “New automatic quality control methods for geometrical treatment planning system tools in external conformal radiotherapy,” in Medical Imaging 2008: Physics of Medical Imaging, 2008, vol. 6913, p. 69133F.\n- M. Babel, B. Parrein, O. Déforges, N. Normand, J. Guédon, and V. Coat, “Joint source-channel coding: secured and progressive transmission of compressed medical images on the Internet,” Computerized Medical Imaging and Graphics, vol. 32, no. 4, pp. 258–269, Apr. 2008.\n- E. Denis, S. Beaumont, J. Guédon, T. Torfeh, N. Normand, and N. Ailleres, “Nouvelle méthode automatique de contrôle de qualité des systèmes de planification géométrique des traitements en radiothérapie externe conformationnelle,” in Journées scientifiques de la Société Française de Physique Médicale, 2008, p. denis.\n- A. Kingston, B. Parrein, and F. Autrusseau, “Redundant Image Representation via Multi-Scale Digital Radon Projection,” in International Conf. of Image Processing, 2008, p. 2069.\n- P. Jia, J. Dong, L. Qi, and F. Autrusseau, “Directionality Measurement and Illumination Estimation of 3D Surface Textures by Using Mojette Transform,” in 19th International Conference on Pattern Recognition, 2010, p. 1144.\n- Y. Ben Hdech, J. Guédon, and S. Beaumont, “Simulations Monte Carlo d’un faisceau de RX issus d’un accélérateur VARIAN : influence du paramétrage des électrons initiaux,” in Journées Scientifiques de la Société Française de Physique Médicale (SFPM) 2009 : Innovations et bénéfices thérapeutiques : quelles limites?, 2009, p. 1.\n- Y. Ben Hdech, J. Guédon, and S. Beaumont, “Des Objets-Tests Numériques (OTN) anatomiques pour le Contrôle Qualité (CQ) de Systèmes de Planification de Traitement (TPS) en radiothérapie,” in Journées Scientifiques de la Société Française de Physique Médicale (SFPM) 2009 : Innovations et bénéfices thérapeutiques : quelles limites?, 2009, p. 1.\n- M. Kalantari, F. Jung, J. Guédon, and N. Paparoditis, “The Five Points Pose Problem : A New and Accurate Solution Adapted to any Geometric Configuration,” in The Pacific-Rim Symposium on Image and Video Technology (PSIVT), 2009, vol. 5414, p. .\n- D. Coeurjolly and N. Normand, “Discrete geometry and projections (chap 1),” in The Mojette Transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 15 pages.\n- J. Guédon and N. Normand, “Reconstructability with the inverse Mojette transform (chap 4),” in The Mojette Transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 15 pages.\n- J. Guédon and N. Normand, “Direct Mojette transform (chap 3),” in The Mojette Transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 23 pages.\n- A. Kingston and F. Autrusseau, “Lossless compression (chap 9),” in The Mojette transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 19 pages.\n- A. Kingston, F. Autrusseau, E. Grall, T. Hamon, and B. Parrein, “Mojette based security (chap 10),” in The Mojette transform: Theory and Applications, J. Guédon, Ed. iste & wiley, 2009, p. 25 pages.\n- A. Kingston, F. Autrusseau, and B. Parrein, “Multiresolution Mojette transform (chap 6),” in The Mojette transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 29 pages.\n- N. Normand, I. Svalbe, P. Evenou, and A. Kingston, “Inverse Mojette transform algorithms (chap 5),” in The Mojette Transform: Theory and Applications, J. Guédon, Ed. iste & wiley, 2009, p. 25 pages.\n- B. Parrein, F. Boulos, N. Normand, and P. Evenou, “Communication, networks and storage (chap 7),” in The Mojette Transform: Theory and Applications, J. Guédon, Ed. iste & wiley, 2009, p. 29 pages.\n- M. Servières, J. Guédon, N. Normand, and Y. Bizais, “Mojette discrete tomography (chap 8),” in The Mojette Transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 29 pages.\n- I. Svalbe and J. Guédon, “Discrete versions of the Radon Transform (chap 2),” in The Mojette Transform: Theory and Applications, jeanpierre Guédon, Ed. iste & wiley, 2009, p. 17 pages.\n- J. Guédon, The Mojette transform. Theory and applications. ISTE-WILEY, 2009.\n- S. Beaumont, J. Guédon, and Y. Ben Hdech, “Contrôle qualité dosimétrique des systèmes de planification de traitement : nouvelle méthode basée sur l’utilisation de PENELOPE et des Objets Tests Numériques anatomiques,” in Journées Scientifiques de la Société Française de Physique Médicale (SFPM), 2010, p. 1.\n- Y. Ben Hdech, S. Beaumont, and J. Guédon, “Développement d’une méthode de Contrôle qualité des Systèmes de Planification des Traitements, utilisés en radiothérapie, au moyen du code Monte-Carlo PENELOPE et des Objets Tests Numériques,” in Journée des doctorants de l’École Doctorale STIM JDOC, 2010, p. 1.\n- Y. Ben Hdech, S. Beaumont, J. Guédon, and T. Torfeh, “New method to perform dosimetric quality control of treatment planning system using PENELOPE Monte-Carlo and anatomical digital test objects,” in SPIE Medical Imaging 2010, 2010, vol. 7622, p. .\n- Y. Amouriq, P. Evenou, A. Arlicot, N. Normand, and P. Layrolle, “Evaluation of trabecular bone patterns on dental radiographic images: influence of cortical bone,” in SPIE Medical Imaging, 2010, vol. 7626, p. 76261M.\n- Y. Amouriq, P. Evenou, A. Arlicot, N. Normand, P. Layrolle, P. Weiss, and J. Guédon, “Evaluation of trabecular bone patterns on dental radiographic images: influence of cortical bone,” in SPIE Medical Imaging, 2010, p. 10 pages.\n- A. Arlicot, Y. Amouriq, P. Evenou, N. Normand, and J. Guédon, “A single scan skeletonization algorithm: application to medical imaging of trabecular bone,” in SPIE Medical Imaging, 2010, vol. 7623, p. 762317.\n- C. Zhang, J. Dong, J. Li, and F. Autrusseau, “A New Information Hiding Method for Image Watermarking Based on Mojette Transform,” in Second International Symposium on Networking and Network Security, 2010, pp. 124–128.\n- N. Normand, I. Svalbe, B. Parrein, and A. Kingston, “Erasure Coding with the Finite Radon Transform,” in Wireless Communications & Networking Conference, 2010, pp. 1–6.\n- S. S. Chandra, N. Normand, A. Kingston, J. Guédon, and I. Svalbe, “Fast Mojette Transform for Discrete Tomography,” 13-Jul-2012.\n- J. Guédon, C. Liu, and J. Guédon, “The 2 and 3 materials scene reconstructed from some line Mojette projections,” in IEEE IPTA Conference, 2010, p. 6.\n- Y. Amouriq, J. Guédon, N. Normand, A. Arlicot, Y. Ben Hdech, and P. Weiss, “Bone texture analysis on dental radiographic images: results with several angulated radiographs on the same region of interest,” in SPIE Medical Imaging 2011: Biomedical Applications in Molecular, Structural, and Functional Imaging, 2012, vol. 7965, p. 796525.\n- S. Beaumont, T. Torfeh, R. Latreille, Y. Ben Hdech, and J. Guédon, “New method to test the gantry, collimator and table rotation angles of a linear accelerator used in radiation therapy,” in SPIE Medical Imaging 2011, 2011, vol. 7961, p. 796153.\n- Y. Ben Hdech, S. Beaumont, J. Guédon, and C. Sylvain, “Dosimetric quality control of Eclipse treatment planning system using pelvic digital test object,” in Medical Imaging 2011: Physics of Medical Imaging, 2011, vol. 7961, p. 79613F.\n- A. Arlicot, P. Evenou, and N. Normand, “Single-scan skeletonization driven by a neighborhood-sequence distance,” in International workshop on Combinatorial Image Analysis, IWCIA, 2011, pp. 61–72.\n- A. Arlicot, N. Normand, Y. Amouriq, and J. Guédon, “Extraction of bone structure with a single-scan skeletonization driven by distance,” in First Sino-French Workshop on Education and Research collaborations in Information and Communication Technologies, SIFWICT, 2011, p. 2 pages.\n- Y. Ben Hdech, D. Autret, S. Beaumont, and J. Guédon, “TPS dosimetric evaluation using 1540-IAEA Package and Monte-Carlo simulations,” in ESTRO International Oncology Forum, 2011, p. 1.\n- C. Liu, J. Guédon, I. Svalbe, and Y. Amouriq, “Line Mojette ternary reconstructions and ghosts,” in IWCIA, 2011, p. 11.\n- C. Liu and J. Guédon, “The limited material scenes reconstructed by line Mojette algorithms,” in Franco-Chinese conference, 2011, p. 2.\n- J. Dong, L. Su, Y. Zhang, F. Autrusseau, and Y. Zhanbin, “Estimating Illumination Direction of 3D Surface Texture Based on Active Basis and Mojette Transform,” Journal of Electronic Imaging, vol. 21, no. 013023, p. 28 pages, Apr. 2012.\n- D. Pertin, G. D’Ippolito, N. Normand, and B. Parrein, “Spatial Implementation for Erasure Coding by Finite Radon Transform,” in International Symposium on signal, Image, Video and Communication 2012, 2012, pp. 1–4.\n- P. Bléry, Y. Amouriq, J. Guédon, P. Pilet, N. Normand, N. Durand, F. Espitalier, A. Arlicot, O. Malard, and P. Weiss, “Microarchitecture of irradiated bone: comparison with healthy bone,” in SPIE Medical Imaging, 2012, vol. 8317, p. 831719.\n- S. Chandra, I. Svalbe, J. Guedon, A. Kingston, and N. Normand, “Recovering Missing Slices of the Discrete Fourier Transform using Ghosts,” IEEE Transactions on Image Processing, vol. 21, no. 10, pp. 4431–4441, Jul. 2012.\n- H. Der Sarkissian, Jp. Guédon, P. Tervé, N. Normand and I. Svalbe. (2012).\" Evaluation of Discrete Angles Rotation Degradation for Myocardial Perfusion Imaging\", EANM Annual Congress 2012.\n- C. Liu and J. Guédon, “Finding all solutions of the 3 materials problem,” in proceedings of SIFWICT, 2013, p. 6.\n- B. Recur, H. Der Sarkissian, Jp. Guédon and I.Svalbe, \"Tomosynthèse à l’aide de transformées discrètes\", in Proceeding TAIMA 2013\n- H. Der Sarkissian, B. Recur, N. Normand and Jp. Guédon, \"Mojette space Transformations\", in proceedings of SWIFCT 2013.\n- B. Recur, H. Der Sarkissian, M. Servières, N.Normand, Jp. Guédon, \"Validation of Mojette Reconstruction from Radon Acquisitions\" in Proceedings of 2013 IEEE International Conference on Image Processing.\n- H. Der Sarkissian, B. Recur, N. Normand, Jp. Guédon. (2013), \"Rotations in the Mojette Space\" in 2013 IEEE International Conference on Image Processing.\n\n- Website of the IVC team of the IRCCyN lab\n- Game on line based on the Mojette transform\n- Official website of ROZOFS company\n- Official website of KEOSYS company\n- Official website of QUALIFORMED company\n", "related": "NONE"}
{"id": "56484", "url": "https://en.wikipedia.org/wiki?curid=56484", "title": "Low-pass filter", "text": "Low-pass filter\n\nA low-pass filter (LPF) is a filter that passes signals with a frequency lower than a selected cutoff frequency and attenuates signals with frequencies higher than the cutoff frequency. The exact frequency response of the filter depends on the filter design. The filter is sometimes called a high-cut filter, or treble-cut filter in audio applications. A low-pass filter is the complement of a high-pass filter.\n\nIn optics, high-pass and low-pass may have the different meanings, depending on whether referring to frequency or wavelength of light, since these variable are inversely related. High-pass frequency filters would act as low-pass wavelength filters, and vice versa. For this reason it is a good practice to refer to wavelength filters as \"Short-pass\" and \"Long-pass\" to avoid confusion, which would correspond to \"high-pass\" and \"low-pass\" frequencies. \n\nLow-pass filters exist in many different forms, including electronic circuits such as a hiss filter used in audio, anti-aliasing filters for conditioning signals prior to analog-to-digital conversion, digital filters for smoothing sets of data, acoustic barriers, blurring of images, and so on. The moving average operation used in fields such as finance is a particular kind of low-pass filter, and can be analyzed with the same signal processing techniques as are used for other low-pass filters. Low-pass filters provide a smoother form of a signal, removing the short-term fluctuations and leaving the longer-term trend.\n\nFilter designers will often use the low-pass form as a prototype filter. That is, a filter with unity bandwidth and impedance. The desired filter is obtained from the prototype by scaling for the desired bandwidth and impedance and transforming into the desired bandform (that is low-pass, high-pass, band-pass or band-stop).\n\nExamples of low-pass filters occur in acoustics, optics and electronics.\n\nA stiff physical barrier tends to reflect higher sound frequencies, and so acts as an acoustic low-pass filter for transmitting sound. When music is playing in another room, the low notes are easily heard, while the high notes are attenuated.\n\nAn optical filter with the same function can correctly be called a low-pass filter, but conventionally is called a \"longpass\" filter (low frequency is long wavelength), to avoid confusion.\n\nIn an electronic low-pass RC filter for voltage signals, high frequencies in the input signal are attenuated, but the filter has little attenuation below the cutoff frequency determined by its RC time constant. For current signals, a similar circuit, using a resistor and capacitor in parallel, works in a similar manner. (See current divider discussed in more detail below.)\n\nElectronic low-pass filters are used on inputs to subwoofers and other types of loudspeakers, to block high pitches that they can't efficiently reproduce. Radio transmitters use low-pass filters to block harmonic emissions that might interfere with other communications. The tone knob on many electric guitars is a low-pass filter used to reduce the amount of treble in the sound. An integrator is another time constant low-pass filter.\n\nTelephone lines fitted with DSL splitters use low-pass and high-pass filters to separate DSL and POTS signals sharing the same pair of wires.\n\nLow-pass filters also play a significant role in the sculpting of sound created by analogue and virtual analogue synthesisers. \"See subtractive synthesis.\"\n\nA low-pass filter is used as an anti-aliasing filter prior to sampling and for reconstruction in digital-to-analog conversion.\n\nAn ideal low-pass filter completely eliminates all frequencies above the cutoff frequency while passing those below unchanged; its frequency response is a rectangular function and is a brick-wall filter. The transition region present in practical filters does not exist in an ideal filter. An ideal low-pass filter can be realized mathematically (theoretically) by multiplying a signal by the rectangular function in the frequency domain or, equivalently, convolution with its impulse response, a sinc function, in the time domain.\n\nHowever, the ideal filter is impossible to realize without also having signals of infinite extent in time, and so generally needs to be approximated for real ongoing signals, because the sinc function's support region extends to all past and future times. The filter would therefore need to have infinite delay, or knowledge of the infinite future and past, in order to perform the convolution. It is effectively realizable for pre-recorded digital signals by assuming extensions of zero into the past and future, or more typically by making the signal repetitive and using Fourier analysis.\n\nReal filters for real-time applications approximate the ideal filter by truncating and windowing the infinite impulse response to make a finite impulse response; applying that filter requires delaying the signal for a moderate period of time, allowing the computation to \"see\" a little bit into the future. This delay is manifested as phase shift. Greater accuracy in approximation requires a longer delay.\n\nAn ideal low-pass filter results in ringing artifacts via the Gibbs phenomenon. These can be reduced or worsened by choice of windowing function, and the design and choice of real filters involves understanding and minimizing these artifacts. For example, \"simple truncation [of sinc] causes severe ringing artifacts,\" in signal reconstruction, and to reduce these artifacts one uses window functions \"which drop off more smoothly at the edges.\"\n\nThe Whittaker–Shannon interpolation formula describes how to use a perfect low-pass filter to reconstruct a continuous signal from a sampled digital signal. Real digital-to-analog converters use real filter approximations.\n\nMany digital filters are designed to give low-pass characteristics. Both infinite impulse response and finite impulse response low pass filters as well as filters using Fourier transforms are widely used.\n\nThe effect of an infinite impulse response low-pass filter can be simulated on a computer by analyzing an RC filter's behavior in the time domain, and then discretizing the model.\nFrom the circuit diagram to the right, according to Kirchhoff's Laws and the definition of capacitance:\nwhere formula_1 is the charge stored in the capacitor at time formula_2. Substituting equation into equation gives formula_3, which can be substituted into equation so that:\n\nThis equation can be discretized. For simplicity, assume that samples of the input and output are taken at evenly spaced points in time separated by formula_5 time. Let the samples of formula_6 be represented by the sequence formula_7, and let formula_8 be represented by the sequence formula_9, which correspond to the same points in time. Making these substitutions:\n\nAnd rearranging terms gives the recurrence relation\n\nThat is, this discrete-time implementation of a simple RC low-pass filter is the exponentially weighted moving average \n\nBy definition, the \"smoothing factor\" formula_13. The expression for formula_14 yields the equivalent time constant formula_15 in terms of the sampling period formula_5 and smoothing factor formula_14:\n\nRecalling that\nthen formula_21 and formula_22 are related by:\nand\n\nIf formula_25, then the formula_26 time constant is equal to the sampling period. If formula_27, then formula_26 is significantly larger than the sampling interval, and formula_29.\n\nThe filter recurrence relation provides a way to determine the output samples in terms of the input samples and the preceding output. The following pseudocode algorithm simulates the effect of a low-pass filter on a series of digital samples:\n\nThe loop that calculates each of the \"n\" outputs can be refactored into the equivalent:\n\nThat is, the change from one filter output to the next is proportional to the difference between the previous output and the next input. This exponential smoothing property matches the exponential decay seen in the continuous-time system. As expected, as the time constant formula_15 increases, the discrete-time smoothing parameter formula_14 decreases, and the output samples formula_9 respond more slowly to a change in the input samples formula_7; the system has more \"inertia\". This filter is an infinite-impulse-response (IIR) single-pole low-pass filter.\n\nFinite-impulse-response filters can be built that approximate to the sinc function time-domain response of an ideal sharp-cutoff low-pass filter. For minimum distortion the finite impulse response filter has an unbounded number of coefficients operating on an unbounded signal. In practice, the time-domain response must be time truncated and is often of a simplified shape; in the simplest case, a running average can be used, giving a square time response.\n\nFor non-realtime filtering, to achieve a low pass filter, the entire signal is usually taken as a looped signal, the Fourier transform is taken, filtered in the frequency domain, followed by an inverse Fourier transform. Only O(n log(n)) operations are required compared to O(n) for the time domain filtering algorithm.\n\nThis can also sometimes be done in real-time, where the signal is delayed long enough to perform the Fourier transformation on shorter, overlapping blocks.\n\nThere are many different types of filter circuits, with different responses to changing frequency. The frequency response of a filter is generally represented using a Bode plot, and the filter is characterized by its cutoff frequency and rate of frequency rolloff. In all cases, at the \"cutoff frequency,\" the filter attenuates the input power by half or 3 dB. So the order of the filter determines the amount of additional attenuation for frequencies higher than the cutoff frequency.\n\n- A first-order filter, for example, reduces the signal amplitude by half (so power reduces by a factor of 4, or , every time the frequency doubles (goes up one octave); more precisely, the power rolloff approaches 20 dB per decade in the limit of high frequency. The magnitude Bode plot for a first-order filter looks like a horizontal line below the cutoff frequency, and a diagonal line above the cutoff frequency. There is also a \"knee curve\" at the boundary between the two, which smoothly transitions between the two straight line regions. If the transfer function of a first-order low-pass filter has a zero as well as a pole, the Bode plot flattens out again, at some maximum attenuation of high frequencies; such an effect is caused for example by a little bit of the input leaking around the one-pole filter; this one-pole–one-zero filter is still a first-order low-pass. \"See Pole–zero plot and RC circuit.\"\n- A second-order filter attenuates high frequencies more steeply. The Bode plot for this type of filter resembles that of a first-order filter, except that it falls off more quickly. For example, a second-order Butterworth filter reduces the signal amplitude to one fourth its original level every time the frequency doubles (so power decreases by 12 dB per octave, or 40 dB per decade). Other all-pole second-order filters may roll off at different rates initially depending on their Q factor, but approach the same final rate of 12 dB per octave; as with the first-order filters, zeroes in the transfer function can change the high-frequency asymptote. See RLC circuit.\n- Third- and higher-order filters are defined similarly. In general, the final rate of power rolloff for an order-formula_34 all-pole filter is formula_35 dB per octave (i.e., formula_36 dB per decade).\n\nOn any Butterworth filter, if one extends the horizontal line to the right and the diagonal line to the upper-left (the asymptotes of the function), they intersect at exactly the \"cutoff frequency\". The frequency response at the cutoff frequency in a first-order filter is 3 dB below the horizontal line. The various types of filters (Butterworth filter, Chebyshev filter, Bessel filter, etc.) all have different-looking \"knee curves\". Many second-order filters have \"peaking\" or resonance that puts their frequency response at the cutoff frequency \"above\" the horizontal line. Furthermore, the actual frequency where this peaking occurs can be predicted without calculus, as shown by Cartwright et al. For third-order filters, the peaking and its frequency of occurrence can also be predicted without calculus as shown by Cartwright et al. \"See electronic filter for other types.\"\n\nThe meanings of 'low' and 'high'—that is, the cutoff frequency—depend on the characteristics of the filter. The term \"low-pass filter\" merely refers to the shape of the filter's response; a high-pass filter could be built that cuts off at a lower frequency than any low-pass filter—it is their responses that set them apart. Electronic circuits can be devised for any desired frequency range, right up through microwave frequencies (above 1 GHz) and higher.\n\nContinuous-time filters can also be described in terms of the Laplace transform of their impulse response, in a way that lets all characteristics of the filter be easily analyzed by considering the pattern of poles and zeros of the Laplace transform in the complex plane. (In discrete time, one can similarly consider the Z-transform of the impulse response.)\n\nFor example, a first-order low-pass filter can be described in Laplace notation as:\n\nwhere \"s\" is the Laplace transform variable, \"τ\" is the filter time constant, and \"K\" is the gain of the filter in the passband.\n\nOne simple low-pass filter circuit consists of a resistor in series with a load, and a capacitor in parallel with the load. The capacitor exhibits reactance, and blocks low-frequency signals, forcing them through the load instead. At higher frequencies the reactance drops, and the capacitor effectively functions as a short circuit. The combination of resistance and capacitance gives the time constant of the filter formula_38 (represented by the Greek letter tau). The break frequency, also called the turnover frequency, corner frequency, or cutoff frequency (in hertz), is determined by the time constant:\n\nor equivalently (in radians per second):\n\nThis circuit may be understood by considering the time the capacitor needs to charge or discharge through the resistor:\n- At low frequencies, there is plenty of time for the capacitor to charge up to practically the same voltage as the input voltage.\n- At high frequencies, the capacitor only has time to charge up a small amount before the input switches direction. The output goes up and down only a small fraction of the amount the input goes up and down. At double the frequency, there's only time for it to charge up half the amount.\n\nAnother way to understand this circuit is through the concept of reactance at a particular frequency:\n- Since direct current (DC) cannot flow through the capacitor, DC input must flow out the path marked formula_41 (analogous to removing the capacitor).\n- Since alternating current (AC) flows very well through the capacitor, almost as well as it flows through solid wire, AC input flows out through the capacitor, effectively short circuiting to ground (analogous to replacing the capacitor with just a wire).\n\nThe capacitor is not an \"on/off\" object (like the block or pass fluidic explanation above). The capacitor variably acts between these two extremes. It is the Bode plot and frequency response that show this variability.\n\nA resistor–inductor circuit or RL filter is an electric circuit composed of resistors and inductors driven by a voltage or current source. A first order RL circuit is composed of one resistor and one inductor and is the simplest type of RL circuit.\n\nA first order RL circuit is one of the simplest analogue infinite impulse response electronic filters. It consists of a resistor and an inductor, either in series driven by a voltage source or in parallel driven by a current source.\n\nAn RLC circuit (the letters R, L and C can be in other orders) is an electrical circuit consisting of a resistor, an inductor, and a capacitor, connected in series or in parallel. The RLC part of the name is due to those letters being the usual electrical symbols for resistance, inductance and capacitance respectively. The circuit forms a harmonic oscillator for current and will resonate in a similar way as an LC circuit will. The main difference that the presence of the resistor makes is that any oscillation induced in the circuit will die away over time if it is not kept going by a source. This effect of the resistor is called damping. The presence of the resistance also reduces the peak resonant frequency somewhat. Some resistance is unavoidable in real circuits, even if a resistor is not specifically included as a component. An ideal, pure LC circuit is an abstraction for the purpose of theory.\n\nThere are many applications for this circuit. They are used in many different types of oscillator circuits. Another important application is for tuning, such as in radio receivers or television sets, where they are used to select a narrow range of frequencies from the ambient radio waves. In this role the circuit is often referred to as a tuned circuit. An RLC circuit can be used as a band-pass filter, band-stop filter, low-pass filter or high-pass filter. The RLC filter is described as a \"second-order\" circuit, meaning that any voltage or current in the circuit can be described by a second-order differential equation in circuit analysis.\n\nHigher order passive filters can also be constructed (see diagram for a third order example).\n\nAnother type of electrical circuit is an \"active\" low-pass filter.\n\nIn the operational amplifier circuit shown in the figure, the cutoff frequency (in hertz) is defined as:\n\nor equivalently (in radians per second):\n\nThe gain in the passband is −\"R\"/\"R\", and the stopband drops off at −6 dB per octave (that is −20 dB per decade) as it is a first-order filter.\n\n", "related": "\n- Baseband\n\n- Low Pass Filter java simulator\n- ECE 209: Review of Circuits as LTI Systems, a short primer on the mathematical analysis of (electrical) LTI systems.\n- ECE 209: Sources of Phase Shift, an intuitive explanation of the source of phase shift in a low-pass filter. Also verifies simple passive LPF transfer function by means of trigonometric identity.\n"}
{"id": "41176434", "url": "https://en.wikipedia.org/wiki?curid=41176434", "title": "Radio spectrum scope", "text": "Radio spectrum scope\n\nThe radio spectrum scope (also radio panoramic receiver, panoramic adapter, pan receiver, pan adapter, panadapter, panoramic radio spectroscope, panoramoscope, panalyzor and band scope) was invented by Marcel Wallace and measures the magnitude of an input signal versus frequency within one or more radio bands - e.g. shortwave bands. A spectrum scope is normally a lot cheaper than a spectrum analyzer, because the aim is not high quality frequency resolution - nor high quality signal strength measurements.\n\nThe spectrum scope use can be to:\n- find radio channels quickly of known and unknown signals when receiving.\n- find radio amateurs activity quickly e.g. with the intent of communicating with them.\n\n- K9rod.net: Spectrum Scope\n- ac8gy.com: Panadapter for FT-950\n- k2za.blogspot.dk: FT-817 Tip - Spectrum Scope\n- portabletubes.co.uk: Panoramic Radio Products PCA-2 Panadapter\n", "related": "NONE"}
{"id": "40714668", "url": "https://en.wikipedia.org/wiki?curid=40714668", "title": "Microwave analog signal processing", "text": "Microwave analog signal processing\n\nReal-time Analog Signal Processing (R-ASP), as an alternative to DSP-based processing, might be defined as the manipulation of signals in their pristine analog form and in real time to realize specific operations enabling microwave or millimeter-wave and terahertz applications.\n\nThe exploding demand for higher spectral efficiency in radio has spurred a renewed interest in analog real-time components and systems beyond conventional purely digital signal processing techniques. Although they are unrivaled at low microwave frequencies, due to their high flexibility, compact size, low cost and strong reliability, digital devices suffer of major issues, such as poor performance, high cost of A/D and D/A converters and excessive power consumption, at higher microwave and millimeter-wave frequencies. At such frequencies, analog devices and related real-time or analog signal processing (ASP) systems, which manipulate broadband signals in the time domain, may be far preferable, as they offer the benefits of lower complexity and higher speed, which may offer unprecedented solutions in the major areas of radio engineering, including communications, but also radars, sensors, instrumentation and imaging. This new technology might be seen as microwave and millimeter-wave counterpart of ultra-fast optics signal processing, and has been recently enabled by a wide range of novel phasers, that are components following arbitrary group delay versus frequency responses.\n", "related": "NONE"}
{"id": "50903", "url": "https://en.wikipedia.org/wiki?curid=50903", "title": "Wavelet", "text": "Wavelet\n\nA wavelet is a wave-like oscillation with an amplitude that begins at zero, increases, and then decreases back to zero. It can typically be visualized as a \"brief oscillation\" like one recorded by a seismograph or heart monitor. Generally, wavelets are intentionally crafted to have specific properties that make them useful for signal processing. Using convolution, wavelets can be combined with known portions of a damaged signal to extract information from the unknown portions.\n\nFor example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly a 32nd note. If this wavelet were to be convolved with a signal created from the recording of a song, then the resulting signal would be useful for determining when the Middle C note was being played in the song. Mathematically, the wavelet will correlate with the signal if the unknown signal contains information of similar frequency. This concept of correlation is at the core of many practical applications of wavelet theory.\n\nAs a mathematical tool, wavelets can be used to extract information from many different kinds of data, including – but not limited to – audio signals and images. Sets of wavelets are generally needed to analyze data fully. A set of \"complementary\" wavelets will decompose data without gaps or overlap so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet based compression/decompression algorithms where it is desirable to recover the original information with minimal loss.\n\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions. This is accomplished through coherent states.\n\nThe word \"wavelet\" has been used for decades in digital signal processing and exploration geophysics. The equivalent French word \"ondelette\" meaning \"small wave\" was used by Morlet and Grossmann in the early 1980s.\n\nWavelet theory is applicable to several subjects. All wavelet transforms may be considered forms of time-frequency representation for continuous-time (analog) signals and so are related to harmonic analysis. Discrete wavelet transform (continuous in time) of a discrete-time (sampled) signal by using discrete-time filterbanks of dyadic (octave band) configuration is a wavelet approximation to that signal. The coefficients of such a filter bank are called the wavelet and scaling coefficients in wavelets nomenclature. These filterbanks may contain either finite impulse response (FIR) or infinite impulse response (IIR) filters. The wavelets forming a continuous wavelet transform (CWT) are subject to the uncertainty principle of Fourier analysis respective sampling theory: Given a signal with some event in it, one cannot assign simultaneously an exact time and frequency response scale to that event. The product of the uncertainties of time and frequency response scale has a lower bound. Thus, in the scaleogram of a continuous wavelet transform of this signal, such an event marks an entire region in the time-scale plane, instead of just one point. Also, discrete wavelet bases may be considered in the context of other forms of the uncertainty principle.\n\nWavelet transforms are broadly divided into three classes: continuous, discrete and multiresolution-based.\n\nIn continuous wavelet transforms, a given signal of finite energy is projected on a continuous family of frequency bands (or similar subspaces of the \"L\" function space \"L\"(R) ). For instance the signal may be represented on every frequency band of the form [\"f\", 2\"f\"] for all positive frequencies \"f\" > 0. Then, the original signal can be reconstructed by a suitable integration over all the resulting frequency components.\n\nThe frequency bands or subspaces (sub-bands) are scaled versions of a subspace at scale 1. This subspace in turn is in most situations generated by the shifts of one generating function ψ in \"L\"(R), the \"mother wavelet\". For the example of the scale one frequency band [1, 2] this function is\nwith the (normalized) sinc function. That, Meyer's, and two other examples of mother wavelets are:\n\nThe subspace of scale \"a\" or frequency band [1/\"a\", 2/\"a\"] is generated by the functions (sometimes called \"child wavelets\")\nwhere \"a\" is positive and defines the scale and \"b\" is any real number and defines the shift. The pair (\"a\", \"b\") defines a point in the right halfplane R × R.\n\nThe projection of a function \"x\" onto the subspace of scale \"a\" then has the form\nwith \"wavelet coefficients\"\n\nFor the analysis of the signal \"x\", one can assemble the wavelet coefficients into a scaleogram of the signal.\n\nSee a list of some Continuous wavelets.\n\nIt is computationally impossible to analyze a signal using all wavelet coefficients, so one may wonder if it is sufficient to pick a discrete subset of the upper halfplane to be able to reconstruct a signal from the corresponding wavelet coefficients. One such system is the affine system for some real parameters \"a\" > 1, \"b\" > 0. The corresponding discrete subset of the halfplane consists of all the points (\"a\", \"nab\") with \"m\", \"n\" in Z. The corresponding \"child wavelets\" are now given as\n\nA sufficient condition for the reconstruction of any signal \"x\" of finite energy by the formula\nis that the functions formula_7 form an orthonormal basis of \"L\"(R).\n\nIn any discretised wavelet transform, there are only a finite number of wavelet coefficients for each bounded rectangular region in the upper halfplane. Still, each coefficient requires the evaluation of an integral. In special situations this numerical complexity can be avoided if the scaled and shifted wavelets form a multiresolution analysis. This means that there has to exist an auxiliary function, the \"father wavelet\" φ in \"L\"(R), and that \"a\" is an integer. A typical choice is \"a\" = 2 and \"b\" = 1. The most famous pair of father and mother wavelets is the Daubechies 4-tap wavelet. Note that not every orthonormal discrete wavelet basis can be associated to a multiresolution analysis; for example, the Journe wavelet admits no multiresolution analysis.\n\nFrom the mother and father wavelets one constructs the subspaces\nThe father wavelet formula_10 keeps the time domain properties, while the mother wavelets formula_11 keeps the frequency domain properties.\n\nFrom these it is required that the sequence\nforms a multiresolution analysis of \"L\" and that the subspaces formula_13 are the orthogonal \"differences\" of the above sequence, that is, \"W\" is the orthogonal complement of \"V\" inside the subspace \"V\",\n\nIn analogy to the sampling theorem one may conclude that the space \"V\" with sampling distance 2 more or less covers the frequency baseband from 0 to 2. As orthogonal complement, \"W\" roughly covers the band [2, 2].\n\nFrom those inclusions and orthogonality relations, especially formula_15, follows the existence of sequences formula_16 and formula_17 that satisfy the identities\nThe second identity of the first pair is a refinement equation for the father wavelet φ. Both pairs of identities form the basis for the algorithm of the fast wavelet transform.\n\nFrom the multiresolution analysis derives the orthogonal decomposition of the space \"L\" as\nFor any signal or function formula_23 this gives a representation in basis functions of the corresponding subspaces as\nwhere the coefficients are\n\nFor practical applications, and for efficiency reasons, one prefers continuously differentiable functions with compact support as mother (prototype) wavelet (functions). However, to satisfy analytical requirements (in the continuous WT) and in general for theoretical reasons, one chooses the wavelet functions from a subspace of the space formula_27 This is the space of measurable functions that are (in absolute value) square integrable:\n\nBeing in this space ensures that one can formulate the conditions of zero mean and square norm one:\n\nFor ψ to be a wavelet for the continuous wavelet transform (see there for exact statement), the mother wavelet must satisfy an admissibility criterion (loosely speaking, a kind of half-differentiability) in order to get a stably invertible transform.\n\nFor the discrete wavelet transform, one needs at least the condition that the wavelet series is a representation of the identity in the space \"L\"(R). Most constructions of discrete WT make use of the multiresolution analysis, which defines the wavelet by a scaling function. This scaling function itself is a solution to a functional equation.\n\nIn most situations it is useful to restrict ψ to be a continuous function with a higher number \"M\" of vanishing moments, i.e. for all integer \"m\" < \"M\"\n\nThe mother wavelet is scaled (or dilated) by a factor of \"a\" and translated (or shifted) by a factor of \"b\" to give (under Morlet's original formulation):\n\nFor the continuous WT, the pair (\"a\",\"b\") varies over the full half-plane R × R; for the discrete WT this pair varies over a discrete subset of it, which is also called \"affine group\".\n\nThese functions are often incorrectly referred to as the basis functions of the (continuous) transform. In fact, as in the continuous Fourier transform, there is no basis in the continuous wavelet transform. Time-frequency interpretation uses a subtly different formulation (after Delprat).\n\nRestriction：\n\n(1) formula_34 when a1 = a and b1 = b,\n\n(2) formula_35 has a finite time interval\n\nThe wavelet transform is often compared with the Fourier transform, in which signals are represented as a sum of sinusoids. In fact, the Fourier transform can be viewed as a special case of the continuous wavelet transform with the choice of the mother wavelet\nformula_36.\nThe main difference in general is that wavelets are localized in both time and frequency whereas the standard Fourier transform is only localized in frequency. The Short-time Fourier transform (STFT) is similar to the wavelet transform, in that it is also time and frequency localized, but there are issues with the frequency/time resolution trade-off.\n\nIn particular, assuming a rectangular window region, one may think of the STFT as a transform with a slightly different kernel\n\nwhere formula_38 can often be written as formula_39, where formula_40 and u respectively denote the length and temporal offset of the windowing function. Using Parseval's theorem, one may define the wavelet's energy as\nFrom this, the square of the temporal support of the window offset by time u is given by\n\nand the square of the spectral support of the window acting on a frequency formula_43\n\nMultiplication with a rectangular window in the time domain corresponds to convolution with a formula_45 function in the frequency domain, resulting in spurious ringing artifacts for short/localized temporal windows. With the continuous-time Fourier Transform, formula_46 and this convolution is with a delta function in Fourier space, resulting in the true Fourier transform of the signal formula_47. The window function may be some other apodizing filter, such as a Gaussian. The choice of windowing function will affect the approximation error relative to the true Fourier transform.\n\nA given resolution cell's time-bandwidth product may not be exceeded with the STFT. All STFT basis elements maintain a uniform spectral and temporal support for all temporal shifts or offsets, thereby attaining an equal resolution in time for lower and higher frequencies. The resolution is purely determined by the sampling width.\n\nIn contrast, the wavelet transform's multiresolutional properties enables large temporal supports for lower frequencies while maintaining short temporal widths for higher frequencies by the scaling properties of the wavelet transform. This property extends conventional time-frequency analysis into time-scale analysis. \n\nThe discrete wavelet transform is less computationally complex, taking O(\"N\") time as compared to O(\"N\" log \"N\") for the fast Fourier transform. This computational advantage is not inherent to the transform, but reflects the choice of a logarithmic division of frequency, in contrast to the equally spaced frequency divisions of the FFT (Fast Fourier Transform) which uses the same basis functions as DFT (Discrete Fourier Transform). It is also important to note that this complexity only applies when the filter size has no relation to the signal size. A wavelet without compact support such as the Shannon wavelet would require O(\"N\"). (For instance, a logarithmic Fourier Transform also exists with O(\"N\") complexity, but the original signal must be sampled logarithmically in time, which is only useful for certain types of signals.)\n\nThere are a number of ways of defining a wavelet (or a wavelet family).\n\nAn orthogonal wavelet is entirely defined by the scaling filter – a low-pass finite impulse response (FIR) filter of length 2\"N\" and sum 1. In biorthogonal wavelets, separate decomposition and reconstruction filters are defined.\n\nFor analysis with orthogonal wavelets the high pass filter is calculated as the quadrature mirror filter of the low pass, and reconstruction filters are the time reverse of the decomposition filters.\n\nDaubechies and Symlet wavelets can be defined by the scaling filter.\n\nWavelets are defined by the wavelet function ψ(\"t\") (i.e. the mother wavelet) and scaling function φ(\"t\") (also called father wavelet) in the time domain.\n\nThe wavelet function is in effect a band-pass filter and scaling that for each level halves its bandwidth. This creates the problem that in order to cover the entire spectrum, an infinite number of levels would be required. The scaling function filters the lowest level of the transform and ensures all the spectrum is covered. See for a detailed explanation.\n\nFor a wavelet with compact support, φ(\"t\") can be considered finite in length and is equivalent to the scaling filter \"g\".\n\nMeyer wavelets can be defined by scaling functions\n\nThe wavelet only has a time domain representation as the wavelet function ψ(\"t\").\n\nFor instance, Mexican hat wavelets can be defined by a wavelet function. See a list of a few Continuous wavelets.\n\nThe development of wavelets can be linked to several separate trains of thought, starting with Haar's work in the early 20th century. Later work by Dennis Gabor yielded Gabor atoms (1946), which are constructed similarly to wavelets, and applied to similar purposes.\n\nWavelet compression, a form of transform coding that uses wavelet transforms in data compression, began after the development of the discrete cosine transform (DCT), a block-based data compression algorithm first proposed by Nasir Ahmed in the early 1970s. The introduction of the DCT led to the development of wavelet coding, a variant of DCT coding that uses wavelets instead of DCT's block-based algorithm.\n\nNotable contributions to wavelet theory since then can be attributed to Zweig’s discovery of the continuous wavelet transform (CWT) in 1975 (originally called the cochlear transform and discovered while studying the reaction of the ear to sound), Pierre Goupillaud, Grossmann and Morlet's formulation of what is now known as the CWT (1982), Jan-Olov Strömberg's early work on discrete wavelets (1983), the LeGall-Tabatabai (LGT) 5/3 wavelet developed by Didier Le Gall and Ali J. Tabatabai (1988), Ingrid Daubechies' orthogonal wavelets with compact support (1988), Mallat's multiresolution framework (1989), Ali Akansu's Binomial QMF (1990), Nathalie Delprat's time-frequency interpretation of the CWT (1991), Newland's harmonic wavelet transform (1993), and set partitioning in hierarchical trees (SPIHT) developed by Amir Said with William A. Pearlman in 1996.\n\nThe JPEG 2000 standard was developed from 1997 to 2000 by a Joint Photographic Experts Group (JPEG) committee chaired by Touradj Ebrahimi (later the JPEG president). In contrast to the DCT algorithm used by the original JPEG format, JPEG 2000 instead uses discrete wavelet transform (DWT) algorithms. It uses the CDF 9/7 wavelet transform (developed by Ingrid Daubechies in 1992) for its lossy compression algorithm, and the LeGall-Tabatabai (LGT) 5/3 wavelet transform (developed by Didier Le Gall and Ali J. Tabatabai in 1988) for its lossless compression algorithm. JPEG 2000 technology, which includes the Motion JPEG 2000 extension, was selected as the video coding standard for digital cinema in 2004.\n\n- First wavelet (Haar Wavelet) by Alfréd Haar (1909)\n- Since the 1970s: George Zweig, Jean Morlet, Alex Grossmann\n- Since the 1980s: Yves Meyer, Didier Le Gall, Ali J. Tabatabai, Stéphane Mallat, Ingrid Daubechies, Ronald Coifman, Ali Akansu, Victor Wickerhauser\n- Since the 1990s: Nathalie Delprat, Newland, Amir Said, William A. Pearlman, Touradj Ebrahimi, JPEG 2000\n\nA wavelet is a mathematical function used to divide a given function or continuous-time signal into different scale components. Usually one can assign a frequency range to each scale component. Each scale component can then be studied with a resolution that matches its scale. A wavelet transform is the representation of a function by wavelets. The wavelets are scaled and translated copies (known as \"daughter wavelets\") of a finite-length or fast-decaying oscillating waveform (known as the \"mother wavelet\"). Wavelet transforms have advantages over traditional Fourier transforms for representing functions that have discontinuities and sharp peaks, and for accurately deconstructing and reconstructing finite, non-periodic and/or non-stationary signals.\n\nWavelet transforms are classified into discrete wavelet transforms (DWTs) and continuous wavelet transforms (CWTs). Note that both DWT and CWT are continuous-time (analog) transforms. They can be used to represent continuous-time (analog) signals. CWTs operate over every possible scale and translation whereas DWTs use a specific subset of scale and translation values or representation grid.\n\nThere are a large number of wavelet transforms each suitable for different applications. For a full list see list of wavelet-related transforms but the common ones are listed below:\n\n- Continuous wavelet transform (CWT)\n- Discrete wavelet transform (DWT)\n- Fast wavelet transform (FWT)\n- Lifting scheme & Generalized Lifting Scheme\n- Wavelet packet decomposition (WPD)\n- Stationary wavelet transform (SWT)\n- Fractional Fourier transform (FRFT)\n- Fractional wavelet transform (FRWT)\n\nThere are a number of generalized transforms of which the wavelet transform is a special case. For example, Yosef Joseph introduced scale into the Heisenberg group, giving rise to a continuous transform space that is a function of time, scale, and frequency. The CWT is a two-dimensional slice through the resulting 3d time-scale-frequency volume.\n\nAnother example of a generalized transform is the chirplet transform in which the CWT is also a two dimensional slice through the chirplet transform.\n\nAn important application area for generalized transforms involves systems in which high frequency resolution is crucial. For example, darkfield electron optical transforms intermediate between direct and reciprocal space have been widely used in the harmonic analysis of atom clustering, i.e. in the study of crystals and crystal defects. Now that transmission electron microscopes are capable of providing digital images with picometer-scale information on atomic periodicity in nanostructure of all sorts, the range of pattern recognition and strain/metrology applications for intermediate transforms with high frequency resolution (like brushlets and ridgelets) is growing rapidly.\n\nFractional wavelet transform (FRWT) is a generalization of the classical wavelet transform in the fractional Fourier transform domains. This transform is capable of providing the time- and fractional-domain information simultaneously and representing signals in the time-fractional-frequency plane.\n\nGenerally, an approximation to DWT is used for data compression if a signal is already sampled, and the CWT for signal analysis. Thus, DWT approximation is commonly used in engineering and computer science, and the CWT in scientific research.\n\nLike some other transforms, wavelet transforms can be used to transform data, then encode the transformed data, resulting in effective compression. For example, JPEG 2000 is an image compression standard that uses biorthogonal wavelets. This means that although the frame is overcomplete, it is a \"tight frame\" (see types of frames of a vector space), and the same frame functions (except for conjugation in the case of complex wavelets) are used for both analysis and synthesis, i.e., in both the forward and inverse transform. For details see wavelet compression.\n\nA related use is for smoothing/denoising data based on wavelet coefficient thresholding, also called wavelet shrinkage. By adaptively thresholding the wavelet coefficients that correspond to undesired frequency components smoothing and/or denoising operations can be performed.\n\nWavelet transforms are also starting to be used for communication applications. Wavelet OFDM is the basic modulation scheme used in HD-PLC (a power line communications technology developed by Panasonic), and in one of the optional modes included in the IEEE 1901 standard. Wavelet OFDM can achieve deeper notches than traditional FFT OFDM, and wavelet OFDM does not require a guard interval (which usually represents significant overhead in FFT OFDM systems).\n\nOften, signals can be represented well as a sum of sinusoids. However, consider a non-continuous signal with an abrupt discontinuity; this signal can still be represented as a sum of sinusoids, but requires an infinite number, which is an observation known as Gibbs phenomenon. This, then, requires an infinite number of Fourier coefficients, which is not practical for many applications, such as compression. Wavelets are more useful for describing these signals with discontinuities because of their time-localized behavior (both Fourier and wavelet transforms are frequency-localized, but wavelets have an additional time-localization property). Because of this, many types of signals in practice may be non-sparse in the Fourier domain, but very sparse in the wavelet domain. This is particularly useful in signal reconstruction, especially in the recently popular field of compressed sensing. (Note that the short-time Fourier transform (STFT) is also localized in time and frequency, but there are often problems with the frequency-time resolution trade-off. Wavelets are better signal representations because of multiresolution analysis.)\n\nThis motivates why wavelet transforms are now being adopted for a vast number of applications, often replacing the conventional Fourier transform. Many areas of physics have seen this paradigm shift, including molecular dynamics, chaos theory, ab initio calculations, astrophysics, gravitational wave transient data analysis, density-matrix localisation, seismology, optics, turbulence and quantum mechanics. This change has also occurred in image processing, EEG, EMG, ECG analyses, brain rhythms, DNA analysis, protein analysis, climatology, human sexual response analysis, general signal processing, speech recognition, acoustics, vibration signals, computer graphics, multifractal analysis, and sparse coding. In computer vision and image processing, the notion of scale space representation and Gaussian derivative operators is regarded as a canonical multi-scale representation.\n\nSuppose we measure a noisy signal formula_48. Assume s has a sparse representation in a certain wavelet bases, and formula_49\n\nSo formula_50.\n\nMost elements in p are 0 or close to 0, and formula_51\n\nSince W is orthogonal, the estimation problem amounts to recovery of a signal in iid Gaussian noise. As p is sparse, one method is to apply a Gaussian mixture model for p.\n\nAssume a prior formula_52, formula_53 is the variance of \"significant\" coefficients, and formula_54 is the variance of \"insignificant\" coefficients.\n\nThen formula_55, formula_56 is called the shrinkage factor, which depends on the prior variances formula_53 and formula_54. The effect of the shrinkage factor is that small coefficients are set early to 0, and large coefficients are unaltered.\n\nSmall coefficients are mostly noises, and large coefficients contain actual signal.\n\nAt last, apply the inverse wavelet transform to obtain formula_59\n\n- Beylkin (18)\n- BNC wavelets\n- Coiflet (6, 12, 18, 24, 30)\n- Cohen-Daubechies-Feauveau wavelet (Sometimes referred to as CDF N/P or Daubechies biorthogonal wavelets)\n- Daubechies wavelet (2, 4, 6, 8, 10, 12, 14, 16, 18, 20, etc.)\n- Binomial-QMF (Also referred to as Daubechies wavelet)\n- Haar wavelet\n- Mathieu wavelet\n- Legendre wavelet\n- Villasenor wavelet\n- Symlet\n\n- Beta wavelet\n- Hermitian wavelet\n- Hermitian hat wavelet\n- Meyer wavelet\n- Mexican hat wavelet\n- Poisson wavelet\n- Shannon wavelet\n- Spline wavelet\n- Stromberg wavelet\n\n- Complex Mexican hat wavelet\n- fbsp wavelet\n- Morlet wavelet\n- Shannon wavelet\n- Modified Morlet wavelet\n\n", "related": "\n- Chirplet transform\n- Curvelet\n- Digital cinema\n- Filter banks\n- Fractal compression\n- Fractional Fourier transform\n- JPEG 2000\n- Multiresolution analysis\n- Noiselet\n- Non-separable wavelet\n- Scale space\n- Scaled correlation\n- Shearlet\n- Short-time Fourier transform\n- Ultra wideband radio- transmits wavelets\n- Wave packet\n- Gabor wavelet#Wavelet space\n- Dimension reduction\n- Fourier-related transforms\n- Spectrogram\n- Huygens–Fresnel principle (physical wavelets)\n\n- Haar A., \"Zur Theorie der orthogonalen Funktionensysteme\", Mathematische Annalen, 69, pp 331–371, 1910.\n- Ingrid Daubechies, \"Ten Lectures on Wavelets\", Society for Industrial and Applied Mathematics, 1992,\n- Ali Akansu and Richard Haddad, \"Multiresolution Signal Decomposition: Transforms, Subbands, Wavelets\", Academic Press, 1992,\n- P. P. Vaidyanathan, \"Multirate Systems and Filter Banks\", Prentice Hall, 1993,\n- Gerald Kaiser, \"A Friendly Guide to Wavelets\", Birkhauser, 1994,\n- Mladen Victor Wickerhauser, \"Adapted Wavelet Analysis From Theory to Software\", A K Peters Ltd, 1994,\n- Martin Vetterli and Jelena Kovačević, \"Wavelets and Subband Coding\", Prentice Hall, 1995,\n- Barbara Burke Hubbard, \"The World According to Wavelets: The Story of a Mathematical Technique in the Making\", AK Peters Ltd, 1998, ,\n- Stéphane Mallat, \"A wavelet tour of signal processing\" 2nd Edition, Academic Press, 1999,\n- Donald B. Percival and Andrew T. Walden, \"Wavelet Methods for Time Series Analysis\", Cambridge University Press, 2000,\n- Ramazan Gençay, Faruk Selçuk and Brandon Whitcher, \"An Introduction to Wavelets and Other Filtering Methods in Finance and Economics\", Academic Press, 2001,\n- Paul S. Addison, \"The Illustrated Wavelet Transform Handbook\", Institute of Physics, 2002,\n- B. Boashash, editor, \"Time-Frequency Signal Analysis and Processing – A Comprehensive Reference\", Elsevier Science, Oxford, 2003, .\n- Tony F. Chan and \"Jackie (Jianhong) Shen\", \"Image Processing and Analysis – Variational, PDE, Wavelet, and Stochastic Methods\", Society of Applied Mathematics, (2005)\n\n- Wavelet Digest\n- Wavelets: Software – a list of useful wavelet transform frameworks, libraries, and other software\n- 1st NJIT Symposium on Wavelets (April 30, 1990) (First Wavelets Conference in USA)\n- Binomial-QMF Daubechies Wavelets\n- Wavelets by Gilbert Strang, American Scientist 82 (1994) 250–255. (A very short and excellent introduction)\n- Course on Wavelets given at UC Santa Barbara, 2004\n- Wavelets for Kids (PDF file) (Introductory (for very smart kids!))\n- WITS: Where Is The Starlet? A dictionary of tens of wavelets and wavelet-related terms ending in -let, from activelets to x-lets through bandlets, contourlets, curvelets, noiselets, wedgelets.\n- The Fractional Spline Wavelet Transform describes a fractional wavelet transform based on fractional b-Splines.\n- A Panorama on Multiscale Geometric Representations, Intertwining Spatial, Directional and Frequency Selectivity provides a tutorial on two-dimensional oriented wavelets and related geometric multiscale transforms.\n- Signal Denoising using Wavelets\n"}
{"id": "1735759", "url": "https://en.wikipedia.org/wiki?curid=1735759", "title": "Recursive filter", "text": "Recursive filter\n\nIn signal processing, a recursive filter is a type of filter which re-uses one or more of its outputs as an input. This feedback typically results in an unending impulse response (commonly referred to as \"infinite impulse response\" (IIR)), characterised by either exponentially growing, decaying, or sinusoidal signal output components.\n\nHowever, a recursive filter does not always have an infinite impulse response. Some implementations of moving average filter are recursive filters but with a finite impulse response.\n\nNon-recursive Filter Example:\ny[n] = 0.5x[n − 1] + 0.5x[n].\n\nRecursive Filter Example:\ny[n] = 0.5y[n − 1] + 0.5x[n].\n\n- Kalman filter\n", "related": "NONE"}
{"id": "42074332", "url": "https://en.wikipedia.org/wiki?curid=42074332", "title": "Half-band filter", "text": "Half-band filter\n\nIn digital signal processing, half-band filters are widely used for their efficiency in multi-rate applications. A half-band filter is a low-pass filter that reduces the maximum bandwidth of sampled data by a factor of 2 (one octave). When multiple octaves of reduction are needed, a cascade of half-band filters is common. And when the goal is downsampling, each half-band filter needs to compute only half as many output samples as input samples. \n\nIt follows from the filter's definition that its transition region, or \"skirt\", can be centered at frequency  formula_1  where  formula_2  is the input sample-rate. That makes it possible to design a FIR filter whose every other coefficient is zero, and whose non-zero coefficients are symmetrical about the center of the impulse response. (See )  Both of those properties can be used to improve efficiency of the implementation.\n\n- https://www.dsprelated.com/showarticle/1113.php\n", "related": "NONE"}
{"id": "3440178", "url": "https://en.wikipedia.org/wiki?curid=3440178", "title": "In-phase and quadrature components", "text": "In-phase and quadrature components\n\nIn electrical engineering, a sinusoid with angle modulation can be decomposed into, or synthesized from, two amplitude-modulated sinusoids that are offset in phase by one-quarter cycle (/2 radians). All three functions have the same center frequency. The amplitude modulated sinusoids are known as the in-phase and quadrature components. \nIn some contexts it is more convenient to refer to only the amplitude modulation (\"baseband\") itself by those terms.\n\nIn vector analysis, a vector with polar coordinates and Cartesian coordinates \nSimilarly in trigonometry, the angle sum identity expresses:\nAnd in functional analysis, when is a linear function of some variable, such as time, these components are sinusoids, and they are orthogonal functions.   A phase-shift of changes the identity to:\nin which case is the in-phase component.  In both conventions is the in-phase amplitude modulation, which explains why some authors refer to it as the actual in-phase component. \n\nThe term \"alternating current\" applies to a voltage vs. time function that is sinusoidal with a frequency   When it is applied to a typical circuit or device, it causes a current that is also sinusoidal. In general there is a constant phase difference, φ, between any two sinusoids. The input sinusoidal voltage is usually defined to have zero phase, meaning that it is arbitrarily chosen as a convenient time reference. So the phase difference is attributed to the current function, e.g. \n  seconds. Note that the term \"in quadrature\" only implies that two sinusoids are orthogonal, not that they are \"components\" of another sinusoid.\n\nIn an angle modulation application, with carrier frequency φ is also a time-variant function, giving:\n\nWhen all three terms above are multiplied by an optional amplitude function, the left-hand side of the equality is known as the \"amplitude/phase\" form, and the right-hand side is the \"quadrature-carrier\" or \"IQ\" form. Because of the modulation, the components are no longer completely orthogonal functions. But when and are slowly varying functions compared to the assumption of orthogonality is a common one.\nAuthors often call it a \"narrowband assumption\", or a narrowband signal model.\n\nThe terms \"I-component\" and \"Q-component\" are common ways of referring to the in-phase and quadrature signals. Both signals comprise a high-frequency sinusoid (or \"carrier\") that is amplitude-modulated by a relatively low-frequency function, usually conveying some sort of information. The two carriers are orthogonal, with I lagging Q by ¼ cycle, or equivalently leading Q by ¾ cycle.  The physical distinction can also be characterized in terms of formula_2:\n- formula_3: The composite signal reduces to just the I-component, which accounts for the term \"in-phase\".\n- formula_4: The composite signal reduces to just the Q-component.\n- formula_5: The amplitude modulations are orthogonal sinusoids, I leading Q by ¼ cycle.\n- formula_6: The amplitude modulations are orthogonal sinusoids, Q leading I by ¼ cycle.\n\n", "related": "\n- IQ imbalance\n- Constellation diagram\n- Phasor\n- Polar modulation\n- Quadrature amplitude modulation\n- Single-sideband modulation\n\n- Steinmetz, Charles Proteus (1917). \"Theory and Calculations of Electrical Apparatus\" 6 (1 ed.). New York: McGraw-Hill Book Company. B004G3ZGTM.\n\n- I/Q Data for Dummies\n"}
{"id": "41918927", "url": "https://en.wikipedia.org/wiki?curid=41918927", "title": "Shearlet", "text": "Shearlet\n\nIn applied mathematical analysis, shearlets are a multiscale framework which allows efficient encoding of anisotropic features in multivariate problem classes. Originally, shearlets were introduced in 2006 for the analysis and sparse approximation of functions formula_1. They are a natural extension of wavelets, to accommodate the fact that multivariate functions are typically governed by anisotropic features such as edges in images, since wavelets, as isotropic objects, are not capable of capturing such phenomena.\n\nShearlets are constructed by parabolic scaling, shearing, and translation applied to a few generating functions. At fine scales, they are essentially supported within skinny and directional ridges following the parabolic scaling law, which reads \"length² ≈ width\". Similar to wavelets, shearlets arise from the affine group and allow a unified treatment of the continuum and digital situation leading to faithful implementations. Although they do not constitute an orthonormal basis for formula_2, they still form a frame allowing stable expansions of arbitrary functions formula_1.\n\nOne of the most important properties of shearlets is their ability to provide optimally sparse approximations (in the sense of optimality in ) for cartoon-like functions formula_4. In imaging sciences, \"cartoon-like functions\" serve as a model for anisotropic features and are compactly supported in formula_5 while being formula_6 apart from a closed piecewise formula_6 singularity curve with bounded curvature. The decay rate of the formula_8-error of the formula_9-term shearlet approximation obtained by taking the formula_9 largest coefficients from the shearlet expansion is in fact optimal up to a log-factor:\nwhere the constant formula_12 depends only on the maximum curvature of the singularity curve and the maximum magnitudes of formula_4, formula_14 and formula_15. This approximation rate significantly improves the best formula_9-term approximation rate of wavelets providing only formula_17 for such class of functions.\n\nShearlets are to date the only directional representation system that provides sparse approximation of anisotropic features while providing a unified treatment of the continuum and digital realm that allows faithful implementation. Extensions of shearlet systems to formula_18 are also available. A comprehensive presentation of the theory and applications of shearlets can be found in .\n\nThe construction of continuous shearlet systems is based on \"parabolic scaling matrices\"\n\nas a mean to change the resolution, on \"shear matrices\"\n\nas a means to change the orientation, and finally on translations to change the positioning. \nIn comparison to curvelets, shearlets use shearings instead of rotations, the advantage being that the shear operator formula_21 leaves the integer lattice invariant in case formula_22, i.e., formula_23 This indeed allows a unified treatment of the continuum and digital realm, thereby guaranteeing a faithful digital implementation.\n\nFor formula_24 the \"continuous shearlet system\" generated by formula_25 is then defined as\n\nand the corresponding \"continuous shearlet transform\" is given by the map\n\nA discrete version of shearlet systems can be directly obtained from formula_28 by discretizing the parameter set formula_29 There are numerous approaches for this but the most popular one is given by\n\nFrom this, the \"discrete shearlet system\" associated with the shearlet generator formula_25 is defined by\n\nand the associated \"discrete shearlet transform\" is defined by\n\nLet formula_34 be a function satisfying the \"discrete Calderón condition\", i.e.,\n\nwith formula_36 and formula_37 \nwhere formula_38 denotes the Fourier transform of formula_39 For instance, one can choose formula_40 to be a Meyer wavelet.\nFurthermore, let formula_41 be such that formula_42 formula_43 and\n\nOne typically chooses formula_45 to be a smooth bump function. Then formula_24 given by\n\nis called a \"classical shearlet\". It can be shown that the corresponding discrete shearlet system formula_48 constitutes a Parseval frame for formula_2 consisting of bandlimited functions.\n\nAnother example are compactly supported shearlet systems, where a compactly supported function formula_24 can be chosen so that formula_48 forms a frame for formula_2. In this case, all shearlet elements in formula_48 are compactly supported providing superior spatial localization compared to the classical shearlets, which are bandlimited. Although a compactly supported shearlet system does not generally form a Parseval frame, any function formula_1 can be represented by the shearlet expansion due to its frame property.\n\nOne drawback of shearlets defined as above is the directional bias of shearlet elements associated with large shearing parameters.\nThis effect is already recognizable in the frequency tiling of classical shearlets (see Figure in Section #Examples), where the frequency support of a shearlet increasingly aligns along the formula_55-axis as the shearing parameter formula_56 goes to infinity. \nThis causes serious problems when analyzing a function whose Fourier transform is concentrated around the formula_55-axis.\nTo deal with this problem, the frequency domain is divided into a low-frequency part and two conic regions (see Figure):\n\nThe associated \"cone-adapted discrete shearlet system\" consists of three parts, each one corresponding to one of these frequency domains.\nIt is generated by three functions formula_59 and a \"lattice sampling\" factor formula_60\n\nwhere\n\nwith\n\nThe systems formula_64 and formula_65 basically differ in the reversed roles of formula_66 and formula_67. \nThus, they correspond to the conic regions formula_68 and formula_69, respectively. \nFinally, the \"scaling function\" formula_70 is associated with the low-frequency part formula_71.\n\n- Image processing and computer sciences\n- Denoising\n- Inverse problems\n- Image enhancement\n- Edge detection\n- Inpainting\n- Image separation\n- PDEs\n- Resolution of the wavefront set\n- Transport equations\n- Coorbit theory, characterization of smoothness spaces\n- Differential geometry: manifold learning\n\n- 3D-Shearlets\n- formula_72-Shearlets\n- Parabolic molecules\n\n", "related": "\n- Wavelet transform\n- Curvelet transform\n- Contourlet transform\n- Bandelet transform\n- Chirplet transform\n- Noiselet transform\n\n- Homepage of Gitta Kutyniok\n- Homepage of Demetrio Labate\n"}
{"id": "43275476", "url": "https://en.wikipedia.org/wiki?curid=43275476", "title": "Passthrough", "text": "Passthrough\n\nIn signal processing, a passthrough is a logic gate that enables a signal to \"pass through\" unaltered, sometimes with little alteration. Sometimes the concept of a \"passthrough\" can also involve daisy chain logic.\n\n- Analog passthrough (for digital TV)\n- Sega 32X (passthrough for Sega Genesis video games)\n- VCRs, DVD recorders, etc. act as a \"pass-through\" for composite video and S-video, though sometimes they act as an RF modulator for use on Channel 3.\n- Tape monitor features allow an AV receiver (sometime the recording device itself) to act as a \"pass-through\" for audio.\n\n", "related": "\n- Dongle, a device that converts signal, instead of just being a \"passthrough\" for unaltered signal\n"}
{"id": "44070426", "url": "https://en.wikipedia.org/wiki?curid=44070426", "title": "Poisson wavelet", "text": "Poisson wavelet\n\nIn mathematics, in functional analysis, several different wavelets are known by the name Poisson wavelet. In one context, the term \"Poisson wavelet\" is used to denote a family of wavelets labeled by the set of positive integers, the members of which are associated with the Poisson probability distribution. These wavelets were first defined and studied by Karlene A. Kosanovich, Allan R. Moser and Michael J. Piovoso in 1995–96. In another context, the term refers to a certain wavelet which involves a form of the Poisson integral kernel. In a still another context, the terminology is used to describe a family of complex wavelets indexed by positive integers which are connected with the derivatives of the Poisson integral kernel.\n\nFor each positive integer \"n\" the Poisson wavelet formula_1 is defined by\n\nTo see the relation between the Poisson wavelet and the Poisson distribution let \"X\" be a discrete random variable having the Poisson distribution with parameter (mean) \"t\" and, for each non-negative integer \"n\", let Prob(\"X\" = \"n\") = \"p\"(\"t\"). Then we have\n\nThe Poisson wavelet formula_1 is now given by\n\n- formula_1 is the backward difference of the values of the Poisson distribution:\n\n- The \"waviness\" of the members of this wavelet family follows from\n\n- The Fourier transform of formula_1 is given\n\n- The admissibility constant associated with formula_1 is\n\n- Poisson wavelet is not an orthogonal family of wavelets.\n\nThe Poisson wavelet family can be used to construct the family of Poisson wavelet transforms of functions defined the time domain. Since the Poisson wavelets satisfy the admissibility condition also, functions in the time domain can be reconstructed from their Poisson wavelet transforms using the formula for inverse continuous-time wavelet transforms.\n\nIf \"f\"(\"t\") is a function in the time domain its \"n\"-th Poisson wavelet transform is given by\n\nIn the reverse direction, given the \"n\"-th Poisson wavelet transform formula_14 of a function \"f\"(\"t\") in the time domain, the function \"f\"(\"t\") can be reconstructed as follows:\n\nPoisson wavelet transforms have been applied in multi-resolution analysis, system identification, and parameter estimation. They are particularly useful in studying problems in which the functions in the time domain consist of linear combinations of decaying exponentials with time delay.\n\nThe Poisson wavelet is defined by the function\n\nThis can be expressed in the form\n\nThe function formula_19 appears as an integral kernel in the solution of a certain initial value problem of the Laplace operator.\n\nThis is the initial value problem: Given any formula_20 in formula_21, find a harmonic function formula_22 defined in the upper half-plane satisfying the following conditions:\n\nThe problem has the following solution: There is exactly one function formula_22 satisfying the two conditions and it is given by\n\nwhere formula_29 and where \"formula_30\" denotes the convolution operation. The function formula_31 is the integral kernel for the function formula_32. The function formula_22 is the harmonic continuation of formula_20 into the upper half plane.\n\n- The \"waviness\" of the function follows from\n\n- The Fourier transform of formula_36 is given by\n\n- The admissibility constant is\n\nThe Poisson wavelet is a family of complex valued functions indexed by the set of positive integers and defined by\n\nThe function formula_1 can be expressed as an \"n\"-th derivative as follows:\nWriting the function formula_43 in terms of the Poisson integral kernel formula_44 as \nwe have\nThus formula_1 can be interpreted as a function proportional to the derivatives of the Poisson integral kernel.\n\nThe Fourier transform of formula_1 is given by\nwhere formula_50 is the unit step function.\n", "related": "NONE"}
{"id": "44086751", "url": "https://en.wikipedia.org/wiki?curid=44086751", "title": "Orban (audio processing)", "text": "Orban (audio processing)\n\nOrban is an international company making audio processors for Radio, TV and Internet broadcasters. It has been operating for over forty years, with founder Bob Orban selling his first product in 1967.\n\nOrban, as a company, started in 1968, when Bob Orban founded Orban Associates. However, the first Orban product was sold the year before, in 1967, to WOR-FM New York City. Over its years of trading, Orban has released many well known audio processing products, including the Orban Optimod 8000, which was the first audio processor ever made to include FM processing and a stereo generator under one package - an innovative idea at the time, as no other processor took into account the 75 μs pre-emphasis curve employed by FM, which lead to low average modulation and a lot of peaks.\n\nThis was followed by the Orban Optimod 8100, which went on to become Orban's most successful product, and the Orban Optimod 8200, the first successful Digital Signal Processor (DSP) to be made. It was entirely digital, and featured a 2 band AGC, followed by 5-band, or 2-band, processing, with distortion cancelled clipping. As well as digital audio processors being made for FM, processors were made for AM and Digital Radio too, including the Orban Optimod 9200, and Orban Optimod 6200, the first processor made exclusively for Digital TV, Digital Radio and Internet Radio.\n\nDuring the noughties, Orban followed on from the 8200 by creating the Orban Optimod 8400 in 2000, and the Orban Optimod 8500, which was released in 2005.\n\nOrban was acquired by Daysequerra in the summer of 2016.\n\nOrban's current product line includes its flagship audio processor, the OPTIMOD-FM 8700i. Other processors include the Orban OPTIMOD-FM 5500i/5700i, the Orban OPTIMOD 6300 for digital, internet and mastering applications, and the Orban Optimod 9400/9300, primarily for AM radio.\n\nThe function of these processors is normally to reduce the dynamic range of the audio being handled. This is particularly important for AM broadcasting where noise and interference can be obtrusive on quiet passages, it is much less so on FM and digital.\n\nTo the listener the overall sound appears louder, which is useful to commercial broadcasters as it draws attention to them when the casual listener is tuning across the band. It does introduce artifacts to the sound, which is an irritation to those with a good musical ear, especially for classical music.\n\n- Orban's website\n- Orban Facebook page\n- Orban history\n- Orban Timeline\n", "related": "NONE"}
{"id": "44235392", "url": "https://en.wikipedia.org/wiki?curid=44235392", "title": "Multidimensional signal processing", "text": "Multidimensional signal processing\n\nIn signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems. While multidimensional signal processing is a subset of signal processing, it is unique in the sense that it deals specifically with data that can only be adequately detailed using more than one dimension. In m-D digital signal processing, useful data is sampled in more than one dimension. Examples of this are image processing and multi-sensor radar detection. Both of these examples use multiple sensors to sample signals and form images based on the manipulation of these multiple signals.\nProcessing in multi-dimension (m-D) requires more complex algorithms, compared to the 1-D case, to handle calculations such as the Fast Fourier Transform due to more degrees of freedom. In some cases, m-D signals and systems can be simplified into single dimension signal processing methods, if the considered systems are separable.\n\nTypically, multidimensional signal processing is directly associated with digital signal processing because its complexity warrants the use of computer modelling and computation. A multidimensional signal is similar to a single dimensional signal as far as manipulations that can be performed, such as sampling, Fourier analysis, and filtering. The actual computations of these manipulations grow with the number of dimensions.\n\nMultidimensional sampling requires different analysis than typical 1-D sampling. Single dimension sampling is executed by selecting points along a continuous line and storing the values of this data stream. In the case of multidimensional sampling, the data is selected utilizing a lattice, which is a \"pattern\" based on the sampling vectors of the m-D data set. These vectors can be single dimensional or multidimensional depending on the data and the application.\n\nMultidimensional sampling is similar to classical sampling as it must adhere to the Nyquist–Shannon sampling theorem. It is affected by aliasing and considerations must be made for eventual Multidimensional Signal Reconstruction.\n\nA multidimensional signal can be represented in terms of sinusoidal components. This is typically done with a type of Fourier transform. The m-D Fourier transform transforms a signal from a signal domain representation to a frequency domain representation of the signal. In the case of digital processing, a discrete Fourier Transform (DFT) is utilized to transform a sampled signal domain representation into a frequency domain representation:\nwhere \"X\" stands for the multidimensional discrete Fourier transform, \"x\" stands for the sampled time/space domain signal, \"m\" stands for the number of dimensions in the system, \"n\" are sample indices and \"k\" are frequency samples.\nComputational complexity is usually the main concern when implementing any Fourier transform. For multidimensional signals, the complexity can be reduced by a number of different methods. The computation may be simplified if there is independence between variables of the multidimensional signal. In general, Fast Fourier Transforms (FFTs), reduce the number of computations by a substantial factor. While there are a number of different implementations of this algorithm for m-D signals, two often used variations are the vector-radix FFT and the row-column FFT.\n\nFiltering is an important part of any signal processing application. Similar to typical single dimension signal processing applications, there are varying degrees of complexity within filter design for a given system. M-D systems utilize digital filters in many different applications. The actual implementation of these m-D filters can pose a design problem depending on whether the multidimensional polynomial is factorable. Typically, a prototype filter is designed in a single dimension and that filter is extrapolated to m-D using a mapping function. One of the original mapping functions from 1-D to 2-D was the McClellan Transform. Both FIR and IIR filters can be transformed to m-D, depending on the application and the mapping function.\n\n- Audio signal processing\n- Image processing\n- Towed array sonar\n- X-ray computed tomography\n", "related": "NONE"}
{"id": "40773", "url": "https://en.wikipedia.org/wiki?curid=40773", "title": "Baseband", "text": "Baseband\n\nBaseband is a signal that has a near-zero frequency range, i.e. a spectral magnitude that is nonzero only for frequencies in the vicinity of the origin (termed \"f\" = 0) and negligible elsewhere. In telecommunications and signal processing, baseband signals are transmitted without modulation, that is, without any shift in the range of frequencies of the signal. Baseband has a low-frequency—contained within the bandwidth frequency close to 0 hertz up to a higher cut-off frequency. Baseband can be synonymous with lowpass or non-modulated, and is differentiated from passband, bandpass, carrier-modulated, intermediate frequency, or radio frequency (RF).\n\nA \"baseband signal\" or \"lowpass signal\" is a signal that can include frequencies that are very near zero, by comparison with its highest frequency (for example, a sound waveform can be considered as a baseband signal, whereas a radio signal or any other modulated signal is not).\n\nA \"baseband bandwidth\" is equal to the highest frequency of a signal or system, or an upper bound on such frequencies, for example the upper cut-off frequency of a low-pass filter. By contrast, passband bandwidth is the difference between a highest frequency and a nonzero lowest frequency.\n\nA \"baseband channel\" or \"lowpass channel\" (or \"system\", or \"network\") is a communication channel that can transfer frequencies that are very near zero. Examples are serial cables and local area networks (LANs), as opposed to passband channels such as radio frequency channels and passband filtered wires of the analog telephone network. Frequency division multiplexing (FDM) allows an analog telephone wire to carry a baseband telephone call, concurrently as one or several carrier-modulated telephone calls.\n\nDigital baseband transmission, also known as line coding, aims at transferring a digital bit stream over baseband channel, typically an unfiltered wire, contrary to passband transmission, also known as \"carrier-modulated\" transmission. Passband transmission makes communication possible over a bandpass filtered channel, such as the telephone network local-loop or a band-limited wireless channel.\n\nThe word \"BASE\" in Ethernet physical layer standards, for example 10BASE5, 100BASE-TX and 1000BASE-SX, implies baseband digital transmission (i.e. that a line code and an unfiltered wire are used).\n\nA baseband processor also known as BP or BBP is used to process the down-converted digital signal to retrieve essential data for the wireless digital system. The baseband processing block in GNSS receivers is usually responsible for providing observable data: code pseudo-ranges and carrier phase measurements, as well as navigation data. \n\nAn \"equivalent baseband signal\" or \"equivalent lowpass signal\" is—in analog and digital modulation methods for (band-pass) signals with constant or varying carrier frequency (for example ASK, PSK QAM, and FSK)—a complex valued representation of the modulated physical signal (the so-called passband signal or RF signal). The equivalent baseband signal is formula_1 where formula_2 is the inphase signal, formula_3 the quadrature phase signal, and formula_4 the imaginary unit. In a digital modulation method, the formula_2 and formula_3 signals of each modulation symbol are evident from the constellation diagram. The frequency spectrum of this signal includes negative as well as positive frequencies. The physical passband signal corresponds to \nwhere formula_8 is the carrier angular frequency in rad/s.\n\nA signal at baseband is often used to modulate a higher frequency carrier signal in order that it may be transmitted via radio. Modulation results in shifting the signal up to much higher frequencies (radio frequencies, or RF) than it originally spanned. A key consequence of the usual double-sideband amplitude modulation (AM) is that the range of frequencies the signal spans (its spectral bandwidth) is doubled. Thus, the RF bandwidth of a signal (measured from the lowest frequency as opposed to 0 Hz) is twice its baseband bandwidth. Steps may be taken to reduce this effect, such as single-sideband modulation. Some transmission schemes such as frequency modulation use even more bandwidth.\n\nThe figure shows what happens with AM modulation:\n\n", "related": "\n- Complex envelope\n- Broadband\n- In-phase and quadrature components\n- Narrowband\n- Wideband\n"}
{"id": "40753", "url": "https://en.wikipedia.org/wiki?curid=40753", "title": "Automatic link establishment", "text": "Automatic link establishment\n\nAutomatic Link Establishment, commonly known as ALE, is the worldwide de facto standard for digitally initiating and sustaining HF radio communications. ALE is a feature in an HF communications radio transceiver system that enables the radio station to make contact, or initiate a circuit, between itself and another HF radio station or network of stations. The purpose is to provide a reliable rapid method of calling and connecting during constantly changing HF ionospheric propagation, reception interference, and shared spectrum use of busy or congested HF channels.\n\nA standalone ALE radio combines an HF SSB radio transceiver with an internal microprocessor and MFSK modem. It is programmed with a unique ALE address, similar to a phone number (or on newer generations, a username). When not actively in contact with another station, the HF SSB transceiver constantly scans through a list of HF frequencies called \"channels\", listening for any ALE signals transmitted by other radio stations. It decodes calls and soundings sent by other stations and uses the bit error rate to store a quality score for that frequency and sender-address.\n\nTo reach a specific station, the caller enters the ALE Address. On many ALE radios this is similar to dialing a phone number. The ALE controller selects the best available idle channel for that destination address. After confirming the channel is indeed idle, it then sends a brief selective calling signal identifying the intended recipient. When the distant scanning station detects ALE activity, it stops scanning and stays on that channel until it can confirm whether or not the call is for it. The two stations' ALE controllers automatically handshake to confirm that a link of sufficient quality has been established, then notify the operators that the link is up. If the callee fails to respond or the handshaking fails, the originating ALE node usually selects another frequency either at random or by making a guess of varying sophistication.\n\nUpon successful linking, the receiving station generally emits an audible alarm and shows a visual alert to the operator, thus indicating the incoming call. It also indicates the callsign or other identifying information of the linked station, similar to Caller ID. The operator then un-mutes the radio and answers the call then can talk in a regular conversation or negotiates a data link using voice or the ALE built-in short text message format. Alternatively, digital data can be exchanged via a built-in or external modem (such as a STANAG 5066 or MIL-STD-188-110B serial tone modem) depending on needs and availability. The ALE built-in text messaging facility can be used to transfer short text messages as an \"orderwire\" to allow operators to coordinate external equipment such as phone patches or non-embedded digital links, or for short tactical messages. \n\nDue to the vagaries of ionospheric communications, HF radio as used by large governmental organizations in the mid-20th century was traditionally the domain of highly skilled and trained radio operators. One of the new characteristics that embedded microprocessors and computers brought to HF radio via ALE, was alleviation of the need for the radio operator to constantly monitor and change the radio frequency manually to compensate for ionospheric conditions or interference. For the average user of ALE, after learning how to work the basic functions of the HF transceiver, it became similar to operating a cellular mobile phone. For more advanced functions and programming of ALE controllers and networks, it became similar to the use of menu-enabled consumer equipment or the optional features typically encountered in software. In a professional or military organization, this does not eliminate the need for skilled and trained communicators to coordinate the per-unit authorized frequency lists and node addresses it merely allows the deployment of relatively unskilled technicians as \"field communicators\" and end-users of the existing coordinated architecture.\n\nAn ALE radio system enables connection for voice conversation, alerting, data exchange, texting, instant messaging, email, file transfer, image, geo-position tracking, or telemetry. With a radio operator initiating a call, the process normally takes a few minutes for the ALE to pick an HF frequency that is optimum for both sides of the communication link. It signals the operators audibly and visually on both ends, so they can begin communicating with each other immediately. In this respect, the longstanding need in HF radio for repetitive calling on pre-determined time schedules or tedious monitoring static is eliminated. It is useful as a tool for finding optimum channels to communicate between stations in real-time. In modern HF communications, ALE has largely replaced HF prediction charts, propagation beacons, chirp sounders, propagation prediction software, and traditional radio operator educated guesswork. ALE is most commonly used for hooking up operators for voice contacts on SSB (single-sideband modulation), HF internet connectivity for email, SMS phone texting or text messaging, real-time chat via HF text, Geo Position Reporting, and file transfer. High Frequency Internet Protocol or HFIP may be used with ALE for internet access via HF.\n\nThe essence of ALE techniques is the use of automatic channel selection, scanning receivers, selective calling, handshaking, and robust burst modems. An ALE node decodes all received ALE signals heard on the channel(s) it monitors. It uses the fact that all ALE messages use forward error correction (FEC) redundancy. By noting how much error-correction occurred in each received and decoded message, an ALE node can detect the \"quality\" of the path between the sending station and itself. This information is coupled with the ALE address of the sending node and the channel the message was received on, and stored in the node's Link Quality Analysis (LQA) memory. When a call is initiated, the LQA lookup table is searched for matches involving the target ALE address and the best historic channel is used to call the target station. This reduces the likelihood that the call has to be repeated on alternate frequencies. Once the target station has heard the call and responded, a bell or other signalling device will notify both operators that a link has been established. At this point, the operators may coordinate further communication via orderwire text messages, voice, or other means. If further digital communication is desired, it may take place via external data modems or via optional modems built into the ALE terminal.\n\nThis unusual usage of FEC redundancy is the primary innovation that differentiates ALE from previous selective calling systems which either decoded a call or failed to decode due to noise or interference. A binary outcome of \"Good enough\" or not gave no way of automatically choosing between two channels, both of which are currently good enough for minimum communications. The redundancy-based scoring inherent in ALE thus allows for selecting the \"best\" available channel and (in more advanced ALE nodes) using all decoded traffic over some time window to sort channels into a list of decreasing probability-to-contact, significantly reducing co-channel interference to other users as well as dramatically decreasing the time needed to successfully link with the target node.\n\nTechniques used in the ALE standard include automatic signaling, automatic station identification (sounding), polling, message store-and-forward, linking protection and anti-spoofing to prevent hostile denial of service by ending the channel scanning process. Optional ALE functions include polling and the exchange of orderwire commands and messages. The orderwire message, known as AMD (Automatic Message Display), is the most commonly used text transfer method of ALE, and the only universal method that all ALE controllers have in common for displaying text. It is common for vendors to offer extensions to AMD for various non-standard features, although dependency on these extensions undermines interoperability. As in all interoperability scenarios, care should be taken to determine if this is acceptable before using such extensions.\n\nALE evolved from older HF radio selective calling technology. It combined existing channel-scanning selective calling concepts with microprocessors (enabling FEC decoding and quality scoring decisions), burst transmissions (minimizing co-channel interference), and transponding (allowing unattended operation and incoming-call signalling). Early ALE systems were developed in the late 1970s and early 1980s by several radio manufacturers. The first ALE-family controller units were external rack mounted controllers connected to control military radios, and were rarely interoperable across vendors.\n\nVarious methods and proprietary digital signaling protocols were used by different manufacturers in first generation ALE, leading to incompatibility. Later, a cooperative effort among manufacturers and the US government resulted in a second generation of ALE that included the features of first generation systems, while improving performance. The second generation 2G ALE system standard in 1986, MIL-STD-188-141A, was adopted in FED-STD-1045 for US federal entities. In the 1980s, military and other entities of the US government began installing early ALE units, using ALE controller products built primarily by US companies. The primary application during the first 10 years of ALE use was government and military radio systems, and the limited customer base combined with the necessity to adhere to MILSPEC standards kept prices extremely high. Over time, demand for ALE capabilities spread and by the late 1990s, most new government HF radios purchased were designed to meet at least the minimum ALE interoperability standard, making them eligible for use with standard ALE node gear. Radios implementing at least minimum ALE node functionality as an option internal to the radio became more common and significantly more affordable. As the standards were adopted by other governments worldwide, more manufacturers produced competitively priced HF radios to meet this demand. The need to interoperate with government organizations prompted many non-government organizations (NGOs) to at least partially adopt ALE standards for communication. As non-military experience spread and prices came down, other civilian entities started using 2G ALE. By the year 2000, there were enough civilian and government organizations worldwide using ALE that it became a de facto HF interoperability standard for situations where a priori channel and address coordination is possible.\n\nIn the late 1990s, a third generation 3G ALE with significantly improved capability and performance was included in MIL-STD-188-141B, retaining backward compatibility with 2G ALE, and was adopted in NATO STANAG 4538. Civilian and non-government adoption rates are much lower than 2G ALE due to the extreme cost as compared to surplus or entry-level 2G gear as well as the significantly increased system and planning complexity necessary to realize the benefits inherent in the 3G specification. For many militaries, whose needs for maximized intra-organizational capability and capacity always strain existing systems, the additional cost and complexity of 3G is far more compelling.\n\nALE enables rapid unscheduled communication and message passing without requiring complex message centers, multiple radios and antennas, or highly trained operators. With the removal of these potential sources of failure, the tactical communication process becomes much more robust and reliable. The effects extend beyond mere force multiplication of existing communications methods; units such as helicopters, when outfitted with ALE radios, can now reliably communicate in situations where the crew are too busy to operate a traditional non-line of sight radio. This ability to enable tactical communication in conditions where dedicated trained operators and hardware are inappropriate is often considered to be the true improvement offered by ALE.\n\nALE is a critical path toward increased interoperability between organizations. By enabling a station to participate nearly simultaneously in many different HF networks, ALE allows for convenient cross-organization message passing and monitoring without requiring dedicated separate equipment and operators for each partner organization. This dramatically reduces staffing and equipment considerations, while enabling small mobile or portable stations to participate in multiple networks and subnetworks. The result is increased resilience, decreased fragility, increased ability to communicate information effectively, and the ability to rapidly add to or replace communication points as the situation demands.\n\nWhen combined with Near Vertical Incidence Skywave (NVIS) techniques and sufficient channels spread across the spectrum, an ALE node can provide greater than 95% success linking on the first call, nearly on par with SATCOM systems. This is significantly more reliable than cellphone infrastructure during disasters or wars yet is mostly immune to such considerations itself.\n\nGlobal standards for ALE are based on the original US MIL-STD 188-141A and FED-1045, known as 2nd Generation (2G) ALE. 2G ALE uses non-synchronised scanning of channels, and it takes several seconds to half a minute to repeatedly scan through an entire list of channels looking for calls. Thus it requires sufficient duration of transmission time for calls to connect or link with another station that is unsynchronised with its calling signal. The vast majority of ALE systems in use in the world at the present time are 2G ALE.\n\nThe more common 2G ALE signal waveform is designed to be compatible with standard 3 kHz SSB narrowband voice channel transceivers. The modulation method is 8ary Frequency Shift Keying or 8FSK, also sometimes called Multi Frequency Shift Keying MFSK, with eight orthogonal tones between 750 and 2500 Hz. Each tone is 8 ms long, resulting in a transmitted over-the-air symbol rate of 125 baud or 125 symbols per second, with a raw data rate of 375 bits per second. The ALE data is formatted in 24-bit frames, which consist of a 3-bit preamble followed by three ASCII characters, each seven bits long. The received signal is usually decoded using digital signal processing techniques that are capable of recovering the 8FSK signal at a negative decibel signal-to-noise ratio (i.e., the signal may be recovered even when it is below the noise level). The over-the-air layers of the protocol involve the use of forward error correction, redundancy, and handshaking transponding similar to those used in ARQ techniques.\n\nNewer standards of ALE, called 3rd Generation or 3G ALE, use accurate time synchronization (via a defined time-synch protocol as well as the option of GPS-locked clocks) to achieve faster and more dependable linking. Through synchronization, the calling time to achieve a link may be reduced to less than 10 seconds. The 3G ALE modem signal also provides better robustness and can work in channel conditions that are less favorable than 2G ALE. Dwell groups, limited callsigns, and shorter burst transmissions enable more rapid intervals of scanning. All stations in the same group scan and receive each channel at precisely the same time window. Although 3G ALE is more reliable and has significantly enhanced channel-time efficiency, the existence of a large installed base of 2G ALE radio systems and the wide availability of moderately priced (often military surplus) equipment, has made 2G the baseline standard for global interoperability.\n\nInteroperability is a critical issue for the disparate entities which use radiocommunications to fulfill the needs of organizations. Largely due to the ubiquity of 2G ALE, it became the primary method for providing interoperability on HF between governmental and non-governmental disaster relief and emergency communications entities, and amateur radio volunteers. With digital techniques increasingly employed in communications equipment, a universal digital calling standard was needed, and ALE filled the gap. Nearly every major HF radio manufacturer in the world builds ALE radios to the 2G standard to meet the high demand that new installations of HF radio systems conform to this standard protocol. Disparate entities that historically used incompatible radio methods were then able to call and converse with each other using the common 2G ALE platform. Some manufacturers and organizations have used the AMD feature of ALE to expand the performance and connectivity. In some cases, this has been successful, and in other cases, the use of proprietary preamble or embedded commands has led to interoperability problems.\n\nALE serves as a convenient method of beyond line of sight communication. Originally developed to support military requirements, ALE is useful to many organizations who find themselves managing widely located units. United States Immigration and Customs Enforcement and United States Coast Guard are two members of the Customs Over the Horizon Enforcement Network (COTHEN), a MIL-STD 188-141A ALE network. All U.S. armed forces operate multiple similar networks. Similarly, shortwave utility listeners have documented frequency and callsign lists for many nations' military and guard units, as well as networks operated by oil exploration and production companies and public utilities in many countries.\n\nALE radio communication systems for both HF regional area networks and HF interoperability communications are in service among emergency and disaster relief agencies as well as military and guard forces. Extraordinary response agencies and organizations use ALE to respond to situations in the world where conventional communications may have been temporarily overloaded or damaged. In many cases, it is in place as alternative back-channel for organizations that may have to respond to situations or scenarios involving the loss of conventional communications. Earthquakes, storms, volcanic eruptions, and power or communication infrastructure failures are typical situations in which organizations may deem ALE necessary to operations. ALE networks are common among organizations engaged in extraordinary situation response such as: natural and man-made disasters, transportation, power, or telecommunication network failures, war, peacekeeping, or stability operations. Organizations known to use ALE for Emergency management, disaster relief, ordinary communication or extraordinary situation response include: Red Cross, FEMA, Disaster Medical Assistance Teams, NATO, Federal Bureau of Investigation, United Nations, AT&T, Civil Air Patrol, SHARES, State of California Emergency Management Agency (CalEMA), other US States' Offices of Emergency Services or Emergency Management Agencies, and Amateur Radio Emergency Service (ARES).\n\nThe International Telecommunications Union (ITU), in response to the need for interoperation in international disaster response spurred largely by humanitarian relief, included ALE in its Telecommunications for Disaster Relief recommendations. The increasing need for instant connectivity for logistical and tactical disaster relief response communications, such as the 2004 Indian Ocean earthquake tsunami led to ITU actions of encouragement to countries around the world toward loosening restrictions on such communications and equipment border transit during catastrophic disasters. The IARU Global Amateur Radio Emergency Communications Conferences (GAREC) and IARU Global Simulated Emergency Tests have included ALE.\n\nAmateur radio operators began sporadic ALE operation on a limited basis in the early to mid-1990s, with commercial ALE radios and ALE controllers. In 2000, the first widely available software ALE controller for the Personal Computer, \"PCALE\", became available, and hams started to set up stations based on it. In 2001, the first organized and coordinated global ALE nets for International Amateur Radio began. In August 2005, ham radio operators supporting communications for emergency Red Cross shelters used ALE for Disaster Relief operations during the Hurricane Katrina disaster. After the event, hams developed more permanent ALE emergency/disaster relief networks, including internet connectivity, with a focus on interoperation between organizations. The amateur radio HFLink Automatic Link Establishment system uses an open net protocol to enable all amateur radio operators and amateur radio nets worldwide to participate in ALE and share the same ALE channels legally and interoperably. Amateur radio operators may use it to call each other for voice or data communications.\n\nAmateur radio operators commonly provide local, regional, national, and international emergency / disaster relief communications. The need for interoperability on HF led to the adoption of Automatic Link Establishment ALE open networks by hams. Amateur radio adapted 2G ALE techniques, by using the common denominators of the 2G ALE protocol, with a limited subset of features found in the majority of all ALE radios and controllers. Each amateur radio ALE station uses the operator's call sign as the address, also known as the ALE Address, in the ALE radio controller. The lowest common denominator technique enables any manufacturer's ALE radios or software to be used for HF interoperability communications and networking. Known as Ham-Friendly ALE, the amateur radio ALE standard is used to establish radio communications, through a combination of active ALE on internationally recognized automatic data frequencies, and passive ALE scanning on voice channels. In this technique, active ALE frequencies include pseudorandom periodic polite station identification, while passive ALE frequencies are silently scanned for selective calling. ALE systems include Listen Before Transmit as a standard function, and in most cases this feature provides better busy channel detection of voice and data signals than the human ear. Ham-Friendly ALE technique is also known as 2.5G ALE, because it maintains 2G ALE compatibility while employing some of the adaptive channel management features of 3G ALE, but without the accurate GPS time synchronization of 3G ALE.\n\nHot standby ALE nets are in constant operation 24/7/365 for International Emergency and Disaster Relief communications. The Ham Radio Global ALE High Frequency Network, which began service in June 2007, is the world's largest intentionally open ALE network. It is a free open network staffed by volunteers, and used by amateur radio operators supporting disaster relief organizations.\n\nInternational amateur radio ALE High Frequency channels are frequency coordinated with all Regions of the International Amateur Radio Union (IARU entity of ITU), for international, regional, national, and local use in the Amateur Radio Service. All Amateur Radio ALE channels use \"USB\" Upper Sideband standard. Different rules, regulations, and bandplans of the region and local country of operation apply to use of various channels. Some channels may not be available in every country. Primary or global channels are in common with most countries and regions.\n\n\"This listing is current as of February 2020. See HFLINK for more information about Amateur Radio Service ALE Automatic Link Establishment .\"\n\nFrequency table notes: \nAutomatic Link Establishment ALE channel frequencies in the Amateur Radio Service are internationally coordinated with selective calling Selcall channels for interoperability purposes. \nNet is the ALE net address or Selcall net name.\n\n! NET !! Protocol !! Content !! Status !! Sounding !! Net Slots !! Purpose\n", "related": "\n- Multiple frequency-shift keying\n- Selective calling\n- Amateur radio\n- Amateur radio emergency communications\n- ARES\n\n"}
{"id": "44789336", "url": "https://en.wikipedia.org/wiki?curid=44789336", "title": "Blackman's theorem", "text": "Blackman's theorem\n\nBlackman's theorem is a general procedure for calculating the change in an impedance due to feedback in a circuit. It was published by Ralph Beebe Blackman in 1943, was connected to signal-flow analysis by John Choma, and was made popular in the extra element theorem by R. D. Middlebrook and the asymptotic gain model of Solomon Rosenstark. Blackman's approach leads to the formula for the impedance \"Z\" between two selected terminals of a negative feedback amplifier as Blackman's formula:\nwhere \"Z\" = impedance with the feedback disabled, \"T\" = loop transmission with a small-signal short across the selected terminal pair, and \"T\" = loop transmission with an open circuit across the terminal pair. The loop transmission also is referred to as the return ratio. Blackman's formula can be compared with Middlebrook's result for the input impedance \"Z\" of a circuit based upon the extra-element theorem:\n\nwhere:\n\nBlackman's formula also can be compared with Choma's signal-flow result:\n\nwhere formula_11 is the value of formula_12 under the condition that a selected parameter \"P\" is set to zero, return ratio formula_13 is evaluated with zero excitation and formula_14 is formula_13 for the case of short-circuited source resistance. As with the extra-element result, differences are in the perspective leading to the formula.\n\n", "related": "\n- Mason's gain formula\n\n"}
{"id": "44982289", "url": "https://en.wikipedia.org/wiki?curid=44982289", "title": "Number theoretic Hilbert transform", "text": "Number theoretic Hilbert transform\n\nThe number theoretic Hilbert transform is an extension of the discrete Hilbert transform to integers modulo a prime formula_1. The transformation operator is a circulant matrix.\n\nThe number theoretic transform is meaningful in the ring formula_2, when the modulus formula_3 is not prime, provided a principal root of order \"n\" exists. \nThe formula_4 NHT matrix, where formula_5, has the form\n\nThe rows are the cyclic permutations of the first row, or the columns may be seen as the cyclic permutations of the first column. The NHT is its own inverse:formula_7 where \"I\" is the identity matrix.\n\nThe number theoretic Hilbert transform can be used to generate sets of orthogonal discrete sequences that have applications in signal processing, wireless systems, and cryptography. Other ways to generate constrained orthogonal sequences also exist.\n\n", "related": "\n- Number theoretic transform\n"}
{"id": "44993448", "url": "https://en.wikipedia.org/wiki?curid=44993448", "title": "Flow graph (mathematics)", "text": "Flow graph (mathematics)\n\nA flow graph is a form of digraph associated with a set of linear algebraic or differential equations:\n\nAlthough this definition uses the terms \"signal flow graph\" and \"flow graph\" interchangeably, the term \"signal flow graph\" is most often used to designate the Mason signal-flow graph, Mason being the originator of this terminology in his work on electrical networks. Likewise, some authors use the term \"flow graph\" to refer strictly to the Coates flow graph. According to Henley & Williams:\n\nA designation \"flow graph\" that includes both the Mason graph and the Coates graph, and a variety of other forms of such graphs appears useful, and agrees with Abrahams and Coverley's and with Henley and Williams' approach.\n\nA directed network – also known as a flow network – is a particular type of flow graph. A \"network\" is a graph with real numbers associated with each of its edges, and if the graph is a digraph, the result is a \"directed network\". A flow graph is more general than a directed network, in that the edges may be associated with \"gains, branch gains\" or \"transmittances\", or even functions of the Laplace operator \"s\", in which case they are called \"transfer functions\".\n\nThere is a close relationship between graphs and matrices and between digraphs and matrices. \"The algebraic theory of matrices can be brought to bear on graph theory to obtain results elegantly\", and conversely, graph-theoretic approaches based upon flow graphs are used for the solution of linear algebraic equations.\n\nAn example of a flow graph connected to some starting equations is presented.\n\nThe set of equations should be consistent and linearly independent. An example of such a set is:\n\nConsistency and independence of the equations in the set is established because the determinant of coefficients is non-zero, so a solution can be found using Cramer's rule.\n\nUsing the examples from the subsection Elements of signal flow graphs, we construct the graph In the figure, a signal-flow graph in this case. To check that the graph does represent the equations given, go to node \"x\". Look at the arrows incoming to this node (colored green for emphasis) and the weights attached to them. The equation for \"x\" is satisfied by equating it to the sum of the nodes attached to the incoming arrows multiplied by the weights attached to these arrows. Likewise, the red arrows and their weights provide the equation for \"x\", and the blue arrows for \"x\".\n\nAnother example is the general case of three simultaneous equations with unspecified coefficients:\nTo set up the flow graph, the equations are recast so each identifies a single variable by adding it to each side. For example:\nUsing the diagram and summing the incident branches into \"x\" this equation is seen to be satisfied.\n\nAs all three variables enter these recast equations in a symmetrical fashion, the symmetry is retained in the graph by placing each variable at the corner of an equilateral triangle. Rotating the figure 120° simply permutes the indices. This construction can be extended to more variables by placing the node for each variable at the vertex of a regular polygon with as many vertices as there are variables.\n\nOf course, to be meaningful the coefficients are restricted to values such that the equations are independent and consistent.\n\n", "related": "\n- Coates graph\n- Signal-flow graph\n\n- A discussion of the Coates and the Mason flow graphs.\n"}
{"id": "44390392", "url": "https://en.wikipedia.org/wiki?curid=44390392", "title": "Log Gabor filter", "text": "Log Gabor filter\n\nIn signal processing it is useful to simultaneously analyze the space and frequency characteristics of a signal. While the Fourier transform gives the frequency information of the signal, it is not localized. This means that we cannot determine which part of a (perhaps long) signal produced a particular frequency. It is possible to use a short time Fourier transform for this purpose, however the short time Fourier transform limits the basis functions to be sinusoidal. To provide a more flexible space-frequency signal decomposition several filters (including wavelets) have been proposed. The Log-Gabor filter is one such filter that is an improvement upon the original Gabor filter. The advantage of this filter over the many alternatives is that it better fits the statistics of natural images compared with Gabor filters and other wavelet filters.\n\nThe Log-Gabor filter is able to describe a signal in terms of the local frequency responses. Because this is a fundamental signal analysis technique, it has many applications in signal processing. Indeed, any application that uses Gabor filters, or other wavelet basis functions may benefit from the Log-Gabor filter. However, there may not be any benefit depending on the particulars of the design problem. Nevertheless, the Log-Gabor filter has been shown to be particularly useful in image processing applications, because it has been shown to better capture the statistics of natural images.\n\nIn image processing, there are a few low-level examples of the use of Log-Gabor filters. Edge detection is one such primitive operation, where the edges of the image are labeled. Because edges appear in the frequency domain as high frequencies, it is natural to use a filter such as the Log-Gabor to pick out these edges. These detected edges can be used as the input to a segmentation algorithm or a recognition algorithm. A related problem is corner detection. In corner detection the goal is to find points in the image that are corners. Corners are useful to find because they represent stable locations that can be used for image matching problems. The corner can be described in terms of localized frequency information by using a Log-Gabor filter.\n\nIn pattern recognition, the input image must be transformed into a feature representation that is easier for a classification algorithm to separate classes. Features formed from the response of Log-Gabor filters may form a good set of features for some applications because it can locally represent frequency information. For example, the filter has been successfully used in face expression classification. There is some evidence that the human visual system processes visual information in a similar way.\n\nThere are a host of other applications that require localized frequency information. The Log-Gabor filter has been used in applications such as image enhancement, speech analysis, contour detection, texture synthesis and image denoising among others.\n\nThere are several existing approaches for computing localized frequency information. These approaches are advantageous because unlike the Fourier transform, these filters can more easily represent discontinuities in the signal. For example, the Fourier transform can represent an edge, but only by using an infinite number of sine waves.\n\nWhen considering filters that extract local frequency information, there is a relationship between the frequency resolution and the time/space resolution. When more samples are taken the resolution of the frequency information is higher, however the time/space resolution will be lower. Likewise taking only a few samples means a higher spatial/temporal resolution, but this is at the cost of less frequency resolution. A good filter should be able to obtain the maximum frequency resolution given a set time/space resolution, and vice versa. The Gabor filter achieves this bound. Because of this, the Gabor filter is a good method for simultaneously localizing spatial/temporal and frequency information. A Gabor filter in the space (or time) domain is formulated as a Gaussian envelope multiplied by a complex exponential. It was found that the cortical responses in the human visual system can be modeled by the Gabor filter. The Gabor filter was modified by Morlet to form an orthonormal continuous wavelet transform.\n\nAlthough the Gabor filter achieves a sense of optimality in terms of the space-frequency tradeoff, in certain applications it might not be an ideal filter. At certain bandwidths, the Gabor filter has a non-zero DC component. This means that the response of the filter depends on the mean value of the signal. If the output of the filter is to be used for an application such as pattern recognition, this DC component is undesirable because it gives a feature that changes with the average value. As we will soon see, the Log-Gabor filter does not exhibit this problem. Also the original Gabor filter has an infinite length impulse response. Finally, the original Gabor filter, while optimum in the sense of uncertainty, does not properly fit the statistics of natural images. As shown in, it is better to choose a filter with a longer sloping tail in an image coding task.\n\nIn certain applications, other decompositions have advantages. Although there are many such decompositions possible, here we briefly present two popular methods: Mexican hat wavelets and the steerable pyramid.\n\nThe Ricker wavelet, commonly called the mexican hat wavelet is another type of filter that is used to model data. In multiple dimensions this becomes the Laplacian of a Gaussian function. For reasons of computational complexity, the Laplacian of a Gaussian function is often simplified as a difference of Gaussians. This difference of Gaussian function has found use in several computer vision applications such as keypoint detection. The disadvantage of the Mexican hat wavelet is that it exhibits some aliasing and does not represent oblique orientations well.\n\nThe steerable pyramid decomposition was presented as an alternative to the Morlet (Gabor) and Ricker wavelets. This decomposition ignores the orthogonality constraint of the wavelet formulation, and by doing this is able to construct a set of filters which are both translation and rotation independent. The disadvantage of the steerable pyramid decomposition is that it is overcomplete. This means that more filters than truly necessary are used to describe the signal.\n\nField introduced the Log-Gabor filter and showed that it is able to better encode natural images compared with the original Gabor filter. Additionally, the Log-Gabor filter does not have the same DC problem as the original Gabor filter. A one dimensional Log-Gabor function has the frequency response:\n\nwhere formula_1 and formula_2 are the parameters of the filter. formula_1 will give the center frequency of the filter. formula_2 affects the bandwidth of the filter. It is useful to maintain the same shape while the frequency parameter is varied. To do this, the ratio formula_5 should remain constant. The following figure shows the frequency response of the Gabor compared with the Log-Gabor:\n\nAnother definition of the Log-Gabor filter is to consider it as a probability distribution function, with a normal distribution, but considering the logarithm of frequencies. This makes sense in contexts where the Weber–Fechner law applies, such as in visual or auditive perception. Following the change of variable rule, a one dimensional Log-Gabor function has thus the modified frequency response:\nNote that this extends to the origin and that we still have formula_6.\n\nIn both definitions, because of the zero at the DC value, it is not possible to derive an analytic expression for the filter in the space domain. In practice the filter is first designed in the frequency domain, and then an inverse Fourier transform gives the time domain impulse response.\n\nLike the Gabor filter, the log-Gabor filter has seen great popularity in image processing. Because of this it is useful to consider the 2-dimensional extension of the log-Gabor filter. With this added dimension the filter is not only designed for a particular frequency, but also is designed for a particular orientation. The orientation component is a Gaussian distance function according to the angle in polar coordinates (see or ):\n\nwhere here there are now four parameters: formula_1 the center frequency, formula_8 the width parameter for the frequency, formula_9 the center orientation, and formula_10 the width parameter of the orientation. An example of this filter is shown below.\n\nThe bandwidth in the frequency is given by:\n\nNote that the resulting bandwidth is in units of octaves.\n\nThe angular bandwidth is given by:\n\nIn many practical applications, a set of filters are designed to form a filter bank. Because the filters do not form a set of orthogonal basis, the design of the filter bank is somewhat of an art and may depend upon the particular task at hand. The necessary parameters that must be chosen are: the minimum and maximum frequencies, the filter bandwidth, the number of orientations, the angular bandwidth, the filter scaling and the number of scales.\n\n", "related": "\n- Gabor transform\n- Gabor wavelet\n- Gabor filter\n- Gabor atom\n- Feature detection (computer vision) for other low-level feature detectors\n- Image derivatives\n- Image noise reduction\n- Ridge detection for relations between edge detectors and ridge detectors\n\n- (obsolete to this date)\n- A python implementation with examples for vision:\n"}
{"id": "42688214", "url": "https://en.wikipedia.org/wiki?curid=42688214", "title": "Chirp spectrum", "text": "Chirp spectrum\n\nThe spectrum of a chirp pulse describes its characteristics in terms of its frequency components. This frequency-domain representation is an alternative to the more familiar time-domain waveform, and the two versions are mathematically related by the Fourier transform. <br>\nThe spectrum is of particular interest when pulses are subject to signal processing. For example, when a chirp pulse is compressed by its matched filter, the resulting waveform contains not only a main narrow pulse but, also, a variety of unwanted artifacts many of which are directly attributable to features in the chirp's spectral characteristics. <br>\nThe simplest way to derive the spectrum of a chirp, now that computers are widely available, is to sample the time-domain waveform at a frequency well above the Nyquist limit and call up an FFT algorithm to obtain the desired result. As this approach was not an option for the early designers, they resorted to analytic analysis, where possible, or to graphical or approximation methods, otherwise. These early methods still remain helpful, however, as they give additional insight into the behavior and properties of chirps.\n\nA general expression for an oscillatory waveform, centered on frequency\n\nformula_1\n\nwhere formula_2 and (t) give the amplitude and phase variations of the waveform formula_3, with time. <br>\nThe frequency spectrum of this waveform is obtained by calculating the Fourier Transform of formula_4, i.e.\n\nformula_5\n<br> so <br>\nformula_6\n\nIn a few special cases, the integral can be solved to give an analytical expression, but often the characteristics of formula_2 and (t) are such that the integral can only be evaluated by an approximation algorithm or by numerical integration.\n\nIn the special case where s(t) is constrained to be a down-chirp, flat topped pulse with its instantaneous frequency varying as a linear function of time, then an analytical solution is possible.\n\nFor convenience, the pulse is considered to have unit amplitude and be of duration T, with the amplitude and phase defined over the time interval -T/2 to +T/2. The total frequency sweep is F, varying in a linear manner from -F/2 to +F/2 in the defined time interval.\n\nWhen the frequency is a linear function of time, the phase is a quadratic function, and s(t) can be written\nThe spectrum of this linear FM signal is\n\nBy completing the square and using the Fresnel integrals C(X) and S(X), defined by\nthe expression can be evaluated to give:\n\nwhere formula_12 and formula_13 are given by \n\nThe linear FM spectrum can be considered to have three major components, namely\n- an Amplitude Term, formula_15\n- a Square Law Phase term, formula_16\n- and a Residual Phase Term formula_17\n\nThe ratio formula_18 is approximately unity over a large part of frequency range of interest so approximates to a constant phase angle /4 there.\nIf a frequency scaling term n is introduced, where formula_19, then the expressions for the Fresnel arguments become\nThe spectra are now functions of the product T.F, independent of any particular values of center frequency and bandwidth. This product, T.F, is often referred to as the time-bandwidth product of the chirp.\n\nTables of the Fresnel integrals have been published, together with mathematical routines with which to compute the integrals manually or by means of a computer program. In addition, a number of mathematical software programs, such as Mathcad, MATLAB and Mathematica have built-in routines to evaluate the integrals, either as standard functions or in extension packages.\n\nSome plots of the power spectrum |S()| as a function of frequency are shown, for time-bandwidth products of 25, 100, 250 and 1000. When the product is small, the Fresnel ripples are very much in evidence, but the spectrum does tend to a more rectangular profile for larger values.\n<br>\nIn the case of the plots of residual phase, 2(), the profiles tend to be very similar over a wide range of time-bandwidth products. Two examples, for TxB = 100 and 250 are shown below. They have a phase angle close to a value of /4 within the chirp range formula_22 and they only start to change significantly for frequencies beyond this range.\n\nConsequently, for frequencies within the sweep range of the chirp, it is the square-law phase term 1() and its group delay function ( = -d1/d() ) that are of most interest. There is a plot of the group delay shown below. Both this function and the phase 1() are independent of the value of the time-bandwidth product. As expected, the group delay is a linear function with a duration T secs, over a frequency sweep of rads.\n\nThe residual phase term adds only minor perturbations to this characteristic within the frequency range formula_23. At frequencies outside this range, 2() deviates rapidly from /4, and so the total phase will deviate seriously from a square law there. Fortunately, the energy content of the chirp spectrum is very small at these frequencies (as is demonstrated in a later section).\n\nWhen the Frequency-Time characteristic is non-linear, the Fourier integral is difficult to evaluate. In such cases, it is possible to resort to an approximation method such as the stationary phase approximation, or to use numerical methods.\n\nOften (as in radar applications) a(t) is a slowly varying function of time and the phase (t) is oscillatory and varies rapidly, over the range of integration. With such waveforms, the stationary phase approximation can be used to investigate the spectrum. The method relies on the fact that the major contributions to the Fourier integral come from the region where the rate of change of phase is minimal, i.e. when\n\nformula_24\n\nUnless (t) is a constant, the point in time t at which the phase is stationary will vary according to the instantaneous frequency .<br>\nExpressing the difference between (-).t and (t) as a Taylor series about the time t, but discarding all but the first three terms (of which the second term is zero, here), the Fourier integral can be written, approximately, as\n\nformula_25\n\nIn this equation t represents a constant time point, so terms depending on t alone may be taken outside the integral. The expression simplifies to <br>\nformula_26 <br>\nso <br>\nformula_27\n\nwhere is used to indicate the dependence of the frequency variable on t. <br>\nThis is a very useful expression linking, as it does, the spectrum profile to the amplitude and phase characteristics of the chirp.\n\nTo carry out the inverse process, i.e. to find the time domain function s(t) given frequency domain data, the inverse Fourier transform is derived.\n\nformula_28\n\nwhere (x) is the phase function of the spectrum. The stationary phase points for this integrand are located at\n\nformula_29\n\nand the corollary relationship, equivalent to that derived for the spectrum, can be obtained by the stationary phase method, and is\n\nformula_30\n\nIn effect, stationary phase analysis gives the following (approximate) Fourier pair relationships:<br>\nformula_31 <br>\nand <br>\nformula_32\n\nConsequently, approximate expressions for a(t) and (t) can be obtained when the spectrum, including its phase function () is given and, similarly, approximate expressions for |S(| and () can be obtained when the signal characteristics are given. Several examples of the procedure are given in the literature\n\nAlthough the relationships are only approximate, their accuracy improves as the time-bandwidth product increases. In cases where the signal envelope and spectrum modulus are defined by smoothly varying Gaussian function then a T.F product as low as 15 will give acceptable results, but if both a(t) and |S()| are defined by rectangular functions, then the product T.F needs to be much greater, typically over 100.\n\n- Examples\nTypically, in the radar case, a(t) is a constant over the duration of the signal and, for convenience, is assumed here to be unity. So the phase and amplitude characteristics, in the frequency domain, are related by\n\nformula_33\n\nThere are two solutions for (), which are complex conjugates of each other. The two filters with these characteristics can be used as the transmitter and receiver filters of a radar system and are interchangeable.<br> \nThe group delay characteristic D(), (where D()=-d/d), is\n\nformula_34 <br>\nso <br>\nformula_35\n\nSo in the case of a rectangular time envelope, the dispersive delay characteristic is given by the integral of the square of the envelope. If the positive sign is taken, then the group delay increases with increasing frequency and vice versa. The result is only approximate, but is more accurate for large values of the time bandwidth product.<br>\nConsider, as an example, the case of a spectrum that is uniform over the range -/2 to /2, then\n\nformula_36 <br>\nso<br>\nformula_37\n\nPut D(-/2) = 0 and D(/2) = T, where T is the pulse duration, then K = T/2 and A = (2πT)/ω<br>\nso, finally <br>\nformula_38\n\nAs expected, a flat topped frequency spectrum corresponds to a linear frequency sweep.\n\nThe linear chirp is just one special case which, in any case, can be calculated more precisely by the methods of the earlier section. The particular usefulness of the stationary phase method lies in its ability to provide results when the frequency sweep is non linear. In such cases the spectral response can be shaped to meet some desired design criteria, for example, low side-lobes when a chirp is compressed. One such family of spectral functions that has been studied is given by\n\nformula_39\n\nIt is possible to find the group delay characteristics of these functions in a similar manner to that carried out above and the results for n = 1 to 4 have been calculated. <br>\nAlthough these cosine functions are amenable to mathematical manipulation, they are rarely chosen to define the spectral characteristics of a chirp, in practice, because when compressed they give broad main pulses with high side-lobe levels. A better characteristic (among many) is the Hamming function, given by\n\nformula_40\nA plot of this characteristic is shown, plotted over the range -/2 to /2.\n\nApplying the equations given above, the group delay characteristic which achieves this spectral shape can be obtained. It is\n\nformula_41\n\nNow because the principle of stationary phase shows that there is a direct relationship between elapsed time and the instantaneous signal delay then, for the Hamming window, t/T can be related to / by\n\nformula_42\n\nThis characteristic which is time as a function of frequency is shown here. Inverting the plot gives the more usual (and more useful) plot of frequency as a function of time, which is also shown. \n<br>\nOther spectral shapes can be investigated in the same way and the results, although approximate, are surprisingly accurate, especially when the time bandwidth product of the pulse is high.\n\nThe stationary phase method does not predict or deal with Fresnell ripples, so it is unable to offer any means by which these ripples can be minimized. As an example, the figure below shows a chirp spectrum with T.F =250 obtained for a non-linear chirp aiming to match the Hamming window, using the methods described above. The figure shows that the spectral profile matches the Hamming characteristic quite well, but Fresnell ripples, not predicted by the method, are very much in evidence. <br>\n\nWhenever a Fourier integral cannot be evaluated by analytical means, an approximate solution is usually possible by numerical analysis. Such a procedure requires the function to be sampled, usually at equi-spaced intervals in time.<br>\nOne consequence of sampling is that the resultant spectrum is periodic in the frequency domain. In addition to the (desired) baseband spectrum, additional versions of the spectrum occur, centered on multiples of the sampling frequency. To ensure that there is no overlapping of frequency data (i.e. no aliasing) the Nyquist sampling theorem must be satisfied. In practice, a sampling rate substantially higher than that dictated by the sampling theorem is advisable\n\nA straightforward way to approximate an integral, such as a Fourier integral, is to use the standard 'rectangle rule' for numerical integration. The method assumes the signal value taken at a sample instant remains constant for one sampling interval, until the next sample is taken. This procedure is sometimes referred to as a 'box-car generator', or a zero order sample and hold. If the time interval between samples is W, then s = s(nW), and the desired integral is obtained, approximately, by summing the rectangular areas.<br>\nThe result so obtained is the convolution of a rectangular pulse with step size W with the impulses located at the sampling instants with weights equal to the sample values. In consequence, the spectrum of interest will have superimposed upon it the frequency response of the sample and hold, and the spectrum of the sampled singnal Ss is given by:\n\nformula_43\n\nThe first part of the expression, i.e. the 'sin(x)/x' part, is the frequency response of the sample and hold. Its amplitude decreases with frequency and it falls to 63% of its peak value at half the sampling frequency and it is zero at multiples of that frequency (since f =1/W).<br>\nThe second term in the equation is called the Fourier transform of the discrete signal s. It is a continuous function over all and involves an infinite number of summations. In practice the summation process can be truncated to a finite number of samples, N, possibly because the waveform is periodic or zero outside the range of samples. Furthermore, because the same spectrum is endlessly repeated, it is possible to confine interest to spectral data within the range -/2 to +/2.\n\nAs an example, an exponential chirp (with its top frequency well below the Nyquist limit) is sampled at 256 points, as shown.<br>\n\nThe sampled spectrum, Ss() of this waveform, calculated using the equation given above, is shown. To simplify the plot, only the results at positive frequencies have been displayed. The influence of the frequency spectrum of the zero order hold circuit is clearly seen in the diagram.<br>\n\nThe baseband portion of the spectrum is shown in more detail in the next figure and the response shows a distinct slope, being significantly lower at the higher frequencies.<br>\n<br>\nAlthough the characteristic of the zero order hold has a small influence on this result, the slope is mainly due to the properties of the chirp. The waveform sweeps relatively quickly over the high frequencies and spends more time sweeping the low frequencies, consequently there is less energy content at the high frequencies with more at the lower ones. (A linear chirp, on the other hand, has a nominally flat spectrum because its frequencies are swept at the same rate, as shown in some earlier plots).\n\nIf we limit interest in the output spectrum to a finite number of discrete data points (= N), at frequencies given by <br>\nformula_44\n\nthen the formula for calculating the discrete Fourier transform is\n\nformula_45\n\nThe calculations can be carried out by means of a straightforward computer algorithm, but this is not very efficient in computer usage. Consequently, more efficient algorithms have been developed, especially Fast Fourier Transforms (FFT). Computer programs which implement the FFT are widely available in the literature and in proprietary CAD programs such as Mathcad, MATLAB, and Mathematica. <br>\nIn the following example a linear chirp with time bandwidth product of 25 is sampled at 128 points (i.e. N = 128). In the figure samples of the real part of the waveform are shown - note that these are samples in the time domain. The FFT process assumes the waveform is cyclic, so these 128 data points can be considered to be part of an endlessly repeating sequence in time.<br>\n\nBy calculating the N-point FFT of this data, the discrete spectrum of the sequence is obtained. The magnitude of this spectrum is shown in the attached figure, where these data points are samples in frequency. The data is cyclic so, in the plot, the zero frequency point is at n = 0 and also at n = 128 (i.e. both points are the same frequency). The point n = 64 corresponds to +fs/2 (and also to -fs/2).\n\nTo display the spectrum in more detail (but not necessarily with more resolution), the time sequence can be extended by zero padding. For example, extending the 128 point time sequence with zeros to give N = 4096 results in that part of the spectrum originally presented in 16 samples, now being presented in 512 samples, as shown. <br>\nThere is very little spectral content beyond the sweep frequency range of a chirp pulse and this is especially true for waveforms where the time-bandwidth product is large. The full line on the graph of the adjacent figure shows results for linear chirps. It shows, for example, that only about 2% of the total power resides at frequencies outside the sweep range F when the time-bandwidth is 100, and it is less than 1/2% when T.F is 500. <br>\nIn the case of a non-linear chirp, or a linear chirp shaped by amplitude weighting, the fraction of power outside F is even lower, as is shown on the graph, where the dashed line is for spectra with Hamming profiles.<br>\nThis low spectral spread is particularly significant when baseband signals are to be digitized since it permits a sampling frequency to be chosen which is only slightly higher than twice the maximum frequency excursion of the chirp.\n\nThe Fresnel ripples on a chirp spectrum are very obtrusive, especially when time-bandwidth products are low (under 50, say) and their presence leads to high time sidelobe levels when chirps are subject to pulse compression as in radar and sonar systems. They arise because of the sudden discontinuities in the chirp waveform at the commencement and termination of the pulse.<br>\nAlthough there are a number of procedures that can be applied to reduce the ripple levels, they are not all equally effective. Furthermore, some of the methods require amplitude shaping, or amplitude modulation, of the chirp pulse and this makes those methods unsuitable when, for example, the chirp pulses are to be transmitted by a power amplifier operating in a near-limiting condition. For such systems only the methods using frequency (or phase) pre-distortion are appropriate.\n\nIf the transitions at the start and end of the chirp are made less sudden (or more 'rounded'), then a reduction in ripple amplitude is achieved. The durations of the two transition regions need only be a small fraction of the pulse duration, and suggested values are between 2/F and 3/F but, as expected, when the time-bandwidth product of the pulse is small, longer transition periods are needed. The actual profiles of these rise and fall regions of a pulse do not seem to be critical and may be provided, for example, by band limiting filters in analogue implementations and a linear slope in digital ones. <br> Two examples show the spectra of linear chirps with finite rise-times. The first is for a chirp with time-bandwidth of 250, where the rise and fall times are 4% of the total pulse duration and the second is for a chirp with time-bandwidth of 25, where the rise and fall times are 10% of the total. These two spectra show a marked reduction in ripple amplitude compared to the spectra of unmodified linear chirps shown earlier. <br>\nA analogous technique can be applied to the frequency characteristic of the chirp waveform by adding linear FM distortion segments (quadratic phase modulation distortion) to the frequency characteristic of the chirp, as shown. The method is effective because amplitude and phase distortions having functional similarity can produce similar effects when the distortion factors are small.<br>\n\nSuggested values for these distortion regions, to give good results are:<br>\nformula_46\n\nLater work proposed slightly different values, namely: <br>\nformula_47 <br>\nbut the outcome can doubtless be improved by optimizing values for each particular situation.<br>\nTwo plots show the effects of frequency pre-correction and can be compared to the results in the earlier sections.<br>\n\nThe ripple reduction achieved by frequency pre-correction, although significant, is seen to be less successful than that achieved by the amplitude modulation methods of the previous section. However, it has been suggested that by implementing cubic (rather than quadratic) phase pre-correction, comparable results can be achieved.\n\nThis method uses an inverse Fourier transform in order to derive a waveform which has a spectrum with the phase characteristic of a chosen chirp but a new amplitude profile which is rectangular and ripple free. The method is very effective but, unfortunately, the waveform that is so derived has a semi-infinite time duration. If, for convenience, the newly derived waveform is truncated to a practical length, then some ripple is reintroduced onto the spectrum.<br>\nAs an example, a linear chirp waveform with a time bandwidth of 25 is shown together with its spectrum magnitude (shown by a full line) which, as demonstrated earlier, has a large ripple component. It is possible to find, by means of an inverse FFT, a chirp waveform which, in the frequency domain, has the same phase characteristic as before, but with the rectangular magnitude characteristic shown by the dashed line on the plot. The chirp waveform resulting from this process has a very long time duration, but when it is truncated to say, a length 2T, then the spectrum acquires some ripple once more, as shown.<br>\n\nThere are many applications in which a spectrum with a rectangular magnitude profile is not ideal. For example, when a chirp waveform is compressed by means of its matched filter, then the resultant waveform approximates to the sinc function and, consequently, has annoyingly high sidelobes. Often, to improve the characteristics of the pulse and lower the sidelobe levels, its spectrum is modified, typically to a bell-shaped profile.\nSimilar problems arise in digital signal processing where the spectral shaping is provided by a window function, a process sometimes called apodization. In the case of an antenna array, similar profiling by \"weighting functions\" is used to reduce the spatial sidelobes of the radiation pattern.<br> \nAlthough spectral shaping of a chirp could be applied in the frequency domain, better results are obtained if the shaping is carried out in the time domain.<br>\nExamples of this process are shown for linear chirps with time-bandwidth products of 250 and 25. They have been shaped by a 3-term Blackman-Harris window given by <br>\nformula_48 <br>\nThe spectra, now bell-shaped, are seen to be free of ripples. \nNon-linear chirps can be devised that have a bell shaped spectrum, such as the Blackman-Harris window just discussed, and consequently will exhibit reduced ripple compared to the linear chirp. By means of the stationary phase method described earlier, an approximate relationship between time and frequency can be obtained and is:<br>\nformula_49 <br>\n\nRearranging the equation, a plot of frequency against time can be plotted, as shown.\n\nAs examples, plots of the spectral magnitudes of non-linear chirps with spectral profiles of Blackman-Harris windows and with time-bandwidth products of 250 and 25 are shown below. As can be seen, there is some ripple reduction, but the disappointing performance can be attributed to the fact that these chirps, although they have reduced energy content in their outer frequency regions, they still have amplitude profiles with fast rise and fall times. <br>\n", "related": "\n- Pulse compression, a process which uses frequency or phase coded waveforms to improve the signal to noise of received signals.\n- Chirp compression, a compression process for chirps only.\n"}
{"id": "950777", "url": "https://en.wikipedia.org/wiki?curid=950777", "title": "Analytic signal", "text": "Analytic signal\n\nIn mathematics and signal processing, an analytic signal is a complex-valued function that has no negative frequency components.  The real and imaginary parts of an analytic signal are real-valued functions related to each other by the Hilbert transform.\n\nThe analytic representation of a real-valued function is an \"analytic signal\", comprising the original function and its Hilbert transform. This representation facilitates many mathematical manipulations. The basic idea is that the negative frequency components of the Fourier transform (or spectrum) of a real-valued function are superfluous, due to the Hermitian symmetry of such a spectrum. These negative frequency components can be discarded with no loss of information, provided one is willing to deal with a complex-valued function instead. That makes certain attributes of the function more accessible and facilitates the derivation of modulation and demodulation techniques, such as single-sideband. \n\nAs long as the manipulated function has no negative frequency components (that is, it is still \"analytic\"), the conversion from complex back to real is just a matter of discarding the imaginary part. The analytic representation is a generalization of the phasor concept: while the phasor is restricted to time-invariant amplitude, phase, and frequency, the analytic signal allows for time-variable parameters.\n\nIf formula_1 is a \"real-valued\" function with Fourier transform formula_2, then the transform has Hermitian symmetry about the formula_3 axis:\nwhere formula_5 is the complex conjugate of formula_2.\nThe function:\nwhere\n- formula_8 is the Heaviside step function,\n- formula_9 is the sign function,\n\ncontains only the \"non-negative frequency\" components of formula_2. And the operation is reversible, due to the Hermitian symmetry of formula_2:\nThe analytic signal of formula_1 is the inverse Fourier transform of formula_14:\nwhere\n- formula_16 is the Hilbert transform of formula_1;\n- formula_18 is the convolution symbol;\n- formula_19 is the imaginary unit.\n\nNoting that formula_20 this can also be expressed as a filtering operation that directly removes negative frequency components:\n\nSince formula_22, restoring the negative frequency components is a simple matter of discarding formula_23 which may seem counter-intuitive. We can also note that the complex conjugate formula_24 comprises \"only\" the negative frequency components. And therefore formula_25 restores the suppressed positive frequency components. Another viewpoint is that the imaginary component in either case is a term that subtracts frequency components from s(t). The formula_26 operator removes the subtraction, giving the appearance of adding new components.\n\nThen:\n\nA corollary of Euler's formula is  formula_31  In general, the analytic representation of a simple sinusoid is obtained by expressing it in terms of complex-exponentials, discarding the negative frequency component, and doubling the positive frequency component. And the analytic representation of a sum of sinusoids is the sum of the analytic representations of the individual sinusoids.\n\nHere we use Euler's formula to identify and discard the negative frequency.\n\nThen:\n\nThis is another example of using the Hilbert transform method to remove negative frequency components. We note that nothing prevents us from computing formula_34 for a complex-valued formula_1. But it might not be a reversible representation, because the original spectrum is not symmetrical in general. So except for this example, the general discussion assumes real-valued formula_1.\n\nThen:\n\nAn analytic signal can also be expressed in polar coordinates:\nwhere the following time-variant quantities are introduced:\n- formula_42 is called the \"instantaneous amplitude\" or the \"envelope\";\n- formula_43 is called the \"instantaneous phase\" or \"phase angle\".\n\nIn the accompanying diagram, the blue curve depicts formula_1 and the red curve depicts the corresponding formula_45.\n\nThe time derivative of the unwrapped instantaneous phase has units of \"radians/second\", and is called the \"instantaneous angular frequency\":\n\nThe \"instantaneous frequency\" (in hertz) is therefore:\n\nThe instantaneous amplitude, and the instantaneous phase and frequency are in some applications used to measure and detect local features of the signal. Another application of the analytic representation of a signal relates to demodulation of modulated signals. The polar coordinates conveniently separate the effects of amplitude modulation and phase (or frequency) modulation, and effectively demodulates certain kinds of signals.\n\nAnalytic signals are often shifted in frequency (down-converted) toward 0 Hz, possibly creating [non-symmetrical] negative frequency components:\nwhere formula_49 is an arbitrary reference angular frequency.\n\nThis function goes by various names, such as \"complex envelope\" and \"complex baseband\". The complex envelope is not unique; it is determined by the choice of formula_49. This concept is often used when dealing with passband signals. If formula_1 is a modulated signal, formula_49 might be equated to its carrier frequency. \n\nIn other cases, formula_49 is selected to be somewhere in the middle of the desired passband. Then a simple low-pass filter with real coefficients can excise the portion of interest. Another motive is to reduce the highest frequency, which reduces the minimum rate for alias-free sampling. A frequency shift does not undermine the mathematical tractability of the complex signal representation. So in that sense, the down-converted signal is still \"analytic\". However, restoring the real-valued representation is no longer a simple matter of just extracting the real component. Up-conversion may be required, and if the signal has been sampled (discrete-time), interpolation (upsampling) might also be necessary to avoid aliasing.\n\nIf formula_49 is chosen larger than the highest frequency of formula_55 then formula_56 has no positive frequencies. In that case, extracting the real component restores them, but in reverse order; the low-frequency components are now high ones and vice versa. This can be used to demodulate a type of single sideband signal called \"lower sideband\" or \"inverted sideband\".\n\n- Other choices of reference frequency:\n\nSometimes formula_49 is chosen to minimize\n\nAlternatively, formula_49 can be chosen to minimize the mean square error in linearly approximating the \"unwrapped\" instantaneous phase formula_60:\nor another alternative (for some optimum formula_62):\n\nIn the field of time-frequency signal processing, it was shown that the analytic signal was needed in the definition of the Wigner–Ville distribution so that the method can have the desirable properties needed for practical applications.\n\nSometimes the phrase \"complex envelope\" is given the simpler meaning of the complex amplitude of a (constant-frequency) phasor;\nother times the complex envelope formula_64 as defined above is interpreted as a time-dependent generalization of the complex amplitude. Their relationship is not unlike that in the real-valued case: varying envelope generalizing constant amplitude.\n\nThe concept of analytic signal is well-defined for signals of a single variable which typically is time. For signals of two or more variables, an analytic signal can be defined in different ways, and two approaches are presented below.\n\nA straightforward generalization of the analytic signal can be done for a multi-dimensional signal once it is established what is meant by \"negative frequencies\" for this case. This can be done by introducing a unit vector formula_65 in the Fourier domain and label any frequency vector formula_66 as negative if formula_67. The analytic signal is then produced by removing all negative frequencies and multiply the result by 2, in accordance to the procedure described for the case of one-variable signals. However, there is no particular direction for formula_65 which must be chosen unless there are some additional constraints. Therefore, the choice of formula_65 is ad hoc, or application specific.\n\nThe real and imaginary parts of the analytic signal correspond to the two elements of the vector-valued monogenic signal, as it is defined for one-variable signals. However, the monogenic signal can be extended to arbitrary number of variables in a straightforward manner, producing an -dimensional vector-valued function for the case of \"n\"-variable signals.\n\n", "related": "\n- Practical considerations for computing Hilbert transforms\n- Negative frequency\n\n- Single sideband modulation\n- Quadrature filter\n- Causal filter\n\n- Leon Cohen, \"Time-frequency analysis\", Prentice Hall, Upper Saddle River, 1995.\n- Frederick W. King, \"Hilbert Transforms\", vol. II, Cambridge University Press, Cambridge, 2009.\n- B. Boashash, \"Time-Frequency Signal Analysis and Processing: A Comprehensive Reference\", Elsevier Science, Oxford, 2003.\n\n- Analytic Signals and Hilbert Transform Filters\n"}
{"id": "46803143", "url": "https://en.wikipedia.org/wiki?curid=46803143", "title": "Chirp compression", "text": "Chirp compression\n\nThe chirp pulse compression process transforms a long duration frequency-coded pulse into a narrow pulse of greatly increased amplitude. It is a technique used in radar and sonar systems because it is a method whereby a narrow pulse with high peak power can be derived from a long duration pulse with low peak power. Furthermore, the process offers good range resolution because the half-power beam width of the compressed pulse is consistent with the system bandwidth.\n\nThe basics of the method for radar applications were developed in the late 1940s and early 1950s, but it was not until 1960, following declassification of the subject matter, that a detailed article on the topic appeared the public domain. Thereafter, the number of published articles grew quickly, as demonstrated by the comprehensive selection of papers to be found in a compilation by Barton.\n\nBriefly, the basic pulse compression properties can be related as follows. For a chirp waveform that sweeps over a frequency range F1 to F2 in a time period T, the nominal bandwidth of the pulse is B, where B = F2 – F1, and the pulse has a time-bandwidth product of T×B. Following pulse compression, a narrow pulse of duration τ is obtained, where τ ≈ 1/B, together with a peak voltage amplification of .\n\nIn order to compress a chirp pulse of duration T seconds, which sweeps linearly in frequency from F1 Hz to F2 Hz, a device with the characteristics of a dispersive delay line is required. This provides most delay for the frequency F1, the first to be generated, but with a delay which reduces linearly with frequency, to be T second less at the end frequency F2. Such a delay characteristic ensures that all frequency components of the chirp pass through the device, to arrive at the detector at the same time instant and so augment one another, to produce a narrow high amplitude pulse, as shown in the figure:\n\nAn expression describing the required delay characteristic is\n\nThis has a phase component (f), where\n\nwhich has a linear slope with frequency, as required. In this expression, the delay characteristic has been normalized (for convenience), so as to give zero delay when the frequency f equals the carrier frequency f. Consequently, when the instantaneous frequency is (f − B/2) or (f + B/2), the required delay is +T/2 or −T/2, respectively, so k = B/T.\n\nThe required dispersive characteristic may be obtained from a lumped element delay network, a SAW device, or by means of digital signal processing \n\nA chirp pulse, however generated, can be considered as the output of one of a pair of filters, which have dispersive characteristics. So, if the transmit filter has a group delay response which increases with frequency, then the receive filter will have one which decreases with frequency and vice versa.\n\nIn principle, the transmitted pulses can be generated by applying impulses to the input of the dispersive transmit filter, with the resultant output chirp amplified as necessary for transmission. Alternatively, a voltage controlled oscillator may be used to generate the chirp signal. To achieve maximum transmitted power (and so achieve maximum range) it is normal for a radar system to transmit chirp pulses at constant amplitude from a transmitter run in a near-limiting condition. The chirp signals reflected from targets are amplified in the receiver and then processed by the compression filter to give narrow pulses of high amplitude, as previously described.\n\nIn general, the compression process is a practical implementation of a matched filter system. For the compression filter to be matched to the radiated chirp signal, its response is the complex conjugate of the time inverse of the transmit filter's impulse response. So, the output of this matched filter is given by the convolution of the signal h(t) with the conjugate impulse response h*(-t):\n\nAlternately, if the frequency response of the coding filter is H(), then that of the matched filter is H*(), the spectrum of the compressed pulse is |H()|. The waveform of this spectrum is obtained from the inverse Fourier transform, i.e.\n\nFor the case of a linear chirp, with constant amplitude and time duration T, compression by the matched filter gives a waveform with the sinc characteristic, with duration 2T, as shown later. So, in addition to the main pulse, there are a large number of time sidelobes (or, more precisely, range sidelobes) present, the biggest of which are only 13.5 dB below the peak signal level.\n\nIn order to achieve a more desirable pulse characteristic (with lower sidelobes, for example), an alternative to the matched filter is often preferred. In this more general case, the compression filter has, say, impulse response g(t) and spectral response G(), so the equations for y(t) becomes:\n\nand\n\nWhen compared to the performance of the true matched filter, there will be some loss of processing gain, the main pulse lobe will be broader, and the total time duration of the compressed waveform will exceed 2T (usually).\n\nThe sinc characteristic of a compressed pulse is a direct consequence of the spectrum of the linear chirp pulse having a rectangular profile. By modifying the spectrum to have a bell-shaped profile, by means of a weighting (or windowing, or apodization) function, lower level sidelobes are obtained. When windowing is implemented, some signal attenuation occurs and there is a broadening of the main pulse, so both signal-to-noise ratio and range resolution are impaired by the process. Preferably, the transmitted and received pulses should be modified in equal measure, but when this is impractical, windowing in the compression filter alone is still beneficial.\n\nWhen the frequency sweep of a chirp is linear, the compression process is found to be very tolerant of Doppler frequency shifts on target returns, for a wide range of time-bandwidth products. It is only when T×B is very large (>2000, say) that loss of performance due to Doppler becomes an issue (with main-pulse broadening and increased sidelobe levels). In these situations a chirp with a hyperbolic frequency law can be used, as it has been shown to be fully tolerant to Doppler shifts. Windowing techniques can still be applied to the compressed pulse spectra, to lower sidelobe levels, in a similar manner to linear chirps.\n\nThere are different concerns when the time-bandwidth product is small. When T×B is less than about 75, the windowing process is not altogether successful, especially when it is applied only within the compressor. In such a situation, even though the close-in sidelobes are lowered by the anticipated amount, further away from the main lobe the sidelobes are found to increase in amplitude once more. These sidelobes tend to reach a maximum at locations ±T/2 on each side of the main lobe of the compressed pulse and they are a consequence of the Fresnel ripples present on the frequency spectrum. This topic is discussed in more detail later.\n\nThere are techniques available that will reduce the amplitude of the spectral ripple (see chirp spectrum) and so reduce the amplitude of these far-out sidelobes, but they are not very effective when T×B. is small. In practice, the technique of “reciprocal ripple correction” gives good results (where the spectrum of the compression filter is designed to have a ripple characteristic which is the inverse of that of the signal), but the method is less successful when signal returns contain large Doppler frequency shifts.\n\nAn alternative method of obtaining a bell-shaped spectral shape in order to achieve lower sidelobes, is to sweep the frequency band in a non-linear manner. The required characteristic is obtained by having rapid changes in frequency near to the band edges, with a slower rate of change around band centre. This is a more efficient way of achieving the required spectral shape than by applying amplitude weighting to the spectrum of the linear chirp because no attenuation of signal power is necessary in order to achieve it. In addition, the procedure gives far-out sidelobes that tend to be lower than those for the comparable linear sweep version. As the mathematics of non-linear chirps is more complicated than that of linear chirps, many early workers resorted to stationary phase methods to design them.\n\nThe results obtained by using a non-linear sweep are particularly good when the time bandwidth product of the pulse is high (T×B >100). However, non-linear sweeps have to be used with caution when target returns are affected by Doppler frequency shifts. Even modest levels of Doppler can seriously degrade the profile of the main compressed pulse and raise sidelobe levels, as is shown later.\n\nMany early dispersive filters were built using lumped element all-pass filter sections., but these proved hard to fabricate with any accuracy and it was difficult to achieve a satisfactory and repeatable performance. Consequently, the time sidelobe levels of the compressed pulses were high with these early systems, even after spectral weighting, with results no better than those achieved by phase coding or chip coding at that time. Typically, sidelobe levels were in the range −20 to −25 dB a poor result when compared to later achievements.\n\nThere were similar problems too, when a voltage controlled oscillator was used as a signal source. Matching the chirp characteristic from a VCO to a dispersive delay line proved difficult and, in addition, achieving adequate temperature compensation proved challenging.\n\nA major improvement in performance of chirp pulse generation and compression systems was achieved with the development of SAW filters. These allowed much more precision in the synthesis of filter characteristics and consequently in the radar performance. The inherent temperature sensitivity of the quartz substrates was overcome by mounting both the transmit and receive filters in a common package, so providing thermal compensation. The increased precision offered by SAW technology, enabled time sidelobe levels approaching −30 dB to become achievable by radar systems. (In actual fact, the performance level now achievable was set more by limitations in the system hardware than in SAW shortcomings).\n\nSAW technology still continues to be relevant to radar systems and is particularly useful for systems using very wideband sweeps, where digital technology (see below) may not always be appropriate or may be difficult to implement.\n\nBy the late 20th century, digital technology was able to offer a new approach to signal processing, with the availability of small high power computers together with fast D/A and A/D converters offering wide dynamic ranges. (see digital-to-analog converter and analog-to-digital converter).\n\nIn a typical installation, data for the transmit pulses are stored in digital memory as a sequence of baseband I/Q samples (see quadrature phase), or as samples of a low IF waveform, and read out to a high speed D/A converters, as required. The analogue signal, so formed, is up-converted for transmission. On reception, returning signals are amplified and, typically, converted to a low IF or to baseband I/Q signals before being digitised by A/D converters. The compression of the chirps and additional signal processing is all performed by a digital computer, which has stored within it the chirp pulse data needed to carry out the compression process numerically.\n\nDigital signal processing is conveniently carried out using FFT methods. If a chirp sequence is a(n) and that for the compression filter is b(n), then the compressed pulse sequence c(n) is given by\n\nIn practice, in a radar system, for example, it is not just a chirp pulse sequence that is to be compressed, but a long data sequence of returns from a given range spoke, within which the returning chirp pulse is located. For convenience, and to permit the use of FFTs of practical size, the data is divided into shorter lengths, which are compressed by repeated use the above equation. By applying the Overlap-save method, reconstruction of the full-duration compressed signal is achieved. In this process, the transform sequence FFT{b(n)} has to be calculated only once, prior to storage in the computer for repeated use.\n\nThere are many reasons why overall system performance is found to be disappointing; the presence of a Doppler shift on signal returns is a common cause for signal degradation, as was mentioned above. Some writers favour using the ambiguity function as a way of assessing the Doppler tolerance of chirps.\n\nOthers causes of signal impairment include amplitude ripple and slope across the passband, phase ripple across the passband, large band-edge phase shifts caused by band-limiting filters, phase modulation due to poorly regulated power supplies, all of which lead to higher sidelobe levels. Tolerances for these various parameters can be derived with the aid of paired echo theory. Fortunately, with the aid of modern processing techniques and using a procedure similar to reciprocal ripple correction, or an optimization method with an adaptive filter it is possible to correct for many of these shortcomings.\n\nAnother type of waveform degradation is caused by eclipsing, where part of a returning chirp pulse is missing. As expected, this results in the loss of signal amplitude and a rise in sidelobe levels.\n\nThe characteristic of a single linear chirp pulse, of unit amplitude, can be described by\n\nwhere rect(z) is defined by rect(z) = 1 if |z| < 1/2 and rect(z) = 0 if |z| > 1/2\n\nThe phase response (t) is given by\n\nand the instantaneous frequency f is\n\nSo, during the T second duration of the pulse, the frequency changes in a linear fashion from \nf – kT/2 to f + kT/2. With the net frequency sweep defined as B, where B = (F1- F2), then k = B/T, as stated previously.\n\nThe spectrum of this waveform may be found from its transform\n\nwhich is an integral which has been evaluated in chirp spectrum.\n\nThe spectrum of the compressed pulse can be found from\n\nWhere Y(f) is the spectrum of the compression filter.\n\nThe time domain waveform formula_14 of the compressed pulse can be found as the inverse transform of formula_15. (This procedure has been described in a paper by Chin and Cook.)\n\nHere it is more convenient to find formula_14 from the convolution of the two time domain responses, i.e.\n\nwhere the convolution of two arbitrary functions is defined by\n\nHowever, in order to use this method, the impulse response of Y(f) is first needed. This is y(t), which is obtained from\n\nA table of standard integrals gives the following transform\nformula_22\n\nComparing the equations, they are equivalent if β = -j/k, so y(t) becomes\n\nformula_23\n\n[Note: the same transform can also be found in Fourier transforms, no. 206, but with replacing ]\n\nWith y(t) determined, the output s(t) can be obtained from the convolution of s(t) and y(t), i.e.\n\nformula_24\n\nwhich can be simplified to\n\nnow as formula_26 then\n\nand finally\n\nSo, for a unit amplitude linear chirp, with a pulse duration T seconds and a frequency sweep B Hz (i.e. with a ‘time-bandwidth product’ T.B), pulse compression gives a waveform with a magnitude given by\n\nformula_31\n\nwhich has the form of the familiar sinc function.\nThe collapsed pulse width , is of the order 1/B (with measured at the −4 dB points). Consequently, a pulse width reduction has occurred given by the ratio T/ where\nformula_32\n\nAlso there is a signal amplification of formula_33\n\nThe main parameters are displayed in the figures below.\nThe product T.B gives the compression ratio of the system and it equates, approximately, to the improvement in signal to noise ratio of the main lobe of the compressed pulse relative to that of the original chirp.\n\nIn the closed form solution, just presented, the compressed waveform has the standard sinc function response, because a rectangular shape was assumed for the amplitude of the pulse spectrum. In practice, the spectrum of a linear chirp has a rectangular profile only when the time-bandwidth product of a pulse is large, i.e. when T×B exceeds 100, say. When the product is small, the spectral profile of the chirp pulse is seriously degraded by Fresnel ripples as shown in chirp spectrum and so is that of the matched filter. To investigate fully the consequences of these ripples it is advisable to consider each situation individually, either by evaluating convolution integrals, or more conveniently, by means of FFTs.\n\nSome examples are shown below, for TB = 1000, 250, 100 and 25. They are dB plots which have all been normalised to have their pulse peaks set at 0 dB.\n\nAs can be seen, at high values of TB, the plots match the sinc characteristic closely, but at low values, significant differences can be seen. As already stated, these degradations in the waveforms, at low TB values, are because the spectral characteristics are no longer truly rectangular. In all cases, the close-in sidelobe levels are consistently high, at about −13.5 dB relative to the main lobe.\n\nThese range sidelobes are an unwelcome presence on the compressed pulse, because they will obscure signals of lower amplitude, which may also be present.\n\nAs the sinc-like characteristics of a compressed pulse are due to the near-rectangular profile of its spectrum, then by modifying that characteristic into a bell shape, for example, it is possible to reduce the sidelobe levels considerably. Previous work for antenna arrays and digital signal processing has already addressed this same problem. So, for example, in the case of antennas, the spatial sidelobes on the beam pattern are improved by applying a weighting function to the array elements, and in the case of digital signal processing, window functions are used to reduce the amplitude of unwanted sidelobes on the sampled functions.\n\nIn an example of the process, the spectrum of a chirp pulse, with a time-bandwidth product of 250, is shown on the left, and is approximately rectangular in profile. Below this plot, also on the left, is shown the waveform after compression of the chirp by its matched filter and is similar to the sinc function, as expected. The top plot, on the right, is the spectrum after Hamming weighting. (This was achieved by applying a root-Hamming characteristic to both the chirp spectrum and to the compressor spectrum.) The compressed pulse corresponding to this spectrum, shown in the lower plots on the right, has much lower sidelobe levels.\n\nAlthough the sidelobe level has been much reduced, there are some undesirable consequences of the weighting process. Firstly, there is an overall loss of gain, with the peak amplitude of the main lobe reduced by about 5.4 dB and, secondly, the half-power beam width of the main lobe has increased by nearly 50%. In, say, a radar system, these effects would cause loss of range and reduced range resolution, respectively.\n\nIn general, the more the sidelobe levels are lowered, the broader the main lobe will become. However, the various windowing functions do perform differently from one other, with some giving main lobes which are unnecessarily broad for the sidelobe levels achieved. The most efficient function is the Dolph–Chebyshev window (see window functions) as this gives the narrowest pulse at a given sidelobe level. A selection of the better performing windowing functions are shown in the graph of Beamwidth × Bandwidth as sidelobe level.\n\nThe lowest full line on the graph is for Dolph-Chebyshev weighting which, as already mentioned, sets the narrowest lobe possible for a given sidelobe level. So, from this plot, if a sidelobe level of −40 dB is desired, the graph shows that the smallest achievable half-power beamwidth × bandwidth is 1.2. Thus a chirp sweeping over a 20 MHz frequency band will have a compressed pulsewidth of 60 nanoseconds (at least).\nAs can be seen from the diagram, Taylor weighting is particularly efficient, and Hamming and the three and four term Blackman-Harris functions all give good results, too. Although the cos functions perform badly, they have been included because they are amenable to mathematical manipulation and were studied in some detail in early work.\n\nThe example of a chirp with TB = 250 and Hamming weighting, given earlier, illustrates the benefits of weighting but is not representative of a normal situation because the results there were achieved by applying weighting equally to both the signal chirp and to its compressor. However, in a typical radar system the chirp pulse is usually transmitted by an amplifier operating in or near to compression, to maximise transmitter efficiency. In such a case, amplitude modulation of the chirp waveform or its spectrum is not possible, so the full window characteristic has to be incorporated into the compressor response. Unfortunately, this arrangement has undesirable consequences for the far-out sidelobes of the compressed pulse, especially when the time-bandwidth of the chirp is small.\n\nConsider first the compressed pulse when TB = 250, which is shown in the left-hand figure below. For this result no weighting has been applied to the transmit pulse, but the full Hamming weighting has been applied in the compressor. As can be seen, the close-in sidelobe levels are consistent with the Hamming weighting (−42 dB), but further out, the sidelobe levels rise to a peak value of −45 dB at T/2 each side of the main lobe. In the right-hand figure, where TB =25, the problems with the far-out sidelobes are much more serious. Here these sidelobes now rise to −25 dB at T/2.\n\nAs a guide, the far-out sidelobe levels are given by\n\nSlight variations on this equation are given in the literature, but they only differ by a few dBs. Best results seem to be obtained when the window function is applied in the time domain to the compressor waveform (as amplitude modulation) rather than in the frequency domain to its spectrum.\n\nAs the far-out sidelobes are a consequence of the Fresnel ripple on the compressed pulse spectrum, any technique that reduces this ripple will also reduce the level of the sidelobes. In fact, there are several ways of achieving this reduction, as shown below. Several of the methods are featured in chirp spectrum.\n\nA chirp with slow rise and fall times has reduced ripple on its spectrum (see chirp spectrum), so will result in lower time sidelobes on the compressed pulse. As an example, consider first the figure shows the compressed spectrum of a linear chirp, which has fast rise and fall times, with T×B = 100 and where Blackman-Harris weighting has been applied. The waveform corresponding to this spectrum has time sidelobes rising to about −40 dB, as predicted.\n\nAfter linear rise and fall times are introduced, using the amplitude template shown, the ripple on the spectrum is much reduced and the time sidelobes are significantly lower, as shown.\n\nThe procedure is most effective when both the signal chirp and the compressor chirp have modified rise-times, when sidelobe levels can be lowered by 15 – 20 dB. However, it is not always possible to apply amplitude modulation in the transmitter, so there is less improvement when only the compressor waveform is modified. Even so, a sidelobe reduction of about 6 dB can still be achieved.\n\nThe precise manner in which the rise and fall times are made less severe is not very critical, so the technique of adding cosine tapers to the compressed pulse spectrum (as with the Tukey weighting function) gives a similar improvement – of several dBs.\n\nImprovements achieved by the method are tolerant of Doppler shifts.\n\nAn alternative form of waveform 'tweaking' is where frequency modulation distortion is applied to the chirps, instead of amplitude modulation. The two types of distortion are functionally similar when the distortion levels are low. As with amplitude modulation, best results are obtained when both the expander and compressor waveforms are modified.\n\nFor best results Cook and Paolillo recommend δf = 0.75×B and δ = 1/B.\n\nAs an example, a pulse considered earlier, with T×B = 100 and Blackman–Harris weighting, is modified with phase tweaking and the results are shown. There is reduced ripple on the compressed pulse spectrum and the far-out sidelobes have been reduced.\n\nThe improvements are maintained even when Doppler frequency shifts are present on the signals.\nIn a more recent paper slightly different parameters have been suggested, namely δ = 0.86/B and δf = 0.73×B.\n\nAlso, Kowatsch and Stocker has reported improved results by applying a cubic distortion function (whereas Cook and Paolillo's technique may be termed ‘square-law modulation distortion’). This new characteristic is also tolerant of Doppler frequency shifts.\n\nThe spectral response of the matched filter has a magnitude which is the mirror image of that of the expanded pulse, when a chirp spectrum has symmetry about its centre frequency, so the Fresnell ripples on the spectrum are augmented by the compression process. What is needed to reduce the ripples is a compression filter whose spectrum has the inverse (reciprocal) ripple to that of the expander. As this will no longer be a matched filter, there will be increased mismatch loss.\nIn his early work Cook did not recommend attempting such a procedure because the filters needed were considered too difficult to make. However, with the advent of SAW technology it became possible to achieve the required characteristics. More recently, digital techniques with mathematically derived look-up tables have provided a convenient way of introducing reciprocal ripple correction.\n\nThe spectrum of the compressed pulse is the product of the spectra of the expander and compressor filters, as given earlier. Now, instead of C(ω), a new output spectrum C'(ω) is defined, which has no Fresnell ripples but which defines a desired sidelobe structure (such as that defined by a Hamming window). The compression filter that will achieve this requirement is determined by the equation\n\nformula_35\n\nwhere H() is the spectrum of the signal, C’() is the target spectrum for the compressed pulse and has the low sidelobes of the chosen weighting function and K() is the spectrum of the compression filter which has the reciprocal ripple properties. Close-in sidelobes are automatically dealt with in the process.\n\nAs an example of the procedure consider a linear chirp with T×B =100. The left-hand figures shows (one half of) the spectrum of the chirp, and the right-hand figure shows the waveform after compression. As expected, close-in sidelobes start at −13.5 dB.\n\nIn the next figure, Blackman-Harris weighting has been applied to the compressed pulse spectrum. Although the close-in sidelobes have been reduced, the far-out sidelobes remain high with a predicted level of, approximately −20×log(100) = -40 dB, as predicted for a time-bandwidth product of 100. With lower time-bandwidth products, these sidelobes will be even higher.\n\nNext, a compression filter that provides reciprocal-ripple correction has been used. As can be seen, a ripple-free spectrum has been achieved resulting in a waveform that is free from high level far-out sidelobes.\n\nHowever, this procedure has a problem. Although the process has found a compressor spectrum that leads to low sidelobes on the compressed pulse, no account was taken of the waveform this spectrum might have. When an inverse Fourier transform is carried out on this spectrum, in order to determine the characteristics of its waveform, it is found that the waveform is of extremely long duration, typically exceeding 10T. Even assuming the waveform is no longer than 10T, it means that the total time needed to process one chirp will be at least 11T, in total, a length of time unacceptable in most circumstances.\n\nIn order to achieve a practical solution Judd proposed that the total length of the compression pulse be truncated to 2T, whereas Butler suggested 1.6T and 1.3T. Extensions as low as 10% have also been used\n\nUnfortunately, when the new compressor waveform is truncated, then far-out sidelobes reappear once more. The next figures shows what happens to the compressed pulse when the compressor is set at 2T duration and then at 1.1T duration. New far-out sidelobes have appeared with amplitudes that make them clearly visible. These sidelobes are often referred to as “gating sidelobes”. They can be irritatingly high but, fortunately, even if the compressor is set have just 10% extension, the sidelobes are still at a level than that achieved without correction.\n\nAny Doppler frequency shift on the received signals will degrade the ripple cancelation process and this is discussed in more detail next.\n\nWhenever the radial distance between a moving target and the radar changes with time, the reflected chirp returns will exhibit a frequency shift (Doppler shift). After compression, the resulting pulses will show some loss in amplitude, a time (range) shift and degradation in sidelobe performance.\nIn a typical radar system, the Doppler frequency is a small fraction of the swept frequency range (i.e. the system bandwidth) of the chirp, so the range errors due to Doppler are found to be minor. For example, for fd<Terman F. E., \"Electronic and Radio Engineering, 4th Edition\", McGraw Hill 1955, p.1033</ref>\n\nand where f is the Doppler frequency, B is the frequency sweep of the chirp, T is the duration of the chirp, f is the mid (centre) frequency of the chirp, V is the radial velocity of the target and c is the velocity of light (= 3×10 m/s).\n\nConsider as an example, a chirp centered on 10 GHz, with pulse duration of 10μs and a bandwidth of 10 MHz. For a target with an approach velocity of Mach1 300 m/s), the Doppler shift will be about 20 kHz and the time shift of the pulse will be about 20ns. This is roughly one fifth of the compressed pulse width and corresponds to a range error of about 7½ metres. In addition there is a tiny loss in signal amplitude (approximately 0.02 dB).\n\nLinear chirps with a time-bandwidth product of less than 2000, say, are found to be very tolerant of Doppler frequency shifts, so main pulse width and the time sidelobe levels show little change for Doppler frequencies up to several percent of system bandwidth. In addition, linear chirps which use phase pre-distortion to lower sidelobe levels, as described in an earlier section, are found to be tolerant of Doppler.\n\nFor very large Doppler values (up to 10% of system bandwidth), time sidelobes are found to increase. In these cases Doppler tolerance can be improved by introducing small frequency extensions onto the spectra of the compressed pulses. The penalty for doing this is, either, an increase in main lobe width, or an increase in bandwidth requirements.\n\nOnly when chirp time-bandwidth products are very high, say well over 2000, is it necessary consider a sweep-frequency law other than linear, to cope with Doppler frequency shifts. A Doppler tolerant characteristic is the linear-period (i.e. hyperbolic) modulation of the chirp, and this has been discussed by several authors, as was mentioned earlier\n\nIf reciprocal-ripple correction has been implemented in order to lower the time-sidelobe levels, then the benefits of the technique diminish as the Doppler frequency is increased. This is because the inverse ripples on the signal spectrum are shifted along in frequency and the reciprocal ripple of the compressor no longer matches those ripples. It is not possible to determine a precise Doppler frequency at which r-r fails because the Fresnell ripples on chirp spectra do not have a single dominant component. However, as a rough guide, r-r correction ceases to be of benefit when\n\nTo ensure that a compressed pulse has low time sidelobes, its spectrum should be approximately bell-shaped. With linear chirp pulses this can be achieved by applying a window function either in the time domain or in the frequency domain, i.e. by amplitude modulating the chirp waveforms or by applying weighting to the compressed pulse spectra. In either case there is a mismatch loss of 1½dB, or more.\n\nAn alternative way to obtain the required spectral shape is to use a non-linear frequency sweep in the chirp. In this case, to achieve the required spectral shape, the frequency sweep changes very rapidly at band edges and more slowly around band centre. Consider, as an example, the frequency versus time plot that achieves the Blackman-Harris windowing profile. When T×B =100, the spectrum of the compressed pulse and the compressed waveform are as shown.\n\nThe required non-linear characteristic can be derived using the method of stationary phase. As this technique does not take account of the Fresnel ripples, these have to be dealt with in additional ways, as was the case with linear chirps.\n\nIn order to achieve the required spectral shape for low time sidelobes, linear chirps require amplitude weighting and consequently incur a mismatch loss. Non-linear chirps, however, have the advantage that by achieving the spectral shaping directly, close-in sidelobe levels can be made low with negligible mismatch loss (typically less than 0.1 dB). Another benefit is that the far out sidelobes, due to Fresnel ripples on the spectrum, tend to be lower than for a linear chirp with the same T×B product (4 to 5 dB lower with large T×B).\n\nHowever, for chirps where the T×B product is low, the far-out sidelobe levels of the compressed pulse can still be disappointingly high, because of high amplitude Fresnel ripples on the spectrum. As with linear chirps, results can be improved by means of reciprocal ripple correction but, as previously, truncation of the compression waveform results in the appearance of gating sidelobes.\n\nAn example of reciprocal ripple and truncation is shown below. The left hand figure shows the spectrum of a non-linear chirp, with a time bandwidth product of 40, aiming to have a Blackman-Harris profile. The right-hand figure shows the compressed pulse for this spectrum,\n\nThe next figures shows the spectrum after r-r compensation, but with truncation of the compression waveform to 1.1T, and the final compressed waveform.\n\nA major disadvantage of non-linear chirps is their sensitivity to Doppler frequency shifts. Even modest values of Doppler will result in broadening of the main pulse, raising of the sidelobe levels, increase in mismatch loss and the appearance of new spurious sidelobes.\n\nAn example of a non-linear chirp pulse and the effects of Doppler are shown. The non-linear characteristic is chosen to achieve −50 dB sidelobes using Taylor weighting. The first figure shows the compressed pulse for a non-linear chirp, with bandwidth 10 MHz, pulse duration 10usec, so T×B = 100, and with no Doppler shift. The next two figures shows the pulse degradation cause by 10 kHz and 100 kHz Doppler, respectively. In addition to the waveform degradation, the mismatch loss increases to 0.5 dB. The final figure shows the effect of 100 kHz Doppler on a linear chirp which has had amplitude weighting applied to give the same spectral shape as that of the non-linear chirp. The greater tolerance to Doppler is clearly seen.\n\nCook, using paired-echo distortion methods, estimated that in order to keep sidelobe levels below −30 dB, the maximum allowed Doppler frequency is given by\n\nformula_38\n\nso, for a 10μs pulse, the maximum Doppler frequency that can be tolerated is 6 kHz. However, more recent work suggests that this is unduly pessimistic. In addition, as the new sidelobes, when at a low level, are very narrow. Consequently, it may be possible to ignore them initially, as they may not be resolvable by the receiver's D to A.\n\nA way of reducing the susceptibility of non-linear chirps to Doppler is to use a ‘hybrid’ scheme, where part of the spectral shaping is achieved by a non-linear sweep, but with additional spectral shaping achieved by amplitude weighting. Such a scheme will have greater mismatch loss than a true non-linear scheme, so the advantage of greater Doppler tolerance has to be weighed against the disadvantage of the increased mismatch loss.\n\nIn the two examples below, the chirps have a non-linear sweep characteristic which gives a spectrum with Taylor weighting which, used alone, will achieve a sidelobe level of −20 dB on its compressed pulses. To achieve lower level sidelobes, this spectral shape is augmented by amplitude weighting so that the final target sidelobe level for the compressed pulses is −50 dB. Comparing the results for Doppler shifts of 10 kHz and 100 kHz with those shown earlier it is seen that the new spurious sidelobes, caused by the Doppler, are seen to be 6 dB lower than before. However, the mismatch loss has increased from 0.1 dB to 0.6 dB, but this is still better than the 1.6 dB figure for linear chirps.\n\nThe amplitude of random noise is not changed by the compression process, so the signal to noise ratios of received chirp signals are increased in the process. In the case of a high power search radars, this extends the range performance of the system, while for stealth systems the property will permit lower transmitter powers to be used.\n\nAs an illustration, a possible received noise sequence is shown, which contains a low amplitude chirp signal obscured within it. After processing by the compressor, the compressed pulse is clearly visible above the noise floor.\n\nWhen pulse compression is carried out in digital signal processing, after the incoming signals are digitised by A/D converters, it is important that level of the noise floor is correctly set. The noise floor at the A/D must be high enough to ensure that the noise is adequately characterised. If the noise level is too low, Nyquist will not be satisfied, and any embedded chirp will not be recovered correctly. On the other hand, setting the noise level unnecessarily high will reduce the dynamic range capability of the system.\n\nFor systems using digital processing, it is important to carry out the chirp compression in the digital domain, after the A/D converters. If the compression process is carried out in the analogue domain before digitization (by a SAW filter, for example), the resulting high-amplitude pulses will place excessive demands on the dynamic range of the A/D converters.\n\nThe transmitter and receiver subsystems of a radar are not distortion free. In consequence system performance is often less than optimum. In particular, the time sidelobe levels of the compressed pulses are found to be disappointingly high.\n\nSome of the characteristics which degrade performance are:\n- Gain slope, or non-linear phase slope, across the system passband.\n- Amplitude and phase ripple ripple across the passband (which may be caused by mismatches on interconnecting cables as well as by imperfections in amplifiers).\n\nDelay modulation by the transmitter (if power supply regulation is poor).\n\nIn addition, filters employed in the frequency conversion processes of the transmitter and receiver all contribute to gain and phase variations across the system passband, especially near to band edges. In particular, major contributors to overall non-linear phase characteristics are the low-pass filters preceding the A/D converters, which are usually sharp-cut filters chosen to ensure maximum bandwidth while minimizing aliased noise. The transient response characteristics of these filters contribute another (unwanted) source of time sidelobes.\n\nFortunately it is possible to compensate for several system properties, provided they are stable and can be characterized adequately when a system is first assembled. This is not difficult to implement in radars using digital look-up tables, since these tables can be easily amended to include compensation data. Phase pre-corrections can be included in the expander tables and phase and amplitude corrections can be included in the compressor tables, as required.\n\nSo, for example, the earlier equation, defining the compressor characteristic to minimize spectral ripple, could be expanded to include additional terms to correct for known amplitude and phase impairments, thus:\n\nwhere, as before, H() is the initial chirp spectrum and C'() is the target spectrum, such as a Taylor window, but now additional terms have been included, namely, (() ) and A() which are the phase and amplitude characteristics that require compensation.\n\nA compressor chirp waveform that includes phase correction data will have additional ripple components present at each end of the waveform (pre-shoots and after-shoots). Any truncation procedure should not remove these new features.\n\nIn addition, it is easy to time shift the compressed pulses by ±t, by multiplying the compressor spectrum by the unity amplitude vector, i.e.\n\nTime shift can be useful to position the main lobes of compressed pulses at a standard location, regardless of chirp pulse length. However, care has to be taken with the overlap and save or overlap and discard algorithm, should time shift be used, to ensure only valid waveform sequences are retained.\n\nThere has been a growth in interest in adaptive filters for pulse compression, made possible by the availability of small fast computers, and some relevant articles are mentioned in the next section. These techniques will also compensate for hardware deficiencies, as part of their optimization procedure\n\nThe growth in digital processing and methods had a significant influence in the field of chirp pulse compression. An introduction to these techniques is provided in a chapter of the Radar Handbook (3rd ed.), edited by Skolnik.\n\nThe main aims of most investigations into pulse compression has been to obtain narrow main lobes, with low sidelobe levels, a tolerance to Doppler frequency shifts and to incur low system losses. The availability of computers has led to a growth in numerical processing and much interest in adaptive networks and optimization methods, to achieve these aims. For example, see the comparison of the various techniques made by Damtie and Lehtinen and, also, various articles by Blunt and Gerlach on these topics. A number of other contributors in the field has included Zrnic et al. Li et al. and Scholnik.\n\nA number of other works, with a variety of approaches to pulse compression, are listed below:\n- New methods of generating non linear chirp waveforms and of improving their Doppler tolerance has been investigated by Doerry\n- Further studies of Hyperbolic chirps have been carried out by Kiss, Readhead, Nagajyothi and Rajarajeswari and Yang and Sarkar.\n- Convolution windows have been investigated by Sahoo and Panda who show that they can result in very low sidelobes yet be Doppler tolerant, but may suffer from some pulse broadening. Wen and his co-workers have also discussed convolution windows.\n- Some new window functions have been proposed by Samad and Sinha and Ferreira, which claim improved performance over the familiar functions.\n- Several techniques to lower the sidelobe levels of the compressed pulses for non-linear FM chirps are compared by Varshney and Thomas.\n- In a paper by Vizitui, sidelobe reduction is considered where phase pre-distortion is applied to non-linear FM chirps, rather than to linear chirps. Lower sidelobes and some improvement in Doppler tolerance is claimed.\n\nThere have been extensive investigations of phase modulation for pulse compression schemes, such as biphase (binary Phase shift keying) and polyphase coding methods, but this work is not considered here.\n\n", "related": "\n- Chirp\n- Chirp spectrum\n- Pulse compression\n- Spread spectrum\n"}
{"id": "47076907", "url": "https://en.wikipedia.org/wiki?curid=47076907", "title": "Carrier frequency offset", "text": "Carrier frequency offset\n\nCarrier frequency offset (CFO) is one of many non-ideal conditions that may affect in baseband receiver design. In designing a baseband receiver, we should notice not only the degradation invoked by non-ideal channel and noise, we should also regard RF and analog parts as the main consideration. Those non-idealities include sampling clock offset, IQ imbalance, power amplifier, phase noise and carrier frequency offset nonlinearity.\n\nCarrier frequency offset often occurs when the local oscillator signal for down-conversion in the receiver does not synchronize with the carrier signal contained in the received signal. This phenomenon can be attributed to two important factors: frequency mismatch in the transmitter and the receiver oscillators; and the Doppler effect as the transmitter or the receiver is moving.\n\nWhen this occurs, the received signal will be shifted in frequency. For an OFDM system, the orthogonality among sub carriers is maintained only if the receiver uses a local oscillation signal that is synchronous with the carrier signal contained in the received signal. Otherwise, mismatch in carrier frequency can result in inter-carrier interference (ICI). The oscillators in the transmitter and the receiver can never be oscillating at identical frequency. Hence, carrier frequency offset always exists even if there is no Doppler effect.\n\nIn a standard-compliant communication system, such as the IEEE 802.11 WLAN the oscillator precision tolerance is specified to be less than ±20 ppm, so that CFO is in the range from - 40 ppm to +40 ppm.\n\nIf the TX oscillator runs at a frequency that is 20 ppm above the nominal frequency and if the RX oscillator is running at 20 ppm below, then the received baseband signal will have a CFO of 40 ppm. With a carrier frequency of 5.2 GHz in this standard, the CFO is up to ±208 kHz. In addition, if the transmitter or the receiver is moving, the Doppler effect adds some hundreds of hertz in frequency spreading.\n\nCompared to the CFO resulting from the oscillator mismatch, the Doppler effect in this case is relatively minor.\n\nGiven a carrier frequency offset,Δformula_1, the received continuous-time signal will be rotated by a constant frequency and is in the form of\n\nformula_2\n\nThe carrier frequency offset can first be normalized with respect to the sub carrier spacing (formula_3 and then decomposed into the integral component formula_4 and fractional component formula_5, that is, formula_6 and formula_7. The received frequency-domain signal then becomes\n\nformula_8\n\nThe second term of the equation denotes the ICI, namely signals from other subcarriers that interfere with the desired subcarrier signal. Also note that formula_9 is the channel noise component. The fractional carrier frequency offset, formula_10, results in attenuation in magnitude, phase shift, and ICI, while the integer carrier frequency offset, formula_11, causes index shift as well as phase shift in the received frequency-domain signals. Note that the phase shift is identical in every subcarrier and is also proportional to the symbol index formula_12.\n\nAn estimate of the CFO, if within a certain limit, can be obtained simultaneously when the coarse symbol timing is acquired by the algorithms mentioned earlier. The ML CFO estimator is given by\n\nformula_13\n\nNote that the phase can only be resolved in formula_14, and the above formula estimates only the part of the CFO that is within formula_15 formula_16. If formula_17, then formula_18, the part of the CFO that is within plus and minus half the subcarrier spacing, also known as the fractional CFO. In the case in which formula_19, frequency ambiguity occurs, and the total CFO must be resolved by additional integer CFO estimation.\n\nIf the preamble has U identical repetitions, where formula_20, then another best linear unbiased estimator (BLUE) exploiting the correlation of the repeated segments is possible. Assume that there are R samples in a segment, so, in total, formula_21 samples are available. The BLUE estimation algorithm starts with computing several linear auto-correlation functions with formula_22 samples of delay,\n\nformula_23\n\nThen the phase differences between all pairs of auto-correlation functions with delay difference formula_24 are computed,\n\nformula_25\n\nwhere formula_26 denotes a modulo-formula_27 operation and formula_28 is a design parameter less than formula_29. Note that each formula_30 represents an estimate of the CFO, scaled by a constant. The smaller the constant formula_31, the better accuracy it achieves. To gain an effective CFO estimate, the BLUE estimator uses a weighted average of all formula_30 and computes\n\nformula_33\n\nwhere\n\nformula_34\n\nThe optimal formula_28 value for achieving the minimal variance of formula_36 is formula_37. The range of estimated carrier frequency offset is formula_38.\n\nWith some modification, this estimator can also be applied to preambles consisting of several repeated segments with specific sign changes. With proper acquired symbol timing, the received formula_29 segments of the preamble are multiplied by their respective signs, and then the same method as the BLUE estimator can be applied.\n\nIn the IEEE 802.16e OFDM mode standard, the oscillator deviation is within ±8 ppm. With the highest possible carrier frequency of 10.68 GHz, the maximum CFO is about ± 171 kHz when the transmitter LO and the receiver LO both have the largest yet opposite-sign frequency deviations, which is also equivalent to ± 1 1 sub carrier spacing formula_40 . In the 6 MHz DVB-T system, assuming that the oscillator deviation is within ±20 ppm and the carrier frequency is around 800 MHz, the maximum CFO can be up to ±38 subcarrier spacing formula_40 in the 8K transmission mode. From the previous discussion, it is clear that the estimated CFO obtained simultaneously in the coarse symbol boundary detection has ambiguity in frequency. In the following, algorithms for resolving such frequency ambiguity in the estimated carrier frequency offset will be presented.\n\nIn the 802.16e OFDM mode, the initial estimated CFO is within formula_42. Besides this estimation, additional frequency offset of formula_43, formula_44, or formula_45, is possible given a CFO range of formula_46. In order to estimate this additional integer CFO, a matched filter matching the fractional CFO-compensated received signal against the modulated long preamble waveforms can be used. The coefficients of the matched filter are the complex conjugate of the long preamble and they are modulated by a sinusoidal wave whose frequency is a possible integer CFO mentioned above. The output of the matched filter will have a maximum peak value if its coefficients are modulated by the carrier with the correct integer CFO. It is possible to deploy one such matched filter for each possible integer CFO. In this case, seven matched filters are needed. However, we can use only one set of matched filter hardware that handles different integer CFOs sequentially. In addition, as suggested previously in the symbol timing detection subsection, the coefficients of the matched filter can be quantized to -1, 0, 1 to reduce hardware complexity.\n\nIn MIMO-OFDM systems, the transmit antennas are often co-located, so are the receive antennas.\n\nHence, it is valid to assume that only one oscillator is referenced in either the transmitter side or the receiver side. As a result, a single CFO set is to be estimated for the multiple receive antennas. The ML estimation for the fractional CFO is quite popular in MIMO-OFDM systems.\n\nAnother fractional CFO estimation algorithm for MIMO-OFDM systems applies different weights to the receive signals according to the respective degrees of channel fading\n\nThe preamble is designed so that each transmit antenna uses non-overlapping sub carriers to facilitate separation of signals from different transmit antennas. At each receive antenna, the cross-correlation between the received signal and the known preamble is examined.\n\nThe magnitude of the cross-correlation output reflects the channel fading between the corresponding transmit and receive antenna pair.\n\nBased on the channel fading information, weights are applied to the received signals to emphasize those with stronger channel gains and at the same time to suppress those that are deeply faded.\n\nThen, the CFO is estimated based on the phase of delay correlation of weighted signals.For integer CFO, frequency-domain cross-correlation and frequency-domain PN correlation can be used with slight modification. First, the received signals must be compensated by the estimated fractional CFO.\n\nThen, the compensated signals are transformed into the frequency domain. The frequency-domain cross-correlation algorithm for one specific receive antenna is similar to that in the SISO case\n\nAlthough the CFO in the received signal has been estimated and compensated in the receiver,some residual CFO may still exist. Besides, the CFO contained in the received signal may very well be time-varying and, thus, it needs to be continuously tracked.\n\nThe received signal also suffers from sampling clock offset (SCO), which may cause a gradual drift of the safe DFT window in addition to extra phase shift in the received frequency-domain signals. In framebased OFDM systems, both the residual CFO tracking and the SCO tracking are inevitable, because the receiver may operate for a long period of time. In packet-based OFDM systems, however, the influences of these two offsets depend on the packet length and the magnitude of the offsets.\n\nThe SCO may not be easily estimated from the time-domain signal. However, it can be examined through the phase shift of the frequency-domain pilot signals. The residual CFO can also be estimated in a similar way. In many OFDM wireless communication standards, for example, DVB-T, IEEE 802.11 a/g/n, and IEEE 802.16e OFDM mode, dedicated pilot subcarriers are allocated to facilitate receiver synchronization.\n\nThe phase shifts in the received frequency-domain signals caused by the CFO are identical at all subcarriers provided that the ICI is ignored. On the other hand, the SCO causes phase shifts that are proportional to the respective sub carrier indices.\n\nThe received signals contain ICI and noise, and therefore the phases deviate from the two ideal straight lines. Conventionally, the SCO can be estimated by computing a slope from the plot of measured pilot subcarrier phase differences versus pilot subcarrier indices. Moreover, joint estimation of CFO and SCO has also been studied extensively.\n\nIn order to suppress the ICI and thereby reduce SNR degradation, the residual CFO must be sufficiently small. For example, when using the 64QAM constellation, it is better to keep the residual CFO below 0. 01/s to ensure that DSNR < 0 . 3 dB for moderate SNR.\n\nOn the other hand, when QPSK is used, the residual CFO can be up to 0.03 fs.\n\n1. G. L. Stuber et ai., 2004. \"Broadband MIMO-OFDM wireless communications,\" Proceedings of the IEEE, 92,271-293.\n2. A. van Zelst and T. C. W. Schenk, 2004. \"Implementation of a MIMO OFDM-based wireless LAN system,\" IEEE Transactions on Signal Processing, 52, 483-494.\n3. E. Zhou, X. Zhang, H. Zhao, and W. Wang, 2005. \"Synchronization algorithms for MIMO OFDM systems,\" in Proceedings of the IEEE Wireless Communications and Networking Conference, March, pp. 18–22.\n4. P. Priotti, 2004. \" Frequency synchronization of MIMO OFDM systems with frequency selective weighting,\" in Proceedings of the IEEE Vehicular Technology Conference, vol. 2, May, pp. 1114–1 118.\n5. Baseband Receiver Design for Wireless MIMO-OFDM Communications\n", "related": "NONE"}
{"id": "4504771", "url": "https://en.wikipedia.org/wiki?curid=4504771", "title": "Frame (linear algebra)", "text": "Frame (linear algebra)\n\nIn linear algebra, a frame of an inner product space is a generalization of a basis of a vector space to sets that may be linearly dependent. In the terminology of signal processing, a frame provides a redundant, stable way of representing a signal. Frames are used in error detection and correction and the design and analysis of filter banks and more generally in applied mathematics, computer science, and engineering.\n\nSuppose we have a set of vectors formula_1 in the vector space \"V\" and we want to express an arbitrary element formula_2 as a linear combination of the vectors formula_3, that is, we want to find coefficients formula_4 such that\n\nIf the set formula_6 does not span formula_7, then such coefficients do not exist for every such formula_8. If formula_6 spans formula_7 and also is linearly independent, this set forms a basis of formula_7, and the coefficients formula_12 are uniquely determined by formula_8. If, however, formula_3 spans formula_7 but is not linearly independent, the question of how to determine the coefficients becomes less apparent, in particular if formula_7 is of infinite dimension.\n\nGiven that formula_1 spans formula_7 and is linearly dependent, one strategy is to remove vectors from the set until it becomes linearly independent and forms a basis. There are some problems with this plan:\n\n1. Removing arbitrary vectors from the set may cause it to be unable to span formula_7 before it becomes linearly independent.\n2. Even if it is possible to devise a specific way to remove vectors from the set until it becomes a basis, this approach may become unfeasible in practice if the set is large or infinite.\n3. In some applications, it may be an advantage to use more vectors than necessary to represent formula_8. This means that we want to find the coefficients formula_4 without removing elements in formula_1. The coefficients formula_4 will no longer be uniquely determined by formula_8. Therefore, the vector formula_8 can be represented as a linear combination of formula_3 in more than one way.\n\nLet \"V\" be an inner product space and formula_27 be a set of vectors in formula_7. These vectors satisfy the \"frame condition\" if there are positive real numbers \"A\" and \"B\" such that <math> 0 and for each formula_8 in \"V\",\nA set of vectors that satisfies the frame condition is a \"frame\" for the vector space.\n\nThe numbers \"A\" and \"B\" are called the lower and upper \"frame bounds\", respectively. The frame bounds are not unique because numbers less than \"A\" and greater than \"B\" are also valid frame bounds. The \"optimal lower bound\" is the supremum of all lower bounds and the \"optimal upper bound\" is the infimum of all upper bounds.\n\nA frame is called \"overcomplete\" (or \"redundant\") if it is not a basis for the vector space.\n\nThe operator mapping formula_2 to a sequence of coefficients formula_4 is called the \"analysis operator\" of the frame. It is defined by:\nBy using this definition we may rewrite the frame condition as\nwhere the left and right norms denote the norm in formula_7 and the middle norm is the formula_36 norm.\n\nThe adjoint operator formula_37 of the analysis operator is called the \"synthesis operator\" of the frame. \n\nWe want that any vector formula_39 can be reconstructed from the coefficients formula_40. This is satisfied if there exists a constant formula_41 such that for all formula_42 we have:\nBy setting formula_44 and applying the linearity of the analysis operator we get that this condition is equivalent to:\nfor all formula_39 which is exactly the lower frame bound condition.\n\nBecause of the various mathematical components surrounding frames, frame theory has roots in harmonic and functional analysis, operator theory, linear algebra, and matrix theory.\n\nThe Fourier transform has been used for over a century as a way of decomposing and expanding signals. However, the Fourier transform masks key information regarding the moment of emission and the duration of a signal. In 1946, Dennis Gabor was able to solve this using a technique that simultaneously reduced noise, provided resiliency, and created quantization while encapsulating important signal characteristics. This discovery marked the first concerted effort towards frame theory.\n\nThe frame condition was first described by Richard Duffin and Albert Charles Schaeffer in a 1952 article on nonharmonic Fourier series as a way of computing the coefficients in a linear combination of the vectors of a linearly dependent spanning set (in their terminology, a \"Hilbert space frame\"). In the 1980s, Stéphane Mallat, Ingrid Daubechies, and Yves Meyer used frames to analyze wavelets. Today frames are associated with wavelets, signal and image processing, and data compression.\n\nA frame satisfies a generalization of Parseval's identity, namely the frame condition, while still maintaining norm equivalence between a signal and its sequence of coefficients.\n\nIf the set formula_1 is a frame of \"V\", it spans \"V\". Otherwise there would exist at least one non-zero formula_2 which would be orthogonal to all formula_49. If we insert formula_8 into the frame condition, we obtain\ntherefore formula_52, which is a violation of the initial assumptions on the lower frame bound.\n\nIf a set of vectors spans \"V\", this is not a sufficient condition for calling the set a frame. As an example, consider formula_53 with the dot product, and the infinite set formula_1 given by\n\nThis set spans \"V\" but since formula_56, we cannot choose a finite upper frame bound \"B\". Consequently, the set formula_1 is not a frame.\n\nIn signal processing, each vector is interpreted as a signal. In this interpretation, a vector expressed as a linear combination of the frame vectors is a redundant signal. Using a frame, it is possible to create a simpler, more sparse representation of a signal as compared with a family of elementary signals (that is, representing a signal strictly with a set of linearly independent vectors may not always be the most compact form). Frames, therefore, provide \"robustness\". Because they provide a way of producing the same vector within a space, signals can be encoded in various ways. This facilitates fault tolerance and resilience to a loss of signal. Finally, redundancy can be used to mitigate noise, which is relevant to the restoration, enhancement, and reconstruction of signals.\n\nIn signal processing, it is common to assume the vector space is a Hilbert space.\n\nA frame is a \"tight frame\" if \"A\" = \"B\"; in other words, the frame satisfies a generalized version of Parseval's identity. For example, the union of \"k\" disjoint orthonormal bases of a vector space is a tight frame with \"A\" = \"B\" = \"k\". A tight frame is a \"Parseval frame\" (sometimes called a \"normalized frame\") if \"A\" = \"B\" = 1. Each orthonormal basis is a Parseval frame, but the converse is not always true.\n\nA frame formula_27 for formula_7 is tight with frame bound \"A\" if and only if\nfor all formula_61.\n\nA frame is an \"equal norm frame\" (sometimes called a \"uniform frame\" or a \"normalized frame\") if there is a constant \"c\" such that formula_62 for each \"i\". An equal norm frame is a \"unit norm frame\" if \"c\" = 1. A Parseval (or tight) unit norm frame is an orthonormal basis; such a frame satisfies Parseval's identity.\n\nA frame is an \"equiangular frame\" if there is a constant \"c\" such that formula_63 for each distinct \"i\" and \"j\".\n\nA frame is an \"exact frame\" if no proper subset of the frame spans the inner product space. Each basis for an inner product space is an exact frame for the space (so a basis is a special case of a frame).\n\nA \"Bessel Sequence\" is a set of vectors that satisfies only the upper bound of the frame condition.\n\nSuppose \"H\" is a Hilbert space, X a locally compact space , and formula_64 is a locally finite \"Borel measure\" on X. Then a set of vectors in \"H\", formula_65 with a measure formula_64 is said to be a \"Continuous Frame\" if there exists constants, formula_67 for all formula_68.\n\nGiven a discrete set formula_69 and a measure formula_70 where formula_71 is the \"Dirac measure\" then the continuous frame property:\nreduces to:\nformula_73\n\nand we see that Continuous Frames are indeed the natural generalization of the frames mentioned above.\n\nJust like in the discrete case we can define the Analysis, Synthesis, and Frame operators when dealing with continuous frames.\n\nGiven a continuous frame formula_65 the \"Continuous Analysis Operator\" is the operator mapping formula_65 to a sequence of coefficients formula_76.\n\nIt is defined as follows:\n\nformula_77 by formula_78\n\nThe adjoint operator of the Continuous Analysis Operator is the \"Continuous Synthesis Operator\" which is the map:\n\nformula_79 by formula_80\n\nThe Composition of the Continuous Analysis Operator and the Continuous Synthesis Operator is known as the \"Continuous Frame Operator\". For a continuous frame formula_65, the \"Continuous Frame Operator\" is defined as follows:\nformula_82 by formula_83\n\nGiven a continuous frame formula_65, and another continuous frame formula_85, then formula_85 is said to be a \"Continuous Dual Frame\" of formula_87 if it satisfies the following condition for all formula_88:\n\nformula_89\n\nThe frame condition entails the existence of a set of \"dual frame vectors\" formula_90 with the property that\nfor any formula_2. This implies that a frame together with its dual frame has the same property as a basis and its dual basis in terms of reconstructing a vector from scalar products.\n\nIn order to construct a dual frame, we first need the linear mapping formula_93, called the frame operator, defined as\n\nFrom this definition of formula_95 and linearity in the first argument of the inner product,\nwhich, when substituted in the frame condition inequality, yields\nfor each formula_2.\n\nThe frame operator formula_95 is self-adjoint, positive definite, and has positive upper and lower bounds. The inverse formula_100 of formula_95 exists and it, too, is self-adjoint, positive definite, and has positive upper and lower bounds.\n\nThe dual frame is defined by mapping each element of the frame with formula_100:\n\nTo see that this makes sense, let formula_8 be an element of formula_7 and let\n\nThus\n\nwhich proves that\n\nAlternatively, we can let\n\nBy inserting the above definition of formula_110 and applying the properties of formula_95 and its inverse,\n\nwhich shows that\n\nThe numbers formula_114 are called \"frame coefficients\". This derivation of a dual frame is a summary of Section 3 in the article by Duffin and Schaeffer. They use the term \"conjugate frame\" for what here is called a dual frame.\n\nThe dual frame formula_115 is called the \"canonical dual\" of formula_3 because it acts similarly as a dual basis to a basis.\n\nWhen the frame formula_3 is overcomplete, a vector formula_8 can be written as a linear combination of formula_3 in more than one way. That is, there are different choices of coefficients formula_120 such that formula_121. This allows us some freedom for the choice of coefficients formula_120 other than formula_114. It is necessary that the frame formula_3 is overcomplete for other such coefficients formula_120 to exist. If so, then there exist frames formula_126 for which \nfor all formula_2. We call formula_129 a dual frame of formula_3.\n\nCanonical duality is a reciprocity relation, i.e. if the frame formula_90 is the canonical dual frame of formula_6, then formula_6 is the canonical dual frame of formula_90.\n\n", "related": "\n- \"k\"-frame\n- Biorthogonal wavelet\n- Orthogonal wavelet\n- Restricted isometry property\n- Schauder basis\n- Harmonic analysis\n- Fourier analysis\n- Functional analysis\n\n"}
{"id": "5167862", "url": "https://en.wikipedia.org/wiki?curid=5167862", "title": "Frequency band", "text": "Frequency band\n\nA frequency band is an interval in the frequency domain, delimited by a lower frequency and an upper frequency. The term may refer to a radio band or an interval of some other spectrum. \n\nThe frequency range of a system is the range over which it is considered to provide satisfactory performance, such as a useful level of signal with acceptable distortion characteristics. A listing of the upper and lower limits of frequency limits for a system is not useful without a criterion for what the range represents.\n\nMany systems are characterized by the range of frequencies to which they respond. Musical instruments produce different ranges of notes within the hearing range. The electromagnetic spectrum can be divided into many different ranges such as visible light, infrared or ultraviolet radiation, radio waves, X-rays and so on, and each of these ranges can in turn be divided into smaller ranges. A radio communications signal must occupy a range of frequencies carrying most of its energy, called its bandwidth. A frequency band may represent one communication channel or be subdivided into many. Allocation of radio frequency ranges to different uses is a major function of radio spectrum allocation.\n", "related": "NONE"}
{"id": "47824717", "url": "https://en.wikipedia.org/wiki?curid=47824717", "title": "Echo removal", "text": "Echo removal\n\nEcho removal is the process of removing echo and reverberation artifacts from audio signals. The reverberation is typically modeled as the convolution of a (sometimes time-varying) impulse response with a hypothetical clean input signal, where both the clean input signal (which is to be recovered) and the impulse response are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the input signal to uniquely determine a plausible original image, making it an ill-posed problem. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions.\n\nThis problem is analogous to deblurring in the image processing domain.\n\n", "related": "\n- Echo suppression and cancellation\n- Digital room correction\n- Noise reduction\n- Linear prediction coder\n"}
{"id": "47801453", "url": "https://en.wikipedia.org/wiki?curid=47801453", "title": "Spectral correlation density", "text": "Spectral correlation density\n\nThe spectral correlation density (SCD), sometimes also called the cyclic spectral density or spectral correlation function, is a function that describes the cross-spectral density of all pairs of frequency-shifted versions of a time-series. The spectral correlation density applies only to cyclostationary processes because stationary processes do not exhibit spectral correlation. Spectral correlation has been used both in signal detection and signal classification. The spectral correlation density is closely related to each of the bilinear time-frequency distributions, but is not considered one of Cohen's class of distributions.\n\nThe cyclic auto-correlation function of a time-series formula_1 is calculated as follows:\n\nformula_2\n\nwhere (*) denotes complex conjugation. By the Wiener–Khinchin theorem, the spectral correlation density is then:\n\nformula_3\n\nThe SCD is estimated in the digital domain with an arbitrary resolution in frequency and time. There are several estimation methods currently used in practice to efficiently estimate the spectral correlation for use in real-time analysis of signals due to its high computational complexity. Some of the more popular ones are the FFT Accumulation Method (FAM) and the Strip-Spectral Correlation Algorithm. One method to calculate the spectral correlation density of a discrete-time signal formula_4 with sample period formula_5 is to first calculate the sliding window discrete Fourier transform:\n\nformula_6The variable \"N′\" determines the frequency resolution of the spectral correlation density and the function formula_7 is a window function of choice. Then, the spectral correlation density is calculated:\n\nformula_8\n\nA fast-spectral-correlation (FSC) algorithm has recently been introduced.\n\n- Napolitano, Antonio (2012-12-07). \"Generalizations of Cyclostationary Signal Processing: Spectral Analysis and Applications\". John Wiley & Sons. .\n- Pace, Phillip E. (2004-01-01). \"Detecting and Classifying Low Probability of Intercept Radar\". Artech House. .\n", "related": "NONE"}
{"id": "48513342", "url": "https://en.wikipedia.org/wiki?curid=48513342", "title": "Non-linear multi-dimensional signal processing", "text": "Non-linear multi-dimensional signal processing\n\nIn signal processing, nonlinear multidimensional signal processing (NMSP) covers all signal processing using nonlinear multidimensional signals and systems. Nonlinear multidimensional signal processing is a subset of signal processing (multidimensional signal processing). Nonlinear multi-dimensional systems can be used in a broad range such as imaging, teletraffic, communications, hydrology, geology, and economics. Nonlinear systems cannot be treated as linear systems, using Fourier transformation and wavelet analysis. Nonlinear systems will have chaotic behavior, limit cycle, steady state, bifurcation, multi-stability and so on. Nonlinear systems do not have a canonical representation, like impulse response for linear systems. But there are some efforts to characterize nonlinear systems, such as Volterra and Wiener series using polynomial integrals as the use of those methods naturally extend the signal into multi-dimensions. Another example is the Empirical mode decomposition method using Hilbert transform instead of Fourier Transform for nonlinear multi-dimensional systems. This method is an empirical method and can be directly applied to data sets. Multi-dimensional nonlinear filters (MDNF) are also an important part of NMSP, MDNF are mainly used to filter noise in real data. There are nonlinear-type hybrid filters used in color image processing, nonlinear edge-preserving filters use in magnetic resonance image restoration. Those filters use both temporal and spatial information and combine the maximum likelihood estimate with the spatial smoothing algorithm.\n\nA linear frequency response function (FRF) can be extended to a nonlinear system by evaluation of higher order transfer functions and impulse response functions by Volterra series. Suppose we have a time series formula_1, which is decomposed formula_1 into components of various order\n\nformula_3\n\nEach component is defined as\n\nformula_4 ,\n\nfor formula_5, formula_6 is the linear convolution. formula_7 is the generalized impulse response of order formula_8.\n\nThe 1D Fourier transform of formula_9 is\n\nformula_10\n\nSchetzen suggested the definition of formula_8th output component as formula_8 time variables formula_13 so as to permit the application of the formula_8-dimensional Fourier transform,\n\nformula_15\n\nTaking the inverse Fourier transform of formula_16 and formula_17 and equalizing formula_18, we obtain the following equation,\n\nformula_19\n\nApplying the formula_8th dimensional Fourier Transform to formula_7 obtain the transfer function\n\nformula_22\n\nOne example of nonlinear filters is the (generalized directional distance rational hybrid filter (GDDRHF)) for multidimensional signal processing. This filter is a two-stage type hybrid filter: 1) the stage formula_23 norm criteria and angular distance criteria to produce three output vectors with respect to the shape models; 2) the stage performs vector rational operation on the above three output vectors to produce the final output vectors. The output vector formula_24 of the GDDRHF is the result of a vector rational function taking into account three input sub-function which form an input function set formula_25,\n\nformula_26\n\nwhere formula_27 plays an important role as an edge sensing term, formula_28 characterizes the constant vector coefficient of the input sub-functions. formula_29 and formula_30 are some positive constants. The parameter formula_30 is used to control the amount of the nonlinear effect.\n\nThis kind of multidimensional filter has been used for MRI imaging processing. This filter uses MRI signal models to implement an approximate maximum likelihood or least squares estimate of each pixel gray level from the gray levels. It is also employs a trimmed mean spatial smoothing algorithm that uses a Euclidean distance discriminator to preserve partial volume and edge information; corresponds to using intra frame information .\n\nA multi-dimensional ensemble empirical mode decomposition method was applied to multi-dimensional data including images and solid with variable density. The decomposition is based on the application of ensemble empirical mode decomposition (EEMD) to slices of data in each and every dimension involved. The final reconstruction of the corresponding intrinsic mode function is based on a comparable minimal scale combination principle.\n\nFor a two-dimensional signal formula_32 using EEMD, the signal is first decomposed the y-direction to obtain formula_33, each row of formula_33 is decomposed using EEMD.\n\nLet formula_35 be sampled as formula_36\n\nThe EEMD decomposition of the formula_37th column of formula_32 is\n\nformula_39\n\nafter all the columns are decomposed we get formula_40 th matrix being\n\nformula_41\n\nThis is the formula_42 component of the original data formula_32\n\nformula_44 row of formula_33 decomposition using EEMD is\n\nformula_46\n\nrearrange the component as\n\nformula_47\n\nSo formula_48 For a multi-dimension decomposition with an formula_8-dimensional function we can use the same method above.\n", "related": "NONE"}
{"id": "48972185", "url": "https://en.wikipedia.org/wiki?curid=48972185", "title": "Block transform", "text": "Block transform\n\nWavelet packet bases are designed by dividing the frequency axis in intervals of varying sizes. These bases are particularly well adapted to decomposing signals that have different behavior in different frequency intervals. If formula_1 has properties that vary in time, it is then more appropriate to decompose formula_1 in a block basis that segments the time axis in intervals with sizes that are adapted to the signal structures.\n\nBlock orthonormal bases are obtained by dividing the time axis in consecutive intervals formula_3 with\n\nformula_4 and formula_5.\n\nThe size formula_6 of each interval is arbitrary. Let formula_7. An interval is covered by the dilated rectangular window\n\nformula_8\n\nTheorem 1. constructs a block orthogonal basis of formula_9 from a single orthonormal basis of formula_10.\n\nif formula_11 is an orthonormal basis of formula_10, then\n\nformula_13\n\nis a block orthonormal basis of formula_9\n\nOne can verify that the dilated and translated family\n\nformula_15\n\nis an orthonormal basis of formula_16. If formula_17, then formula_18 since their supports do not overlap. Thus, the family formula_13 is orthonormal. To expand a signal formula_1 in this family, it is decomposed as a sum of separate blocks\n\nformula_21\n\nand each block formula_22 is decomposed in the basis formula_15\n\nA block basis is constructed with the Fourier basis of formula_10:\n\nformula_25\n\nThe time support of each block Fourier vector formula_26 is formula_3 of size formula_28. The Fourier transform of formula_29 is\n\nformula_30\n\nand\n\nformula_31\n\nIt is centered at formula_32 and has a slow asymptotic decay proportional to formula_33 Because of this poor frequency localization, even though a signal formula_1 is smooth, its decomposition in a block Fourier basis may include large high-frequency coefficients. This can also be interpreted as an effect of periodization.\n\nFor all formula_35, we suppose that formula_36. Discrete block bases are built with discrete rectangular windows having supports on intervals formula_37:\n\nformula_38.\n\nSince dilations are not defined in a discrete framework, we generally cannot derive bases of intervals of varying sizes from a single basis. Thus,Theorem 2. supposes that we can construct an orthonormal basis of formula_39 for any formula_40. The proof is straightforward.\n\nSuppose that formula_41 is an orthogonal basis of formula_39 for any formula_40. The family\n\nformula_44\n\nis a block orthonormal basis of formula_45.\n\nA discrete block basis is constructed with discrete Fourier bases\n\nformula_46\n\nThe resulting block Fourier vectors formula_26 have sharp transitions at the window border, and thus are not well localized in frequency. As in the continuous case, the decomposition of smooth signals formula_1 may produce large-amplitude, high-frequency coefficients because of border effects.\n\nGeneral block bases of images are constructed by partitioning the plane formula_49 into rectangles formula_50 of arbitrary length formula_51 and width formula_52. Let formula_53 be an orthonormal basis of formula_10 and formula_29. We denote\n\nformula_56.\n\nThe family formula_57 is an orthonormal basis of formula_58.\n\nFor discrete images,we define discrete windows that cover each rectangle\n\nformula_59.\n\nIf formula_41 is an orthogonal basis of formula_39 for any formula_40, then\n\nformula_63\n\nis a block basis of formula_64\n\n1. St´ephane Mallat, A Wavelet Tour of Signal Processing, 3rd\n", "related": "NONE"}
{"id": "3352292", "url": "https://en.wikipedia.org/wiki?curid=3352292", "title": "Wavelet transform", "text": "Wavelet transform\n\nIn mathematics, a wavelet series is a representation of a square-integrable (real- or complex-valued) function by a certain orthonormal series generated by a wavelet. This article provides a formal, mathematical definition of an orthonormal wavelet and of the integral wavelet transform. \n\nA function formula_1 is called an orthonormal wavelet if it can be used to define a Hilbert basis, that is a complete orthonormal system, for the Hilbert space formula_2 of square integrable functions.\n\nThe Hilbert basis is constructed as the family of functions formula_3 by means of dyadic translations and dilations of formula_4,\n\nfor integers formula_6.\n\nIf under the standard inner product on formula_2,\n\nthis family is orthonormal, it is an orthonormal system:\n\nwhere formula_10 is the Kronecker delta.\n\nCompleteness is satisfied if every function formula_11 may be expanded in the basis as\n\nwith convergence of the series understood to be convergence in norm. Such a representation of \"f\" is known as a wavelet series. This implies that an orthonormal wavelet is self-dual.\n\nThe integral wavelet transform is the integral transform defined as\n\nThe wavelet coefficients formula_14 are then given by\n\nHere, formula_16 is called the binary dilation or dyadic dilation, and formula_17 is the binary or dyadic position.\n\nThe fundamental idea of wavelet transforms is that the transformation should allow only changes in time extension, but not shape. This is affected by choosing suitable basis functions that allow for this. Changes in the time extension are expected to conform to the corresponding analysis frequency of the basis function. Based on the uncertainty principle of signal processing,\n\nwhere t represents time and ω angular frequency (ω = 2πf, where f is temporal frequency).\n\nThe higher the required resolution in time, the lower the resolution in frequency has to be. The larger the extension of the analysis windows is chosen, the larger is the value of formula_19.\n\nWhen Δt is large,\n1. Bad time resolution\n2. Good frequency resolution\n3. Low frequency, large scaling factor\n\nWhen Δt is small\n1. Good time resolution\n2. Bad frequency resolution\n3. High frequency, small scaling factor\n\nIn other words, the basis function Ψ can be regarded as an impulse response of a system with which the function x(t) has been filtered. The transformed signal provides information about the time and the frequency. Therefore, wavelet-transformation contains information similar to the short-time-Fourier-transformation, but with additional special properties of the wavelets, which show up at the resolution in time at higher analysis frequencies of the basis function. The difference in time resolution at ascending frequencies for the Fourier transform and the wavelet transform is shown below. Note however, that the frequency resolution is decreasing for increasing frequencies while the temporal resolution increases. This consequence of Heisenberg's uncertainty principle is not correctly displayed in the Figure.\n\nThis shows that wavelet transformation is good in time resolution of high frequencies, while for slowly varying functions, the frequency resolution is remarkable.\n\nAnother example: The analysis of three superposed sinusoidal signals formula_20 with STFT and wavelet-transformation.\n\nWavelet compression is a form of data compression well suited for image compression (sometimes also video compression and audio compression). Notable implementations are JPEG 2000, DjVu and ECW for still images, CineForm, and the BBC's Dirac. The goal is to store image data in as little space as possible in a file. Wavelet compression can be either lossless or lossy. Wavelet coding is a variant of discrete cosine transform (DCT) coding that uses wavelets instead of DCT's block-based algorithm.\n\nUsing a wavelet transform, the wavelet compression methods are adequate for representing transients, such as percussion sounds in audio, or high-frequency components in two-dimensional images, for example an image of stars on a night sky. This means that the transient elements of a data signal can be represented by a smaller amount of information than would be the case if some other transform, such as the more widespread discrete cosine transform, had been used.\n\nDiscrete wavelet transform has been successfully applied for the compression of electrocardiograph (ECG) signals In this work, the high correlation between the corresponding wavelet coefficients of signals of successive cardiac cycles is utilized employing linear prediction.\nWavelet compression is not good for all kinds of data: transient signal characteristics mean good wavelet compression, while smooth, periodic signals are better compressed by other methods, particularly traditional harmonic compression (frequency domain, as by Fourier transforms and related).\n\nSee Diary Of An x264 Developer: The problems with wavelets (2010) for discussion of practical issues of current methods using wavelets for video compression.\n\nFirst a wavelet transform is applied. This produces as many coefficients as there are pixels in the image (i.e., there is no compression yet since it is only a transform). These coefficients can then be compressed more easily because the information is statistically concentrated in just a few coefficients. This principle is called transform coding. After that, the coefficients are quantized and the quantized values are entropy encoded and/or run length encoded.\n\nA few 1D and 2D applications of wavelet compression use a technique called \"wavelet footprints\".\n\nWavelets have some slight benefits over Fourier transforms in reducing computations when examining specific frequencies. However, they are rarely more sensitive, and indeed, the common Morlet wavelet is mathematically identical to a short-time Fourier transform using a Gaussian window function. The exception is when searching for signals of a known, non-sinusoidal shape (e.g., heartbeats); in that case, using matched wavelets can outperform standard STFT/Morlet analyses.\n\nThe wavelet transform can provide us with the frequency of the signals and the time associated to those frequencies, making it very convenient for its application in numerous fields. For instance, signal processing of accelerations for gait analysis, for fault detection, for design of low power pacemakers and also in ultra-wideband (UWB) wireless communications.\n\n\\cdot\\Psi\\left[\\frac{k - m c_0^n}{c_0^n}T\\right] = \\frac{1}{\\sqrt{c_0^n}}\\cdot\\Psi\\left[\\left(\\frac{k}{c_0^n} - m\\right)T\\right]</math>\n\nSuch discrete wavelets can be used for the transformation:\n\nAs apparent from wavelet-transformation representation (shown below) \n\nwhere c is scaling factor, τ represents time shift factor\n\nand as already mentioned in this context, the wavelet-transformation corresponds to a convolution of a function y(t) and a wavelet-function. A convolution can be implemented as a multiplication in the frequency domain. With this the following approach of implementation results into:\n- Fourier-transformation of signal y(k) with the FFT\n- Selection of a discrete scaling factor formula_23\n- Scaling of the wavelet-basis-function by this factor formula_23 and subsequent FFT of this function\n- Multiplication with the transformed signal YFFT of the first step\n- Inverse transformation of the product into the time domain results in \"Yformula_25\" for different discrete values of τ and a discrete value of formula_23\n- Back to the second step, until all discrete scaling values for formula_23are processed\n\nThere are many different types of wavelet transforms for specific purposes. See also a full list of wavelet-related transforms but the common ones are listed below: Mexican hat wavelet, Haar Wavelet, Daubechies wavelet, triangular wavelet.\n\n", "related": "\n- Continuous wavelet transform\n- Discrete wavelet transform\n- Complex wavelet transform\n- Constant-Q transform\n- Stationary wavelet transform\n- Dual wavelet\n- Multiresolution analysis\n- MrSID, the image format developed from original wavelet compression research at Los Alamos National Laboratory (LANL).\n- ECW, a wavelet-based geospatial image format designed for speed and processing efficiency\n- JPEG 2000, a wavelet-based image compression standard\n- DjVu format uses wavelet-based IW44 algorithm for image compression\n- scaleograms, a type of spectrogram generated using wavelets instead of a short-time Fourier transform.\n- Wavelet\n- Haar wavelet\n- Daubechies wavelet\n- Morlet wavelet\n- Gabor wavelet\n- Chirplet transform\n- Time-frequency representation\n- S transform\n- Set partitioning in hierarchical trees\n- Short-time Fourier transform\n\n\n"}
{"id": "155319", "url": "https://en.wikipedia.org/wiki?curid=155319", "title": "Data acquisition", "text": "Data acquisition\n\nData acquisition is the process of sampling signals that measure real world physical conditions and converting the resulting samples into digital numeric values that can be manipulated by a computer. Data acquisition systems, abbreviated by the initialisms \"DAS\" or \"DAQ\", typically convert analog waveforms into digital values for processing. The components of data acquisition systems include:\n\n- Sensors, to convert physical parameters to electrical signals.\n- Signal conditioning circuitry, to convert sensor signals into a form that can be converted to digital values.\n- Analog-to-digital converters, to convert conditioned sensor signals to digital values.\n\nData acquisition applications are usually controlled by software programs developed using various general purpose programming languages such as Assembly, BASIC, C, C++, C#, Fortran, Java, LabVIEW, Lisp, Pascal, etc. Stand-alone data acquisition systems are often called data loggers.\n\nThere are also open-source software packages providing all the necessary tools to acquire data from different hardware equipment. These tools come from the scientific community where complex experiment requires fast, flexible and adaptable software. Those packages are usually custom fit but more general DAQ packages like the Maximum Integrated Data Acquisition System can be easily tailored and is used in several physics experiments worldwide.\n\nIn 1963, IBM produced computers which specialized in data acquisition. These include the IBM 7700 Data Acquisition System, and its successor, the IBM 1800 Data Acquisition and Control System. These expensive specialized systems were surpassed in 1974 by general purpose S-100 computers and data acquisitions cards produced by Tecmar/Scientific Solutions Inc. In 1981 IBM introduced the IBM Personal Computer and Scientific Solutions introduced the first PC data acquisition products.\n\nData acquisition begins with the physical phenomenon or physical property to be measured. Examples of this include temperature, light intensity, gas pressure, fluid flow, and force. Regardless of the type of physical property to be measured, the physical state that is to be measured must first be transformed into a unified form that can be sampled by a data acquisition system. The task of performing such transformations falls on devices called \"sensors\". A data acquisition system is a collection of software and hardware that allows one to measure or control physical characteristics of something in the real world. A complete data acquisition system consists of DAQ hardware, sensors and actuators, signal conditioning hardware, and a computer running DAQ software.\n\nA sensor, which is a type of \"transducer\", is a device that converts a physical property into a corresponding electrical signal (e.g., strain gauge, thermistor). An acquisition system to measure different properties depends on the sensors that are suited to detect those properties. Signal conditioning may be necessary if the signal from the transducer is not suitable for the DAQ hardware being used. The signal may need to be filtered or amplified in most cases. Various other examples of signal conditioning might be bridge completion, providing current or voltage excitation to the sensor, isolation, linearization. For transmission purposes, single ended analog signals, which are more susceptible to noise can be converted to differential signals. Once digitized, the signal can be encoded to reduce and correct transmission errors.\n\nDAQ hardware is what usually interfaces between the signal and a PC. It could be in the form of modules that can be connected to the computer's ports (parallel, serial, USB, etc.) or cards connected to slots (S-100 bus, AppleBus, ISA, MCA, PCI, PCI-E, etc.) in the motherboard. Usually the space on the back of a PCI card is too small for all the connections needed, so an external breakout box is required. The cable between this box and the PC can be expensive due to the many wires, and the required shielding. \n\nDAQ cards often contain multiple components (multiplexer, ADC, DAC, TTL-IO, high speed timers, RAM). These are accessible via a bus by a microcontroller, which can run small programs. A controller is more flexible than a hard wired logic, yet cheaper than a CPU so that it is permissible to block it with simple polling loops. For example:\nWaiting for a trigger, starting the ADC, looking up the time, waiting for the ADC to finish, move value to RAM, switch multiplexer, get TTL input, let DAC proceed with voltage ramp.\n\nDAQ device drivers are needed in order for the DAQ hardware to work with a PC. The device driver performs low-level register writes and reads on the hardware, while exposing API for developing user applications in a variety of programs.\n\n- 3D scanner\n- Analog-to-digital converter\n- Time-to-digital converter\n\n- Computer Automated Measurement and Control (CAMAC)\n- Industrial Ethernet\n- Industrial USB\n- LAN eXtensions for Instrumentation\n- NIM\n- PowerLab\n- PCI eXtensions for Instrumentation\n- VMEbus\n- VXI\n\nSpecialized DAQ software may be delivered with the DAQ hardware. Software tools used for building large-scale data acquisition systems include EPICS. Other programming environments that are used to build DAQ applications include ladder logic, Visual C++, Visual Basic, LabVIEW, and MATLAB.\nSee also:\n- LabChart\n- MIDAS\n- BioChart\n- PowerChrom\n\n- Tomaž Kos, Tomaž Kosar, and Marjan Mernik. Development of data acquisition systems by using a domain-specific modeling language. \"Computers in Industry\", 63(3):181–192, 2012.\n\n", "related": "\n- Black box\n- Data logger\n- Data storage device\n- Sensor\n- Signal processing\n- Transducer\n\n- Visual DAQ - An Analytics Platform (Software) for Connecting DAQ Systems\n- DAS DATA - Cloud Platform (Software) for connecting Internet of Things]\n"}
{"id": "41222", "url": "https://en.wikipedia.org/wiki?curid=41222", "title": "Group delay and phase delay", "text": "Group delay and phase delay\n\nIn signal processing, group delay is the time delay of the amplitude envelopes of the various sinusoidal components of a signal through a device under test, and is a function of frequency for each component. Phase delay, in contrast, is the time delay of the \"phase\" as opposed to the time delay of the \"amplitude envelope\".\n\nAll frequency components of a signal are delayed when passed through a device such as an amplifier, a loudspeaker, or propagating through space or a medium, such as air. This signal delay will be different for the various frequencies unless the device has the property of being linear phase. The delay variation means that signals consisting of multiple frequency components will suffer distortion because these components are not delayed by the same amount of time at the output of the device. This changes the shape of the signal in addition to any constant delay or scale change. A sufficiently large delay variation can cause problems such as poor fidelity in audio or intersymbol interference (ISI) in the demodulation of digital information from an analog carrier signal. High speed modems use adaptive equalizers to compensate for non-constant group delay.\n\nGroup delay is a useful measure of time distortion, and is calculated by differentiating, with respect to frequency, the phase response of the device under test (DUT): the group delay is a measure of the slope of the phase response at any given frequency. Variations in group delay cause signal distortion, just as deviations from linear phase cause distortion.\n\nIn linear time-invariant (LTI) system theory, control theory, and in digital or analog signal processing, the relationship between the input signal, formula_1, to output signal, formula_2, of an LTI system is governed by a convolution operation:\n\nOr, in the frequency domain,\n\nwhere\n\nand\n\nHere formula_8 is the time-domain impulse response of the LTI system and formula_9, formula_10, formula_11, are the Laplace transforms of the input formula_1, output formula_2, and impulse response formula_8, respectively. formula_11 is called the transfer function of the LTI system and, like the impulse response formula_8, \"fully\" defines the input-output characteristics of the LTI system.\n\nSuppose that such a system is driven by a quasi-sinusoidal signal, that is a sinusoid having an amplitude envelope formula_17 that is slowly changing relative to the frequency formula_18 of the sinusoid. Mathematically, this means that the quasi-sinusoidal driving signal has the form\n\nand the slowly changing amplitude envelope formula_20 means that\n\nThen the output of such an LTI system is very well approximated as\n\nHere formula_23 and formula_24, the group delay and phase delay respectively, are given by the expressions below (and potentially are functions of the angular frequency formula_18). The sinusoid, as indicated by the zero crossings, is delayed in time by phase delay, formula_24. The envelope of the sinusoid is delayed in time by the group delay, formula_23. \n\nIn a linear phase system (with non-inverting gain), both formula_23 and formula_24 are constant (i.e. independent of formula_18) and equal, and their common value equals the overall delay of the system; and the unwrapped phase shift of the system (namely formula_31) is negative, with magnitude increasing linearly with frequency formula_18.\n\nMore generally, it can be shown that for an LTI system with transfer function formula_11 driven by a complex sinusoid of unit amplitude,\n\nthe output is\n\nwhere the phase shift formula_36 is\n\nAdditionally, it can be shown that the group delay, formula_23, and phase delay, formula_24, are frequency-dependent, and they can be computed from the properly unwrapped phase shift formula_36 by\n\nIn physics, and in particular in optics, the term group delay has the following meanings:\n\nIt is often desirable for the group delay to be constant across all frequencies; otherwise there is temporal smearing of the signal. Because group delay is formula_48, as defined in (1), it therefore follows that a constant group delay can be achieved if the transfer function of the device or medium has a linear phase response (i.e., formula_49 where the group delay formula_50 is a constant).\nThe degree of nonlinearity of the phase indicates the deviation of the group delay from a constant.\n\nGroup delay has some importance in the audio field and especially in the sound reproduction field. Many components of an audio reproduction chain, notably loudspeakers and multiway loudspeaker crossover networks, introduce group delay in the audio signal. It is therefore important to know the threshold of audibility of group delay with respect to frequency, especially if the audio chain is supposed to provide high fidelity reproduction. The best thresholds of audibility table has been provided by .\n\nFlanagan, Moore and Stone conclude that at 1, 2 and 4 kHz, a group delay of about 1.6 ms is audible with headphones in a non-reverberant condition.\n\nA transmitting apparatus is said to have true time delay (TTD) if the time delay is independent of the frequency of the electrical signal. TTD is an important characteristic of lossless and low-loss, dispersion free, transmission lines. TTD allows for a wide instantaneous signal bandwidth with virtually no signal distortion such as pulse broadening during pulsed operation.\n\n", "related": "\n- Audio system measurements\n- Bessel filter\n- Eye pattern\n- Group velocity — \"The group velocity of light in a medium is the inverse of the group delay per unit length.\"\n- Spectral phase — \"The group delay can be defined as the derivative of the spectral phase with respect to angular frequency.\"\n\n- Discussion of Group Delay in Loudspeakers\n- Group Delay Explanations and Applications\n- Blauert, J.; Laws, P. (May 1978), \"Group Delay Distortions in Electroacoustical Systems\", Journal of the Acoustical Society of America 63 (5): 1478–1483\n- \"Introduction to Digital Filters with Audio Applications\", Julius O. Smith III, (September 2007 Edition).\n"}
{"id": "81242", "url": "https://en.wikipedia.org/wiki?curid=81242", "title": "Pulse-width modulation", "text": "Pulse-width modulation\n\nPulse width modulation (PWM), or pulse-duration modulation (PDM), is a method of reducing the average power delivered by an electrical signal, by effectively chopping it up into discrete parts. The average value of voltage (and current) fed to the load is controlled by turning the switch between supply and load on and off at a fast rate. The longer the switch is on compared to the off periods, the higher the total power supplied to the load. Along with MPPT maximum power point tracking, it is one of the primary methods of reducing the output of solar panels to that which can be utilized by a battery. PWM is particularly suited for running inertial loads such as motors, which are not as easily affected by this discrete switching, because they have inertia to react slow. The PWM switching frequency has to be high enough not to affect the load, which is to say that the resultant waveform perceived by the load must be as smooth as possible. \n\nThe rate (or frequency) at which the power supply must switch can vary greatly depending on load and application. For example, switching has to be done several times a minute in an electric stove; 120 Hz in a lamp dimmer; between a few kilohertz (kHz) and tens of kHz for a motor drive; and well into the tens or hundreds of kHz in audio amplifiers and computer power supplies. The main advantage of PWM is that power loss in the switching devices is very low. When a switch is off there is practically no current, and when it is on and power is being transferred to the load, there is almost no voltage drop across the switch. Power loss, being the product of voltage and current, is thus in both cases close to zero. PWM also works well with digital controls, which, because of their on/off nature, can easily set the needed duty cycle. PWM has also been used in certain communication systems where its duty cycle has been used to convey information over a communications channel.\n\nThe term \"duty cycle\" describes the proportion of 'on' time to the regular interval or 'period' of time; a low duty cycle corresponds to low power, because the power is off for most of the time. Duty cycle is expressed in percent, 100% being fully on. When a digital signal is on half of the time and off the other half of the time, the digital signal has a duty cycle of 50% and resembles a \"square\" wave. When a digital signal spends more time in the on state than the off state, it has a duty cycle of >50%. When a digital signal spends more time in the off state than the on state, it has a duty cycle of <50%. Here is a pictorial that illustrates these three scenarios:\nSome machines (such as a sewing machine motor) require partial or variable power. In the past, control (such as in a sewing machine's foot pedal) was implemented by use of a rheostat connected in series with the motor to adjust the amount of current flowing through the motor. It was an inefficient scheme, as this also wasted power as heat in the resistor element of the rheostat, but tolerable because the total power was low. While the rheostat was one of several methods of controlling power (see autotransformers and Variac for more info), a low cost and efficient power switching/adjustment method was yet to be found. This mechanism also needed to be able to drive motors for fans, pumps and robotic servos, and needed to be compact enough to interface with lamp dimmers. PWM emerged as a solution for this complex problem.\n\nOne early application of PWM was in the Sinclair X10, a 10 W audio amplifier available in kit form in the 1960s. At around the same time PWM started to be used in AC motor control.\n\nOf note, for about a century, some variable-speed electric motors have had decent efficiency, but they were somewhat more complex than constant-speed motors, and sometimes required bulky external electrical apparatus, such as a bank of variable power resistors or rotating converters such as the Ward Leonard drive.\n\nPulse-width modulation uses a rectangular pulse wave whose pulse width is modulated resulting in the variation of the average value of the waveform. If we consider a pulse waveform formula_1, with period formula_2, low value formula_3, a high value formula_4 and a duty cycle D (see figure 1), the average value of the waveform is given by:\n\nAs formula_1 is a pulse wave, its value is formula_4 for formula_8 and formula_3 for formula_10. The above expression then becomes:\n\nThis latter expression can be fairly simplified in many cases where formula_12 as formula_13. From this, the average value of the signal (formula_14) is directly dependent on the duty cycle D.\n\nThe simplest way to generate a PWM signal is the intersective method, which requires only a sawtooth or a triangle waveform (easily generated using a simple oscillator) and a comparator. When the value of the reference signal (the red sine wave in figure 2) is more than the modulation waveform (blue), the PWM signal (magenta) is in the high state, otherwise it is in the low state.\n\nIn the use of delta modulation for PWM control, the output signal is integrated, and the result is compared with limits, which correspond to a Reference signal offset by a constant. Every time the integral of the output signal reaches one of the limits, the PWM signal changes state. Figure 3\n\nIn delta-sigma modulation as a PWM control method, the output signal is subtracted from a reference signal to form an error signal. This error is integrated, and when the integral of the error exceeds the limits, the output changes state. Figure 4\n\nSpace vector modulation is a PWM control algorithm for multi-phase AC generation, in which the reference signal is sampled regularly; after each sample, non-zero active switching vectors adjacent to the reference vector and one or more of the zero switching vectors are selected for the appropriate fraction of the sampling period in order to synthesize the reference signal as the average of the used vectors.\n\nDirect torque control is a method used to control AC motors. It is closely related with the delta modulation (see above). Motor torque and magnetic flux are estimated and these are controlled to stay within their hysteresis bands by turning on a new combination of the device's semiconductor switches each time either signal tries to deviate out of its band.\n\nMany digital circuits can generate PWM signals (e.g., many microcontrollers have PWM outputs). They normally use a counter that increments periodically (it is connected directly or indirectly to the clock of the circuit) and is reset at the end of every period of the PWM. When the counter value is more than the reference value, the PWM output changes state from high to low (or low to high). This technique is referred to as time proportioning, particularly as time-proportioning control – which \"proportion\" of a fixed cycle time is spent in the high state.\n\nThe incremented and periodically reset counter is the discrete version of the intersecting method's sawtooth. The analog comparator of the intersecting method becomes a simple integer comparison between the current counter value and the digital (possibly digitized) reference value. The duty cycle can only be varied in discrete steps, as a function of the counter resolution. However, a high-resolution counter can provide quite satisfactory performance.\n\nThree types of pulse-width modulation (PWM) are possible:\n1. The pulse center may be fixed in the center of the time window and both edges of the pulse moved to compress or expand the width.\n2. The lead edge can be held at the lead edge of the window and the tail edge modulated.\n3. The tail edge can be fixed and the lead edge modulated.\n\nThe resulting spectra (of the three cases) are similar, and each contains a dc component—a base sideband containing the modulating signal and phase modulated carriers at each harmonic of the frequency of the pulse. The amplitudes of the harmonic groups are restricted by a formula_15 envelope (sinc function) and extend to infinity.\nThe infinite bandwidth is caused by the nonlinear operation of the pulse-width modulator. In consequence, a digital PWM suffers from aliasing distortion that significantly reduce its applicability for modern communications system. By limiting the bandwidth of the PWM kernel, aliasing effects can be avoided.\n\nOn the contrary, the delta modulation is a random process that produces continuous spectrum without distinct harmonics.\n\nThe process of PWM conversion is non-linear and it is generally supposed that low pass filter signal recovery is imperfect for PWM. The PWM sampling theorem shows that PWM conversion can be perfect. The theorem states that \"Any bandlimited baseband signal within ±0.637 can be represented by a pulsewidth modulation (PWM) waveform with unit amplitude. The number of pulses in the waveform is equal to the number of Nyquist samples and the peak constraint is independent of whether the waveform is two-level or three-level.\"\n\n• Nyquist-Shannon Sampling Theorem:\n“If you have a signal that is perfectly band limited to a bandwidth of f then you can collect all the\ninformation there is in that signal by sampling it at discrete times, as long as your sample\nrate is greater than 2f.”\n\nPWM is used to control servomechanisms; see servo control.\n\nIn telecommunications, PWM is a form of signal modulation where the widths of the pulses correspond to specific data values encoded at one end and decoded at the other.\n\nPulses of various lengths (the information itself) will be sent at regular intervals (the carrier frequency of the modulation).\n\nThe inclusion of a clock signal is not necessary, as the leading edge of the data signal can be used as the clock if a small offset is added to the data value in order to avoid a data value with a zero length pulse.\n\nPWM can be used to control the amount of power delivered to a load without incurring the losses that would result from linear power delivery by resistive means. Drawbacks to this technique are that the power drawn by the load is not constant but rather discontinuous (see Buck converter), and energy delivered to the load is not continuous either. However, the load may be inductive, and with a sufficiently high frequency and when necessary using additional passive electronic filters, the pulse train can be smoothed and average analog waveform recovered. Power flow into the load can be continuous. Power flow from the supply is not constant and will require energy storage on the supply side in most cases. (In the case of an electrical circuit, a capacitor to absorb energy stored in (often parasitic) supply side inductance.)\n\nHigh frequency PWM power control systems are easily realisable with semiconductor switches. As explained above, almost no power is dissipated by the switch in either on or off state. However, during the transitions between on and off states, both voltage and current are nonzero and thus power is dissipated in the switches. By quickly changing the state between fully on and fully off (typically less than 100 nanoseconds), the power dissipation in the switches can be quite low compared to the power being delivered to the load.\n\nModern semiconductor switches such as MOSFETs or insulated-gate bipolar transistors (IGBTs) are well suited components for high-efficiency controllers. Frequency converters used to control AC motors may have efficiencies exceeding 98%. Switching power supplies have lower efficiency due to low output voltage levels (often even less than 2 V for microprocessors are needed) but still more than 70–80% efficiency can be achieved.\n\nVariable-speed computer fan controllers usually use PWM, as it is far more efficient when compared to a potentiometer or rheostat. (Neither of the latter is practical to operate electronically; they would require a small drive motor.)\n\nLight dimmers for home use employ a specific type of PWM control. Home-use light dimmers typically include electronic circuitry which suppresses current flow during defined portions of each cycle of the AC line voltage. Adjusting the brightness of light emitted by a light source is then merely a matter of setting at what voltage (or phase) in the AC half-cycle the dimmer begins to provide electric current to the light source (e.g. by using an electronic switch such as a triac). In this case the PWM duty cycle is the ratio of the conduction time to the duration of the half AC cycle defined by the frequency of the AC line voltage (50 Hz or 60 Hz depending on the country).\n\nThese rather simple types of dimmers can be effectively used with inert (or relatively slow reacting) light sources such as incandescent lamps, for example, for which the additional modulation in supplied electrical energy which is caused by the dimmer causes only negligible additional fluctuations in the emitted light. Some other types of light sources such as light-emitting diodes (LEDs), however, turn on and off extremely rapidly and would perceivably flicker if supplied with low frequency drive voltages. Perceivable flicker effects from such rapid response light sources can be reduced by increasing the PWM frequency. If the light fluctuations are sufficiently rapid (faster than the flicker fusion threshold), the human visual system can no longer resolve them and the eye perceives the time average intensity without flicker.\n\nIn electric cookers, continuously variable power is applied to the heating elements such as the hob or the grill using a device known as a simmerstat. This consists of a thermal oscillator running at approximately two cycles per minute and the mechanism varies the duty cycle according to the knob setting. The thermal time constant of the heating elements is several minutes, so that the temperature fluctuations are too small to matter in practice.\n\nPWM is also used in efficient voltage regulators. By switching voltage to the load with the appropriate duty cycle, the output will approximate a voltage at the desired level. The switching noise is usually filtered with an inductor and a capacitor.\n\nOne method measures the output voltage. When it is lower than the desired voltage, it turns on the switch. When the output voltage is above the desired voltage, it turns off the switch.\n\nPWM is sometimes used in sound (music) synthesis, in particular subtractive synthesis, as it gives a sound effect similar to chorus or slightly detuned oscillators played together. (In fact, PWM is equivalent to the difference of two sawtooth waves with one of them inverted.) The ratio between the high and low level is typically modulated with a low frequency oscillator. In addition, varying the duty cycle of a pulse waveform in a subtractive-synthesis instrument creates useful timbral variations. Some synthesizers have a duty-cycle trimmer for their square-wave outputs, and that trimmer can be set by ear; the 50% point (true square wave) was distinctive, because even-numbered harmonics essentially disappear at 50%. Pulse waves, usually 50%, 25%, and 12.5%, make up the soundtracks of classic video games.\n\nA new class of audio amplifiers based on the PWM principle is becoming popular. Called class-D amplifiers, they produce a PWM equivalent of the analog input signal which is fed to the loudspeaker via a suitable filter network to block the carrier and recover the original audio. These amplifiers are characterized by very good efficiency figures (≥ 90%) and compact size/light weight for large power outputs. For a few decades, industrial and military PWM amplifiers have been in common use, often for driving servo motors. Field-gradient coils in MRI machines are driven by relatively high-power PWM amplifiers.\n\nHistorically, a crude form of PWM has been used to play back PCM digital sound on the PC speaker, which is driven by only two voltage levels, typically 0 V and 5 V. By carefully timing the duration of the pulses, and by relying on the speaker's physical filtering properties (limited frequency response, self-inductance, etc.) it was possible to obtain an approximate playback of mono PCM samples, although at a very low quality, and with greatly varying results between implementations.\n\nIn more recent times, the Direct Stream Digital sound encoding method was introduced, which uses a generalized form of pulse-width modulation called pulse density modulation, at a high enough sampling rate (typically in the order of MHz) to cover the whole acoustic frequencies range with sufficient fidelity. This method is used in the SACD format, and reproduction of the encoded audio signal is essentially similar to the method used in class-D amplifiers.\n\nSPWM (Sine–triangle pulse width modulation) signals are used in micro-inverter design (used in solar and wind power applications). These switching signals are fed to the FETs that are used in the device. The device's efficiency depends on the harmonic content of the PWM signal. There is much research on eliminating unwanted harmonics and improving the fundamental strength, some of which involves using a modified carrier signal instead of a classic sawtooth signal in order to decrease power losses and improve efficiency. Another common application is in robotics where PWM signals are used to control the speed of the robot by controlling the motors.\n\n", "related": "\n- Analog signal to discrete time interval converter\n- Class-D amplifier\n- Computer fan control\n- Delta-sigma modulation\n- Pulse-amplitude modulation\n- Pulse-code modulation\n- Pulse-density modulation\n- Pulse-position modulation\n- Radio control\n- RC servo\n- Sliding mode control - produces smooth behavior by way of discontinuous switching in systems\n- Space vector modulation\n- Sound chip\n\n- An Introduction to Delta Sigma Converters\n- Pulse Width Modulation in PID control loop - free simulator\n- Pulse Width Modulation in Desktop monitors - monitor flicker\n"}
{"id": "50364516", "url": "https://en.wikipedia.org/wiki?curid=50364516", "title": "Optomyography", "text": "Optomyography\n\nOptomyography (OMG) was proposed in 2015 as a technique that could be used to monitor muscular activity. It is possible to use OMG for the same applications where Electromyography (EMG) and Mechanomyography (MMG) are used. However, OMG offers superior signal-to-noise ratio and improved robustness against the disturbing factors and limitations of EMG and MMG.\nThe basic principle of OMG is to use active near-infra-red optical sensors to measure the variations in the measured signals that are reflected from the surface of the skin while activating the muscles below and around the skin spot where the photoelectric sensor is focusing to measure the signals reflected from this spot.\n\nGenerating proper control signals is the most important task to be able to control any kind of a prosthesis, computer game or any other system which contains a human-computer interaction unit or module. For this purpose, surface-Electromyographic (s-EMG) and Mechanomyographic (MMG) signals are measured during muscular activities and used, not only for monitoring and assessing these activities, but also to help in providing efficient rehabilitation treatment for patients suffering from disabilities as well as in constructing and controlling sophisticated prostheses for various types of amputees and disabilities. However, while the existing s-EMG and MMG based systems have compelling benefits, many engineering challenges still remain unsolved, especially with regard to the sensory control system.\n", "related": "NONE"}
{"id": "201605", "url": "https://en.wikipedia.org/wiki?curid=201605", "title": "Sampling (signal processing)", "text": "Sampling (signal processing)\n\nIn signal processing, sampling is the reduction of a continuous-time signal to a discrete-time signal. A common example is the conversion of a sound wave (a continuous signal) to a sequence of samples (a discrete-time signal).\n\nA sample is a value or set of values at a point in time and/or space. A sampler is a subsystem or operation that extracts samples from a continuous signal. A theoretical ideal sampler produces samples equivalent to the instantaneous value of the continuous signal at the desired points.\n\nThe original signal is retrievable from a sequence of samples, up to the Nyquist limit, by passing the sequence of samples through a type of low pass filter called a reconstruction filter.\n\nSampling can be done for functions varying in space, time, or any other dimension, and similar results are obtained in two or more dimensions.\n\nFor functions that vary with time, let \"s\"(\"t\") be a continuous function (or \"signal\") to be sampled, and let sampling be performed by measuring the value of the continuous function every T seconds, which is called the sampling interval or the sampling period.  Then the sampled function is given by the sequence:\n\nThe sampling frequency or sampling rate, f, is the average number of samples obtained in one second (\"samples per second\"), thus f = 1/T.\n\nReconstructing a continuous function from samples is done by interpolation algorithms. The Whittaker–Shannon interpolation formula is mathematically equivalent to an ideal lowpass filter whose input is a sequence of Dirac delta functions that are modulated (multiplied) by the sample values. When the time interval between adjacent samples is a constant (\"T\"), the sequence of delta functions is called a Dirac comb. Mathematically, the modulated Dirac comb is equivalent to the product of the comb function with \"s\"(\"t\"). That purely mathematical abstraction is sometimes referred to as \"impulse sampling\".\n\nMost sampled signals are not simply stored and reconstructed. But the fidelity of a theoretical reconstruction is a customary measure of the effectiveness of sampling. That fidelity is reduced when \"s\"(\"t\") contains frequency components whose periodicity is smaller than two samples; or equivalently the ratio of cycles to samples exceeds ½ (see Aliasing). The quantity ½ \"cycles/sample\" × f \"samples/sec\" = f/2 \"cycles/sec\" (hertz) is known as the Nyquist frequency of the sampler. Therefore, \"s\"(\"t\") is usually the output of a lowpass filter, functionally known as an \"anti-aliasing filter\". Without an anti-aliasing filter, frequencies higher than the Nyquist frequency will influence the samples in a way that is misinterpreted by the interpolation process.\n\nIn practice, the continuous signal is sampled using an analog-to-digital converter (ADC), a device with various physical limitations. This results in deviations from the theoretically perfect reconstruction, collectively referred to as distortion.\n\nVarious types of distortion can occur, including:\n\n- Aliasing. Some amount of aliasing is inevitable because only theoretical, infinitely long, functions can have no frequency content above the Nyquist frequency. Aliasing can be made arbitrarily small by using a sufficiently large order of the anti-aliasing filter.\n- Aperture error results from the fact that the sample is obtained as a time average within a sampling region, rather than just being equal to the signal value at the sampling instant . In a capacitor-based sample and hold circuit, aperture errors are introduced by multiple mechanisms. For example, the capacitor cannot instantly track the input signal and the capacitor can not instantly be isolated from the input signal.\n- Jitter or deviation from the precise sample timing intervals.\n- Noise, including thermal sensor noise, analog circuit noise, etc.\n- Slew rate limit error, caused by the inability of the ADC input value to change sufficiently rapidly.\n- Quantization as a consequence of the finite precision of words that represent the converted values.\n- Error due to other non-linear effects of the mapping of input voltage to converted output value (in addition to the effects of quantization).\n\nAlthough the use of oversampling can completely eliminate aperture error and aliasing by shifting them out of the pass band, this technique cannot be practically used above a few GHz, and may be prohibitively expensive at much lower frequencies. Furthermore, while oversampling can reduce quantization error and non-linearity, it cannot eliminate these entirely. Consequently, practical ADCs at audio frequencies typically do not exhibit aliasing, aperture error, and are not limited by quantization error. Instead, analog noise dominates. At RF and microwave frequencies where oversampling is impractical and filters are expensive, aperture error, quantization error and aliasing can be significant limitations.\n\nJitter, noise, and quantization are often analyzed by modeling them as random errors added to the sample values. Integration and zero-order hold effects can be analyzed as a form of low-pass filtering. The non-linearities of either ADC or DAC are analyzed by replacing the ideal linear function mapping with a proposed nonlinear function.\n\nDigital audio uses pulse-code modulation and digital signals for sound reproduction. This includes analog-to-digital conversion (ADC), digital-to-analog conversion (DAC), storage, and transmission. In effect, the system commonly referred to as digital is in fact a discrete-time, discrete-level analog of a previous electrical analog. While modern systems can be quite subtle in their methods, the primary usefulness of a digital system is the ability to store, retrieve and transmit signals without any loss of quality.\n\nA commonly seen unit of sampling rate is Hz, which stands for Hertz and means \"samples per second\". As an example, 48 kHz is 48,000 samples per second.\n\nWhen it is necessary to capture audio covering the entire 20–20,000 Hz range of human hearing, such as when recording music or many types of acoustic events, audio waveforms are typically sampled at 44.1 kHz (CD), 48 kHz, 88.2 kHz, or 96 kHz. The approximately double-rate requirement is a consequence of the Nyquist theorem. Sampling rates higher than about 50 kHz to 60 kHz cannot supply more usable information for human listeners. Early professional audio equipment manufacturers chose sampling rates in the region of 40 to 50 kHz for this reason.\n\nThere has been an industry trend towards sampling rates well beyond the basic requirements: such as 96 kHz and even 192 kHz Even though ultrasonic frequencies are inaudible to humans, recording and mixing at higher sampling rates is effective in eliminating the distortion that can be caused by foldback aliasing. Conversely, ultrasonic sounds may interact with and modulate the audible part of the frequency spectrum (intermodulation distortion), \"degrading\" the fidelity. One advantage of higher sampling rates is that they can relax the low-pass filter design requirements for ADCs and DACs, but with modern oversampling sigma-delta converters this advantage is less important.\n\nThe Audio Engineering Society recommends 48 kHz sampling rate for most applications but gives recognition to 44.1 kHz for Compact Disc (CD) and other consumer uses, 32 kHz for transmission-related applications, and 96 kHz for higher bandwidth or relaxed anti-aliasing filtering. Both Lavry Engineering and J. Robert Stuart state that the ideal sampling rate would be about 60 kHz, but since this is not a standard frequency, recommend 88.2 or 96 kHz for recording purposes. \n\nA more complete list of common audio sample rates is:\n\nAudio is typically recorded at 8-, 16-, and 24-bit depth, which yield a theoretical maximum signal-to-quantization-noise ratio (SQNR) for a pure sine wave of, approximately, 49.93 dB, 98.09 dB and 122.17 dB. CD quality audio uses 16-bit samples. Thermal noise limits the true number of bits that can be used in quantization. Few analog systems have signal to noise ratios (SNR) exceeding 120 dB. However, digital signal processing operations can have very high dynamic range, consequently it is common to perform mixing and mastering operations at 32-bit precision and then convert to 16- or 24-bit for distribution.\n\nSpeech signals, i.e., signals intended to carry only human speech, can usually be sampled at a much lower rate. For most phonemes, almost all of the energy is contained in the 100 Hz–4 kHz range, allowing a sampling rate of 8 kHz. This is the sampling rate used by nearly all telephony systems, which use the G.711 sampling and quantization specifications.\n\nStandard-definition television (SDTV) uses either 720 by 480 pixels (US NTSC 525-line) or 720 by 576 pixels (UK PAL 625-line) for the visible picture area.\n\nHigh-definition television (HDTV) uses 720p (progressive), 1080i (interlaced), and 1080p (progressive, also known as Full-HD).\n\nIn digital video, the temporal sampling rate is defined the frame rate or rather the field rate rather than the notional pixel clock. The image sampling frequency is the repetition rate of the sensor integration period. Since the integration period may be significantly shorter than the time between repetitions, the sampling frequency can be different from the inverse of the sample time:\n\n- 50 Hz – PAL video\n- 60 / 1.001 Hz ~= 59.94 Hz – NTSC video\n\nVideo digital-to-analog converters operate in the megahertz range (from ~3 MHz for low quality composite video scalers in early games consoles, to 250 MHz or more for the highest-resolution VGA output).\n\nWhen analog video is converted to digital video, a different sampling process occurs, this time at the pixel frequency, corresponding to a spatial sampling rate along scan lines. A common pixel sampling rate is:\n\n- 13.5 MHz – CCIR 601, D1 video\n\nSpatial sampling in the other direction is determined by the spacing of scan lines in the raster. The sampling rates and resolutions in both spatial directions can be measured in units of lines per picture height.\n\nSpatial aliasing of high-frequency luma or chroma video components shows up as a moiré pattern.\n\nThe process of volume rendering samples a 3D grid of voxels to produce 3D renderings of sliced (tomographic) data. The 3D grid is assumed to represent a continuous region of 3D space. Volume rendering is common in medial imaging, X-ray computed tomography (CT/CAT), magnetic resonance imaging (MRI), positron emission tomography (PET) are some examples. It is also used for seismic tomography and other applications.\n\nWhen a bandpass signal is sampled slower than its Nyquist rate, the samples are indistinguishable from samples of a low-frequency alias of the high-frequency signal. That is often done purposefully in such a way that the lowest-frequency alias satisfies the Nyquist criterion, because the bandpass signal is still uniquely represented and recoverable. Such undersampling is also known as \"bandpass sampling\", \"harmonic sampling\", \"IF sampling\", and \"direct IF to digital conversion.\"\n\nOversampling is used in most modern analog-to-digital converters to reduce the distortion introduced by practical digital-to-analog converters, such as a zero-order hold instead of idealizations like the Whittaker–Shannon interpolation formula.\n\n\"Complex sampling\" (I/Q sampling) is the simultaneous sampling of two different, but related, waveforms, resulting in pairs of samples that are subsequently treated as complex numbers.  When one waveformformula_1  is the Hilbert transform of the other waveformformula_2  the complex-valued function,  formula_3  is called an analytic signal,  whose Fourier transform is zero for all negative values of frequency. In that case, the Nyquist rate for a waveform with no frequencies ≥ \"B\" can be reduced to just \"B\" (complex samples/sec), instead of 2\"B\" (real samples/sec). More apparently, the equivalent baseband waveform,  formula_4  also has a Nyquist rate of \"B\", because all of its non-zero frequency content is shifted into the interval [-B/2, B/2).\n\nAlthough complex-valued samples can be obtained as described above, they are also created by manipulating samples of a real-valued waveform. For instance, the equivalent baseband waveform can be created without explicitly computing formula_5  by processing the product sequenceformula_6  through a digital lowpass filter whose cutoff frequency is B/2. Computing only every other sample of the output sequence reduces the sample-rate commensurate with the reduced Nyquist rate. The result is half as many complex-valued samples as the original number of real samples. No information is lost, and the original s(t) waveform can be recovered, if necessary.\n\n", "related": "\n- Downsampling\n- Upsampling\n- Multidimensional sampling\n- Sample rate conversion\n- Digitizing\n- Sample and hold\n- Beta encoder\n- Kell factor\n- Bit rate\n\n- Matt Pharr, Wenzel Jakob and Greg Humphreys, \"Physically Based Rendering: From Theory to Implementation, 3rd ed.\", Morgan Kaufmann, November 2016. . The chapter on sampling (available online) is nicely written with diagrams, core theory and code sample.\n\n- Journal devoted to Sampling Theory\n- I/Q Data for Dummies a page trying to answer the question \"Why I/Q Data?\"\n- Sampling of analog signals an interactive presentation in a web-demo at the Institute of Telecommunications, University of Stuttgart\n"}
{"id": "48795622", "url": "https://en.wikipedia.org/wiki?curid=48795622", "title": "Reconstruction from Projections", "text": "Reconstruction from Projections\n\nThe problem of reconstructing a multidimensional signal from its projection is uniquely multidimensional, having no 1-D counterpart. It has applications that range from computer-aided tomography to geophysical signal processing. It is a problem which can be explored from several points of view—as a deconvolution problem, a modeling problem, an estimation problem, or an interpolation problem.\n\nThe problem of reconstruction form projections has arisen independently in a large number of scientific fields, since it is widely applied in areas such as medical imaging, geophysical tomography, industrial radiography and so on. For example, by using CT scanner, the lesion information of the patients can be presented in 3D on the computer, which offers a new and accurate approach in diagnosis and thus has vital clinical value.\n\nA projection is a linear mapping of an M-D signal into an N-D one, where N<M. And the objective of reconstruction is to restore the M-D signal based on the N-D signal. The following case is a 2-D signal projected into 1D signal.\nThe signal in the original coordinate is denoted as formula_1. Now consider a collimated beam of radiation coming from the opposite orientation of formula_2, producing a projection along formula_3. formula_2 and formula_3 are normal to each other, and the angle between u and formula_3 is theta. The signal obtained along formula_3 axis is defined to be formula_8. The relationship between the original coordinate and the rotated coordinate is given by\n\nformula_9\n\nor inversely,\n\nformula_10\n\nThen we have\n\nformula_11\n\nBy varying theta, a large number of projections can be obtained.\n\nGiven the projection-slice theorem, formula_12,the slice of the Fourier transform of formula_1 at angle theta, is equivalent to formula_14, the Fourier Transform of the projection formula_8. Therefore, the unknown formula_1 can be obtained from its Fourier transform by means of the Fourier transform inversion integral\n\nformula_17\nformula_18\nformula_19\nformula_20\nformula_21\n\nBy taking the inverse Fourier Transform and assumingformula_22, we get formula_23\n\nIn practice, there are a wide rarity of methods that are utilized, most of which are reconstruct 3-D information (volume) from 2-D signals (image). Typically used methods are CT, MRI, PET and SPECT. And the filtered back projection based on the principles introduced above are commonly applied.\n\nIn CT, a volume is formed by stacking the axial slices. The software cuts the volume in a different plane (usually orthogonal). Commonly, slice data is generated using an X-ray source that rotates around the object. X-ray sensors are positioned on the opposite side of the circle from the X-ray source.\n\nIn MRI, energy from an oscillating magnetic field is temporarily applied to the patient at the appropriate resonance frequency. The protons (hydrogen atoms) emit a radio frequency signal which is measured by a receiving coil. The radio signal can be made to encode position information by varying the main magnetic field using gradient coils.\n\nThe system detects pairs of gamma rays emitted indirectly by a positron-emitting radionuclide (tracer), which is introduced into the body on a biologically active molecule. Three-dimensional images of tracer concentration within the body are then constructed by computer analysis. In modern PET-CT scanners, three dimensional imaging is often accomplished with the aid of a CT X-ray scan performed on the patient during the same session, in the same machine.\n\nSPECT imaging is performed by using a gamma camera to acquire multiple 2-D images (projections) from multiple angles. Multiple projections are used to yield a 3-D data set. This data set may then be manipulated to show thin slices along any chosen axis of the body. SPECT is similar to PET in its use of radioactive tracer material and detection of gamma rays, while the tracers used in SPECT emit gamma radiation that is measured more directly.\n\n", "related": "\n- 3D scanner\n- filtered back projection\n- Algebraic Reconstruction Technique\n- 3D data acquisition and object reconstruction\n\n- http://www.tecn.upf.es/~afrangi/ibi/reconstruction_color_2.pdf\n- http://opax.swin.edu.au/~dliley/lectures/het408/backproj.pdf\n"}
{"id": "33353907", "url": "https://en.wikipedia.org/wiki?curid=33353907", "title": "Digital storage oscilloscope", "text": "Digital storage oscilloscope\n\nA digital storage oscilloscope (often abbreviated DSO) is an oscilloscope which stores and analyses the signal digitally rather than using analog techniques. It is now the most common type of oscilloscope in use because of the advanced trigger, storage, display and measurement features which it typically provides.\n\nThe input analogue signal is sampled and then converted into a digital record of the amplitude of the signal at each sample time. The sampling frequency should be not less than the Nyquist rate to avoid aliasing. These digital values are then turned back into an analogue signal for display on a cathode ray tube (CRT), or transformed as needed for the various possible types of output—liquid crystal display, chart recorder, plotter or network interface.\n\nDigital storage oscilloscope costs vary widely; bench-top self-contained instruments (complete with displays) start at or even less, with high-performance models selling for tens of thousands of dollars. Small, pocket-size models, limited in function, may retail for as little as US$50.\n\nThe principal advantage over analog storage is that the stored traces are as bright, as sharply defined, and written as quickly as non-stored traces. Traces can be stored indefinitely or written out to some external data storage device and reloaded. This allows, for example, comparison of an acquired trace from a system under test with a standard trace acquired from a known-good system. Many models can display the waveform prior to the trigger signal.\n\nDigital oscilloscopes usually analyze waveforms and provide numerical values as well as visual displays. These values typically include averages, maxima and minima, root mean square (RMS) and frequencies. They may be used to capture transient signals when operated in a single sweep mode, without the brightness and writing speed limitations of an analog storage oscilloscope.\n\nThe displayed trace can be manipulated after acquisition; a portion of the display can be magnified to make fine detail more visible, or a long trace can be examined in a single display to identify areas of interest. Many instruments allow a stored trace to be annotated by the user.\n\nMany digital oscilloscopes use flat panel displays similar to those made in high volumes for computers and television displays.\n\n- Digital Storage Oscilloscope Measurement Basics\n- The Effective Number of Bits (ENOB)\n- The Impact of Digital Oscilloscope Blind Time on Your Measurements\n- Benefits of a Digital Trigger System\n", "related": "NONE"}
{"id": "52211120", "url": "https://en.wikipedia.org/wiki?curid=52211120", "title": "Multidimensional empirical mode decomposition", "text": "Multidimensional empirical mode decomposition\n\nIn signal processing, the multidimensional empirical mode decomposition (multidimensional EMD) is the extension of the 1-D EMD algorithm into multiple-dimensional signal. The Hilbert–Huang empirical mode decomposition (EMD) process decomposes a signal into intrinsic mode functions combined with the Hilbert spectral analysis known as Hilbert–Huang transform (HHT). The multidimensional EMD extends the 1-D EMD algorithm into multiple-dimensional signals. This decomposition can be applied to image processing, audio signal processing and various other multidimensional signals.\n\nMultidimensional empirical mode decomposition is a popular method because of its applications in many fields, such as texture analysis, financial applications, image processing, ocean engineering, seismic research and so on. Recently, several methods of Empirical Mode Decomposition have been used to analyze characterization of multidimensional signals. In this article, we will introduce the basics of Multidimensional Empirical Mode Decomposition, and then look into various approaches used for Multidimensional Empirical Mode Decomposition.\n\nThe \"empirical mode decomposition\" method can extract global structure and deal with fractal-like signals.\n\nThe EMD method was developed so that the data can be examined in an adaptive time–frequency–amplitude space for nonlinear and non-stationary signals.\n\nThe EMD method decomposes the input signal into few Intrinsic Mode functions (IMF) and a residue. The given equation will be as follows:\n\nwhere formula_2 is the multi-component signal. formula_3 is the formula_4 intrinsic mode function, and formula_5 represents residue corresponding to formula_6 intrinsic modes.\n\nTo improve the accuracy of measurements, the ensemble mean is a powerful approach, where data are collected by separate observations, each of which contains different noise over an ensemble of universe's. To generalize this ensemble idea, noise is introduced to the single data set, x(t), as if separate observations were indeed being made as an analogue to a physical experiment that could be repeated many times. The added white noise is treated as the possible random noise that would be encountered in the measurement process. Under such conditions, the i th ‘artificial’ observation will be formula_7\n\nIn the case of only one observation, one of the multiple-observation ensembles is mimicked by adding not arbitrary but different copies of white noise, wi(t), to that single observation as given in the equation. Although adding noise may result in smaller signal to-noise ratio, the added white noise will provide a uniform reference scale distribution to facilitate EMD; therefore, the low signal-noise ratio does not affect the decomposition\nmethod but actually enhances it to avoid the mode mixing. Based on this argument, an additional step is taken by arguing that adding white noise may help to extract the true signals in the data, a method that is termed Ensemble Empirical Mode Decomposition (EEMD)\n\nThe EEMD consists of the following steps:\n\n1. Adding a white noise series to the original data.\n2. Decomposing the data with added white noise into oscillatory components.\n3. Repeating step 1 and step 2 again and again, but with different white noise series added each time.\n4. Obtaining the (ensemble) means of the corresponding intrinsic mode functions of the decomposition as the final result.\n\nIn these steps, EEMD uses two properties of white noise:\n\n1. The added white noise leads to relatively even distribution of extrema distribution on all timescales.\n2. The dyadic filter bank property provides a control on the periods of oscillations contained in an oscillatory component, significantly reducing the chance of scale mixing in a component. Through ensemble average, the added noise is averaged out.\n\nIt should be pointed out here that the “pseudo-BEMD” method is not limited to only one-spatial dimension; rather, it can be applied to data of any number of spatial-temporal dimensions. Since the spatial structure is essentially determined by timescales of the variability of a physical quantity at each location and the decomposition is completely based on the characteristics of individual time series at each spatial location, there is no assumption of spatial coherent structures of this physical quantity. When a coherent spatial structure emerges, it reflects better the physical processes that drive the evolution of the physical quantity on the timescale of each component. Therefore, we expect this method to have significant applications in spatial-temporal data analysis.\n\nTo design a pseudo-BEMD algorithm the key step is to translate the algorithm of the 1D EMD into a Bi-dimensional Empirical Mode Decomposition(BEMD) and further extend the algorithm to three or more dimensions which is similar to the BEMD by extending the procedure on successive dimensions.For a 3D data cube of i × j × k elements, the pseudo-BEMD will yield detailed 3D components of m × n × q where m, n and q are the number of the IMFs decomposed from each dimension having i, j, and k elements, respectively.\n\nMathematically let us represent a 2D signal in the form of ixj matrix form with a finite number of elements.\n\nAt first we perform EMD in one direction of \"X\"(\"i\",\"j\"), Row wise for instance, to decompose the data of each row into m components, then to collect the components of the same level of m from the result of each row decomposition to make a 2D decomposed signal at that level of m. Therefore, m set of 2D spatial data are obtained\n\nwhere RX (1, i, j), RX (2, i, j), and RX (m, i, j) are the \"m\" sets of signal as stated (also here we use \"R\" to indicate row decomposing). The relation between these m 2D decomposed signals and the original signal is given as formula_10 \n\nThe first row of the matrix RX (m, i, j) is the mth EMD component decomposed from the first row of the matrix X (i, j). The second row of the matrix RX (m, i, j) is the mth EMD component decomposed from the second row of the matrix X (i, j), and so on.\n\nSuppose that the previous decomposition is along the horizontal direction, the next step is to decompose each one of the previously row decomposed components RX(m, i, j), in the vertical direction into n components. This step will generate n components from each RX component.\n\nFor example, the component\n\n1. RX(1,i,j) will be decomposed into CRX(1,1,i,j), CRX(1,2,i,j),…,CRX(1,n,i,j)\n2. RX(2,i,j) will be decomposed into CRX(2,1,i,j), CRX(2,2,i,j),…, CRX(2,n,i,j)\n3. RX(m,i,j) will be decomposed into CRX(m,1,i,j), CRX(m,2,i,j),…, CRX(m,n,i,j)\n\nwhere C means column decomposing.Finally, the 2D decomposition will result into m× n matrices which are the 2D EMD components of the original data X(i,j). The matrix expression for the result of the 2D decomposition is\n\nwhere each element in the matrix CRX is an i × j sub-matrix representing a 2D EMD decomposed component. We use the arguments (or suffixes) m and n to represent the component number of row decomposition and column decomposition, respectively rather than the subscripts indicating the row and the column of a matrix. Notice that the m and n indicate the number of components resulting from row(horizontal) decomposition and then column (vertical) decomposition, respectively.\n\nBy combining the components of the same scale or the comparable scales with minimal difference will yield a 2D feature with best physical significance. The components of the first row and the first column are approximately the same or comparable scale although their scales are increasing gradually along the row or column. Therefore, combining the components of the first row and the first column will obtain the first complete 2D component (C2D1). The subsequent process is to perform the same combination technique to the rest of the components, the contribution of the noises are distributed to the separate component according to their scales. As a result, the coherent structures of the components emerge, In this way, the pseudo-BEMD method can be applied to reveal the evolution of spatial structures of data.\n\nFollowing the convention of 1D EMD, the last component of the complete 2D components is called residue.\n\nThe decomposition scheme proposed here could be extended to data of any dimensions such as data of a solid with different density or other measurable properties\n\ngiven as formula_13\n\nIn which the subscription, n, indicated the number of dimensions. The procedure is identical as stated above: the decomposition starts with the first dimension,and proceeds to the second and third till all the dimensions are exhausted. The decomposition is still implemented by slices.This new approach is based on separating the original data into one-dimensional slices, then applying ensemble EMD to each one-dimensional slice. The key part of the method is in the construction of the IMF according to the principle of combination of the comparable minimal scale components.\n\nFor example, the matrix expression for the result of a 3D decomposition is TCRX(m,n,q,i,j,k) where T denotes the depth (or time) decomposition. Based on the comparable minimal scale combination principle as applied in the 2D case, the number of complete 3D components will be the smallest value of \"m\", \"n\", and \"q\". The general equation for deriving 3D components is\nwhere denotes the level of the C3D i.e.\n\nThe pseudo-BEMD method has several advantages. For instance, the sifting procedure of the pseudo-BEMD is a combination of one dimensional sifting. It employs 1D curve fitting in the sifting process of each dimension, and has no difficulty as encountered in the 2D EMD algorithms using surface fitting which has the problem of determining the saddle point as a local maximum or minimum.Sifting is the process which separates the IMF and repeats the process until the residue is obtained. The first step of performing sifting is to determine the upper and lower envelopes encompassing all the data by using the spline method. Sifting scheme for pseudo-BEMD is like the 1D sifting where the local mean of the standard EMD is replaced by the mean of multivariate envelope curves.\n\nThe major disadvantage of this method is that although we could extend this algorithm to any dimensional data we only use it for Two dimension applications. Because the computation time of higher dimensional data would be proportional to the number of IMF's of the succeeding dimensions. Hence it could exceed the computation capacity for a Geo-Physical data processing system when the number of EMD in the algorithm is large. Hence we have mentioned below faster and better techniques to tackle this disadvantage.\n\nA Fast and efficient data analysis is very important for large sequences hence the MDEEMD focuses on two important things\n\n1. Data compression which involves decomposing data into simpler forms.\n2. EEMD on the compressed data; this is the most challenging since on decomposing the compressed data there is a high probability to lose key information. A data compression method that uses principal component analysis (PCA)/empirical orthogonal function (EOF) analysis or principal oscillation pattern analysis is used to compress data.\n\nThe principal component analysis/empirical orthogonal function analysis (PCA/EOF) has been widely used in data analysis and image compression,its main objective is to reduce a data set containing a large number of variables to a data set containing fewer variables, but that still represents a large fraction of the variability contained in the original data set. In climate studies, EOF analysis is often used to study possible spatial modes (i.e., patterns) of variability and how they change with time . In statistics, EOF analysis is known as principal component analysis (PCA).\n\nTypically, the EOFs are found by computing the eigenvalues and eigen vectors of a spatially weighted anomaly covariance matrix of a field. Most commonly, the spatial weights are the cos(latitude) or, better for EOF analysis, the sqrt(cos(latitude)). The derived eigenvalues provide a measure of the percent variance explained by each mode. Unfortunately, the eigenvalues are not necessarily distinct due to sampling issues. North et al. (Mon. Wea. Rev., 1982, eqns 24-26) provide a 'rule of thumb' for determining if a particular eigenvalue (mode) is distinct from its nearest neighbor.\n\nAtmospheric and oceanographic processes are typically 'red' which means that most of the variance (power) is contained within the first few modes. The time series of each mode (aka, principle components) are determined by projecting the derived eigen vectors onto the spatially weighted anomalies. This will result in the amplitude of each mode over the period of record.\n\nBy construction, the EOF patterns and the principal components are independent. Two factors inhibit physical interpretation of EOFs: (i)The orthogonality constraint and (ii) the derived patterns may be domain dependent. Physical systems are not necessarily orthogonal and if the patterns depend on the region used they may not exist if the domain changes.\n\nAssume, we have a spatio-temporal data \"T\"(\"s\", \"t\"), where \"s\" is spatial locations (not necessary one dimensional originally but needed to be rearranged into a single spatial dimension) from 1 to \"N\" and \"t\" temporal locations from 1 to \"M\".\n\nUsing PCA/EOF, one can express \"T\"(\"s\", \"t\") into formula_16\n\nwhere \"Y\"(\"t\") is the \"i\"th principal component and \"V\"(\"t\") the \"i\"th empirical orthogonal function (EOF) pattern and \"K\" is the smaller one of \"M\" and \"N\". PC and EOFs are often obtained by solving the eigenvalue/eigenvector problem of either temporal co-variance matrix or spatial co-variance matrix based on which dimension is smaller. The variance explained by one pair of PCA/EOF is its corresponding eigenvalue divided by the sum of all eigenvalues of the co-variance matrix.\n\nIf the data subjected to PCA/EOF analysis is all white noise, all eigenvalues are theoretically equal and there is no preferred vector direction for the principal component in PCA/EOF space. To retain most of the information of the data, one needs to retain almost all the PC's and EOF's, making the size of PCA/EOF expression even larger than that of the original but If the original data contain only one spatial structure and oscillate with time, then the original data can be expressed as the product of one PC and one EOF, implying that the original data of large size can be expressed by small size data without losing information, i.e. highly compressible.\n\nThe variability of a smaller region tends to be more spatio-temporally coherent than that of a bigger region containing that smaller region, and, therefore, it is expected that fewer PC/EOF components are required to account for a threshold level of variance hence one way to improve the efficiency of the representation of data in terms of PC/EOF component is to divide the global spatial domain into a set of sub-regions. If we divide the original global spatial domain into n sub-regions containing N1, N2, . . . , Nn spatial grids, respectively, with all Ni, where i=1, . . . , n, greater than M, where M denotes the number of temporal locations, we anticipate that the numbers of the retained PC/EOF pairs for all sub-regions K1, K2, . . . , Kn are all smaller than K, the total number of data values in PCA/EOF representation of the original data of the global spatial domain by the equation given up is K×(N+M).For the new approach of using spatial division, the total number of values in PCA/EOF representatio is\n\nwhere\n\nTherefore, the compression rate of the spatial domain is as follows\n\nThe advantage of this algorithm is that an optimized division and an optimized selection of PC/EOF pairs for each region would lead to a higher rate of compression and result into significantly lower computation as compared to a Pseudo BEMD extended to higher dimensions.\n\nFor a temporal signal of length \"M\", the complexity of cubic spline sifting through its local extrema is about the order of \"M,\" and so is that of the EEMD as it only repeats the spline fitting operation with a number that is not dependent on \"M\". However, as the sifting number (often selected as 10) and the ensemble number (often a few hundred) multiply to the spline sifting operations, hence the EEMD is time consuming compared with many other time series analysis methods such as Fourier transforms and wavelet transforms.The MEEMD employs EEMD decomposition of the time serie s at each division grids of the initial temporal signal, the EEMD operation is repeated by the number of total grid points of the domain. The idea of the fast MEEMD is very simple. As PCA/EOF-based compression expressed the original data in terms of pairs of PCs and EOFs, through decomposing PCs, instead of time series of each grid, and using the corresponding spatial structure depicted by the corresponding EOFs, the computational burden can be significantly reduced.\n\nThe fast MEEMD includes the following steps:\n\n1. All pairs of EOF's, \"V\", and their corresponding PCs, \"Y\", of spatio-temporal data over a compressed sub-domain are computed.\n2. The number of pairs of PC/EOF that are retained in the compressed data is determined by the calculation of the accumulated total variance of leading EOF/PC pairs.\n3. Each PC \"Y\" is decomposed using EEMD, i.e.\n\nIn this compressed computation, we have used the approximate dyadic filter bank properties of EMD/EEMD.\n\nNote that a detailed knowledge of the intrinsic mode functions of a noise corrupted signal can help in estimating the significance of that mode. It is usually assumed that the first IMF captures most of the noise and hence from this IMF we could estimate the Noise level and estimate the noise corrupted signal eliminating the effects of noise approximately. This method is known as denoising and detrending. Another advantage of using the MEEMD is that the mode mixing is reduced significantly due to the function of the EEMD.The denoising and detrending strategy can be used for image processing to enhance an image and similarly it could be applied to Audio Signals to remove corrupted data in speech. The MDEEMD could be used to break down images and audio signals into IMF and based on the knowledge of the IMF perform necessary operations. The decomposition of an image is very advantageous for radar-based application the decomposition of an image could reveal land mines etc.\n\nIn MEEMD, although ample parallelism potentially exists in the ensemble dimensions and/or the non-operating dimensions, several challenges still face a high performance MEEMD implementation.\n1. Dynamic data variations: In EEMD, white noises change the number of extrema causing some irregularity and load imbalance, and thus slowing down the parallel execution.\n2. Stride memory accesses of high-dimensional data: High dimensional data are stored in non-continuous memory locations. Accesses along high dimensions are thus strided and uncoalesced, wasting available memory bandwidth.\n3. Limited resources to harness parallelism: While the independent EMDs and/or EEMDs comprising an MEEMD provide high parallelism, the computational capacities of multi-core and many-core processors may not be sufficient to fully exploit the inherent parallelism of MEEMD. Moreover, increased parallelism may increase memory requirements beyond the memory capacities of these processors.In MEEMD, when a high degree of parallelism is given by the ensemble dimension and/or the non-operating dimensions, the benefits of using a thread-level parallel algorithm are threefold.\n\n1. It can exploit more parallelism than a block-level parallel algorithm.\n2. It does not incur any communication or synchronization between the threads until the results are merged since the execution of each EMD or EEMD is independent.\n3. Its implementation is like the sequential one, which makes it more straightforward.\n\nThe EEMDs comprising MEEMD are assigned to independent threads for parallel execution, relying on the OpenMP runtime to resolve any load imbalance issues. Stride memory accesses of high-dimensional data are eliminated by transposing these data to lower dimensions, resulting in better utilization of cache lines. The partial results of each EEMD are made thread-private for correct functionality. The required memory depends on the number of OpenMP threads and is managed by OpenMP runtime\n\nIn the GPU CUDA implementation, each EMD, is mapped to a thread. The memory layout, especially of high-dimensional data, is rearranged to meet memory coalescing requirements and fit into the 128-byte cache lines. The data is first loaded along the lowest dimension and then consumed along a higher dimension. This step is performed when the Gaussian noise is added to form the ensemble data. In the new memory layout, the ensemble dimension is added to the lowest dimension to reduce possible branch divergence. The impact of the unavoidable branch divergence from data irregularity, caused by the noise, is minimized via a regularization technique using the on-chip memory. Moreover, the cache memory is utilized to amortize unavoidable uncoalesced memory accesses.\n\nThe fast and adaptive bidimensional empirical mode decomposition (FABEMD) is an improved version of traditional BEMD. The FABEMD can be used in many areas, including medical image analysis, texture analysis and so on. The order statistics filter can help in solving the problems of efficiency and restriction of size in BEMD.\n\nBased on the algorithm of BEMD, the implementation method of FABEMD is really similar to BEMD, but the FABEMD approach just changes the interpolation step into a direct envelop estimation method and restricts the number of iterations for every BIMF to one. As a result, two order statistics, including MAX and MIN, will be used for approximating the upper and lower envelope. The size of the filter will depend on the maxima and minima maps obtained from the input. The steps of the FABEMD algorithm are listed below.\n\n- Step 1 – Determine and detect local maximum and minimum\n\nAs the traditional BEMD approach, we can find the jth ITS-BIMF formula_22 of any source of input formula_23 by neighboring window method. For FABEMD approach, we choose a different implementation approach.\n\nFrom the input data, we can get an 2D matrix represent\n\nwhere formula_25 is the element location in the matrix A, and we can define the window size to be formula_26. Thus, we can obtain the maximum and minimum value from the matrix as follows:\n\nwhere\n\n- Step 2 – Obtain the size of window for order-statistic filter\n\nAt first, we define formula_30 and formula_31 to be the maximum and minimum distance in the array which is calculated from each local maximum or minimum point to the nearest nonzero element. Also, formula_30 and formula_31 will be sorted in descending order in the array according to the convenient selection. Otherwise, we will only consider square window. Thus, the gross window width will be as follows:\n\n- Step 3 – Apply order statistics and smoothing filters to obtain the MAX and MIN filter output\n\nTo obtain the upper and lower envelopes, there should be defined two parameter formula_38 and formula_39, and the equation will be as follows:\n\nwhere formula_42 is defined as the square region of window size, and formula_43 is the window width of the smoothing filter which formula_43 equals to formula_45. Therefore, the MAX and MIN filter will form a new 2-D matrix for envelope surface which will not change the original 2-D input data.\n\n- Step 4 – Set up an estimation from upper and lower envelopes\n\nThis step is to make sure that the envelope estimation in FABEMD is nearly closed to the result from BEMD by using interpolation. In order to do comparison, we need to form corresponding matrices for upper envelope, lower envelope and mean envelope by using thin-plate spline surface interpolation to the max and min maps.\n\nThis method(FABEMD) provides a way to use less computation to obtain the result rapidly, and it allows us to ensure more accurate estimation of the BIMFs. Even more, the FABEMD is more adaptive to handle the large size input than the traditional BEMD. Otherwise, the FABEMD is an efficient method that we do not need to consider the boundary effects and overshoot-undershoot problems.\n\nThere is one particular problem that we will face in this method. Sometimes, there will be only one local maxima or minima element in the input data, so it will cause the distance array to be empty.\n\nThe Partial Differential Equation-Based Multidimensional Empirical Mode Decomposition(PDE-based MEMD) approach is a way to improve and overcome the difficulties of mean-envelope estimation of a signal from the traditional EMD. The PDE-based MEMD focus on modifying the original algorithm for MEMD. Thus, the result will provide an analytical formulation which can facilitate theoretical analysis and performance observation. In order to perform multidimensional EMD, we need to extend the 1-D PDE-based sifting process to 2-D space as shown by the steps below.\n\nHere, we take 2-D PDE-based EMD as an example.\n\n- Step 1 – Extend super diffusion model from 1-D to 2-D\n\nConsidered a super diffusion matrix function\n\nwhere formula_47 represent the qth-order stopping function in direction i.\n\nThen, based on the Navier–Strokes equations, diffusion equation will be:\n\nwhere formula_49 is the tension parameter, and we assumed that formula_50 .\n\n- Step 2 – Connect the relationship between diffusion model and PDEs on implicit surface\n\nIn order to relate to PDEs, the given equation will be\n\nwhere formula_52 is the 2qth-order differential operator on u intrinsic to surface S, and the initial condition for the equation will be formula_53 for any y on surface S.\n\n- Step 3 – Consider all the numerical resolutions\n\nTo obtain the theoretical and analysis result from the previous equation, we need to make an assumption.\n\nAssumption:\n\nThe numerical resolution schemes are assumed to be 4th-order PDE with no tension, and the equation for 4th-order PDE will be\n\nFirst of all, we will explicit scheme by approximating the PDE-based sifting process.\n\nwhere formula_56 is a vector which consists of the value of each pixel, formula_57 is a matrix which is a difference approximation to the operator, and formula_58 is a small time step.\n\nSecondly, we can use additive operator splitting (AOS) scheme to improve the property of stability, because the small time step formula_58 will be unstable when it comes to a large time step.\n\nFinally, we can use the alternating direction implicit (ADI) scheme. By using ADI-type schemes, it is suggested that to mix a derivative term to overcome the problem that ADI-type schemes can only be used in second-order diffusion equation. The numerically solved equation will be :\n\nwhere formula_61 is a matrix which is the central difference approximation to the operator formula_62\n\nBased on the Navier–Stokes equations directly, this approach provides a good way to obtain and develop theoretical and numerical results. In particular, the PDE-based BEMD can work well for image decomposition fields. This approach can be applied to extract transient signal and avoiding the indeterminacy characterization in some signals.\n\nThere are some problems in BEMD and boundary extending implementation in the iterative sifting process, including time consuming, shape and continuity of the edges, decomposition results comparison and so on. In order to fix these problems, the Boundary Processing in Bidimensional Empirical Decomposition (BPBEMD) method was created. The main points of the new method algorithm will be described next.\n\nThe few core steps for BPBEMD algorithm are:\n\n- Step 1\n\nAssuming the size of original input data and resultant data to be formula_63 and formula_64, respectively, we can also define that original input data matrix to be in the middle of resultant data matrix.\n\n- Step 2\n\nDivide both original input data matrix and resultant data matrix into blocks of formula_65 size.\n\n- Step 3\n\nFind the block which is the most similar to its neighbor block in the original input data matrix, and put it into the corresponding resultant data matrix.\n\n- Step 4\n\nForm a distance matrix which the matrix elements are weighted by different distances between each block from those boundaries.\n\n- Step 5\n\nImplement iterative extension when resultant data matrix faces a huge boundary extension, we can see that the block in original input data matrix is corresponding to the block in resultant data matrix.\n\nThis method can process larger number of elements than traditional BEMD method. Also, it can shorten the time consuming for the process. Depended on using nonparametric sampling based texture synthesis, the BPBEMD could obtain better result after decomposing and extracting.\n\nBecause most of image inputs are non-stationary which don’t exist boundary problems, the BPBEMD method is still lack of enough evidence that it is adaptive to all kinds of input data. Also, this method is narrowly restricted to be use on texture analysis and image processing.\n\nIn the first part, these MEEMD techniques can be used on Geophysical data sets such as climate, magnetic, seismic data variability which takes advantage of the fast algorithm of MEEMD. The MEEMD is often used for nonlinear geophysical data filtering due to its fast algorithms and its ability to handle large amount of data sets with the use of compression without losing key information. The IMF can also be used as a signal enhancement of Ground Penetrating Radar for nonlinear data processing; it is very effective to detect geological boundaries from the analysis of field anomalies.\n\nIn the second part, the PDE-based MEMD and FAMEMD can be implemented on audio processing, image processing and texture analysis. Because of its several properties, including stability, less time consuming and so on, PDE-based MEMD method works well for adaptive decomposition, data denoising and texture analysis. Furthermore, the FAMEMD is a great method to reduce computation time and have a precise estimation in the process. Finally, the BPBEMD method has good performance for image processing and texture analysis due to its property to solve the extension boundary problems in recent techniques.\n", "related": "NONE"}
{"id": "995908", "url": "https://en.wikipedia.org/wiki?curid=995908", "title": "Tomographic reconstruction", "text": "Tomographic reconstruction\n\nTomographic reconstruction is a type of multidimensional inverse problem where the challenge is to yield an estimate of a specific system from a finite number of projections. The mathematical basis for tomographic imaging was laid down by Johann Radon. A notable example of applications is the reconstruction of computed tomography (CT) where cross-sectional images of patients are obtained in non-invasive manner. Recent developments have seen the Radon transform and its inverse used for tasks related to realistic object insertion required for testing and evaluating computed tomography use in airport security.\n\nThis article applies in general to reconstruction methods for all kinds of tomography, but some of the terms and physical descriptions refer directly to the reconstruction of X-ray computed tomography.\n\nThe projection of an object, resulting from the tomographic measurement process at a given angle formula_1, is made up of a set of line integrals (see Fig. 1). A set of many such projections under different angles organized in 2D is called sinogram (see Fig. 3). In X-ray CT, the line integral represents the total attenuation of the beam of x-rays as it travels in a straight line through the object. As mentioned above, the resulting image is a 2D (or 3D) model of the attenuation coefficient. That is, we wish to find the image formula_2. The simplest and easiest way to visualise the method of scanning is the system of parallel projection, as used in the first scanners. For this discussion we consider the data to be collected as a series of parallel rays, at position formula_3, across a projection at angle formula_1. This is repeated for various angles. Attenuation occurs exponentially in tissue:\n\nwhere formula_2 is the attenuation coefficient as a function of position. Therefore, generally the total attenuation formula_7 of a ray at position formula_3, on the projection at angle formula_1, is given by the line integral:\n\nUsing the coordinate system of Figure 1, the value of formula_3 onto which the point formula_12 will be projected at angle formula_1 is given by:\n\nSo the equation above can be rewritten as\n\nwhere formula_16 represents formula_2 and formula_18 is the Dirac delta function. This function is known as the Radon transform (or \"sinogram\") of the 2D object.\n\nThe Fourier Transform of the projection can be written as\n\nformula_19 where formula_20\n\nformula_21 represents a slice of the 2D Fourier transform of formula_16 at angle formula_1. Using the inverse Fourier transform, the inverse Radon transform formula can be easily derived.\n\nformula_24\n\nwhere formula_25 is the derivative of the Hilbert transform of formula_26\n\nIn theory, the inverse Radon transformation would yield the original image. The projection-slice theorem tells us that if we had an infinite number of one-dimensional projections of an object taken at an infinite number of angles, we could perfectly reconstruct the original object, formula_16. However, there will only be a finite number of projections available in practice.\n\nAssuming formula_16 has effective diameter formula_29 and desired resolution is formula_30, rule of thumb number of projections needed for reconstruction is formula_31\n\nPractical reconstruction algorithms have been developed to implement the process of reconstruction of a 3-dimensional object from its projections. These algorithms are designed largely based on the mathematics of the Radon transform, statistical knowledge of the data acquisition process and geometry of the data imaging system.\n\nReconstruction can be made using interpolation. Assume formula_32-projections of formula_16 are generated at equally spaced angles, each sampled at same rate. The Discrete Fourier transform on each projection will yield sampling in the frequency domain. Combining all the frequency-sampled projections would generate a polar raster in the frequency domain. The polar raster will be sparse so interpolation is used to fill the unknown DFT points and reconstruction can be done through inverse Discrete Fourier transform. Reconstruction performance may improve by designing methods to change the sparsity of the polar raster, facilitating the effectiveness of interpolation.\n\nFor instance, a concentric square raster in the frequency domain can be obtained by changing the angle between each projection as follow:\n\nformula_34\n\nwhere formula_35 is highest frequency to be evaluated.\n\nThe concentric square raster improves computational efficiency by allowing all the interpolation positions to be on rectangular DFT lattice. Furthermore, it reduces the interpolation error. Yet, the Fourier-Transform algorithm has a disadvantage of producing inherently noisy output.\n\nIn practice of tomographic image reconstruction, often a stabilized and discretized version of the inverse Radon transform is used, known as the filtered back projection algorithm.\n\nWith a sampled discrete system, the inverse Radon Transform is\n\nformula_36\n\nformula_37\n\nwhere formula_38 is the angular spacing between the projections and formula_39 is radon kernel with frequency response formula_40.\n\nThe name back-projection comes from the fact that 1D projection needs to be filtered by 1D Radon kernel (back-projected) in order to obtain a 2D signal. The filter used does not contain DC gain, thus adding DC bias may be desirable. Reconstruction using back-projection allows better resolution than interpolation method described above. However, it induces greater noise because the filter is prone to amplify high-frequency content.\n\nIterative algorithm is computationally intensive but it allows to include \"a priori\" information about the system formula_16.\n\nLet formula_32 be the number of projections, formula_43 be the distortion operator for formula_44th projection taken at an angle formula_45. formula_46 are set of parameters to optimize the conversion of iterations.\n\nformula_47\n\nformula_48\n\nAn alternative family of recursive tomographic reconstruction algorithms are the Algebraic Reconstruction Techniques and iterative Sparse Asymptotic Minimum Variance.\n\nUse of a noncollimated fan beam is common since a collimated beam of radiation is difficult to obtain. Fan beams will generate series of line integrals, not parallel to each other, as projections. The fan-beam system will require 360 degrees range of angles which impose mechanical constraint, however, it allows faster signal acquisition time which may be advantageous in certain settings such as in the field of medicine. Back projection follows a similar 2 step procedure that yields reconstruction by computing weighted sum back-projections obtained from filtered projections.\n\nDeep learning methods are widely applied to image reconstruction nowadays and have achieved impressive results in various image reconstruction tasks, including low-dose denoising, sparse-view reconstruction, limited angle tomography and metal artifact reduction. An excellent overview can be found in the special issue of IEEE Transaction on Medical Imaging. One group of deep learning reconstruction algorithms apply post-processing neural networks to achieve image-to-image reconstruction, where input images are reconstructed by conventional reconstruction methods. Artifact reduction using the U-Net in limited angle tomography is such an example application . However, incorrect structures may occur in an image reconstructed by such a completely data-driven method , as displayed in the figure. Therefore, integration of know operators into the architecture design of neural networks appears beneficial, as described in the concept of precision learning . For example, direct image reconstruction from projection data can be learnt from the framework of filtered back-projection . Another example is to build neural networks by unrolling iterative reconstruction algorithms . Except for precision learning, using conventional reconstruction methods with deep learning reconstruction prior is also an alternative approach to improve the image quality of deep learning reconstruction.\n\nFor flexible tomographic reconstruction, open source toolboxes are available, such as PYRO-NN, TomoPy, CONRAD, ODL, the ASTRA toolbox, and TIGRE. TomoPy is an open-source Python toolbox to perform tomographic data processing and image reconstruction tasks at the Advanced Photon Source at Argonne National Laboratory. TomoPy toolbox is specifically designed to be easy to use and deploy at a synchrotron facility beamline. It supports reading many common synchrotron data formats from disk through Scientific Data Exchange, and includes several other processing algorithms commonly used for synchrotron data. TomoPy also includes several reconstruction algorithms, which can be run on multi-core workstations and large-scale computing facilities. The ASTRA Toolbox is a MATLAB and Python toolbox of high-performance GPU primitives for 2D and 3D tomography, from 2009–2014 developed by iMinds-Vision Lab, University of Antwerp and since 2014 jointly developed by iMinds-VisionLab (now imec-VisionLab), UAntwerpen and CWI, Amsterdam. The toolbox supports parallel, fan, and cone beam, with highly flexible source/detector positioning. A large number of reconstruction algorithms are available through TomoPy and the ASTRA toolkit, including FBP, Gridrec, ART, SIRT, SART, BART, CGLS, PML, MLEM and OSEM. Recently, the ASTRA toolbox has been integrated in the TomoPy framework. By integrating the ASTRA toolbox in the TomoPy framework, the optimized GPU-based reconstruction methods become easily available for synchrotron beamline users, and users of the ASTRA toolbox can more easily read data and use TomoPy’s other functionality for data filtering and artifact correction.\n\nShown in the gallery is the complete process for a simple object tomography and the following tomographic reconstruction based on ART.\n", "related": "\n- Operation of computed tomography#Tomographic reconstruction\n- Cone beam reconstruction\n- Industrial CT scanning\n- Industrial Tomography Systems\n\n- Avinash Kak & Malcolm Slaney (1988), Principles of Computerized Tomographic Imaging, IEEE Press, .\n- Bruyant, P.P. \"Analytic and iterative reconstruction algorithms in SPECT\" Journal of Nuclear Medicine 43(10):1343-1358, 2002\n\n- Insight ToolKit; open source tomographic support software\n- ASTRA (All Scales Tomographic Reconstruction Antwerp) toolbox; very flexible, fast and open source software for computed tomographic reconstruction\n- NiftyRec; comprehensive open source tomographic reconstruction software; Matlab and Python scriptable\n- Open-source tomographic reconstruction and visualization tool\n"}
{"id": "52279775", "url": "https://en.wikipedia.org/wiki?curid=52279775", "title": "Dependent component analysis", "text": "Dependent component analysis\n\nDependent component analysis (DCA) is a blind signal separation (BSS) method and an extension of Independent component analysis (ICA). ICA is the separating of mixed signals to individual signals without knowing anything about source signals. DCA is used to separate mixed signals into individual sets of signals that are dependent on signals within their own set, without knowing anything about the original signals. DCA can be ICA if all sets of signals only contain a single signal within their own set.\n\nFor simplicity, assume all individual sets of signals are the same size, k, and total N sets. Building off the basic equations of BSS (seen below) instead of independent source signals, one has independent sets of signals, s(t) = ({s(t)...,s(t)}...,{s(t)...,s(t)}), which are mixed by coefficients A=[a]εR that produce a set of mixed signals, x(t)=(x(t)...,x(t)). The signals can be multidimensional.\n\nformula_1\n\nThe following equation BSS separates the set of mixed signals, x(t), by finding and using coefficients, B=[B]εR, to separate and get the set of approximation of the original signals, y(t)=({y(t)...,y(t)}...,{y(t)...,y(t)}).\n\nformula_2\n\nSub-Band Decomposition ICA (SDICA) is based on the fact that wideband source signals are dependent, but that other subbands are independent. It uses an adaptive filter by choosing subbands using a minimum of mutual information (MI) to separate mixed signals. After finding subband signals, ICA can be used to reconstruct, based off subband signals, by using ICA. Below is a formula to find MI based on entropy, where H is entropy.\n\nformula_3\n\nformula_4\n\nformula_5\n", "related": "NONE"}
{"id": "48259106", "url": "https://en.wikipedia.org/wiki?curid=48259106", "title": "Transfer function matrix", "text": "Transfer function matrix\n\nIn control system theory, and various branches of engineering, a transfer function matrix, or just transfer matrix is a generalisation of the transfer functions of single-input single-output (SISO) systems to multiple-input and multiple-output (MIMO) systems. The matrix relates the outputs of the system to its inputs. It is a particularly useful construction for linear time-invariant (LTI) systems because it can be expressed in terms of the s-plane.\n\nIn some systems, especially ones consisting entirely of passive components, it can be ambiguous which variables are inputs and which are outputs. In electrical engineering, a common scheme is to gather all the voltage variables on one side and all the current variables on the other regardless of which are inputs or outputs. This results in all the elements of the transfer matrix being in units of impedance. The concept of impedance (and hence impedance matrices) has been borrowed into other energy domains by analogy, especially mechanics and acoustics.\n\nMany control systems span several different energy domains. This requires transfer matrices with elements in mixed units. This is needed both to describe transducers that make connections between domains and to describe the system as a whole. If the matrix is to properly model energy flows in the system, compatible variables must be chosen to allow this.\n\nA MIMO system with outputs and inputs is represented by a matrix. Each entry in the matrix is in the form of a transfer function relating an output to an input. For example, for a three-input, two-output system, one might write,\n\nwhere the are the inputs, the are the outputs, and the are the transfer functions. This may be written more succinctly in matrix operator notation as,\n\nwhere is a column vector of the outputs, is a matrix of the transfer functions, and is a column vector of the inputs.\n\nIn many cases, the system under consideration is a linear time-invariant (LTI) system. In such cases, it is convenient to express the transfer matrix in terms of the Laplace transform (in the case of continuous time variables) or the z-transform (in the case of discrete time variables) of the variables. This may be indicated by writing, for instance,\n\nwhich indicates that the variables and matrix are in terms of , the complex frequency variable of the s-plane arising from Laplace transforms, rather than time. The examples in this article are all assumed to be in this form, although that is not explicitly indicated for brevity. For discrete time systems is replaced by from the z-transform, but this makes no difference to subsequent analysis. The matrix is particular useful when it is a proper rational matrix, that is, all its elements are proper rational functions. In this case the state-space representation can be applied.\n\nIn systems engineering, the overall system transfer matrix is decomposed into two parts: representing the system being controlled, and representing the control system. takes as its inputs the inputs of and the outputs of . The outputs of form the inputs for .\n\nIn electrical systems it is often the case that the distinction between input and output variables is ambiguous. They can be either, depending on circumstance and point of view. In such cases the concept of port (a place where energy is transferred from one system to another) can be more useful than input and output. It is customary to define two variables for each port (): the voltage across it () and the current entering it (). For instance, the transfer matrix of a two-port network can be defined as follows,\n\nwhere the are called the impedance parameters, or \"z\"-parameters. They are so called because they are in units of impedance and relate port currents to a port voltage. The z-parameters are not the only way that transfer matrices are defined for two-port networks. There are six basic matrices that relate voltages and currents each with advantages for particular system network topologies. However, only two of these can be extended beyond two ports to an arbitrary number of ports. These two are the \"z\"-parameters and their inverse, the admittance parameters or \"y\"-parameters.\n\nTo understand the relationship between port voltages and currents and inputs and outputs, consider the simple voltage divider circuit. If we only wish to consider the output voltage () resulting from applying the input voltage () then the transfer function can be expressed as,\n\nwhich can be considered the trivial case of a 1×1 transfer matrix. The expression correctly predicts the output voltage if there is no current leaving port 2, but is increasingly inaccurate as the load increases. If, however, we attempt to use the circuit in reverse, driving it with a voltage at port 2 and calculate the resulting voltage at port 1 the expression gives completely the wrong result even with no load on port 1. It predicts a greater voltage at port 1 than was applied at port 2, an impossibility with a purely resistive circuit like this one. To correctly predict the behaviour of the circuit, the currents entering or leaving the ports must also be taken into account, which is what the transfer matrix does. The impedance matrix for the voltage divider circuit is,\n\nwhich fully describes its behaviour under all input and output conditions.\n\nAt microwave frequencies, none of the transfer matrices based on port voltages and currents are convenient to use in practice. Voltage is difficult to measure directly, current next to impossible, and the open circuits and short circuits required by the measurement technique cannot be achieved with any accuracy. For waveguide implementations, circuit voltage and current are entirely meaningless. Transfer matrices using different sorts of variables are used instead. These are the powers transmitted into, and reflected from a port which are readily measured in the transmission line technology used in distributed-element circuits in the microwave band. The most well known and widely used of these sorts of parameters is the scattering parameters, or s-parameters.\n\nThe concept of impedance can be extended into the mechanical, and other domains through a mechanical-electrical analogy, hence the impedance parameters, and other forms of 2-port network parameters, can be extended to the mechanical domain also. To do this an effort variable and a flow variable are made analogues of voltage and current respectively. For mechanical systems under translation these variables are force and velocity respectively.\n\nExpressing the behaviour of a mechanical component as a two-port or multi-port with a transfer matrix is a useful thing to do because, like electrical circuits, the component can often be operated in reverse and its behaviour is dependent on the loads at the inputs and outputs. For instance, a gear train is often characterised simply by its gear ratio, a SISO transfer function. However, the gearbox output shaft can be driven round to turn the input shaft requiring a MIMO analysis. In this example the effort and flow variables are torque and angular velocity respectively. The transfer matrix in terms of z-parameters will look like,\n\nHowever, the z-parameters are not necessarily the most convenient for characterising gear trains. A gear train is the analogue of an electrical transformer and the h-parameters (\"hybrid\" parameters) better describe transformers because they directly include the turns ratios (the analogue of gear ratios). The gearbox transfer matrix in h-parameter format is,\n\nFor an ideal gear train with no losses (friction, distortion etc), this simplifies to,\n\nwhere is the gear ratio.\n\nIn a system that consists of multiple energy domains, transfer matrices are required that can handle components with ports in different domains. In robotics and mechatronics, actuators are required. These usually consist of a transducer converting, for instance, signals from the control system in the electrical domain into motion in the mechanical domain. The control system also requires sensors that detect the motion and convert it back into the electrical domain through another transducer so that the motion can be properly controlled through a feedback loop. Other sensors in the system may be transducers converting yet other energy domains into electrical signals, such as optical, audio, thermal, fluid flow and chemical. Another application is the field of mechanical filters which require transducers between the electrical and mechanical domains in both directions.\n\nA simple example is an electromagnetic electromechanical actuator driven by an electronic controller. This requires a transducer with an input port in the electrical domain and an output port in the mechanical domain. This might be represented simplistically by a SISO transfer function, but for similar reasons to those already stated, a more accurate representation is achieved with a two-input, two-output MIMO transfer matrix. In the z-parameters, this takes the form,\n\nwhere is the force applied to the actuator and is the resulting velocity of the actuator. The impedance parameters here are a mixture of units; is an electrical impedance, is a mechanical impedance and the other two are transimpedances in a hybrid mix of units.\n\nAcoustic systems are a subset of fluid dynamics, and in both fields the primary input and output variables are pressure, , and volumetric flow rate, , except in the case of sound travelling through solid components. In the latter case, the primary variables of mechanics, force and velocity, are more appropriate. An example of a two-port acoustic component is a filter such as a muffler on an exhaust system. A transfer matrix representation of it may look like,\n\nHere, the are the transmission parameters, also known as ABCD-parameters. The component can be just as easily described by the z-parameters, but transmission parameters have a mathematical advantage when dealing with a system of two-ports that are connected in a cascade of the output of one into the input port of another. In such cases the overall transmission parameters are found simply by the matrix multiplication of the transmission parameter matrices of the constituent components.\n\nWhen working with mixed variables from different energy domains consideration needs to be given on which variables to consider analogous. The choice depends on what the analysis is intended to achieve. If it is desired to correctly model energy flows throughout the entire system then a pair of variables whose product is power (power conjugate variables) in one energy domain must map to power conjugate variables in other domains. Power conjugate variables are not unique so care needs to be taken to use the same mapping of variables throughout the system.\n\nA common mapping (used in some of the examples in this article) maps the effort variables (ones that initiate an action) from each domain together and maps the flow variables (ones that are a property of an action) from each domain together. Each pair of effort and flow variables is power conjugate. This system is known as the impedance analogy because a ratio of the effort to the flow variable in each domain is analogous to electrical impedance.\n\nThere are two other power conjugate systems on the same variables that are in use. The mobility analogy maps mechanical force to electric current instead of voltage. This analogy is widely used by mechanical filter designers and frequently in audio electronics also. The mapping has the advantage of preserving network topologies across domains but does not maintain the mapping of impedances. The Trent analogy classes the power conjugate variables as either \"across\" variables, or \"through\" variables depending on whether they act across an element of a system or through it. This largely ends up the same as the mobility analogy except in the case of the fluid flow domain (including the acoustics domain). Here pressure is made analogous to voltage (as in the impedance analogy) instead of current (as in the mobility analogy). However, force in the mechanical domain \"is\" analogous to current because force acts \"through\" an object.\n\nThere are some commonly used analogies that do not use power conjugate pairs. For sensors, correctly modelling energy flows may not be so important. Sensors often extract only tiny amounts of energy into the system. Choosing variables that are convenient to measure, particularly ones that the sensor is sensing, may be more useful. For instance, in the thermal resistance analogy, thermal resistance is considered analogous to electrical resistance, resulting in temperature difference and thermal power mapping to voltage and current respectively. The power conjugate of temperature difference is not thermal power, but rather entropy flow rate, something that cannot be directly measured. Another analogy of the same sort occurs in the magnetic domain. This maps magnetic reluctance to electrical resistance, resulting in magnetic flux mapping to current instead of magnetic flux rate of change as required for compatible variables.\n\nThe matrix representation of linear algebraic equations has been known for some time. Poincaré in 1907 was the first to describe a transducer as a pair of such equations relating electrical variables (voltage and current) to mechanical variables (force and velocity). Wegel, in 1921, was the first to express these equations in terms of mechanical impedance as well as electrical impedance.\n\nThe first use of transfer matrices to represent a MIMO control system was by Boksenbom and Hood in 1950, but only for the particular case of the gas turbine engines they were studying for the National Advisory Committee for Aeronautics. Cruickshank provided a firmer basis in 1955 but without complete generality. Kavanagh in 1956 gave the first completely general treatment, establishing the matrix relationship between system and control and providing criteria for realisability of a control system that could deliver a prescribed behaviour of the system under control.\n\n", "related": "\n- Transfer-matrix method (optics)\n\n- Bessai, Horst, \"MIMO Signals and Systems\", Springer, 2006 .\n- Bakshi, A.V.; Bakshi, U.A., \"Network Theory\", Technical Publications, 2008 .\n- Boksenbom, Aaron S.; Hood, Richard, \"General algebraic method applied to control analysis of complex engine types\", NACA Report 980, 1950.\n- Busch-Vishniac, Ilene J., \"Electromechanical Sensors and Actuators\", Springer, 1999 .\n- Chen, Wai Kai, \"The Electrical Engineering Handbook\", Academic Press, 2004 .\n- Choma, John, \"Electrical Networks: Theory and Analysis\", Wiley, 1985 .\n- Cruickshank, A. J. O., \"Matrix formulation of control system equations\", \"The Matrix and Tensor Quarterly\", vol. 5, no. 3, p. 76, 1955.\n- Iyer, T. S. K. V., \"Circuit Theory\", Tata McGraw-Hill Education, 1985 .\n- Kavanagh, R. J., \"The application of matrix methods to multi-variable control systems\", \"Journal of the Franklin Institute\", vol. 262, iss. 5, pp. 349–367, November 1956.\n- Koenig, Herman Edward; Blackwell, William A., \"Electromechanical System Theory\", McGraw-Hill, 1961\n- Levine, William S., \" The Control Handbook\", CRC Press, 1996 .\n- Nguyen, Cam, \"Radio-Frequency Integrated-Circuit Engineering\", Wiley, 2015 .\n- Olsen A., \"Characterization of Transformers by h-Paraameters\", \"IEEE Transactions on Circuit Theory\", vol. 13, iss. 2, pp. 239–240, June 1966.\n- Pierce, Allan D. \"Acoustics: an Introduction to its Physical Principles and Applications\", Acoustical Society of America, 1989 .\n- Poincaré, H., \"Etude du récepteur téléphonique\", \"Eclairage Electrique\", vol. 50, pp. 221–372, 1907.\n- Wegel, R. L., \"Theory of magneto-mechanical systems as applied to telephone receivers and similar structures\", \"Journal of the American Institute of Electrical Engineers\", vol. 40, pp. 791–802, 1921.\n- Yang, Won Y.; Lee, Seung C., \"Circuit Systems with MATLAB and PSpice\", Wiley 2008, .\n"}
{"id": "155562", "url": "https://en.wikipedia.org/wiki?curid=155562", "title": "Bode plot", "text": "Bode plot\n\nIn electrical engineering and control theory, a Bode plot is a graph of the frequency response of a system. It is usually a combination of a Bode magnitude plot, expressing the magnitude (usually in decibels) of the frequency response, and a Bode phase plot, expressing the phase shift.\n\nAs originally conceived by Hendrik Wade Bode in the 1930s, the plot is an asymptotic approximation of the frequency response, using straight line segments.\n\nAmong his several important contributions to circuit theory and control theory, engineer Hendrik Wade Bode, while working at Bell Labs in the United States in the 1930s, devised a simple but accurate method for graphing gain and phase-shift plots. These bear his name, \"Bode gain plot\" and \"Bode phase plot\". \"Bode\" is often pronounced although the Dutch pronunciation is Bo-duh. ().\n\nBode was faced with the problem of designing stable amplifiers with feedback for use in telephone networks. He developed the graphical design technique of the Bode plots to show the gain margin and phase margin required to maintain stability under variations in circuit characteristics caused during manufacture or during operation. The principles developed were applied to design problems of servomechanisms and other feedback control systems. The Bode plot is an example of analysis in the frequency domain.\n\nThe Bode plot for a linear, time-invariant system with transfer function formula_1 (formula_2 being the complex frequency in the Laplace domain) consists of a magnitude plot and a phase plot.\n\nThe Bode magnitude plot is the graph of the function formula_3 of frequency formula_4 (with formula_5 being the imaginary unit). The formula_4-axis of the magnitude plot is logarithmic and the magnitude is given in decibels, i.e., a value for the magnitude formula_7 is plotted on the axis at formula_8.\n\nThe Bode phase plot is the graph of the phase, commonly expressed in degrees, of the transfer function formula_9 as a function of formula_10. The phase is plotted on the same logarithmic formula_10-axis as the magnitude plot, but the value for the phase is plotted on a linear vertical axis.\n\nThis section illustrates that a Bode Plot is a visualization of the frequency response of a system.\n\nConsider a linear, time-invariant system with transfer function formula_12. Assume that the system is subject to a sinusoidal input with frequency formula_10,\n\nthat is applied persistently, i.e. from a time formula_15 to a time formula_16. The response will be of the form\n\ni.e., also a sinusoidal signal with amplitude formula_18 shifted in phase with respect to the input by a phase formula_19.\n\nIt can be shown that the magnitude of the response is\n\nand that the phase shift is\n\nA sketch for the proof of these equations is given in the appendix.\n\nIn summary, subjected to an input with frequency formula_10 the system responds at the same frequency with an output that is amplified by a factor formula_21 and phase-shifted by formula_22. These quantities, thus, characterize the frequency response and are shown in the Bode plot.\n\nFor many practical problems, the detailed Bode plots can be approximated with straight-line segments that are asymptotes of the precise response. The effect of each of the terms of a multiple element transfer function can be approximated by a set of straight lines on a Bode plot. This allows a graphical solution of the overall frequency response function. Before widespread availability of digital computers, graphical methods were extensively used to reduce the need for tedious calculation; a graphical solution could be used to identify feasible ranges of parameters for a new design.\n\nThe premise of a Bode plot is that one can consider the log of a function in the form:\n\nas a sum of the logs of its zeros and poles:\n\nThis idea is used explicitly in the method for drawing phase diagrams. The method for drawing amplitude plots implicitly uses this idea, but since the log of the amplitude of each pole or zero always starts at zero and only has one asymptote change (the straight lines), the method can be simplified.\n\nAmplitude decibels is usually done using formula_25 to define decibels. Given a transfer function in the form\n\nwhere formula_27 and formula_28 are constants, formula_29, formula_30, and formula_31 is the transfer function:\n\n- at every value of s where formula_32 (a zero), increase the slope of the line by formula_33 per decade.\n- at every value of s where formula_34 (a pole), decrease the slope of the line by formula_35 per decade.\n- The initial value of the graph depends on the boundaries. The initial point is found by putting the initial angular frequency formula_10 into the function and finding formula_37.\n- The initial slope of the function at the initial value depends on the number and order of zeros and poles that are at values below the initial value, and is found using the first two rules.\n\nTo handle irreducible 2nd order polynomials, formula_38 can, in many cases, be approximated as formula_39.\n\nNote that zeros and poles happen when formula_10 is \"equal to\" a certain formula_27 or formula_28. This is because the function in question is the magnitude of formula_43, and since it is a complex function, formula_44. Thus at any place where there is a zero or pole involving the term formula_45, the magnitude of that term is formula_46.\n\nTo correct a straight-line amplitude plot:\n\n- at every zero, put a point formula_47 above the line,\n- at every pole, put a point formula_48 below the line,\n- draw a smooth curve through those points using the straight lines as asymptotes (lines which the curve approaches).\n\nNote that this correction method does not incorporate how to handle complex values of formula_27 or formula_28. In the case of an irreducible polynomial, the best way to correct the plot is to actually calculate the magnitude of the transfer function at the pole or zero corresponding to the irreducible polynomial, and put that dot over or under the line at that pole or zero.\n\nGiven a transfer function in the same form as above:\n\nthe idea is to draw separate plots for each pole and zero, then add them up. The actual phase curve is given by\nformula_52.\n\nTo draw the phase plot, for each pole and zero:\n\n- if formula_53 is positive, start line (with zero slope) at formula_54\n- if formula_53 is negative, start line (with zero slope) at formula_56\n- if the sum of the number of unstable zeros and poles is odd, add 180 degrees to that basis\n- at every formula_57 (for stable zeros – formula_58), increase the slope by formula_59 degrees per decade, beginning one decade before formula_57 (E.g.: formula_61)\n- at every formula_62 (for stable poles – formula_63), decrease the slope by formula_64 degrees per decade, beginning one decade before formula_62 (E.g.: formula_61)\n- \"unstable\" (right half plane) poles and zeros (formula_67) have opposite behavior\n- flatten the slope again when the phase has changed by formula_68 degrees (for a zero) or formula_69 degrees (for a pole),\n- After plotting one line for each pole or zero, add the lines together to obtain the final phase plot; that is, the final phase plot is the superposition of each earlier phase plot.\n\nTo create a straight-line plot for a first-order (one-pole) lowpass filter, one considers the transfer function in terms of the angular frequency:\nThe above equation is the normalized form of the transfer function. The Bode plot is shown in Figure 1(b) above, and construction of the straight-line approximation is discussed next.\n\nThe magnitude (in decibels) of the transfer function above, (normalized and converted to angular frequency form), given by the decibel gain expression formula_71:\nthen plotted versus input frequency formula_10 on a logarithmic scale, can be approximated by two lines and it forms the asymptotic (approximate) magnitude Bode plot of the transfer function:\n- for angular frequencies below formula_74 it is a horizontal line at 0 dB since at low frequencies the formula_75 term is small and can be neglected, making the decibel gain equation above equal to zero,\n- for angular frequencies above formula_74 it is a line with a slope of −20 dB per decade since at high frequencies the formula_75 term dominates and the decibel gain expression above simplifies to formula_78 which is a straight line with a slope of formula_79 per decade.\n\nThese two lines meet at the corner frequency. From the plot, it can be seen that for frequencies well below the corner frequency, the circuit has an attenuation of 0 dB, corresponding to a unity pass band gain, i.e. the amplitude of the filter output equals the amplitude of the input. Frequencies above the corner frequency are attenuated – the higher the frequency, the higher the attenuation.\n\nThe phase Bode plot is obtained by plotting the phase angle of the transfer function given by\n\nversus formula_10, where formula_10 and formula_74 are the input and cutoff angular frequencies respectively. For input frequencies much lower than corner, the ratio formula_84 is small and therefore the phase angle is close to zero. As the ratio increases the absolute value of the phase increases and becomes –45 degrees when formula_85. As the ratio increases for input frequencies much greater than the corner frequency, the phase angle asymptotically approaches −90 degrees. The frequency scale for the phase plot is logarithmic.\n\nThe horizontal frequency axis, in both the magnitude and phase plots, can be replaced by the normalized (nondimensional) frequency ratio formula_75. In such a case the plot is said to be normalized and units of the frequencies are no longer used since all input frequencies are now expressed as multiples of the cutoff frequency formula_74.\n\nFigures 2-5 further illustrate construction of Bode plots. This example with both a pole and a zero shows how to use superposition. To begin, the components are presented separately.\n\nFigure 2 shows the Bode magnitude plot for a zero and a low-pass pole, and compares the two with the Bode straight line plots. The straight-line plots are horizontal up to the pole (zero) location and then drop (rise) at 20 dB/decade. The second Figure 3 does the same for the phase. The phase plots are horizontal up to a frequency factor of ten below the pole (zero) location and then drop (rise) at 45°/decade until the frequency is ten times higher than the pole (zero) location. The plots then are again horizontal at higher frequencies at a final, total phase change of 90°.\n\nFigure 4 and Figure 5 show how superposition (simple addition) of a pole and zero plot is done. The Bode straight line plots again are compared with the exact plots. The zero has been moved to higher frequency than the pole to make a more interesting example. Notice in Figure 4 that the 20 dB/decade drop of the pole is arrested by the 20 dB/decade rise of the zero resulting in a horizontal magnitude plot for frequencies above the zero location. Notice in Figure 5 in the phase plot that the straight-line approximation is pretty approximate in the region where both pole and zero affect the phase. Notice also in Figure 5 that the range of frequencies where the phase changes in the straight line plot is limited to frequencies a factor of ten above and below the pole (zero) location. Where the phase of the pole and the zero both are present, the straight-line phase plot is horizontal because the 45°/decade drop of the pole is arrested by the overlapping 45°/decade rise of the zero in the limited range of frequencies where both are active contributors to the phase.\n\nBode plots are used to assess the stability of negative feedback amplifiers by finding the gain and phase margins of an amplifier. The notion of gain and phase margin is based upon the gain expression for a negative feedback amplifier given by\n\nwhere A is the gain of the amplifier with feedback (the closed-loop gain), β is the feedback factor and \"A\" is the gain without feedback (the open-loop gain). The gain \"A\" is a complex function of frequency, with both magnitude and phase. Examination of this relation shows the possibility of infinite gain (interpreted as instability) if the product β\"A\" = −1. (That is, the magnitude of β\"A\" is unity and its phase is −180°, the so-called Barkhausen stability criterion). Bode plots are used to determine just how close an amplifier comes to satisfying this condition.\n\nKey to this determination are two frequencies. The first, labeled here as \"f\", is the frequency where the open-loop gain flips sign. The second, labeled here \"f\", is the frequency where the magnitude of the product | β \"A\" | = 1 (in dB, magnitude 1 is 0 dB). That is, frequency \"f\" is determined by the condition:\n\nwhere vertical bars denote the magnitude of a complex number (for example, formula_90), and frequency \"f\" is determined by the condition:\n\nOne measure of proximity to instability is the gain margin. The Bode phase plot locates the frequency where the phase of β\"A\" reaches −180°, denoted here as frequency \"f\". Using this frequency, the Bode magnitude plot finds the magnitude of β\"A\". If |β\"A\"| = 1, the amplifier is unstable, as mentioned. If \"A\"| < 1, instability does not occur, and the separation in dB of the magnitude of |β\"A\"| from |β\"A\"| = 1 is called the \"gain margin\". Because a magnitude of one is 0 dB, the gain margin is simply one of the equivalent forms: formula_92.\n\nAnother equivalent measure of proximity to instability is the phase margin. The Bode magnitude plot locates the frequency where the magnitude of |β\"A\"| reaches unity, denoted here as frequency \"f\". Using this frequency, the Bode phase plot finds the phase of β\"A\". If the phase of β\"A\"( \"f\") > −180°, the instability condition cannot be met at any frequency (because its magnitude is going to be < 1 when \"f = f\"), and the distance of the phase at \"f\" in degrees above −180° is called the \"phase margin\".\n\nIf a simple \"yes\" or \"no\" on the stability issue is all that is needed, the amplifier is stable if \"f\" < \"f\". This criterion is sufficient to predict stability only for amplifiers satisfying some restrictions on their pole and zero positions (minimum phase systems). Although these restrictions usually are met, if they are not another method must be used, such as the Nyquist plot.\nOptimal gain and phase margins may be computed using Nevanlinna–Pick interpolation theory.\n\nFigures 6 and 7 illustrate the gain behavior and terminology. For a three-pole amplifier, Figure 6 compares the Bode plot for the gain without feedback (the \"open-loop\" gain) \"A\" with the gain with feedback \"A\" (the \"closed-loop\" gain). See negative feedback amplifier for more detail.\n\nIn this example, \"A\" = 100 dB at low frequencies, and 1 / β = 58 dB. At low frequencies, \"A\" ≈ 58 dB as well.\n\nBecause the open-loop gain \"A\" is plotted and not the product β \"A\", the condition \"A\" = 1 / β decides \"f\". The feedback gain at low frequencies and for large \"A\" is \"A\" ≈ 1 / β (look at the formula for the feedback gain at the beginning of this section for the case of large gain \"A\"), so an equivalent way to find \"f\" is to look where the feedback gain intersects the open-loop gain. (Frequency \"f\" is needed later to find the phase margin.)\n\nNear this crossover of the two gains at \"f\", the Barkhausen criteria are almost satisfied in this example, and the feedback amplifier exhibits a massive peak in gain (it would be infinity if β \"A\" = −1). Beyond the unity gain frequency \"f\", the open-loop gain is sufficiently small that \"A\" ≈ \"A\" (examine the formula at the beginning of this section for the case of small \"A\").\n\nFigure 7 shows the corresponding phase comparison: the phase of the feedback amplifier is nearly zero out to the frequency \"f\" where the open-loop gain has a phase of −180°. In this vicinity, the phase of the feedback amplifier plunges abruptly downward to become almost the same as the phase of the open-loop amplifier. (Recall, \"A\" ≈ \"A\" for small \"A\".)\n\nComparing the labeled points in Figure 6 and Figure 7, it is seen that the unity gain frequency \"f\" and the phase-flip frequency \"f\" are very nearly equal in this amplifier, \"f\" ≈ \"f\" ≈ 3.332 kHz, which means the gain margin and phase margin are nearly zero. The amplifier is borderline stable.\n\nFigures 8 and 9 illustrate the gain margin and phase margin for a different amount of feedback β. The feedback factor is chosen smaller than in Figure 6 or 7, moving the condition | β \"A\" | = 1 to lower frequency. In this example, 1 / β = 77 dB, and at low frequencies \"A\" ≈ 77 dB as well.\n\nFigure 8 shows the gain plot. From Figure 8, the intersection of 1 / β and \"A\" occurs at \"f\" = 1 kHz. Notice that the peak in the gain \"A\" near \"f\" is almost gone.\nFigure 9 is the phase plot. Using the value of \"f\" = 1 kHz found above from the magnitude plot of Figure 8, the open-loop phase at \"f\" is −135°, which is a phase margin of 45° above −180°.\n\nUsing Figure 9, for a phase of −180° the value of \"f\" = 3.332 kHz (the same result as found earlier, of course). The open-loop gain from Figure 8 at \"f\" is 58 dB, and 1 / β = 77 dB, so the gain margin is 19 dB.\n\nStability is not the sole criterion for amplifier response, and in many applications a more stringent demand than stability is good step response. As a rule of thumb, good step response requires a phase margin of at least 45°, and often a margin of over 70° is advocated, particularly where component variation due to manufacturing tolerances is an issue. See also the discussion of phase margin in the step response article.\n\nThe Bode plotter is an electronic instrument resembling an oscilloscope, which produces a Bode diagram, or a graph, of a circuit's voltage gain or phase shift plotted against frequency in a feedback control system or a filter. An example of this is shown in Figure 10. It is extremely useful for analyzing and testing filters and the stability of feedback control systems, through the measurement of corner (cutoff) frequencies and gain and phase margins.\n\nThis is identical to the function performed by a vector network analyzer, but the network analyzer is typically used at much higher frequencies.\n\nFor education/research purposes, plotting Bode diagrams for given transfer functions facilitates better understanding and getting faster results (see external links).\n\nTwo related plots that display the same data in different coordinate systems are the Nyquist plot and the Nichols plot. These are parametric plots, with frequency as the input and magnitude and phase of the frequency response as the output. The Nyquist plot displays these in polar coordinates, with magnitude mapping to radius and phase to argument (angle). The Nichols plot displays these in rectangular coordinates, on the log scale.\n\nThis section shows that the frequency response is given by the magnitude and phase of the transfer function in Eqs.()-().\n\nSlightly changing the requirements for Eqs.()-() one assumes that the input has been applied starting at time formula_93 and one calculates the output in the limit formula_94. In this case, the output is given by the convolution\n\nof the input signal with the inverse Laplace transform of the transfer function formula_96. Assuming that the signal becomes periodic with mean 0 and period T after a while, we can add as many periods as we want to the interval of the integral\n\nThus, inserting the sinusodial input signal one obtains\n\nSince formula_96 is a real function this can be written as\n\nThe term in brackets is the definition of the Laplace transform of formula_101 at formula_102. Inserting the definition in the form formula_103 one obtains the output signal\n\nstated in Eqs.()-().\n\n", "related": "\n- Analog signal processing\n- Phase margin\n- Bode's sensitivity integral\n- Bode's magnitude (gain)–phase relation\n- Electrochemical impedance spectroscopy\n\n- Explanation of Bode plots with movies and examples\n- How to draw piecewise asymptotic Bode plots\n- Summarized drawing rules (PDF)\n- Bode plot applet - Accepts transfer function coefficients as input, and calculates magnitude and phase response\n- Circuit analysis in electrochemistry\n- Tim Green: \"Operational amplifier stability\" Includes some Bode plot introduction\n- Gnuplot code for generating Bode plot:\n- MATLAB function for creating a Bode plot of a system\n- MATLAB Tech Talk videos explaining Bode plots and showing how to use them for control design\n- Insert the poles and zeros and this website will draw the asymptotic and accurate Bode plots\n- Mathematica function for creating the Bode plot\n"}
{"id": "25849661", "url": "https://en.wikipedia.org/wiki?curid=25849661", "title": "Orthogonal signal correction", "text": "Orthogonal signal correction\n\nOrthogonal Signal Correction (OSC) is a spectral preprocessing technique that removes variation from a data matrix X that is orthogonal to the response matrix Y. OSC was introduced by researchers at the University of Umea in 1998 and has since found applications in domains including metabolomics.\n", "related": "NONE"}
{"id": "2407650", "url": "https://en.wikipedia.org/wiki?curid=2407650", "title": "Kaczmarz method", "text": "Kaczmarz method\n\nThe Kaczmarz method or Kaczmarz's algorithm is an iterative algorithm for solving linear equation systems formula_1. It was first discovered by the Polish mathematician Stefan Kaczmarz, and was rediscovered in the field of image reconstruction from projections by Richard Gordon, Robert Bender, and Gabor Herman in 1970, where it is called the Algebraic Reconstruction Technique (ART). ART includes the positivity constraint, making it nonlinear.\n\nThe Kaczmarz method is applicable to any linear system of equations, but its computational advantage relative to other methods depends on the system being sparse. It has been demonstrated to be superior, in some biomedical imaging applications, to other methods such as the filtered backprojection method.\n\nIt has many applications ranging from computed tomography (CT) to signal processing. It can be obtained also by applying to the hyperplanes, described by the linear system, the method of successive projections onto convex sets (POCS).\n\nLet formula_2 be a system of linear equations, let formula_3 the number of rows of \"A\", formula_4 be the formula_5th row of complex-valued matrix formula_6, and let formula_7 be arbitrary complex-valued initial approximation to the solution of formula_8. For formula_9 compute:\n\nwhere formula_10 and formula_11 denotes complex conjugation of formula_12.\n\nIf the system is consistent, formula_13 converges to the minimum-norm solution, provided that the iterations start with the zero vector.\n\nA more general algorithm can be defined using a relaxation parameter formula_14\n\nThere are versions of the method that converge to a regularized weighted least squares solution when applied to a system of inconsistent equations and, at least as far as initial behavior is concerned, at a lesser cost than other iterative methods, such as the conjugate gradient method.\n\nIn 2009, a randomized version of the Kaczmarz method for overdetermined linear systems was introduced by Thomas Strohmer and Roman Vershynin in which the \"i\"-th equation is selected randomly with probability proportional to formula_16\n\nThis method can be seen as a particular case of stochastic gradient descent.\n\nUnder such circumstances formula_17 converges exponentially fast to the solution of formula_18 and the rate of convergence depends only on the scaled condition number formula_19.\n\nWe have\n\nUsing\n\nwe can write () as\n\nThe main point of the proof is to view the left hand side in () as an expectation of some random variable. Namely, recall that the solution space of the formula_25 equation of formula_8 is the hyperplane\n\nwhose normal is formula_28 Define a random vector \"Z\" whose values are the normals to all the equations of formula_29, with probabilities as in our algorithm:\n\nThen () says that\n\nThe orthogonal projection formula_32 onto the solution space of a random equation of formula_8 is given by formula_34\n\nNow we are ready to analyze our algorithm. We want to show that the error formula_35 reduces at each step in average (conditioned on the previous steps) by at least the factor of formula_36 The next approximation formula_37 is computed from formula_38 as formula_39 where formula_40 are independent realizations of the random projection formula_41 The vector formula_42 is in the kernel of formula_43 It is orthogonal to the solution space of the equation onto which formula_44 projects, which contains the vector formula_45 (recall that formula_46 is the solution to all equations). The orthogonality of these two vectors then yields\n\nTo complete the proof, we have to bound formula_48 from below. By the definition of formula_37, we have\n\nwhere formula_51 are independent realizations of the random vector formula_52\n\nThus\n\nNow we take the expectation of both sides conditional upon the choice of the random vectors formula_54 (hence we fix the choice of the random projections formula_55 and thus the random vectors formula_56 and we average over the random vector formula_57). Then\n\nBy () and the independence,\n\nTaking the full expectation of both sides, we conclude that\n\nThe superiority of this selection was illustrated with the reconstruction of a bandlimited function from its nonuniformly spaced sampling values. However, it has been pointed out that the reported success by Strohmer and Vershynin depends on the specific choices that were made there in translating the underlying problem, whose geometrical nature is to \"find a common point of a set of hyperplanes\", into a system of algebraic equations. There will always be legitimate algebraic representations of the underlying problem for which the selection method in will perform in an inferior manner.\n\nThe Kaczmarz iteration () has a purely geometric interpretation: the algorithm successively projects the current iterate onto the hyperplane defined by the next equation. Hence, any scaling of the equations is irrelevant; it can also be seen from () that any (nonzero) scaling of the equations cancels out. Thus, in RK, one can use formula_61 or any other weights that may be relevant. Specifically, in the above-mentioned reconstruction example, the equations were chosen with probability proportional to the average distance of each sample point from its two nearest neighbors — a concept introduced by Feichtinger and . For additional progress on this topic, see, and the references therein.\n\nIn 2015, Robert M. Gower and Peter Richtarik developed a versatile randomized iterative method for solving a consistent system of linear equations formula_62 which includes the randomized Kaczmarz algorithm as a special case. Other special cases include randomized coordinate descent, randomized Gaussian descent and randomized Newton method. Block versions and versions with importance sampling of all these methods also arise as special cases. The method is shown to enjoy exponential rate decay (in expectation) - also known as linear convergence, under very mild conditions on the way randomness enters the algorithm. The Gower-Richtarik method is the first algorithm uncovering a \"sibling\" relationship between these methods, some of which were independently proposed before, while many of which were new.\n\nInteresting new insights about the randomized Kaczmarz method that can be gained from the analysis of the method include:\n\n- The general rate of the Gower-Richtarik algorithm precisely recovers the rate of the randomized Kaczmarz method in the special case when it reduced to it.\n- The choice of probabilities for which the randomized Kaczmarz algorithm was originally formulated and analyzed (probabilities proportional to the squares of the row norms) is not optimal. Optimal probabilities are the solution of a certain semidefinite program. The theoretical complexity of randomized Kaczmarz with the optimal probabilities can be arbitrarily better than the complexity for the standard probabilities. However, the amount by which it is better depends on the matrix formula_6. There are problems for which the standard probabilities are optimal.\n- When applied to a system with matrix formula_6 which is positive definite, Randomized Kaczmarz method is equivalent to the Stochastic Gradient Descent (SGD) method (with a very special stepsize) for minimizing the strongly convex quadratic function formula_65 Note that since formula_66 is convex, the minimizers of formula_66 must satisfy formula_68, which is equivalent to formula_69 The \"special stepsize\" is the stepsize which leads to a point which in the one-dimensional line spanned by the stochastic gradient minimizes the Euclidean distance from the unknown(!) minimizer of formula_70, namely, from formula_71 This insight is gained from a dual view of the iterative process (below described as \"Optimization Viewpoint: Constrain and Approximate\").\n\nThe Gower-Richtarik method enjoys six seemingly different but equivalent formulations, shedding additional light on how to interpret it (and, as a consequence, how to interpret its many variants, including randomized Kaczmarz):\n\n- 1. Sketching viewpoint: Sketch & Project\n- 2. Optimization viewpoint: Constrain and Approximate\n- 3. Geometric viewpoint: Random Intersect\n- 4. Algebraic viewpoint 1: Random Linear Solve\n- 5. Algebraic viewpoint 2: Random Update\n- 6. Analytic viewpoint: Random Fixed Point\n\nWe now describe some of these viewpoints. The method depends on 2 parameters:\n\n- a positive definite matrix formula_72 giving rise to a weighted Euclidean inner product formula_73 and the induced norm\n\n- and a random matrix formula_75 with as many rows as formula_6 (and possibly random number of columns).\n\nGiven previous iterate formula_77 the new point formula_78 is computed by drawing a random matrix formula_75 (in an iid fashion from some fixed distribution), and setting\n\nThat is, formula_78 is obtained as the projection of formula_82 onto the randomly sketched system formula_83. The idea behind this method is to pick formula_75 in such a way that a projection onto the sketched system is substantially simpler than the solution of the original system formula_85. Randomized Kaczmarz method is obtained by picking formula_72 to be the identity matrix, and formula_75 to be the formula_88 unit coordinate vector with probability formula_89 Different choices of formula_72 and formula_75 lead to different variants of the method.\n\nA seemingly different but entirely equivalent formulation of the method (obtained via Lagrangian duality) is\n\nwhere formula_93 is also allowed to vary, and where formula_94 is any solution of the system formula_21 Hence, formula_78 is obtained by first constraining the update to the linear subspace spanned by the columns of the random matrix formula_97, i.e., to\n\nand then choosing the point formula_20 from this subspace which best approximates formula_100. This formulation may look surprising as it seems impossible to perform the approximation step due to the fact that formula_94 is not known (after all, this is what we are trying the compute!). However, it is still possible to do this, simply because formula_78 computed this way is the same as formula_78 computed via the sketch and project formulation and since formula_94 does not appear there.\n\nThe update can also be written explicitly as\n\nwhere by formula_106 we denote the Moore-Penrose pseudo-inverse of matrix formula_107. Hence, the method can be written in the form formula_108, where formula_109 is a random update vector.\n\nLetting formula_110 it can be shown that the system formula_111 always has a solution formula_112, and that for all such solutions the vector formula_113 is the same. Hence, it does not matter which of these solutions is chosen, and the method can be also written as formula_114. The pseudo-inverse leads just to one particular solution. The role of the pseudo-inverse is twofold:\n\n- It allows the method to be written in the explicit \"random update\" form as above,\n- It makes the analysis simple through the final, sixth, formulation.\n\nIf we subtract formula_94 from both sides of the random update formula, denote\n\nand use the fact that formula_117 we arrive at the last formulation:\n\nwhere formula_119 is the identity matrix. The iteration matrix, formula_120 is random, whence the name of this formulation.\n\nBy taking conditional expectations in the 6th formulation (conditional on formula_121), we obtain\n\nBy taking expectation again, and using the tower property of expectations, we obtain\n\nGower and Richtarik show that\n\nwhere the matrix norm is defined by\n\nMoreover, without any assumptions on formula_75 one has formula_127 By taking norms and unrolling the recurrence, we obtain\n\n\"Remark\". A sufficient condition for the expected residuals to converge to 0 is formula_129 This can be achieved if formula_6 has a full column rank and under very mild conditions on formula_131 Convergence of the method can be established also without the full column rank assumption in a different way.\n\nIt is also possible to show a stronger result:\n\nThe expected squared norms (rather than norms of expectations) converge at the same rate:\n\n\"Remark\". This second type of convergence is stronger due to the following identity which holds for any random vector formula_20 and any fixed vector formula_94:\n\nWe have seen that the randomized Kaczmarz method appears as a special case of the Gower-Richtarik method for formula_136 and formula_75 being the formula_88 unit coordinate vector with probability formula_139 where formula_12 is the formula_88 row of formula_142 It can be checked by direct calculation that\n\n\n- A randomized Kaczmarz algorithm with exponential convergence\n- Comments on the randomized Kaczmarz method\n", "related": "NONE"}
{"id": "54523041", "url": "https://en.wikipedia.org/wiki?curid=54523041", "title": "Tone-Lok", "text": "Tone-Lok\n\nTone-Lok Effects are guitar effects pedals from a (now discontinued) product line, introduced by Ibanez in 1999. In contrast with other guitar pedals, they included a \"Lok\" feature, engaged for each adjustment by pressing down on its corresponding potentiometer's control knob.\n\nAP7 Analog Phaser<br>\nAW7 Autowah<br>\nCF7 Stereo Chorus/Flanger<br>\nDE7 Stereo Delay/Echo<br>\nDS7 Distortion<br>\nFZ7 Fuzz<br>\nLF7 Lo Fi<br>\nPH7 Phaser<br>\nPM7 Phase Modulator<br>\nSH7 Seventh Heaven<br>\nSM7 Smashbox<br>\nTC7 Tri Mode Chorus<br>\nTS7 Tubescreamer<br>\nWD7 Weeping Demon<br>\nWD7JR Weeping Demon Junior\n\nPD7 Phat-Hed Bass Overdrive<br>\nSB7 Synthesizer Bass<br>\n", "related": "NONE"}
{"id": "28448", "url": "https://en.wikipedia.org/wiki?curid=28448", "title": "Speech processing", "text": "Speech processing\n\nSpeech processing is the study of speech signals and the processing methods of signals. The signals are usually processed in a digital representation, so speech processing can be regarded as a special case of digital signal processing, applied to speech signals. Aspects of speech processing includes the acquisition, manipulation, storage, transfer and output of speech signals. The input is called speech recognition and the output is called speech synthesis.\n\nEarly attempts at speech processing and recognition were primarily focused on understanding a handful of simple phonetic elements such as vowels. In 1952, three researchers at Bell Labs, Stephen. Balashek, R. Biddulph, and K. H. Davis, developed a system that could recognize digits spoken by a single speaker. \n\nLinear predictive coding (LPC), a speech processing algorithm, was first proposed by Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966. Further developments in LPC technology were made by Bishnu S. Atal and Manfred R. Schroeder at Bell Labs during the 1970s. LPC was the basis for voice-over-IP (VoIP) technology, as well as speech synthesizer chips, such as the Texas Instruments LPC Speech Chips used in the Speak & Spell toys from 1978.\n\nOne of the first commercially available speech recognition products was Dragon Dictate, released in 1990. In 1992, technology developed by Lawrence Rabiner and others at Bell Labs was used by AT&T in their Voice Recognition Call Processing service to route calls without a human operator. By this point, the vocabulary of these systems was larger than the average human vocabulary. \n\nBy the early 2000s, the dominant speech processing strategy started to shift away from Hidden Markov Models towards more modern neural networks and deep learning.\n\nDynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed. In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules. The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.\n\nA hidden Markov model can be represented as the simplest dynamic Bayesian network. The goal of the algorithm is to estimate a hidden variable x(t) given a list of observations y(t). By applying the Markov property, the conditional probability distribution of the hidden variable \"x\"(\"t\") at time \"t\", given the values of the hidden variable \"x\" at all times, depends \"only\" on the value of the hidden variable \"x\"(\"t\" − 1). Similarly, the value of the observed variable \"y\"(\"t\") only depends on the value of the hidden variable \"x\"(\"t\") (both at time \"t\").\n\nAn artificial neural network (ANN) is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs.\n\n- Interactive Voice Systems\n- Virtual Assistants\n- Voice Identification\n- Emotion Recognition\n- Call Center Automation\n- Robotics\n\n", "related": "\n- Neurocomputational speech processing\n- Speech coding\n- Speech technology\n- Natural Language Processing\n"}
{"id": "3458672", "url": "https://en.wikipedia.org/wiki?curid=3458672", "title": "Pole–zero plot", "text": "Pole–zero plot\n\nIn mathematics, signal processing and control theory, a pole–zero plot is a graphical representation of a rational transfer function in the complex plane which helps to convey certain properties of the system such as:\n\n- Stability\n- Causal system / anticausal system\n- Region of convergence (ROC)\n- Minimum phase / non minimum phase\n\nA pole-zero plot shows the location in the complex plane of the poles and zeros of the transfer function of a dynamic system, such as a controller, compensator, sensor, equalizer, filter, or communications channel. By convention, the poles of the system are indicated in the plot by an X while the zeros are indicated by a circle or O.\n\nA pole-zero plot can represent either a continuous-time (CT) or a discrete-time (DT) system. For a CT system, the plane in which the poles and zeros appear is the s plane of the Laplace transform. In this context, the parameter \"s\" represents the complex angular frequency, which is the domain of the CT transfer function. For a DT system, the plane is the z plane, where \"z\" represents the domain of the Z-transform.\n\nIn general, a rational transfer function for a continuous-time LTI system has the form:\n\nwhere\n- formula_2 and formula_3 are polynomials in formula_4,\n- formula_5 is the order of the numerator polynomial,\n- formula_6 is the \"m\"-th coefficient of the numerator polynomial,\n- formula_7 is the order of the denominator polynomial, and\n- formula_8 is the \"n\"-th coefficient of the denominator polynomial.\n\nEither M or N or both may be zero, but in real systems, it should be the case that formula_9; otherwise the gain would be unbounded at high frequencies.\n\n- the zeros of the system are roots of the numerator polynomial:\n\n- the poles of the system are roots of the denominator polynomial:\n\nThe region of convergence (ROC) for a given CT transfer function is a half-plane or vertical strip, either of which contains no poles. In general, the ROC is not unique, and the particular ROC in any given case depends on whether the system is causal or anti-causal.\n\n- If the ROC includes the imaginary axis, then the system is bounded-input, bounded-output (BIBO) stable.\n- If the ROC extends rightward from the pole with the largest real-part (but not at infinity), then the system is causal.\n- If the ROC extends leftward from the pole with the smallest real-part (but not at negative infinity), then the system is anti-causal.\n\nThe ROC is usually chosen to include the imaginary axis since it is important for most practical systems to have BIBO stability.\n\nThis system has no (finite) zeros and two poles:\n\nand\n\nThe pole-zero plot would be:\n\nNotice that these two poles are complex conjugates, which is the necessary and sufficient condition to have real-valued coefficients in the differential equation representing the system.\n\nIn general, a rational transfer function for a discrete-time LTI system has the form:\n\nwhere\n- formula_5 is the order of the numerator polynomial,\n- formula_6 is the \"m\"-th coefficient of the numerator polynomial,\n- formula_7 is the order of the denominator polynomial, and\n- formula_8 is the \"n\"-th coefficient of the denominator polynomial.\n\nEither M or N or both may be zero.\n\n- formula_22 such that formula_23 are the zeros of the system\n- formula_24 such that formula_25 are the poles of the system.\n\nThe region of convergence (ROC) for a given DT transfer function is a disk or annulus which contains no poles. In general, the ROC is not unique, and the particular ROC in any given case depends on whether the system is causal or anti-causal.\n\n- If the ROC includes the unit circle, then the system is bounded-input, bounded-output (BIBO) stable.\n- If the ROC extends outward from the pole with the largest (but not infinite) magnitude, then the system has a right-sided impulse response. If the ROC extends outward from the pole with the largest magnitude and there is no pole at infinity, then the system is causal.\n- If the ROC extends inward from the pole with the smallest (nonzero) magnitude, then the system is anti-causal.\n\nThe ROC is usually chosen to include the unit circle since it is important for most practical systems to have BIBO stability.\n\nIf formula_26 and formula_27 are completely factored, their solution can be easily plotted in the z-plane. For example, given the following transfer function:\n\nThe only (finite) zero is located at: formula_29, and the two poles are located at: formula_30, where \"j\" is the imaginary unit.\n\nThe pole–zero plot would be:\n\n", "related": "\n- Root locus\n- Laplace transform\n- Z-transform\n- Rational function\n\n"}
{"id": "54698311", "url": "https://en.wikipedia.org/wiki?curid=54698311", "title": "Video line selector", "text": "Video line selector\n\nA video line selector is an electronic circuit or device for picking a line from an analog video signal. The input of the circuit is connected to an analog video source, the output triggers an oscilloscope, so display the selected line on the oscilloscope or similar device.\n\nVideo line selectors are circuits or units of other devices, fitted to the demand of the unit or a separate device for use in workshops, production and laboratories. They contain analog and digital circuits and an internal or external DC power supply. There's a video signal input, sometimes an output to prevent reflexions of the video signal and the cause of shadows of the video picture, also a trigger output. There is also an input or adjust for the line number(s) to be picked out and as an option an automatic or manual setting to fit other video standards and non-interlaced video. Video line selectors do not need all the picture signal, just the synchronisation signals are needed. Sometimes inputs for H- and V-sync were installed, only.\n\nThe video signal input is 75 Ω terminated or connected to the video output for a monitor. The amplified video signal is connected to the inputs of the H- und V-sync detector circuits. The H-sync detector outputs the horizontal synchronisation pulse filtered from the video signal. This is the line synchronisation and makes the lines fit vertically. The V-sync detector filters the vertical synchronisation and makes the picture fit the same position on the screen than the previous one.\n\nBoth synchronisation output pulses are fed to a digital synchron counter. The V-sync resets the counter. The H-sync is being counted. On every frame picture, the counter is being reset and the lines were counted. Most often interlaced video was used, spitting up a picture in the odd numbered lines, followed by the even-numbered lines in a half picture each. (→deninterlacing).\n\nInterlace video requires a V-sync detector which detects first a second scan of the interlaced frame. \nSome reset the counter and toggle an interlace bit, others ignore the sync after the odd-numbered lines and continue counting.\nBroadcast television systems were based on a nearly identical monochrome video signal with minor changes all over the world, which a number of lines can be covered by 10 bit counter (2 < lines < 2 → 512 < 576 < 1024). The digital comparator, feed by the line number preset and the counter detects the logical equivalence as match of the binary numbers, which is the output pulse of the video line selector. When fed to the trigger input of an oscilloscope, the signal of the selected video line is displayed on the oscilloscope when the test probe is fed by the video signal. A precision timer can trigger a pixel or dot of the line.\n\nIn order to simplify the digital part of the circuit, it is possible to load the preset line number into the counter and have it count descending. When the counter reaches zero, the trigger output is set. A 10 inputs NOR gate is more sufficient than a 10 bits digital comparator, but evaluating several lines per picture is no longer possible. Decreasing the line number by one, the carry bit of the counter can be used as trigger output, replacing a 10 inputs NOR-Gate.\n\nVideo line selecting was used in laboratory, production, and workshop: (selection only)\n- focussing CCD-Sensors in cameras (all areas of the picture)\n- analyzing a television signal on quality and troubleshooting video devices\nMonitoring television picture content:\n- decoding teletext\n- was used on decoding Channel Videodat, a former television service in Germany, broadcasting software and data over television\n- Restoring data like „ArVid“, using a videocassette recorder for data storage\nFor modifying television signals:\n- teletext output to screen\n- merge on-screen displays, logos or text into the television picture\nAs precise optical sensor:\n- use a camera as optical sensor for analyzing a taken picture in automation,\n- use a camera as a line sensor,\n- use a camera as vertical selective line sensor.\n\n", "related": "\n- component video, HD-MAC, back porch\n\n- \"Video Line Selector\" Circuit and documentation at elm-chan.org 12 April 2002\n- (German) PAL Line-Selector at controller-designs.de\n"}
{"id": "51518803", "url": "https://en.wikipedia.org/wiki?curid=51518803", "title": "Steered-Response Power Phase Transform", "text": "Steered-Response Power Phase Transform\n\nSteered-Response Power Phase Transform (SRP-PHAT) is a popular algorithm for acoustic source localization, well known for its robust performance in adverse acoustic environments. The algorithm can be interpreted as a beamforming-based approach that searches for the candidate position that maximizes the output of a steered delay-and-sum beamformer.\n\nConsider a system of formula_1 microphones, where each microphone is denoted by a subindex formula_2. The discrete-time output signal from a microphone is formula_3. The steered-response power (SRP) at a spatial point formula_4 can be expressed as\n\nformula_5\n\nwhere formula_6 denotes the set of integer numbers and formula_7 would be the time-lag due to the propagation from a source located at formula_8 to the formula_9-th microphone.\n\nThe SRP can be rewritten as \nformula_10\n\nwhere formula_11 denotes complex conjugation, formula_12 represents the discrete-time Fourier transform of formula_13 and formula_14 is a weighting function in the frequency domain (later discussed). The term formula_15 is the discrete time-difference of arrival (TDOA) of a signal emitted at position formula_8 to microphones formula_17 and formula_18, given by\n\nformula_19\nwhere formula_20 is the sampling frequency of the system, formula_21 is the sound propagation speed, formula_22 is the position of the formula_9-th microphone, formula_24 is the 2-norm and formula_25 denotes the rounding operator.\n\nThe above SRP objective function can be expressed as a sum of Generalized Cross-Correlations (GCCs) for the different microphone pairs at the time-lag corresponding to their TDOA\n\nformula_26\n\nwhere the GCC for a microphone pair formula_27 is defined as\n\nformula_28\n\nThe phase transform (PHAT) is an effective GCC weighting for time delay estimation in reverberant environments, that forces the GCC to consider only the phase information of the involved signals:\n\nformula_29\n\nThe SRP-PHAT algorithm consists in a grid-search procedure that evaluates the objective function formula_30 on a grid of candidate source locations formula_31 to estimate the spatial location of the sound source, formula_32, as the point of the grid that provides the maximum SRP:\n\nformula_33\n\nModifications of the classical SRP-PHAT algorithm have been proposed to reduce the computational cost of the grid-search step of the algorithm and to increase the robustness of the method. In the classical SRP-PHAT, for each microphone pair and for each point of the grid, a unique integer TDOA value is selected to be the acoustic delay corresponding to that grid point. This procedure does not guarantee that all TDOAs are associated to points on the grid, nor that the spatial grid is consistent, since some of the points may not correspond to an intersection of hyperboloids. This issue becomes more problematic with coarse grids since, when the number of points is reduced, part of the TDOA information gets lost because most delays are not anymore associated to any point in the grid.\n\nThe modified SRP-PHAT collects and uses the TDOA information related to the volume surrounding each spatial point of the search grid by considering a modified objective function:\n\nformula_34\n\nwhere formula_35 and formula_36 are the lower and upper accumulation limits of GCC delays, which depend on the spatial location formula_8.\n\nThe accumulation limits can be calculated beforehand in an exact way by exploring the boundaries separating the regions corresponding to the points of the grid. Alternatively, they can be selected by considering the spatial gradient of the TDOA formula_38, where each component formula_39 of the gradient is:\n\nformula_40\n\nFor a rectangular grid where neighboring points are separated a distance formula_41, the lower and upper accumulation limits are given by:\n\nformula_42\nformula_43\nwhere formula_44 and the gradient direction angles are given by\n\nformula_45\n\nformula_46\n\n", "related": "\n- Acoustic source localization\n- Multilateration\n- Audio signal processing\n"}
{"id": "49968788", "url": "https://en.wikipedia.org/wiki?curid=49968788", "title": "Masreliez's theorem", "text": "Masreliez's theorem\n\nMasreliez theorem describes a recursive algorithm\nwithin the technology of extended Kalman filter, named after the Swedish-American physicist John Masreliez, who is its author. The algorithm estimates the state of a dynamic system with the help of often incomplete measurements marred by distortion.\n\nMasreliez's theorem produces estimates that are quite good approximations to the exact conditional mean in non-Gaussian additive outlier (AO) situations. Some evidence for this can be had by Monte Carlo simulations.\n\nThe key approximation property used to construct these filters is that the state prediction density is approximately Gaussian. Masreliez discovered in 1975 that this approximation yields an intuitively appealing non-Gaussian filter recursions, with data dependent covariance (unlike the Gaussian case) this derivation also provides one of the nicest ways of establishing the standard Kalman filter recursions. Some theoretical justification for use of the Masreliez approximation is provided by the \"continuity of state prediction densities\" theorem in Martin (1979).\n\n", "related": "\n- Control engineering\n- Hidden Markov model\n- Bayes' theorem\n- Robust optimization\n- Probability theory\n- Nyquist–Shannon sampling theorem\n"}
{"id": "857897", "url": "https://en.wikipedia.org/wiki?curid=857897", "title": "Time–frequency analysis", "text": "Time–frequency analysis\n\nIn signal processing, time–frequency analysis comprises those techniques that study a signal in both the time and frequency domains \"simultaneously,\" using various time–frequency representations. Rather than viewing a 1-dimensional signal (a function, real or complex-valued, whose domain is the real line) and some transform (another function whose domain is the real line, obtained from the original via some transform), time–frequency analysis studies a two-dimensional signal – a function whose domain is the two-dimensional real plane, obtained from the signal via a time–frequency transform.\n\nThe mathematical motivation for this study is that functions and their transform representation are often tightly connected, and they can be understood better by studying them jointly, as a two-dimensional object, rather than separately. A simple example is that the 4-fold periodicity of the Fourier transform – and the fact that two-fold Fourier transform reverses direction – can be interpreted by considering the Fourier transform as a 90° rotation in the associated time–frequency plane: 4 such rotations yield the identity, and 2 such rotations simply reverse direction (reflection through the origin).\n\nThe practical motivation for time–frequency analysis is that classical Fourier analysis assumes that signals are infinite in time or periodic, while many signals in practice are of short duration, and change substantially over their duration. For example, traditional musical instruments do not produce infinite duration sinusoids, but instead begin with an attack, then gradually decay. This is poorly represented by traditional methods, which motivates time–frequency analysis.\n\nOne of the most basic forms of time–frequency analysis is the short-time Fourier transform (STFT), but more sophisticated techniques have been developed, notably wavelets.\n\nIn signal processing, time–frequency analysis is a body of techniques and methods used for characterizing and manipulating signals whose statistics vary in time, such as transient signals.\n\nIt is a generalization and refinement of Fourier analysis, for the case when the signal frequency characteristics are varying with time. Since many signals of interest – such as speech, music, images, and medical signals – have changing frequency characteristics, time–frequency analysis has broad scope of applications.\n\nWhereas the technique of the Fourier transform can be extended to obtain the frequency spectrum of any slowly growing locally integrable signal, this approach requires a complete description of the signal's behavior over all time. Indeed, one can think of points in the (spectral) frequency domain as smearing together information from across the entire time domain. While mathematically elegant, such a technique is not appropriate for analyzing a signal with indeterminate future behavior. For instance, one must presuppose some degree of indeterminate future behavior in any telecommunications systems to achieve non-zero entropy (if one already knows what the other person will say one cannot learn anything).\n\nTo harness the power of a frequency representation without the need of a complete characterization in the time domain, one first obtains a time–frequency distribution of the signal, which represents the signal in both the time and frequency domains simultaneously. In such a representation the frequency domain will only reflect the behavior of a temporally localized version of the signal. This enables one to talk sensibly about signals whose component frequencies vary in time.\n\nFor instance rather than using tempered distributions to globally transform the following function into the frequency domain one could instead use these methods to describe it as a signal with a time varying frequency.\n\nOnce such a representation has been generated other techniques in time–frequency analysis may then be applied to the signal in order to extract information from the signal, to separate the signal from noise or interfering signals, etc.\n\nThere are several different ways to formulate a valid time–frequency distribution function, resulting in several well-known time–frequency distributions, such as:\n\n- Short-time Fourier transform (including the Gabor transform),\n- Wavelet transform,\n- Bilinear time–frequency distribution function (Wigner distribution function, or WDF),\n- Modified Wigner distribution function, Gabor–Wigner distribution function, and so on (see Gabor–Wigner transform).\n- Hilbert–Huang transform\n\nMore information about the history and the motivation of development of time–frequency distribution can be found in the entry Time–frequency representation.\n\nA time–frequency distribution function ideally has the following properties:\n\n1. High resolution in both time and frequency, to make it easier to be analyzed and interpreted.\n2. No cross-term to avoid confusing real components from artifacts or noise.\n3. A list of desirable mathematical properties to ensure such methods benefit real-life application.\n4. Lower computational complexity to ensure the time needed to represent and process a signal on a time–frequency plane allows real-time implementations.\n\nBelow is a brief comparison of some selected time–frequency distribution functions.\nTo analyze the signals well, choosing an appropriate time–frequency distribution function is important. Which time–frequency distribution function should be used depends on the application being considered, as shown by reviewing a list of applications. The high clarity of the Wigner distribution function (WDF) obtained for some signals is due to the auto-correlation function inherent in its formulation; however, the latter also causes the cross-term problem. Therefore, if we want to analyze a single-term signal, using the WDF may be the best approach; if the signal is composed of multiple components, some other methods like the Gabor transform, Gabor-Wigner distribution or Modified B-Distribution functions may be better choices.\n\nAs an illustration, Fourier analysis cannot distinguish the signals:\n\nBut time–frequency analysis can.\n\nThe following applications need not only the time–frequency distribution functions but also some operations to the signal. The Linear canonical transform (LCT) is really helpful. By LCTs, the shape and location on the time–frequency plane of a signal can be in the arbitrary form that we want it to be. For example, the LCTs can shift the time–frequency distribution to any location, dilate it in the horizontal and vertical direction without changing its area on the plane, shear (or twist) it, and rotate it (Fractional Fourier transform). This powerful operation, LCT, make it more flexible to analyze and apply the time–frequency distributions.\n\nThe definition of instantaneous frequency is the time rate of change of phase, or\n\nwhere formula_5 is the instantaneous phase of a signal. We can know the instantaneous frequency from the time–frequency plane directly if the image is clear enough. Because the high clarity is critical, we often use WDF to analyze it.\n\nThe goal of filter design is to remove the undesired component of a signal. Conventionally, we can just filter in the time domain or in the frequency domain individually as shown below.\n\nThe filtering methods mentioned above can’t work well for every signal which may overlap in the time domain or in the frequency domain. By using the time–frequency distribution function, we can filter in the Euclidean time–frequency domain or in the fractional domain by employing the fractional Fourier transform. An example is shown below.\n\nFilter design in time–frequency analysis always deals with signals composed of multiple components, so one cannot use WDF due to cross-term. The Gabor transform, Gabor–Wigner distribution function, or Cohen's class distribution function may be better choices.\n\nThe concept of signal decomposition relates to the need to separate one component from the others in a signal; this can be achieved through a filtering operation which require a filter design stage. Such filtering is traditionally done in the time domain or in the frequency domain; however, this may not be possible in the case of non-stationary signals that are multicomponent as such components could overlap in both the time domain and also in the frequency domain; as a consequence, the only possible way to achieve component separation and therefore a signal decomposition is to implement a time–frequency filter.\n\nBy the Nyquist–Shannon sampling theorem, we can conclude that the minimum number of sampling points without aliasing is equivalent to the area of the time–frequency distribution of a signal. (This is actually just an approximation, because the TF area of any signal is infinite.) Below is an example before and after we combine the sampling theory with the time–frequency distribution:\n\nIt is noticeable that the number of sampling points decreases after we apply the time–frequency distribution.\n\nWhen we use the WDF, there might be the cross-term problem (also called interference). On the other hand, using Gabor transform causes an improvement in the clarity and readability of the representation, therefore improving its interpretation and application to practical problems.\n\nConsequently, when the signal we tend to sample is composed of single component, we use the WDF; however, if the signal consists of more than one component, using the Gabor transform, Gabor-Wigner distribution function, or other reduced interference TFDs may achieve better results.\n\nThe Balian–Low theorem formalizes this, and provides a bound on the minimum number of time–frequency samples needed.\n\nConventionally, the operation of modulation and multiplexing concentrates in time or in frequency, separately. By taking advantage of the time–frequency distribution, we can make it more efficient to modulate and multiplex. All we have to do is to fill up the time–frequency plane. We present an example as below.\nAs illustrated in the upper example, using the WDF is not smart since the serious cross-term problem make it difficult to multiplex and modulate.\n\nWe can represent an electromagnetic wave in the form of a 2 by 1 matrix\n\nwhich is similar to the time–frequency plane. When electromagnetic wave propagates through free-space, the Fresnel diffraction occurs. We can operate with the 2 by 1 matrix\n\nby LCT with parameter matrix\n\nwhere \"z\" is the propagation distance and formula_9 is the wavelength. When electromagnetic wave pass through a spherical lens or be reflected by a disk, the parameter matrix should be\n\nand\n\nrespectively, where ƒ is the focal length of the lens and \"R\" is the radius of the disk. These corresponding results can be obtained from\n\nLight is a kind of electromagnetic wave, so we apply the time–frequency analysis to optics in the same way as to electromagnetic wave propagation. In the same way, a characteristic of acoustic signals is that, often, its frequency varies really severely with time. Because the acoustic signals usually contain a lot of data, it is suitable to use simpler TFDs such as the Gabor transform to analyze the acoustic signals due to the lower computational complexity. If speed is not an issue, then a detailed comparison with well defined criteria should be made before selecting a particular TFD. Another approach is to define a signal dependent TFD that is adapted to the data.\nIn biomedicine, one can use time–frequency distribution to analyze the electromyography (EMG), electroencephalography (EEG), electrocardiogram (ECG) or otoacoustic emissions (OAEs).\n\nEarly work in time–frequency analysis can be seen in the Haar wavelets (1909) of Alfréd Haar, though these were not significantly applied to signal processing. More substantial work was undertaken by Dennis Gabor, such as Gabor atoms (1947), an early form of wavelets, and the Gabor transform, a modified short-time Fourier transform. The Wigner–Ville distribution (Ville 1948, in a signal processing context) was another foundational step.\n\nParticularly in the 1930s and 1940s, early time–frequency analysis developed in concert with quantum mechanics (Wigner developed the Wigner–Ville distribution in 1932 in quantum mechanics, and Gabor was influenced by quantum mechanics – see Gabor atom); this is reflected in the shared mathematics of the position-momentum plane and the time–frequency plane – as in the Heisenberg uncertainty principle (quantum mechanics) and the Gabor limit (time–frequency analysis), ultimately both reflecting a symplectic structure.\n\nAn early practical motivation for time–frequency analysis was the development of radar – see ambiguity function.\n\n", "related": "\n- Cone-shape distribution function\n- Multiresolution analysis\n- Spectral density estimation\n- Time–frequency analysis for music signal\n"}
{"id": "714163", "url": "https://en.wikipedia.org/wiki?curid=714163", "title": "Cross-correlation", "text": "Cross-correlation\n\nIn signal processing, cross-correlation is a measure of similarity of two series as a function of the displacement of one relative to the other. This is also known as a \"sliding dot product\" or \"sliding inner-product\". It is commonly used for searching a long signal for a shorter, known feature. It has applications in pattern recognition, single particle analysis, electron tomography, averaging, cryptanalysis, and neurophysiology. The cross-correlation is similar in nature to the convolution of two functions. In an autocorrelation, which is the cross-correlation of a signal with itself, there will always be a peak at a lag of zero, and its size will be the signal energy.\n\nIn probability and statistics, the term \"cross-correlations\" refers to the correlations between the entries of two random vectors formula_1 and formula_2, while the \"correlations\" of a random vector formula_1 are the correlations between the entries of formula_1 itself, those forming the correlation matrix of formula_1. If each of formula_1 and formula_2 is a scalar random variable which is realized repeatedly in a time series, then the correlations of the various temporal instances of formula_1 are known as \"autocorrelations\" of formula_1, and the cross-correlations of formula_1 with formula_2 across time are temporal cross-correlations. In probability and statistics, the definition of correlation always includes a standardising factor in such a way that correlations have values between −1 and +1.\n\nIf formula_12 and formula_13 are two independent random variables with probability density functions formula_14 and formula_15, respectively, then the probability density of the difference formula_16 is formally given by the cross-correlation (in the signal-processing sense) formula_17; however, this terminology is not used in probability and statistics. In contrast, the convolution formula_18 (equivalent to the cross-correlation of formula_19 and formula_20) gives the probability density function of the sum formula_21.\n\nFor continuous functions formula_14 and formula_15, the cross-correlation is defined as:\n\nwhich is equivalent to\n\nwhere formula_25 denotes the complex conjugate of formula_19, and formula_27 is the displacement, also known as \"lag\" (a feature in formula_14 at formula_29 occurs in formula_15 at formula_31).\n\nSimilarly, for discrete functions, the cross-correlation is defined as:\n\nwhich is equivalent to\n\nAs an example, consider two real valued functions formula_14 and formula_15 differing only by an unknown shift along the x-axis. One can use the cross-correlation to find how much formula_15 must be shifted along the x-axis to make it identical to formula_14. The formula essentially slides the formula_15 function along the x-axis, calculating the integral of their product at each position. When the functions match, the value of formula_38 is maximized. This is because when peaks (positive areas) are aligned, they make a large contribution to the integral. Similarly, when troughs (negative areas) align, they also make a positive contribution to the integral because the product of two negative numbers is positive.\nWith complex-valued functions formula_14 and formula_15, taking the conjugate of formula_14 ensures that aligned peaks (or aligned troughs) with imaginary components will contribute positively to the integral.\n\nIn econometrics, lagged cross-correlation is sometimes referred to as cross-autocorrelation.\n\n- The cross-correlation of functions formula_19 and formula_43 is equivalent to the convolution (denoted by formula_44) of formula_45 and formula_43. That is:\n- formula_47\n- formula_48\n- If formula_14 is a Hermitian function, then formula_50\n- If both formula_14 and formula_15 are Hermitian, then formula_53.\n- formula_54.\n- Analogous to the convolution theorem, the cross-correlation satisfies\n- formula_55\n- The cross-correlation is related to the spectral density (see Wiener–Khinchin theorem).\n- The cross-correlation of a convolution of formula_14 and formula_61 with a function formula_15 is the convolution of the cross-correlation of formula_15 and formula_14 with the kernel formula_61:\n- formula_66.\n\nIf formula_14 and formula_15 are both continuous periodic functions of period formula_69, the integration from formula_70 to formula_71 is replaced by integration over any interval formula_72 of length formula_69:\n\nwhich is equivalent to\n\nFor random vectors formula_76 and formula_77, each containing random elements whose expected value and variance exist, the cross-correlation matrix of formula_1 and formula_2 is defined by\n\nand has dimensions formula_80. Written component-wise:\n\nThe random vectors formula_1 and formula_2 need not have the same dimension, and either might be a scalar value.\n\nFor example, if formula_84 and formula_85 are random vectors, then\nformula_86 is a formula_87 matrix whose formula_88-th entry is formula_89.\n\nIf formula_90 and formula_91 are complex random vectors, each containing random variables whose expected value and variance exist, the cross-correlation matrix of formula_92 and formula_93 is defined by\n\nwhere formula_95 denotes Hermitian transposition.\n\nIn time series analysis and statistics, the cross-correlation of a pair of random process is the correlation between values of the processes at different times, as a function of the two times. Let formula_96 be a pair of random processes, and formula_29 be any point in time (formula_29 may be an integer for a discrete-time process or a real number for a continuous-time process). Then formula_99 is the value (or realization) produced by a given run of the process at time formula_29.\n\nSuppose that the process has means formula_101 and formula_102 and variances formula_103 and formula_104 at time formula_29, for each formula_29. Then the definition of the cross-correlation between times formula_107 and formula_108 is\n\nwhere formula_109 is the expected value operator. Note that this expression may be not defined.\n\nSubtracting the mean before multiplication yields the cross-covariance between times formula_107 and formula_108:\n\nNote that this expression is not well-defined for all-time series or processes, because the mean may not exist, or the variance may not exist.\n\nLet formula_96 represent a pair of stochastic processes that are jointly wide-sense stationary. Then the Cross-covariance function and the cross-correlation function are given as follows.\n\nor equivalently\n\nor equivalently\n\nwhere formula_115 and formula_116 are the mean and standard deviation of the process formula_117, which are constant over time due to stationarity; and similarly for formula_118, respectively. formula_119 indicates the expected value. That the cross-covariance and cross-correlation are independent of formula_29 is precisely the additional information (beyond being individually wide-sense stationary) conveyed by the requirement that formula_96 are \"jointly\" wide-sense stationary.\n\nThe cross-correlation of a pair of jointly wide sense stationary stochastic processes can be estimated by averaging the product of samples measured from one process and samples measured from the other (and its time shifts). The samples included in the average can be an arbitrary subset of all the samples in the signal (e.g., samples within a finite time window or a sub-sampling of one of the signals). For a large number of samples, the average converges to the true cross-correlation.\n\nIt is common practice in some disciplines (e.g. statistics and time series analysis) to normalize the cross-correlation function to get a time-dependent Pearson correlation coefficient. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms \"cross-correlation\" and \"cross-covariance\" are used interchangeably.\n\nThe definition of the normalized cross-correlation of a stochastic process is\n\nIf the function formula_123 is well-defined, its value must lie in the range formula_124, with 1 indicating perfect correlation and −1 indicating perfect anti-correlation.\n\nFor jointly wide-sense stationary stochastic processes, the definition is\n\nThe normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of statistical dependence, and because the normalization has an effect on the statistical properties of the estimated autocorrelations.\n\nFor jointly wide-sense stationary stochastic processes, the cross-correlation function has the following symmetry property:\nRespectively for jointly WSS processes:\n\nCross-correlations are useful for determining the time delay between two signals, e.g., for determining time delays for the propagation of acoustic signals across a microphone array. After calculating the cross-correlation between the two signals, the maximum (or minimum if the signals are negatively correlated) of the cross-correlation function indicates the point in time where the signals are best aligned; i.e., the time delay between the two signals is determined by the argument of the maximum, or arg max of the cross-correlation, as in\n\nFor image-processing applications in which the brightness of the image and template can vary due to lighting and exposure conditions, the images can be first normalized. This is typically done at every step by subtracting the mean and dividing by the standard deviation. That is, the cross-correlation of a template, formula_129 with a subimage formula_130 is\n\nwhere formula_132 is the number of pixels in formula_129 and formula_130,\nformula_135 is the average of formula_14 and formula_137 is standard deviation of formula_14.\n\nIn functional analysis terms, this can be thought of as the dot product of two normalized vectors. That is, if\nand\nthen the above sum is equal to\nwhere formula_142 is the inner product and formula_143 is the \"L\"² norm.\n\nThus, if formula_14 and formula_29 are real matrices, their normalized cross-correlation equals the cosine of the angle between the unit vectors formula_146 and formula_69, being thus formula_148 if and only if formula_146 equals formula_69 multiplied by a positive scalar.\n\nNormalized correlation is one of the methods used for template matching, a process used for finding incidences of a pattern or object within an image. It is also the 2-dimensional version of Pearson product-moment correlation coefficient.\n\nNCC is similar to ZNCC with the only difference of not subtracting the local mean value of intensities:\n\nCaution must be applied when using cross correlation for nonlinear systems. In certain circumstances, which depend on the properties of the input, cross correlation between the input and output of a system with nonlinear dynamics can be completely blind to certain nonlinear effects. This problem arises because some quadratic moments can equal zero and this can incorrectly suggest that there is little \"correlation\" (in the sense of statistical dependence) between two signals, when in fact the two signals are strongly related by nonlinear dynamics.\n\n", "related": "\n- Autocorrelation\n- Autocovariance\n- Coherence\n- Convolution\n- Correlation\n- Correlation function\n- Cross-correlation matrix\n- Cross-covariance\n- Cross-spectrum\n- Digital image correlation\n- Phase correlation\n- Scaled correlation\n- Spectral density\n- Wiener–Khinchin theorem\n\n- Cross Correlation from Mathworld\n- http://scribblethink.org/Work/nvisionInterface/nip.html\n- http://www.staff.ncl.ac.uk/oliver.hinton/eee305/Chapter6.pdf\n"}
{"id": "19447404", "url": "https://en.wikipedia.org/wiki?curid=19447404", "title": "Wideband audio", "text": "Wideband audio\n\nWideband audio, also known as wideband voice or HD voice, is high definition voice quality for telephony audio, contrasted with standard digital telephony \"toll quality\". It extends the frequency range of audio signals transmitted over telephone lines, resulting in higher quality speech. The range of the human voice extends from 80 Hz to 14 kHz but traditional, voiceband or narrowband telephone calls limit audio frequencies to the range of 300 Hz to 3.4 kHz. Wideband audio relaxes the bandwidth limitation and transmits in the audio frequency range of 50 Hz to 7 kHz or even up to 22 kHz. In addition, some wideband codecs may use a higher audio bit depth of 16-bits to encode samples, also resulting in much better voice quality.\n\nIn 1987, the International Telecommunication Union (ITU) standardized a version of wideband audio known as G.722. Radio broadcasters began using G.722 over Integrated Services Digital Network (ISDN) to provide high-quality audio for remote broadcasts, such as commentary from sports venues. AMR-WB (G.722.2) was developed by Nokia and VoiceAge and it was first specified by 3GPP.\n\nThe traditional telephone network (PSTN) is generally limited to narrowband audio by the intrinsic nature of its transmission technology, TDM (time-division multiplexing), and by the analogue-to-digital converters used at the edge of the network, as well as the speakers, microphones and other elements in the endpoints themselves.\n\nWideband audio has been broadly deployed in conjunction with videoconferencing. Providers of this technology quickly discovered that despite the explicit emphasis on video transmission, the quality of the participant experience was significantly influenced by the fidelity of the associated audio signal.\n\nCommunications via Voice over Internet Protocol (VoIP) can readily employ wideband audio. When PC-to-PC calls are placed via VoIP services, such as Skype, and the participants use a high-quality headset, the resulting call quality can be noticeably superior to conventional PSTN calls. A number of audio codecs have emerged to support these services, supplementing G.722.\n\nManufacturers of audio conferencing equipment have introduced wideband-capable models that include support for G.722 over VoIP.\n\nConference calls are a direct beneficiary of the enhancements offered by wideband audio. Participants often struggle to figure out who is talking, or to understand accented speakers. Misunderstandings are commonplace due primarily to generally poor audio quality and an accumulation of background noise.\n\nSome listener benefits cited of wideband audio compared to traditional (narrowband):\n- clearer overall sound quality\n- easier to recognize voices, distinguish confusing sounds and understand accented speakers\n- ease of deciphering words that have the close sounds of ‘s’ and ‘f’ and others, often indistinguishable over telephone lines\n- ability to hear faint talkers and to understand double-talk (when more than one person is speaking at the same time)\n- reduced listening effort (decreased cognitive load), resulting in increased productivity and lessened listener fatigue\n- better understanding amidst other impairments, such as when talkers are using a speakerphone or in the presence of background noise\n\nDespite its reputation for poor audio quality, the mobile telephone industry has started to make some progress on wideband audio. The 3GPP standards group has designated G.722.2 as its wideband codec and calls it Adaptive Multirate – Wideband (AMR-WB). More than a hundred handsets have been introduced supporting this codec (for example, Apple, Google, HTC, Nokia, Samsung and Sony), and network demonstrations have been conducted.\n\nAs business telephone systems have adopted VoIP technology, support for wideband audio has grown rapidly. Telephone sets from Avaya, Cisco, NEC Unified Solutions, Grandstream, Gigaset, Panasonic (which brands wideband audio \"HD Sonic\"), Polycom (which brands wideband audio \"HD Voice\"), Snom, AudioCodes (which brands wideband audio \"HDVoIP\") and others now incorporate G.722, as well as varying degrees of higher-quality audio components. AMR-WB is natively supported in Android since Android Gingerbread, and in iOS since the iPhone 5.\n\nSuppliers of integrated circuits for telephony equipment, including DSP Group, Broadcom, Infineon, and Texas Instruments, include wideband audio in their feature portfolios. There are audio conferencing service providers that support wideband connections from these and other VoIP endpoints, while also permitting PSTN participants to join the conference in narrowband. sipXtapi is an open source solution for VoIP media processing engine supporting wideband and HD voice that provides RTP and codecs through a plugin framework for use with SIP and other VoIP protocols. Skype uses an audio codec called Silk which allows for extremely high quality audio.\n\nA number of carriers around the world have rolled out HD voice services based on the G.722 wideband standard. In North America, hosted service providers have recently deployed the Aastra Hi-Q upgrade to its installed user base and as of January 2010 claimed around 70,000 HD voice endpoints. Consumer service provider ooma has an estimated 25,000 HD voice endpoints deployed stemming from its roll out of its second-generation Telo hardware.\n\nAs of December 2015, a report announces 117 commercial mobile HD Voice networks launched in 76 countries.\n\nThe following are wideband audio coding standards and audio codecs used in telecommunication.\n\n- Extended Adaptive Multi-Rate – Wideband (AMR-WB+)\n- Variable-Rate Multimode Wideband (VMR-WB)\n- AAC-LD (MPEG-4 ER AAC-LD)\n- Speex\n- Skype SILK\n- Microsoft RTAudio\n- internet Speech Audio Codec (iSAC)\n- Opus\n\n", "related": "\n- List of Smartphones using HD Voice\n\n- The Effect of Bandwidth on Speech Intelligibility – Polycom technical White Paper\n- VoIP transitioning to High Definition Voice – InfoWorld blog on wideband audio\n- Texas Instruments HD Audio website\n- Can You Hear What I Mean? Polycom Delivers HD Voice – sponsored IDC White Paper\n- International Telecommunications Union\n- VoIP Planet Article: High-Definition Voice: The Future of Phone\n"}
{"id": "1257591", "url": "https://en.wikipedia.org/wiki?curid=1257591", "title": "Near–far problem", "text": "Near–far problem\n\nThe near–far problem or hearability problem is the effect of a strong signal from a near signal source in making it hard for a receiver to hear a weaker signal from a further source due to adjacent-channel interference, co-channel interference, distortion, capture effect, dynamic range limitation, or the like. Such a situation is common in wireless communication systems, in particular CDMA. In some signal jamming techniques, the near–far problem is exploited to disrupt (\"jam\") communications.\n\nConsider a receiver and two transmitters, one close to the receiver, the other far away. If both transmitters transmit simultaneously and at equal powers, then due to the inverse square law the receiver will receive more power from the nearer transmitter. Since one transmission's signal is the other's noise, the signal-to-noise ratio (SNR) for the further transmitter is much lower. This makes the farther transmitter more difficult, if not impossible, to understand. In short, the near–far problem is one of detecting or filtering out a weaker signal amongst stronger signals.\n\nTo place this problem in more common terms, imagine you are talking to someone 6 meters away. If the two of you are in a quiet, empty room then a conversation is quite easy to hold at normal voice levels. In a loud, crowded bar, it would be impossible to hear the same voice level, and the only solution (for that distance) is for both you and your friend to speak louder. Of course, this increases the overall noise level in the bar, and every other patron has to talk louder too (this is equivalent to power control runaway). Eventually, everyone has to shout to make themselves heard by a person standing right beside them, and it is impossible to communicate with anyone more than half a meter away. In general, however, a human is very capable of filtering out loud sounds; similar techniques can be deployed in signal processing where suitable criteria for distinguishing between signals can be established (see signal processing and notably adaptive signal processing.)\n\nTaking this analogy back to wireless communications, the far transmitter would have to drastically increase transmission power which simply may not be possible.\n\nIn CDMA systems and similar cellular phone-like networks, the problem is commonly solved by dynamic output power adjustment of the transmitters. That is, the closer transmitters use less power so that the SNR for all transmitters at the receiver is roughly the same. This sometimes can have a noticeable impact on battery life, which can be dramatically different depending on distance from the base station. In high-noise situations, however, closer transmitters may boost their output power, which forces distant transmitters to boost their output to maintain a good SNR. Other transmitters react to the rising noise floor by increasing their output. This process continues, and eventually distant transmitters lose their ability to maintain a usable SNR and drop from the network. This process is called \"power control runaway\". This principle may be used to explain why an area with low signal is perfectly usable when the cell isn't heavily loaded, but when load is higher, service quality degrades significantly, sometimes to the point of unusability.\n\nOther possible solutions to the near–far problem:\n1. Increased receiver dynamic range - Use a higher resolution ADC. Increase the dynamic range of receiver stages that are saturating.\n2. Dynamic output power control – Nearby transmitters decrease their output power so that all signals arrive at the receiver with similar signal strengths.\n3. TDMA – Transmitters use some scheme to avoid transmitting at the same time.\n\n", "related": "\n- Direct-sequence spread spectrum\n- Hidden node problem\n- Signal-to-noise ratio\n"}
{"id": "9166436", "url": "https://en.wikipedia.org/wiki?curid=9166436", "title": "Multitaper", "text": "Multitaper\n\nIn signal processing, the multitaper method is a technique developed by David J. Thomson to estimate the power spectrum \"S\" of a stationary ergodic finite-variance random process \"X\", given a finite contiguous realization of \"X\" as data. It is one of a number of approaches to spectral density estimation.\n\nThe multitaper method overcomes some of the limitations of conventional Fourier analysis. When applying the Fourier transform to extract spectral information from a signal, we assume that each Fourier coefficient is a reliable representation of the amplitude and relative phase of the corresponding component frequency. This assumption, however, is not always valid. For instance, a single trial represents only one noisy realization of the underlying process of interest. A comparable situation arises in statistics when estimating measures of central tendency i.e., it is bad practice to estimate qualities of a population using individuals or very small samples. Likewise, a single sample of a process does not necessarily provide a reliable estimate of its spectral properties. Moreover, the naive power spectral density obtained from the signal's Fourier transform is a biased estimate of the true spectral content.\n\nThese problems are often overcome by averaging over many realizations of the same event. However, this method is unreliable with small data sets and undesirable when one does not wish to attenuate signal components that vary across trials. Instead of ensemble averaging, the multitaper method reduces estimation bias by obtaining multiple independent estimates from the same sample. Each data taper is multiplied element-wise by the signal to provide a windowed trial from which one estimates the power at each component frequency. As each taper is pairwise orthogonal to all other tapers, the windowed signals provide statistically independent estimates of the underlying spectrum. The final spectrum is obtained by averaging over all the tapered spectra. Thomson chose the Slepian or discrete prolate spheroidal sequences as tapers since these vectors are mutually orthogonal and possess desirable spectral concentration properties (see the section on Slepian sequences). In practice, a weighted average is often used to compensate for increased energy loss at higher order tapers.\n\nConsider a p-dimensional zero mean stationary stochastic process\n\nHere \"T\" denotes the matrix transposition. In neurophysiology for example, \"p\" refers to the total number of channels and\nhence formula_2 can represent simultaneous measurement of\nelectrical activity of those \"p\" channels. Let the sampling interval\nbetween observations be formula_3, so that the Nyquist frequency is formula_4.\n\nThe multitaper spectral estimator utilizes several different data tapers which are orthogonal to each other. The multitaper cross-spectral estimator between channel \"l\" and \"m\" is the average of K direct cross-spectral estimators between the same pair of channels (\"l\" and \"m\") and hence takes the form\n\nHere, formula_6 (for formula_7) is the \"k\" direct cross spectral estimator between channel \"l\" and \"m\" and is given by\n\nwhere\n\nThe sequence formula_10 is the data taper for the\n\"k\" direct cross-spectral estimator formula_11 and is chosen as follows:\n\nWe choose a set of \"K\" orthogonal data tapers such that each one provides a good protection against leakage. These are given by the Slepian sequences, after David Slepian (also known in literature as discrete prolate spheroidal sequences or DPSS for short) with parameter \"W\" and orders \"k\" = 0 to \"K\" − 1. The maximum order \"K\" is chosen to be less than the Shannon number formula_12. The quantity 2\"W\" defines the resolution bandwidth for the spectral concentration problem and formula_13. When \"l\" = \"m\", we get the multitaper estimator for the auto-spectrum of the \"l\" channel. In recent years, a dictionary based on modulated DPSS was proposed as an overcomplete alternative to DPSS.\n\nSee also Window function:DPSS or Slepian window\n\nThis technique is currently used in the spectral analysis toolkit of Chronux. An extensive treatment about the application of this method to analyze multi-trial, multi-channel data generated in neuroscience experiments, biomedical engineering and others can be found here. Not limited to time series, the multitaper method can be reformulated for spectral estimation on the sphere using Slepian functions constructed from spherical harmonics for applications in geophysics and cosmology among others.\n\n", "related": "\n- Periodogram\n\n- C++/Octave libraries for the multitaper method, including adaptive weighting (hosted on GitHub)\n- Documentation on the multitaper method from the SSA-MTM Toolkit implementation\n- Fortran 90 library with additional multivariate applications\n- Python module\n- R (programming language) multitaper Package\n- S-Plus script to generate Slepian sequences (dpss)\n"}
{"id": "56343589", "url": "https://en.wikipedia.org/wiki?curid=56343589", "title": "SAMV (algorithm)", "text": "SAMV (algorithm)\n\nSAMV (iterative sparse asymptotic minimum variance) is a parameter-free superresolution algorithm for the linear inverse problem in spectral estimation, direction-of-arrival (DOA) estimation and tomographic reconstruction with applications in signal processing, medical imaging and remote sensing. The name was coined in 2013 to emphasize its basis on the asymptotically minimum variance (AMV) criterion. It is a powerful tool for the recovery of both the amplitude and frequency characteristics of multiple highly correlated sources in challenging environment (e.g., limited number of snapshots, low signal-to-noise ratio. Applications include synthetic-aperture radar, computed tomography scan, and magnetic resonance imaging (MRI). \n\nThe formulation of the SAMV algorithm is given as an inverse problem in the context of DOA estimation. Suppose an formula_1-element uniform linear array (ULA) receive formula_2 narrow band signals emitted from sources located at locations formula_3, respectively. The sensors in the ULA accumulates formula_4 snapshots over a specific time. The formula_5 dimensional snapshot vectors are\n\nwhere formula_7 is the steering matrix, formula_8 contains the source waveforms, and formula_9 is the noise term. Assume that formula_10, where formula_11 is the Dirac delta and it equals to 1 only if formula_12 and 0 otherwise. Also assume that formula_9 and formula_14 are independent, and that formula_15, where formula_16. Let formula_17 be a vector containing the unknown signal powers and noise variance, formula_18.\n\nThe covariance matrix of formula_19 that contains all information about formula_20 is\n\nThis covariance matrix can be traditionally estimated by the sample covariance matrix formula_22 where formula_23. After applying the vectorization operator to the matrix formula_24, the obtained vector formula_25 is linearly related to the unknown parameter formula_20 as\n\nformula_27,\n\nwhere formula_28, formula_29, formula_30, formula_31, and let formula_32.\n\nTo estimate the parameter formula_20 from the statistic formula_34, we develop a series of iterative SAMV approaches based on the asymptotically minimum variance criterion. From , the covariance matrix formula_35 of an arbitrary consistent estimator of formula_36 based on the second-order statistic formula_34 is bounded by the real symmetric positive definite matrix\n\nwhere formula_39. In addition, this lower bound is attained by the covariance matrix of the asymptotic distribution of formula_40 obtained by minimizing,\n\nwhere\nformula_42\n\nTherefore, the estimate of formula_20 can be obtained iteratively.\n\nThe formula_44 and formula_45 that minimize formula_46 can be computed as follows. Assume formula_47 and formula_48 have been approximated to a certain degree in the formula_49th iteration, they can be refined at the formula_50th iteration by,\n\nwhere the estimate of formula_24 at the formula_49th iteration is given by formula_55 with formula_56.\n\nThe resolution of most compressed sensing based source localization techniques is limited by the fineness of the direction grid that covers the location parameter space. In the sparse signal recovery model, the sparsity of the truth signal formula_57 is dependent on the distance between the adjacent element in the overcomplete dictionary formula_58, therefore, the difficulty of choosing the optimum overcomplete dictionary arises. The computational complexity is directly proportional to the fineness of the direction grid, a highly dense grid is not computational practical. To overcome this resolution limitation imposed by the grid, the grid-free SAMV-SML (iterative Sparse Asymptotic Minimum Variance - Stochastic Maximum Likelihood) is proposed, which refine the location estimates formula_59 by iteratively minimizing a stochastic maximum likelihood cost function with respect to a single scalar parameter formula_60.\n\nA typical application with the SAMV algorithm in SISO radar/sonar range-Doppler imaging problem. This imaging problem is a single-snapshot application, and algorithms compatible with single-snapshot estimation are included, i.e., matched filter (MF, similar to the periodogram or backprojection, which is often efficiently implemented as fast Fourier transform (FFT)), IAA, and a variant of the SAMV algorithm (SAMV-0). The simulation conditions are identical to : A formula_61-element polyphase pulse compression P3 code is employed as the transmitted pulse, and a total of nine moving targets are simulated. Of all the moving targets, three are of formula_62 dB power and the rest six are of formula_63 dB power. The received signals are assumed to be contaminated with uniform white Gaussian noise of formula_64 dB power.\n\nThe matched filter detection result suffers from severe smearing and leakage effects both in the Doppler and range domain, hence it is impossible to distinguish the formula_62 dB targets. On contrary, the IAA algorithm offers enhanced imaging results with observable target range estimates and Doppler frequencies. The SAMV-0 approach provides highly sparse result and eliminates the smearing effects completely, but it misses the weak formula_62 dB targets.\n\nAn open source MATLAB implementation of SAMV algorithm could be downloaded here.\n\n", "related": "\n- Array processing\n- Matched filter\n- Periodogram\n- Filtered backprojection (Radon transform)\n- MUltiple SIgnal Classification (MUSIC), a popular parametric superresolution method\n- Pulse-Doppler radar\n- Super-resolution imaging\n- Compressed sensing\n- Inverse problem\n- Tomographic reconstruction\n"}
{"id": "5225", "url": "https://en.wikipedia.org/wiki?curid=5225", "title": "Code", "text": "Code\n\nIn communications and information processing, code is a system of rules to convert information—such as a letter, word, sound, image, or gesture—into another form or representation, sometimes shortened or secret, for communication through a communication channel or storage in a storage medium. An early example is the invention of language, which enabled a person, through speech, to communicate what they saw, heard, felt, or thought to others. But speech limits the range of communication to the distance a voice can carry, and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.\n\nThe process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands, such as English or Spanish.\n\nOne reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaler or the arms of a semaphore tower encodes parts of the message, typically individual letters and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.\n\nIn information theory and computer science, a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet, by \"encoded\" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.\n\nBefore giving a mathematically precise definition, this is a brief example. The mapping\nis a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001011 can be grouped into codewords as 0 011 0 01 011, and these in turn can be decoded to the sequence of source symbols \"acabc\".\n\nUsing terms from formal language theory, the precise mathematical definition of this concept is as follows: let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T, and the extension of formula_5 to a homomorphism of formula_6 into formula_7, which naturally maps each sequence of source symbols to a sequence of target symbols, is referred to as its extension.\n\nCodes that encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string. Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.\n\nA \"prefix code\" is a code with the \"prefix property\": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as \"Huffman codes\" even when the code was not produced by a Huffman algorithm. Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS WCDMA 3G Wireless Standard.\n\nKraft's inequality characterizes the sets of codeword lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessary a prefix one, must satisfy Kraft's inequality.\n\nCodes may also be used to represent data in a way more resistant to errors in transmission or storage. This so-called error-correcting code works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed–Solomon, Reed–Muller, Walsh–Hadamard, Bose–Chaudhuri–Hochquenghem, Turbo, Golay, Goppa, low-density parity-check codes, and space–time codes.\nError detecting codes can be optimised to detect \"burst errors\", or \"random errors\".\n\nA cable code replaces words (e.g. \"ship\" or \"invoice\") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and less expensively.\n\nCodes can be used for brevity. When telegraph messages were the state of the art in rapid long distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such \"words\" as \"BYOXO\" (\"Are you trying to weasel out of our deal?\"), \"LIOUY\" (\"Why do you not answer my question?\"), \"BMULD\" (\"You're a skunk!\"), or \"AYYLU\" (\"Not clearly coded, repeat more clearly.\"). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.\n\nCharacter encodings are representations of textual data. A given character encoding may be associated with a specific character set (the collection of characters which it can represent), though some character sets have multiple character encodings and vice versa. Character encodings may be broadly grouped according to the number of bytes required to represent a single character: there are single byte encodings, multibyte (also called wide) encodings, and variable-width (also called variable-length) encodings. The earliest character encodings were single-byte, the best known example of which is ASCII. ASCII remains in use today, for example in HTTP headers. However, single-byte encodings cannot model character sets with more than 256 characters. Scripts which require large character sets such as Chinese, Japanese and Korean must be represented with multibyte encodings. Early multibyte encodings were fixed-length, meaning that although each character was represented by more than one byte, all characters used the same number of bytes (\"word length\"), making them suitable for decoding with a lookup table. The final group, variable-width encodings, is a subset of multibyte encodings. These use more complex encoding and decoding logic to efficiently represent large character sets while keeping the representations of more commonly used characters shorter or maintaining backwards compatibility properties. This group includes UTF-8, an encoding of the Unicode character set; UTF-8 is the most common encoding of text media on the Internet.\n\nBiological organisms contain genetic material that is used to control their function and development. This is DNA, which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a genetic code in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.\n\nIn mathematics, a Gödel code was the basis for the proof of Gödel's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a Gödel numbering).\n\nThere are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, organic, etc.).\n\nIn marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from a (usually internet) retailer.\n\nIn military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry on the battlefield, etc.\n\nCommunication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.\n\nMusical scores are the most common way to encode music.\n\nSpecific games have their own code systems to record the matches, e.g. chess notation.\n\nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead.\n\nSecret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomacy, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requisite is the pre-agreement on the meaning by both the sender and the receiver.\n\nOther examples of encoding include:\n- Encoding (in cognition) - a basic perceptual process of interpreting incoming stimuli; technically speaking, it is a complex, multi-stage process of converting relatively objective sensory input (e.g., light, sound) into subjectively meaningful experience.\n- A content format - a specific encoding format for converting a specific type of data to information.\n- Text encoding uses a markup language to tag the structure and other features of a text to facilitate processing by computers. (See also Text Encoding Initiative.)\n- Semantics encoding of formal language A in formal language B is a method of representing all terms (e.g. programs or descriptions) of language A using language B.\n- Data compression transforms a signal into a code optimized for transmission or storage, generally done with a codec.\n- Neural encoding - the way in which information is represented in neurons.\n- Memory encoding - the process of converting sensations into memories.\n- Television encoding: NTSC, PAL and SECAM\n\nOther examples of decoding include:\n- Decoding (computer science)\n- Decoding methods, methods in communication theory for decoding code words sent over a noisy channel\n- Digital signal processing, the study of signals in a digital representation and the processing methods of these signals\n- Digital-to-analog converter, the use of analog circuit for decoding operations\n- Word decoding, the use of phonics to decipher print patterns and translate them into the sounds of language\n\nAcronyms and abbreviations can be considered codes, and in a sense all languages and writing systems are codes for human thought.\n\nInternational Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways, but are usually national, so the same code can be used for different stations if they are in different countries.\n\nOccasionally, a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean \"end of story\", and has been used in other contexts to signify \"the end\".\n\n", "related": "\n- Asemic writing\n- Cipher\n- Code (semiotics)\n- Equipment codes\n- Quantum error correction\n- Semiotics\n- Universal language\n"}
{"id": "6680502", "url": "https://en.wikipedia.org/wiki?curid=6680502", "title": "Field-programmable analog array", "text": "Field-programmable analog array\n\nA field-programmable analog array (FPAA) is an integrated circuit device containing computational analog blocks (CAB) and interconnects between these blocks offering field-programmability. Unlike their digital cousin, the FPGA, the devices tend to be more application driven than general purpose as they may be current mode or voltage mode devices. For voltage mode devices, each block usually contains an operational amplifier in combination with programmable configuration of passive components. The blocks can, for example, act as summers or integrators.\n\nFPAAs usually operate in one of two modes: \"continuous time\" and \"discrete time\".\n- \"Discrete-time devices\" possess a system sample clock. In a switched capacitor design, all blocks sample their input signals with a sample and hold circuit composed of a semiconductor switch and a capacitor. This feeds a programmable op amp section which can be routed to a number of other blocks. This design requires more complex semiconductor construction. An alternative, switched-current design, offers simpler construction and does not require the input capacitor, but can be less accurate, and has lower fan-out - it can drive only one following block. Both discrete-time device types must compensate for switching noise, aliasing at the system sample rate, and sample-rate limited bandwidth, during the design phase.\n- \"Continuous-time devices\" work more like an array of transistors or op amps which can operate at their full bandwidth. The components are connected in a particular arrangement through a configurable array of switches. During circuit design, the switch matrix's parasitic inductance, capacitance and noise contributions must be taken into account.\n\nCurrently there are very few manufactures of FPAAs. On-chip resources are still very limited when compared to that of an FPGA. This resource deficit is often cited by researchers as a limiting factor in their research.\n\nThe term FPAA was first used in 1991 by Lee and Gulak. They put forward the concept of CABs that are connected via a routing network and configured digitally. Subsequently, in 1992 and 1995 they further elaborated the concept with the inclusion of op-amps, capacitors, and resistors. This original chip was manufactured using 1.2 µm CMOS technology and operates in the 20 kHz range at a power consumption of 80 mW.\n\nPierzchala et al introduced a similar concept named electronically-programmable analog circuit (EPAC). It featured only a single integrator. However, they proposed a local interconnect architecture in order to try and avoid the bandwidth limitations.\n\nThe reconfigurable analog signal processor (RASP) and a second version were introduced in 2002 by Hall et al. Their design incorporated high-level elements such as second order bandpass filters and 4 by 4 vector matrix multipliers into the CABs. Because of its architecture, it is limited to around 100 kHz and the chip itself is not able to support independent reconfiguration.\n\nIn 2004 Joachim Becker picked up the parallel connection of OTAs (operational transconductance amplifiers) and proposed its use in a hexagonal local interconnection architecture. It did not require a routing network and eliminated switching the signal path that enhances the frequency response.\n\nIn 2005 Fabian Henrici worked with Joachim Becker to develop a switchable and invertible OTA which doubled the maximum FPAA bandwidth. This collaboration resulted in the first manufactured FPAA in a 0.13 µm CMOS technology.\n\nIn 2016 Dr. Jennifer Hasler from Georgia Tech. university designed a FPAA System on Chip that uses analog technology to achieve unprecedented power and size reductions.\n\n", "related": "\n- Field-programmable RF – Field programmable radio frequency devices\n- CPLD: Complex Programmable Logic Device\n- PSoC: Programmable System-on-Chip\n- NoC: Network on a Chip\n- Network architecture\n\n- \"Analog's Answer to FPGA Opens Field to Masses\" Sunny Bains, \"EE Times\", February 21, 2008. Issue 1510.\n- \"Field programmable analog arrays\" Tim Edwards, Johns Hopkins University project, 1999.\n- \"Field programmable analog arrays\" Joachim Becker, et al., University of Freiburg, Department of Microsystems Engineering. Hex FPAA Research Project.\n- \"Integrated Computational Electronics (ICE) Laboratory\" Georgia Institute of Technology Project\n"}
{"id": "302033", "url": "https://en.wikipedia.org/wiki?curid=302033", "title": "Frequency response", "text": "Frequency response\n\nFrequency response is the quantitative measure of the output spectrum of a system or device in response to a stimulus, and is used to characterize the dynamics of the system. It is a measure of magnitude and phase of the output as a function of frequency, in comparison to the input. In simplest terms, if a sine wave is injected into a system at a given frequency, a linear system will respond at that same frequency with a certain magnitude and a certain phase angle relative to the input. Also for a linear system, doubling the amplitude of the input will double the amplitude of the output. In addition, if the system is time-invariant (so LTI), then the frequency response also will not vary with time. Thus for LTI systems, the frequency response can be seen as applying the system's transfer function to a purely imaginary number argument representing the frequency of the sinusoidal excitation.\n\nTwo applications of frequency response analysis are related but have different objectives.\n\nFor an audio system, the objective may be to reproduce the input signal with no distortion. That would require a uniform (flat) magnitude of response up to the bandwidth limitation of the system, with the signal delayed by precisely the same amount of time at all frequencies. That amount of time could be seconds, or weeks or months in the case of recorded media.\n\nIn contrast, for a feedback apparatus used to control a dynamic system, the objective is to give the closed-loop system improved response as compared to the uncompensated system. The feedback generally needs to respond to system dynamics within a very small number of cycles of oscillation (usually less than one full cycle), and with a definite phase angle relative to the commanded control input. For feedback of sufficient amplification, getting the phase angle wrong can lead to instability for an open-loop stable system, or failure to stabilize a system that is open-loop unstable.\n\nDigital filters may be used for both audio systems and feedback control systems, but since the objectives are different, generally the phase characteristics of the filters will be significantly different for the two applications.\n\nEstimating the frequency response for a physical system generally involves exciting the system with an input signal, measuring both input and output time histories, and comparing the two through a process such as the Fast Fourier Transform (FFT). One thing to keep in mind for the analysis is that the frequency content of the input signal must cover the frequency range of interest because the results will not be valid for the portion of the frequency range not covered.\n\nThe frequency response of a system can be measured by applying a \"test signal\", for example:\n- applying an impulse to the system and measuring its response (see impulse response)\n- sweeping a constant-amplitude pure tone through the bandwidth of interest and measuring the output level and phase shift relative to the input\n- applying a signal with a wide frequency spectrum (for example multifrequency signals (nonorthogonal frequency-discrete multiplexing of signals (N-OFDM or as the same SEFDM) and OFDM), digitally-generated maximum length sequence noise, or analog filtered white noise equivalent, like pink noise), and calculating the impulse response by deconvolution of this input signal and the output signal of the system.\n\nThe frequency response is characterized by the \"magnitude\" of the system's response, typically measured in decibels (dB) or as a decimal, and the \"phase\", measured in radians or degrees, versus frequency in radians/sec or Hertz (Hz).\n\nThese response measurements can be plotted in three ways: by plotting the magnitude and phase measurements on two rectangular plots as functions of frequency to obtain a Bode plot; by plotting the magnitude and phase angle on a single polar plot with frequency as a parameter to obtain a Nyquist plot; or by plotting magnitude and phase on a single rectangular plot with frequency as a parameter to obtain a Nichols plot.\n\nFor audio systems with nearly uniform time delay at all frequencies, the magnitude versus frequency portion of the Bode plot may be all that is of interest. For the design of control systems, any of the three types of plots (Bode, Nyquist, Nichols) can be used to infer closed-loop stability and stability margins (gain and phase margins) from the open-loop frequency response, provided that for the Bode analysis the phase-versus-frequency plot is included.\n\nThe form of frequency response for digital systems (as example FFT filters) are periodical with multiple main lobes and sidelobes.\n\nIf the system under investigation is nonlinear then applying purely linear frequency domain analysis will not reveal all the nonlinear characteristics. To overcome these limitations, generalized frequency response functions and nonlinear output frequency response functions have been defined that allow the user to analyze complex nonlinear dynamic effects. The nonlinear frequency response methods reveal complex resonance, inter modulation, and energy transfer effects that cannot be seen using a purely linear analysis and are becoming increasingly important in a nonlinear world.\n\nIn electronics this stimulus would be an input signal. In the audible range it is usually referred to in connection with electronic amplifiers, microphones and loudspeakers. Radio spectrum frequency response can refer to measurements of coaxial cable, twisted-pair cable, video switching equipment, wireless communications devices, and antenna systems. Infrasonic frequency response measurements include earthquakes and electroencephalography (brain waves).\n\nFrequency response requirements differ depending on the application. In high fidelity audio, an amplifier requires a frequency response of at least 20–20,000 Hz, with a tolerance as tight as ±0.1 dB in the mid-range frequencies around 1000 Hz, however, in telephony, a frequency response of 400–4,000 Hz, with a tolerance of ±1 dB is sufficient for intelligibility of speech.\n\nFrequency response curves are often used to indicate the accuracy of electronic components or systems. When a system or component reproduces all desired input signals with no emphasis or attenuation of a particular frequency band, the system or component is said to be \"flat\", or to have a flat frequency response curve. In other case can be use 3D-form of frequency response surface.\n\nOnce a frequency response has been measured (e.g., as an impulse response), provided the system is linear and time-invariant, its characteristic can be approximated with arbitrary accuracy by a digital filter. Similarly, if a system is demonstrated to have a poor frequency response, a digital or analog filter can be applied to the signals prior to their reproduction to compensate for these deficiencies.\n\nThe form of a frequency response curve is very important for anti-jamming protection of radars, communications and other systems.\n\n", "related": "\n- Audio system measurements\n- Bandwidth (signal processing)\n- Bode plot\n- Frequency response\n- Impulse response\n- Spectral sensitivity\n- Steady state (electronics)\n- Transient response\n- Universal dielectric response\n- Notes\n\n- Bibliography\n- Luther, Arch C.; Inglis, Andrew F. \"Video engineering\", McGraw-Hill, 1999.\n- Stark, Scott Hunter. \"Live Sound Reinforcement\", Vallejo, California, Artistpro.com, 1996–2002.\n- L. R. Rabiner and B. Gold. Theory and Application of Digital Signal Processing. – Englewood Cliffs, NJ: Prentice-Hall, 1975. – 720 pp\n\n- University of Michigan: Frequency Response Analysis and Design Tutorial\n- Smith, Julius O. III: Introduction to Digital Filters with Audio Applications has a nice chapter on Frequency Response\n"}
{"id": "59384115", "url": "https://en.wikipedia.org/wiki?curid=59384115", "title": "EEG analysis", "text": "EEG analysis\n\nEEG analysis is exploiting mathematical signal analysis methods and computer technology to extract information from electroencephalography (EEG) signals. The targets of EEG analysis are to help researchers gain a better understanding of the brain; assist physicians in diagnosis and treatment choices; and to boost brain-computer interface (BCI) technology. There are many ways to roughly categorize EEG analysis methods. If a mathematical model is exploited to fit the sampled EEG signals, the method can be categorized as parametric, otherwise, it is a non-parametric method. Traditionally, most EEG analysis methods fall into four categories: time domain, frequency domain, time-frequency domain, and nonlinear methods. There are also later methods including deep neural networks (DNNs).\n\nFrequency domain analysis, also known as spectral analysis, is the most conventional yet one of the most powerful and standard methods for EEG analysis. It gives insight to information contained in the frequency domain of EEG waveforms by adopting statistical and Fourier Transform methods. Among all the spectral methods, power spectral analysis is the most commonly used, since the power spectrum reflects the 'frequency content' of the signal or the distribution of signal power over frequency.\n\nThere are two important methods for time domain EEG analysis: Linear Prediction and Component Analysis. Generally, Linear Prediction gives the estimated value equal to a linear combination of the past output value with the present and past input value. And Component Analysis is an unsupervised method in which the data set is mapped to a feature set. Notably, the parameters in time domain methods are entirely based on time, but they can also be extracted from statistical moments of the power spectrum. As a result, time domain method builds a bridge between physical time interpretation and conventional spectral analysis. Besides, time domain methods offer a way to on-line measurement of basic signal properties by means of a time-based calculation, which requires less complex equipment compared to conventional frequency analysis.\n\nWavelet Transform, a typical time-frequency domain method, can extract and represent properties from transient biological signals. Specifically, through wavelet decomposition of the EEG records, transient features can be accurately captured and localized in both time and frequency context. Thus Wavelet transform is like a mathematical microscope that can analyze different scales of neural rhythms and investigate small-scale oscillations of the brain signals while ignoring the contribution of other scales. Apart from Wavelet Transform, there is another prominent time-frequency method called Hilbert-Huang Transform, which can decompose EEG signals into a set of oscillatory components called Intrinsic Mode Function(IMF) in order to capture instantaneous frequency data.\n\nMany phenomena in nature are nonlinear and non-stationary, and so are EEG signals. This attribute adds more complexity to the interpretation of EEG signals, rendering linear methods(methods mentioned above) limited. Since 1985 when two pioneers in nonlinear EEG analysis, Rapp and Bobloyantz, published their first results, the theory of nonlinear dynamic systems, also called 'chaos theory', has been broadly applied to the field of EEG analysis. To conduct nonlinear EEG analysis, researchers have adopted many useful nonlinear parameters such as Lyapunov Exponent, Correlation Dimension, and entropies like Approximate Entropy and Sample Entropy.\n\nThe implementation of Artificial Neural Networks(ANN) is presented for classification of electroencephalogram (EEG) signals. In most cases, EEG data involves a preprocess of wavelet transform before putting into the neural networks. RNN(recurrent neural networks) was once considerably applied in studies of ANN implementations in EEG analysis. Until the boom of deep leaning and CNN(Convolutional Neural Networks), CNN method becomes a new favorite in recent studies of EEG analysis employing deep leaning. With cropped training for the deep CNN to reach competitive accuracies on the dataset, deep CNN has presented a superior decoding performance. Moreover, the big EEG data, as the input of ANN, calls for the need for safe storage and high computational resources for real-time processing. To address these challenges, a cloud-based deep learning has been proposed and presented for real-time analysis of big EEG data.\n\nEEG analysis is widely used in brain-disease diagnosis and assessment. In the domain of epileptic seizures, the detection of epileptiform discharges in the EEG is an important component in the diagnosis of epilepsy. Careful analyses of the EEG records can provide valuable insight and improved understanding of the mechanisms causing epileptic disorders. Besides, EEG analysis also helps much with the detection of Alzheimer's disease, tremor, etc.\n\nEEG recordings during right and left motor imagery allow one to establish a new com-munication channel. Based on real-time EEG analysis with subject-specific spatial patterns, a brain–computer interface (BCI) can be used to develop a simple binary response for the control of a device. Such an EEG-based BCI can help, e.g., patients with amyotrophic lateral sclerosis, with some daily activities.\n\nBrainstorm is a collaborative, open-source application dedicated to the analysis of brain recordings including MEG, EEG, fNIRS, ECoG, depth electrodes and animal invasive neurophysiology. The objective of Brainstorm is to share a comprehensive set of user-friendly tools with the scientific community using MEG/EEG as an experimental technique. Brainstorm offers rich and intuitive graphic interface for physicians and researchers, which does not require any programming knowledge. Some other relative open source analysis softwares include FieldTrip, etc.\n\nCombined with facial expressions analysis, EEG analysis offers the function of continuous emotion detection, which can be used to find the emotional traces of videos. Some other applications include EEG-based brain mapping, personalized EEG-based encryptor, EEG-Based image annotation system, etc.\n", "related": "NONE"}
{"id": "294927", "url": "https://en.wikipedia.org/wiki?curid=294927", "title": "Cross-correlation matrix", "text": "Cross-correlation matrix\n\nThe cross-correlation matrix of two random vectors is a matrix containing as elements the cross-correlations of all pairs of elements of the random vectors. The cross-correlation matrix is used in various digital signal processing algorithms.\n\nFor two random vectors formula_1 and formula_2, each containing random elements whose expected value and variance exist, the cross-correlation matrix of formula_3 and formula_4 is defined by\n\nand has dimensions formula_5. Written component-wise:\n\nThe random vectors formula_3 and formula_4 need not have the same dimension, and either might be a scalar value.\n\nFor example, if formula_9 and formula_10 are random vectors, then\nformula_11 is a formula_12 matrix whose formula_13-th entry is formula_14.\n\nIf formula_15 and formula_16 are complex random vectors, each containing random variables whose expected value and variance exist, the cross-correlation matrix of formula_17 and formula_18 is defined by\n\nwhere formula_20 denotes Hermitian transposition.\n\nTwo random vectors formula_21 and formula_22 are called uncorrelated if\n\nThey are uncorrelated if and only if their cross-covariance matrix formula_24 matrix is zero.\n\nIn the case of two complex random vectors formula_17 and formula_18 they are called uncorrelated if\nand\n\nThe cross-correlation is related to the \"cross-covariance matrix\" as follows:\n\n", "related": "\n- Autocorrelation\n- Correlation does not imply causation\n- Covariance function\n- Pearson product-moment correlation coefficient\n- Correlation function (astronomy)\n- Correlation function (statistical mechanics)\n- Correlation function (quantum field theory)\n- Mutual information\n- Rate distortion theory\n- Radial distribution function\n\n- Hayes, Monson H., \"Statistical Digital Signal Processing and Modeling\", John Wiley & Sons, Inc., 1996. .\n- Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n"}
{"id": "1291319", "url": "https://en.wikipedia.org/wiki?curid=1291319", "title": "Time-invariant system", "text": "Time-invariant system\n\nA time-invariant (TIV) system has a time-dependent system function that is not a direct function of time. Such systems are regarded as a class of systems in the field of system analysis. The time-dependent system function is a function of the time-dependent input function. If this function depends \"only\" indirectly on the time-domain (via the input function, for example), then that is a system that would be considered time-invariant. Conversely, any direct dependence on the time-domain of the system function could be considered as a \"time-varying system\".\n\nMathematically speaking, \"time-invariance\" of a system is the following property:\n\nIn the language of signal processing, this property can be satisfied if the transfer function of the system is not a direct function of time except as expressed by the input and output.\n\nIn the context of a system schematic, this property can also be stated as follows:\nIf a time-invariant system is also linear, it is the subject of linear time-invariant theory (linear time-invariant) with direct applications in NMR spectroscopy, seismology, circuits, signal processing, control theory, and other technical areas. Nonlinear time-invariant systems lack a comprehensive, governing theory. Discrete time-invariant systems are known as shift-invariant systems. Systems which lack the time-invariant property are studied as time-variant systems.\n\nTo demonstrate how to determine if a system is time-invariant, consider the two systems:\n- System A: formula_10\n- System B: formula_11\n\nSince system A explicitly depends on \"t\" outside of formula_2 and formula_1, it is not time-invariant. System B, however, does not depend explicitly on \"t\" so it is time-invariant.\n\nA more formal proof of why systems A and B above differ is now presented.\nTo perform this proof, the second definition will be used.\n\nSystem A:\n\nSystem B:\n\nMore generally, the relationship between the input and output is formula_28, and its variation with time is\nFor time-invariant systems, the system properties remain constant with time, formula_30. Applied to Systems A and B above:\n\nWe can denote the shift operator by formula_33 where formula_34 is the amount by which a vector's index set should be shifted. For example, the \"advance-by-1\" system\n\ncan be represented in this abstract notation by\n\nwhere formula_37 is a function given by\n\nwith the system yielding the shifted output\n\nSo formula_40 is an operator that advances the input vector by 1.\n\nSuppose we represent a system by an operator formula_41. This system is time-invariant if it commutes with the shift operator, i.e.,\n\nIf our system equation is given by\n\nthen it is time-invariant if we can apply the system operator formula_41 on formula_37 followed by the shift operator formula_33, or we can apply the shift operator formula_33 followed by the system operator formula_41, with the two computations yielding equivalent results.\n\nApplying the system operator first gives\n\nApplying the shift operator first gives\n\nIf the system is time-invariant, then\n\n", "related": "\n- Finite impulse response\n- Sheffer sequence\n- State space (controls)\n- Signal-flow graph\n- LTI system theory\n"}
{"id": "47021271", "url": "https://en.wikipedia.org/wiki?curid=47021271", "title": "Array factor", "text": "Array factor\n\nThe array factor formula_1 is the complex-valued far-field radiation pattern obtained for an array of formula_2 isotropic radiators located at coordinates formula_3, as determined by:\n\nformula_4\n\nwhere formula_5 are the complex-valued excitation coefficients, and formula_6 is the direction unit vector. The array factor is defined in the transmitting mode, with the time convention formula_7. A corresponding expression can be derived for the receiving mode, where a negative sign appears in the exponential factors. \n\n", "related": "\n- Array antenna\n"}
{"id": "58037076", "url": "https://en.wikipedia.org/wiki?curid=58037076", "title": "Kernel-phase", "text": "Kernel-phase\n\nKernel-phases are observable quantities used in high resolution astronomical imaging used for superresolution image creation. It can be seen as a generalization of closure phases for redundant arrays. For this reason, when the wavefront quality requirement are met, it is an alternative to aperture masking interferometry that can be executed without a mask while retaining phase error rejection properties. The observables are computed through linear algebra from the Fourier transform of direct images. They can then be used for statistical testing, model fitting, or image reconstruction.\nIn order to extract kernel-phases from an image, some requirements must be met:\n- Images are nyquist-sampled (at least 2 pixels per resolution element (formula_1))\n- Images are taken in near monochromatic light\n- Exposure time is shorter than the timescale of aberrations\n- Strehl ratio is high (good adaptive optics)\n- Linearity of the pixel response (i.e. no saturation)\nDeviations from these requirements are known to be acceptable, but lead to observational bias that should be corrected by the observation of calibrators.\n\nThe method relies on a discrete model of the instrument's pupil plane and the corresponding list of baselines to provide corresponding vectors formula_2 of pupil plane errors and formula_3 of image plane Fourier Phases. When the wavefront error in the pupil plane is small enough (i.e. when the Strehl ratio of the imaging system is sufficiently high), the complex amplitude associated to the instrumental phase in one point of the pupil formula_4, can be approximated by formula_5 . This permits the expression of the pupil-plane phase aberrations formula_2 to the image plane Fourier phase as a linear transformation described by the matrix formula_7:\nWhere formula_9 is the theoretical Fourier phase vector of the object. In this formalism, Singular Value Decomposition can be used to find a matrix formula_10 satisfying formula_11. The rows of formula_10 constitute a basis of the kernel of formula_13.\nThe vector formula_15 is called the kernel-phase vector of observables. This equation can be used for model-fitting as it represents the interpretation of a sub-space of the Fourier phase that is immune to the instrumental phase errors to the first order.\n\nThe technique was first used in the re-analysis of archival images from the Hubble Space Telescope where it enabled the discovery of a number of brown dwarf in close binary systems.\n\nThe technique is used as an alternative to aperture masking interferometry, especially for fainter stars because it does not require the use of masks that typically block 90% of the light, and therefore allows higher throughput. It is also considered to be an alternative to coronagraphy for direct detection of exoplanets at very small separations (below formula_16) where coronagraphs are limited by the wavefront errors of adaptive optics.\n\nThe same framework can be used for wavefront sensing. In the case of an asymmetric aperture, a pseudo-inverse of formula_7 can be used to reconstruct the wavefront errors directly from the image.\n\nA Python library called xara is available on GitHub and maintained by Frantz Martinache to facilitate the extraction and interpretation of kernel-phases.\n\nThe KERNEL project, has received fundings from the European Research Council to explore the potential of these observables for a number of use-cases, including direct detection of exoplanets, image reconstruction, and image plane wavefront sensing for adaptive optics.\n", "related": "NONE"}
{"id": "59657820", "url": "https://en.wikipedia.org/wiki?curid=59657820", "title": "Square-law detector", "text": "Square-law detector\n\nIn electronic signal processing, a square law detector is a device that produces an output proportional to the square of some input. For example, in demodulating radio signals, a semiconductor diode can be used as a square law detector, providing an output voltage proportional to the square of the amplitude of the input voltage over some range of input amplitudes. A square law detector responds to the power of the input electrical signal.\n", "related": "NONE"}
{"id": "2201447", "url": "https://en.wikipedia.org/wiki?curid=2201447", "title": "Angle of arrival", "text": "Angle of arrival\n\nThe angle of arrival (AoA) of a signal is the direction from which the signal (e.g. radio, optical or acoustic) is received.\n\nMeasurement of AoA can be done by determining the direction of propagation of a radio-frequency wave incident on an antenna array or determined from maximum signal strength during antenna rotation.\n\nThe AoA can be calculated by measuring the time difference of arrival (TDOA) between individual elements of the array. \n\nGenerally this TDOA measurement is made by measuring the difference in received phase at each element in the antenna array. This can be thought of as beamforming in reverse. In beamforming, the signal from each element is weighed to \"steer\" the gain of the antenna array. In AoA, the delay of arrival at each element is measured directly and converted to an AoA measurement.\n\nConsider, for example, a two element array spaced apart by one-half the wavelength of an incoming RF wave. If a wave is incident upon the array at boresight, it will arrive at each antenna simultaneously. This will yield 0° phase-difference measured between the two antenna elements, equivalent to a 0° AoA. If a wave is incident upon the array at broadside, then a 180° phase difference will be measured between the elements, corresponding to a 90° AoA.\n\nIn optics, AoA can be calculated using interferometry.\n\nAn application of AoA is in the geolocation of cell phones. The aim is either for the cell system to report the location of a cell phone placing an emergency call or to provide a service to tell the user of the cell phone where they are. Multiple receivers on a base station would calculate the AoA of the cell phone's signal, and this information would be combined to determine the phone's location. \n\nAoA is generally used to discover the location of pirate radio stations or of any military radio transmitter.\n\nIn submarine acoustics, AoA is used to localize objects with active or passive ranging.\n\n", "related": "\n- Geolocation\n- GNSS\n- GSM localization\n- Multilateration\n- Radiolocation\n- Time of arrival\n- Triangulation\n- Trilateration\n- Wideband Space Division Multiple Access\n"}
{"id": "29324", "url": "https://en.wikipedia.org/wiki?curid=29324", "title": "Signal processing", "text": "Signal processing\n\nSignal processing is an electrical engineering subfield that focuses on analysing, modifying and synthesizing signals such as sound, images and biological measurements. Signal processing techniques can be used to improve transmission, storage efficiency and subjective quality and to also emphasize or detect components of interest in a measured signal.\n\nAccording to Alan V. Oppenheim and Ronald W. Schafer, the principles of signal processing can be found in the classical numerical analysis techniques of the 17th century. Oppenheim and Schafer further state that the digital refinement of these techniques can be found in the digital control systems of the 1940s and 1950s.\n\nIn 1948, Claude Shannon wrote the influential paper \"A Mathematical Theory of Communication\" which was published in the Bell System Technical Journal. The paper laid the groundwork for later development of information communication systems. Around the same time, methods of signal transmission were being rapidly developed, as a new type of signal emerged called \"processing signals\".\n\nElectronic signal processing was revolutionized by the MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor), which was originally invented by Mohamed M. Atalla and Dawon Kahng in 1959. MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s, and then the first single-chip digital signal processor (DSP) in 1979.\n\nAnalog signal processing is for signals that have not been digitized, as in legacy radio, telephone, radar, and television systems. This involves linear electronic circuits as well as non-linear ones. The former are, for instance, passive filters, active filters, additive mixers, integrators and delay lines. Non-linear circuits include compandors, multiplicators (frequency mixers and voltage-controlled amplifiers), voltage-controlled filters, voltage-controlled oscillators and phase-locked loops.\n\nContinuous-time signal processing is for signals that vary with the change of continuous domain (without considering some individual interrupted points).\n\nThe methods of signal processing include time domain, frequency domain, and complex frequency domain. This technology mainly discusses the modeling of linear time-invariant continuous system, integral of the system's zero-state response, setting up system function and the continuous time filtering of deterministic signals\n\nDiscrete-time signal processing is for sampled signals, defined only at discrete points in time, and as such are quantized in time, but not in magnitude.\n\n\"Analog discrete-time signal processing\" is a technology based on electronic devices such as sample and hold circuits, analog time-division multiplexers, analog delay lines and analog feedback shift registers. This technology was a predecessor of digital signal processing (see below), and is still used in advanced processing of gigahertz signals.\n\nThe concept of discrete-time signal processing also refers to a theoretical discipline that establishes a mathematical basis for digital signal processing, without taking quantization error into consideration.\n\nDigital signal processing is the processing of digitized discrete-time sampled signals. Processing is done by general-purpose computers or by digital circuits such as ASICs, field-programmable gate arrays or specialized digital signal processors (DSP chips). Typical arithmetical operations include fixed-point and floating-point, real-valued and complex-valued, multiplication and addition. Other typical operations supported by the hardware are circular buffers and lookup tables. Examples of algorithms are the fast Fourier transform (FFT), finite impulse response (FIR) filter, Infinite impulse response (IIR) filter, and adaptive filters such as the Wiener and Kalman filters.\n\nNonlinear signal processing involves the analysis and processing of signals produced from nonlinear systems and can be in the time, frequency, or spatio-temporal domains. Nonlinear systems can produce highly complex behaviors including bifurcations, chaos, harmonics, and subharmonics which cannot be produced or analyzed using linear methods.\n\nStatistical signal processing is an approach which treats signals as stochastic processes, utilizing their statistical properties to perform signal processing tasks. Statistical techniques are widely used in signal processing applications. For example, one can model the probability distribution of noise incurred when photographing an image, and construct techniques based on this model to reduce the noise in the resulting image.\n\n- Audio signal processing for electrical signals representing sound, such as speech or music\n- Speech signal processing for processing and interpreting spoken words\n- Image processing in digital cameras, computers and various imaging systems\n- Video processing for interpreting moving pictures\n- Wireless communication waveform generations, demodulation, filtering, equalization\n- Control systems\n- Array processing for processing signals from arrays of sensors\n- Process control a variety of signals are used, including the industry standard 4-20 mA current loop\n- Seismology\n- Financial signal processing analyzing financial data using signal processing techniques, especially for prediction purposes.\n- Feature extraction, such as image understanding and speech recognition.\n- Quality improvement, such as noise reduction, image enhancement, and echo cancellation.\n- (Source coding), including audio compression, image compression, and video compression.\n- Genomics, Genomic signal processing\n\nIn communication systems, signal processing may occur at:\n\n- OSI layer 1 in the seven layer OSI model, the Physical Layer (modulation, equalization, multiplexing, etc.);\n- OSI layer 2, the Data Link Layer (Forward Error Correction);\n- OSI layer 6, the Presentation Layer (source coding, including analog-to-digital conversion and signal compression).\n\n- Filters for example analog (passive or active) or digital (FIR, IIR, frequency domain or stochastic filters, etc.)\n- Samplers and analog-to-digital converters for signal acquisition and reconstruction, which involves measuring a physical signal, storing or transferring it as digital signal, and possibly later rebuilding the original signal or an approximation thereof.\n- Signal compressors\n- Digital signal processors (DSPs)\n\n- Differential equations\n- Recurrence relation\n- Transform theory\n- Time-frequency analysis for processing non-stationary signals\n- Spectral estimation for determining the spectral content (i.e., the distribution of power over frequency) of a time series\n- Statistical signal processing analyzing and extracting information from signals and noise based on their stochastic properties\n- Linear time-invariant system theory, and transform theory\n- Polynomial signal processing analysis of systems which relate input and output using polynomials\n- System identification and classification\n- Calculus\n- Complex analysis\n- Vector spaces and Linear algebra\n- Functional analysis\n- Probability and stochastic processes\n- Detection theory\n- Estimation theory\n- Optimization\n- Numerical methods\n- Time series\n- Data mining for statistical analysis of relations between large quantities of variables (in this context representing many physical signals), to extract previously unknown interesting patterns\n\n", "related": "\n- Audio filter\n- Bounded variation\n- Digital image processing\n- Dynamic range compression, companding, limiting, and noise gating\n- Information theory\n- Non-local means\n- Reverberation\n\n- Kainam Thomas Wong : Statistical Signal Processing lecture notes at the University of Waterloo, Canada.\n- Ali H. Sayed, Adaptive Filters, Wiley, NJ, 2008, .\n- Thomas Kailath, Ali H. Sayed, and Babak Hassibi, Linear Estimation, Prentice-Hall, NJ, 2000, .\n\n- Signal Processing for Communications – free online textbook by Paolo Prandoni and Martin Vetterli (2008)\n- Scientists and Engineers Guide to Digital Signal Processing – free online textbook by Stephen Smith\n- Signal Processing Techniques for Determining Powerplant Characteristics\n- The IEEE Signal Processing Society\n- Bio-Medical Signal processing at a Glance\n- IPython notebooks for Python for Signal Processing Book\n"}
{"id": "38516236", "url": "https://en.wikipedia.org/wiki?curid=38516236", "title": "Pairwise error probability", "text": "Pairwise error probability\n\nPairwise error probability is the error probability that for a transmitted signal (formula_1) its corresponding but distorted version (formula_2) will be received. This type of probability is called ″pair-wise error probability″ because the probability exists with a pair of signal vectors in a signal constellation. It's mainly used in communication systems.\n\nIn general, the received signal is a distorted version of the transmitted signal. Thus, we introduce the symbol error probability, which is the probability formula_3 that the demodulator will make a wrong estimation formula_4 of the transmitted symbol formula_5 based on the received symbol, which is defined as follows:\n\nwhere is the size of signal constellation.\n\nThe pairwise error probability formula_7 is defined as the probability that, when formula_1 is transmitted, formula_2 is received.\n\nUsing the upper bound to the probability of a union of events, it can be written:\n\nFinally:\n\nFor the simple case of the additive white Gaussian noise (AWGN) channel:\n\nThe PEP can be computed in closed form as follows:\n\nformula_18 is a Gaussian random variable with mean 0 and variance formula_19.\n\nFor a zero mean, variance formula_20 Gaussian random variable:\nHence,\n\n", "related": "\n- Signal Processing\n- Telecommunication\n- Electrical engineering\n- Random variable\n\n"}
{"id": "63057517", "url": "https://en.wikipedia.org/wiki?curid=63057517", "title": "Generalized pencil-of-function method", "text": "Generalized pencil-of-function method\n\nGeneralized pencil-of-function method (GPOF), also known as matrix pencil method, is a signal processing technique for estimating a signal or extracting information with complex exponentials. Being similar to Prony and original pencil-of-function methods, it is generally preferred to the those for its robustness and computational efficiency.\n\nThe method was originally developed by Y. Hua and T. Sankar for estimating the behaviour of electromagnetic systems by its transient response, building on Sankar's past work on the original pencil-of-function method. The method has a plethora of applications in electrical engineering, particularly related to problems in computational electromagnetics, microwave engineering and antenna theory.\n\nA transient electromagnetic signal can be represented as:\n\nwhere\n\nThe same sequence, sampled by a period of formula_11, can be written as the following:\n\nwhere formula_13 by the identities of Z-transform. Generalized pencil-of-function estimates the optimal formula_14 and formula_15.\n\nFor the noiseless case, two formula_16 matrices, formula_17 and formula_18, are produced:\n\nwhere formula_21 is defined as the pencil parameter. formula_17 and formula_18 can decomposed into following matrices:\n\nwhere\n\nformula_28 and formula_29 are formula_30 diagonal matrices with sequentially-placed formula_31 and formula_32 values, respectively.\n\nIf formula_33, the generalized eigenvalues of the matrix pencil\n\nyield the poles of the system, which are formula_35. Then, the generalized eigenvectors formula_36 can be obtained by the following identities:\n\nwhere the formula_41 denotes the Moore–Penrose inverse, also known as the pseudo-inverse. Singular value decomposition can be employed to compute the pseudo-inverse.\n\nIf noise is present in the system, formula_42 and formula_43 are combined in a general data matrix, formula_44:\n\nwhere formula_46 is the noisy data. For efficient filtering, L is chosen between formula_47 and formula_48. A singular value decomposition on formula_44 yields:\n\nIn this decomposition, formula_51 and formula_52 are unitary matrices with respective eigenvectors formula_53 and formula_54 and formula_55 is a diagonal matrix with singular values of formula_44. formula_57 superscript denotes the conjugate transpose.\n\nThen the formula_58 parameter is chosen for filtering. Singular values after formula_58, which are below the filtering threshold, are set to zero; for an arbitrary singular value formula_60, the threshold is denoted by the following formula:\n\nformula_62 and are the maximum singular value and significant decimal digits, respectively. For a data with significant digits accurate up to , singular values below formula_63 are considered noise.\n\nformula_64 and formula_65 are obtained through removing the last and first rows of the filtered matrix formula_66, respectively; formula_58 columns of formula_55 represent formula_69. Filtered formula_42 and formula_43 matrices are obtained as:\n\nPrefiltering can be used to combat noise and enhance signal-to-noise ratio (SNR). Band-pass matrix pencil (BPMP) method is a modification of the GPOF method via FIR or IIR band-pass filters.\n\nGPOF can handle up to 25 dB SNR. For GPOF, as well as for BPMP, variace of the estimates approximately reaches Cramér–Rao bound.\n\nResidues of the complex poles are obtained through the least squares problem:\n\nThe method is generally used for the evaluation of Sommerfeld integrals in discrete complex image method for method of moments, where the spectral Green's function is approximated as a sum of complex exponentials. Additionally, the method is used in antenna analysis, S-parameter-estimation in microwave integrated circuits, wave propagation analysis, moving target indication and radar signal processing.\n\n", "related": "\n- Generalized eigenvalue problem\n- Matrix pencil\n- Prony's method\n"}
