{"id": "41107", "url": "https://en.wikipedia.org/wiki?curid=41107", "title": "Emphasis (telecommunications)", "text": "Emphasis (telecommunications)\n\nTypically, prior to some process, such as transmission over cable, or recording to phonograph record or tape, the input frequency range most susceptible to noise is boosted. This is referred to as \"pre-emphasis\"before the process the signal will undergo. Later, when the signal is received, or retrieved from recording, the reverse transformation is applied (\"de-emphasis\") so that the output accurately reproduces the original input. Any noise added by transmission or record/playback, to the frequency range previously boosted, is now attenuated in the de-emphasis stage.\n\nThe high-frequency signal components are emphasized to produce a more equal modulation index for the transmitted frequency spectrum, and therefore a better signal-to-noise ratio for the entire frequency range.\n\nEmphasis is commonly used in FM broadcasting and vinyl (e.g. LP) records.\n\nIn processing electronic audio signals, pre-emphasis refers to a system process designed to increase (within a frequency band) the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system. The mirror operation is called de-emphasis, and the system as a whole is called emphasis.\n\nPre-emphasis is achieved with a pre-emphasis network which is essentially a calibrated filter. The frequency response is decided by special time constants. The cutoff frequency can be calculated from that value.\n\nPre-emphasis is commonly used in telecommunications, digital audio recording, record cutting, in FM broadcasting transmissions, and in displaying the spectrograms of speech signals. One example of this is the RIAA equalization curve on 33 rpm and 45 rpm vinyl records. Another is the Dolby noise-reduction system as used with magnetic tape.\n\nPre-emphasis is employed in frequency modulation or phase modulation transmitters to equalize the modulating signal drive power in terms of deviation ratio. The receiver demodulation process includes a reciprocal network, called a de-emphasis network, to restore the original signal power distribution.\n\nIn telecommunication, de-emphasis is the complement of pre-emphasis, in the antinoise system called emphasis. De-emphasis is a system process designed to decrease, (within a band of frequencies), the magnitude of some (usually higher) frequencies with respect to the magnitude of other (usually lower) frequencies in order to improve the overall signal-to-noise ratio by minimizing the adverse effects of such phenomena as attenuation distortion or saturation of recording media in subsequent parts of the system.\n\nSpecial time constants dictate the frequency response curve, from which one can calculate the cutoff frequency.\n\nAlthough rarely used, there exists the capability for standardized emphasis in Red Book CD mastering. As CDs were intended to work on 14-bit audio, a specification for 'pre-emphasis' was included to compensate for quantization noise. After production spec was set at 16 bits, quantization noise became less of a concern, but emphasis remained an option through standards revisions. The pre-emphasis is described as a first-order filter with a gain of 10 dB (at 20 dB/decade) and time constants 50 μs and 15 μs.\n\nIn high speed digital transmission, pre-emphasis is used to improve signal quality at the output of a data transmission. In transmitting signals at high data rates, the transmission medium may introduce distortions, so pre-emphasis is used to distort the transmitted signal to correct for this distortion. When done properly this produces a received signal which more closely resembles the original or desired signal, allowing the use of higher frequencies or producing fewer bit errors.\n\nIn serial data transmission, de-emphasis has a different meaning, which is to reduce the level of all bits except the first one after a transition. That causes the high frequency content due to the transition to be emphasized compared to the low frequency content which is de-emphasized. This is a form of transmitter equalization; it compensates for losses over the channel which are larger at higher frequencies. Well known serial data standards such as PCI Express, SATA and SAS require transmitted signals to use de-emphasis.\n\n- EmphasisFrequency response and equalization EQConversion: time constant to cut-off frequency and vice versa\n- DeemphasisFrequency response and equalization EQConversion: time constant to cut-off frequency and vice versa\n", "related": "NONE"}
{"id": "1315049", "url": "https://en.wikipedia.org/wiki?curid=1315049", "title": "Downsampling (signal processing)", "text": "Downsampling (signal processing)\n\nIn digital signal processing, downsampling, compression, and decimation are terms associated with the process of \"resampling\" in a multi-rate digital signal processing system. Both \"downsampling\" and \"decimation\" can be synonymous with \"compression\", or they can describe an entire process of bandwidth reduction (filtering) and sample-rate reduction.\n  When the process is performed on a sequence of samples of a \"signal\" or other continuous function, it produces an approximation of the sequence that would have been obtained by sampling the signal at a lower rate (or density, as in the case of a photograph).\n\n\"Decimation\" is a term that historically means the \"removal of every tenth one\". But in signal processing, \"decimation by a factor of 10\" actually means \"keeping\" only every tenth sample. This factor multiplies the sampling interval or, equivalently, divides the sampling rate. For example, if compact disc audio at 44,100 samples/second is \"decimated\" by a factor of 5/4, the resulting sample rate is 35,280. A system component that performs decimation is called a \"decimator\". Decimation by an integer factor is also called \"compression\".\n\nRate reduction by an integer factor \"M\" can be explained as a two-step process, with an equivalent implementation that is more efficient:\n1. Reduce high-frequency signal components with a digital lowpass filter.\n2. \"Decimate\" the filtered signal by \"M\"; that is, keep only every \"M\" sample.\n\nStep 2 alone allows high-frequency signal components to be misinterpreted by subsequent users of the data, which is a form of distortion called aliasing. Step 1, when necessary, suppresses aliasing to an acceptable level. In this application, the filter is called an anti-aliasing filter, and its design is discussed below. Also see undersampling for information about decimating bandpass functions and signals.\n\nWhen the anti-aliasing filter is an IIR design, it relies on feedback from output to input, prior to the second step. With FIR filtering, it is an easy matter to compute only every \"M\" output. The calculation performed by a decimating FIR filter for the \"n\" output sample is a dot product:\n\nwhere the \"h\"[•] sequence is the impulse response, and \"K\" is its length.  \"x\"[•] represents the input sequence being downsampled. In a general purpose processor, after computing \"y\"[\"n\"], the easiest way to compute \"y\"[\"n\"+1] is to advance the starting index in the \"x\"[•] array by \"M\", and recompute the dot product. In the case \"M\"=2, \"h\"[•] can be designed as a half-band filter, where almost half of the coefficients are zero and need not be included in the dot products.\n\nImpulse response coefficients taken at intervals of \"M\" form a subsequence, and there are \"M\" such subsequences (phases) multiplexed together. The dot product is the sum of the dot products of each subsequence with the corresponding samples of the \"x\"[•] sequence. Furthermore, because of downsampling by \"M\", the stream of \"x\"[•] samples involved in any one of the \"M\" dot products is never involved in the other dot products. Thus \"M\" low-order FIR filters are each filtering one of \"M\" multiplexed \"phases\" of the input stream, and the \"M\" outputs are being summed. This viewpoint offers a different implementation that might be advantageous in a multi-processor architecture. In other words, the input stream is demultiplexed and sent through a bank of M filters whose outputs are summed. When implemented that way, it is called a polyphase filter.\n\nFor completeness, we now mention that a possible, but unlikely, implementation of each phase is to replace the coefficients of the other phases with zeros in a copy of the \"h\"[•] array, process the original \"x\"[•] sequence at the input rate (which means lots of multiplying by zeros), and decimate the output by a factor of \"M\". The equivalence of this inefficient method and the implementation described above is known as the \"first Noble identity\".\n\nLet \"X\"(\"f\") be the Fourier transform of any function, \"x\"(\"t\"), whose samples at some interval, \"T\", equal the \"x\"[\"n\"] sequence. Then the discrete-time Fourier transform (DTFT) is a Fourier series representation of a periodic summation of \"X\"(\"f\"):\n\nWhen \"T\" has units of seconds, formula_3 has units of hertz. Replacing \"T\" with \"MT\" in the formulas above gives the DTFT of the decimated sequence, \"x\"[\"nM\"]:\n\nThe periodic summation has been reduced in amplitude and periodicity by a factor of \"M\".  An example of both these distributions is depicted in the two traces of Fig 1.\nAliasing occurs when adjacent copies of \"X\"(\"f\") overlap. The purpose of the anti-aliasing filter is to ensure that the reduced periodicity does not create overlap. The condition that ensures the copies of \"X\"(\"f\") do not overlap each other is: formula_5 so that is the maximum cutoff frequency of an \"ideal\" anti-aliasing filter.\n\nLet \"M/L\" denote the decimation factor, where: M, L ∈ ℤ; M > L.\n\n1. Increase (resample) the sequence by a factor of \"L\". This is called Upsampling, or \"interpolation\".\n2. Decimate by a factor of \"M\"\n\nStep 1 requires a lowpass filter after increasing (\"expanding\") the data rate, and step 2 requires a lowpass filter before decimation. Therefore, both operations can be accomplished by a single filter with the lower of the two cutoff frequencies. For the M > L case, the anti-aliasing filter cutoff,  formula_6 \"cycles per intermediate sample\", is the lower frequency.\n\n", "related": "\n- Upsampling\n- Posterization\n- Sample-rate conversion\n\n- T. Schilcher. RF applications in digital signal processing//” Digital signal processing”. Proceedings, CERN Accelerator School, Sigtuna, Sweden, May 31-June 9, 2007. - Geneva, Switzerland: CERN (2008). - P. 258. - DOI: 10.5170/CERN-2008-003.\n- Sliusar I.I., Slyusar V.I., Voloshko S.V., Smolyar V.G. Next Generation Optical Access based on N-OFDM with decimation.// Third International Scientific-Practical Conference “Problems of Infocommunications. Science and Technology (PIC S&T’2016)”. – Kharkiv. - October 3 –6, 2016.\n- Saska Lindfors, Aarno Pärssinen, Kari A. I. Halonen. A 3-V 230-MHz CMOS Decimation Subsampler.// IEEE transactions on circuits and systems— Vol. 52, No. 2, February 2005. – P. 110.\n"}
{"id": "1315510", "url": "https://en.wikipedia.org/wiki?curid=1315510", "title": "Upsampling", "text": "Upsampling\n\nIn digital signal processing, upsampling, expansion, and interpolation are terms associated with the process of resampling in a multi-rate digital signal processing system. \"Upsampling\" can be synonymous with \"expansion\", or it can describe an entire process of \"expansion\" and filtering (\"interpolation\").\n  When upsampling is performed on a sequence of samples of a \"signal\" or other continuous function, it produces an approximation of the sequence that would have been obtained by sampling the signal at a higher rate (or density, as in the case of a photograph). For example, if compact disc audio at 44,100 samples/second is upsampled by a factor of 5/4, the resulting sample-rate is 55,125.\n\nRate increase by an integer factor \"L\" can be explained as a 2-step process, with an equivalent implementation that is more efficient:\n1. Expansion: Create a sequence, formula_1, comprising the original samples, formula_2 separated by \"L\" − 1 zeros.  A notation for this operation is:  formula_3\n2. Interpolation: Smooth out the discontinuities with a lowpass filter, which replaces the zeros.\n\nIn this application, the filter is called an interpolation filter, and its design is discussed below. When the interpolation filter is an FIR type, its efficiency can be improved, because the zeros contribute nothing to its dot product calculations. It is an easy matter to omit them from both the data stream and the calculations. The calculation performed by a multirate interpolating FIR filter for each output sample is a dot product:\n\nwhere the \"h\"[•] sequence is the impulse response, and \"K\" is the largest value of \"k\" for which \"h\"[\"j\" + \"kL\"] is non-zero. In the case \"L\" = 2, \"h\"[•] can be designed as a half-band filter, where almost half of the coefficients are zero and need not be included in the dot products. Impulse response coefficients taken at intervals of \"L\" form a subsequence, and there are \"L\" such subsequences (called phases) multiplexed together. Each of \"L\" phases of the impulse response is filtering the same sequential values of the \"x\"[•] data stream and producing one of \"L\" sequential output values. In some multi-processor architectures, these dot products are performed simultaneously, in which case it is called a polyphase filter.\n\nFor completeness, we now mention that a possible, but unlikely, implementation of each phase is to replace the coefficients of the other phases with zeros in a copy of the \"h\"[•] array, and process the formula_4  sequence at L times faster than the original input rate.  Then \"L-1\" of every \"L\" outputs are zero. The desired \"y\"[•] sequence is the sum of the phases, where \"L-1\" terms of the each sum are identically zero.  Computing \"L-1\" zeros between the useful outputs of a phase and adding them to a sum is effectively decimation. It's the same result as not computing them at all. That equivalence is known as the \"second Noble identity\".\n\nLet \"X\"(\"f\") be the Fourier transform of any function, \"x\"(\"t\"), whose samples at some interval, \"T\", equal the \"x\"[\"n\"] sequence. Then the discrete-time Fourier transform (DTFT) of the \"x\"[\"n\"] sequence is the Fourier series representation of a periodic summation of \"X\"(\"f\"):\n\nWhen \"T\" has units of seconds, formula_5 has units of hertz (Hz). Sampling \"L\" times faster (at interval \"T\"/\"L\") increases the periodicity by a factor of L:\nwhich is also the desired result of interpolation. An example of both these distributions is depicted in the first and third graphs of Fig.2.\n\nWhen the additional samples are inserted zeros, they increase the data rate, but they have no effect on the frequency distribution until the zeros are replaced by the interpolation filter, depicted in the second graph. Its application makes the first two graphs resemble the third one. Its bandwidth is the Nyquist frequency of the original x[n] sequence.  In units of Hz that value is formula_6  but filter design applications usually require normalized units. (see Fig 2, table)\n\nLet \"L\"/\"M\" denote the upsampling factor, where \"L\" > \"M\".\n\n1. Upsample by a factor of \"L\"\n2. Downsample by a factor of \"M\"\n\nUpsampling requires a lowpass filter after increasing the data rate, and downsampling requires a lowpass filter before decimation. Therefore, both operations can be accomplished by a single filter with the lower of the two cutoff frequencies. For the \"L\" > \"M\" case, the interpolation filter cutoff,  formula_7 \"cycles per intermediate sample\", is the lower frequency.\n\n", "related": "\n- Downsampling\n- Multi-rate digital signal processing\n- Half-band filter\n- Oversampling\n- Sampling (information theory)\n- Signal (information theory)\n- Data conversion\n- Interpolation\n- Poisson summation formula\n\n- (discusses a technique for bandlimited interpolation)\n"}
{"id": "41605", "url": "https://en.wikipedia.org/wiki?curid=41605", "title": "Pulse duration", "text": "Pulse duration\n\nIn signal processing and telecommunication, pulse duration is the interval between the time, during the first transition, that the amplitude of the pulse reaches a specified fraction (level) of its final amplitude, and the time the pulse amplitude drops, on the last transition, to the same level.\n\nThe interval between the 50% points of the final amplitude is usually used to determine or define pulse duration, and this is understood to be the case unless otherwise specified. Other fractions of the final amplitude, e.g., 90% or 1/e, may also be used, as may the root mean square (rms) value of the pulse amplitude.\n\nIn radar, the pulse duration is the time the radar's transmitter is energized during each cycle.\n\n", "related": "NONE"}
{"id": "756197", "url": "https://en.wikipedia.org/wiki?curid=756197", "title": "Vector signal analyzer", "text": "Vector signal analyzer\n\nA vector signal analyzer is an instrument that measures the magnitude and phase of the input signal at a single frequency within the IF bandwidth of the instrument. The primary use is to make in-channel measurements, such as error vector magnitude, code domain power, and spectral flatness, on known signals.\n\nVector signal analyzers are useful in measuring and demodulating digitally modulated signals like W-CDMA, LTE, and WLAN. These measurements are used to determine the quality of modulation and can be used for design validation and compliance testing of electronic devices.\n\nThe vector signal analyzer spectrum analysis process typically has a down-convert & digitizing stage and a DSP & display stage.\n\nA vector signal analyzer operates by first down-converting the signal spectra by using superheterodyne techniques.\nA portion of the input signal spectrum is down-converted (using a voltage-controlled oscillator and a mixer) to the center frequency of a band-pass filter. The use of a voltage-controlled oscillator allows consideration of different carrier frequencies.\n\nAfter the conversion to an intermediate frequency, the signal is filtered in order to band-limit the signal and prevent aliasing. The signal is then digitized using an analog-to-digital converter. Sampling rate is often varied in relation to the frequency span under consideration.\n\nOnce the signal is digitized, it is separated into quadrature and in-phase components using a quadrature detector, which is typically implemented with a discrete Hilbert transform. Several measurements are made and displayed using these signal components and various DSP processes, such as the ones below.\n\nA FFT is used to compute the frequency spectrum of the signal. Usually there is a windowing function option to limit spectral leakage and enhance frequency resolution. This window is implemented by multiplying it with the digitized values of the sample period before computing the FFT.\n\nA constellation diagram represents a signal modulated by a digital modulation scheme such as quadrature amplitude modulation or phase-shift keying. This diagram maps the magnitude of the quadrature and in-phase components to the vertical and horizontal directions respectively. Qualitative assessments of signal integrity can be made based on interpretation of this diagram.\n\nBy representing the quadrature and in-phase components as the vertical and horizontal axes, the error vector magnitude can be computed as the distance between the ideal and measured constellation points on the diagram. This requires knowledge of the modulated signal in order to compare the received signal with the ideal signal.\n\nTypical vector signal analyzer displays feature the spectrum of the signal measured within the IF bandwidth, a constellation diagram of the demodulated signal, error vector magnitude measurements, and a time-domain plot of the signal. Many more measurement results can be displayed depending on the type of modulation being used (symbol decoding, MIMO measurements, radio frame summary, etc.).\n\n- Communications Systems Analysis Using Hardware and Software-Based Vector Signal Analyzers\n", "related": "NONE"}
{"id": "1314272", "url": "https://en.wikipedia.org/wiki?curid=1314272", "title": "Dirac comb", "text": "Dirac comb\n\nIn mathematics, a Dirac comb (also known as an impulse train and sampling function in electrical engineering) is a periodic tempered distribution constructed from Dirac delta functions\n\nfor some given period \"T\". The symbol formula_2, where the period is omitted, represents a Dirac comb of unit period. Some authors, notably Bracewell, as well as some textbook authors in electrical engineering and circuit theory, refer to it as the Shah function (possibly because its graph resembles the shape of the Cyrillic letter sha Ш). Because the Dirac comb function is periodic, it can be represented as a Fourier series:\n\nThe Dirac comb function allows one to represent both continuous and discrete phenomena, such as sampling and aliasing, in a single framework of continuous Fourier analysis on Schwartz distributions, without any reference to Fourier series. Owing to the Poisson summation formula, in signal processing, the Dirac comb allows modelling sampling by \"multiplication\" with it, but it also allows modelling periodization by \"convolution\" with it.\n\nThe Dirac comb can be constructed in two ways, either by using the \"comb\" operator (performing sampling) applied to the function that is constantly formula_4, or, alternatively, by using the \"rep\" operator (performing periodization) applied to the Dirac delta formula_5. Formally, this yields (; )\n\nwhere\n\nIn signal processing, this property on one hand allows sampling a function formula_9 by \"multiplication\" with formula_10, and on the other hand it also allows the periodization of formula_9 by \"convolution\" with formula_10 ().\n\nThe scaling property of the Dirac comb follows from the properties of the Dirac delta function. \nSince formula_13 for positive real numbers formula_14, it follows that:\n\nNote that requiring positive scaling numbers formula_14 instead of negative ones is not a restriction because the negative sign would only reverse the order of the summation within formula_10, which does not affect the result.\n\nIt is clear that formula_19 is periodic with period formula_20. That is,\n\nfor all \"t\". The complex Fourier series for such a periodic function is\n\nwhere the Fourier coefficients are\n\nAll Fourier coefficients are 1/\"T\" resulting in\n\nWhen the period is one unit, this simplifies to \n\nThe Fourier transform of a Dirac comb is also a Dirac comb. This is evident when one considers that all the Fourier components add constructively whenever formula_26 is an integer multiple of formula_27.\n\nUnitary transform to ordinary frequency domain (Hz):\n\nNotably, the unit period Dirac comb transforms to itself:\n\nThe specific rule depends on the form of the Fourier transform used. When using a unitary transform of angular frequency (radian/s), the rule is\n\nMultiplying any function by a Dirac comb transforms it into a train of impulses with integrals equal to the value of the function at the nodes of the comb. This operation is frequently used to represent sampling.\n\nDue to the self-transforming property of the Dirac comb and the convolution theorem, this corresponds to convolution with the Dirac comb in the frequency domain.\n\nSince convolution with a delta function formula_33 is equivalent to shifting the function by formula_34, convolution with the Dirac comb corresponds to replication or periodic summation:\n\nThis leads to a natural formulation of the Nyquist–Shannon sampling theorem. If the spectrum of the function formula_36 contains no frequencies higher than B (i.e., its spectrum is nonzero only in the interval formula_37) then samples of the original function at intervals formula_38 are sufficient to reconstruct the original signal. It suffices to multiply the spectrum of the sampled function by a suitable rectangle function, which is equivalent to applying a brick-wall lowpass filter.\n\nIn the time domain, this multiplication is equivalent to convolving a sinc function with the samples of the signal, leading to the Whittaker–Shannon interpolation formula.\n\nIn directional statistics, the Dirac comb of period 2 is equivalent to a wrapped Dirac delta function and is the analog of the Dirac delta function in linear statistics.\n\nIn linear statistics, the random variable (\"x\") is usually distributed over the real-number line, or some subset thereof, and the probability density of \"x\" is a function whose domain is the set of real numbers, and whose integral from formula_41 to formula_42 is unity. In directional statistics, the random variable (θ) is distributed over the unit circle, and the probability density of θ is a function whose domain is some interval of the real numbers of length 2 and whose integral over that interval is unity. Just as the integral of the product of a Dirac delta function with an arbitrary function over the real-number line yields the value of that function at zero, so the integral of the product of a Dirac comb of period 2 with an arbitrary function of period 2 over the unit circle yields the value of that function at zero.\n\n", "related": "\n- Frequency comb\n- Poisson summation formula\n\n"}
{"id": "2000394", "url": "https://en.wikipedia.org/wiki?curid=2000394", "title": "Reconstruction from zero crossings", "text": "Reconstruction from zero crossings\n\nThe problem of reconstruction from zero crossings can be stated as: given the zero crossings of a continuous signal, is it possible to reconstruct the signal (to within a constant factor)? Worded differently, what are the conditions under which a signal can be reconstructed from its zero crossings?\n\nThis problem has two parts. Firstly, proving that there is a unique reconstruction of the signal from the zero crossings, and secondly, how to actually go about reconstructing the signal. Though there have been quite a few attempts, no conclusive solution has yet been found. Ben Logan from Bell Labs wrote an article in 1977 in the \"Bell System Technical Journal\" giving some criteria under which unique reconstruction is possible. Though this has been a major step towards the solution, many people are dissatisfied with the type of condition that results from his article.\n\nAccording to Logan, a signal is uniquely reconstructible from its zero crossings if:\n1. The signal \"x\"(\"t\") and its Hilbert transform \"x\" have no zeros in common with each other.\n2. The frequency-domain representation of the signal is at most 1 octave long, in other words, it is bandpass-limited between some frequencies \"B\" and 2\"B\".\n\n- B. F. Logan, Jr. \"Information in the Zero Crossings of Bandpass Signals\", \"Bell System Technical Journal\", vol. 56, pp. 487–510, April 1977.\n\n- Reconstruction of two-dimensional signals from threshold crossings\n", "related": "NONE"}
{"id": "2198965", "url": "https://en.wikipedia.org/wiki?curid=2198965", "title": "Cross-covariance", "text": "Cross-covariance\n\nIn probability and statistics, given two stochastic processes formula_1 and formula_2, the cross-covariance is a function that gives the covariance of one process with the other at pairs of time points. With the usual notation formula_3; for the expectation operator, if the processes have the mean functions formula_4 and formula_5, then the cross-covariance is given by\n\nCross-covariance is related to the more commonly used cross-correlation of the processes in question.\n\nIn the case of two random vectors formula_7 and formula_8, the cross-covariance would be a formula_9 matrix formula_10 (often denoted formula_11) with entries formula_12 Thus the term \"cross-covariance\" is used in order to distinguish this concept from the covariance of a random vector formula_13, which is understood to be the matrix of covariances between the scalar components of formula_13 itself.\n\nIn signal processing, the cross-covariance is often called cross-correlation and is a measure of similarity of two signals, commonly used to find features in an unknown signal by comparing it to a known one. It is a function of the relative time between the signals, is sometimes called the \"sliding dot product\", and has applications in pattern recognition and cryptanalysis.\n\nThe definition of cross-covariance of random vector may be generalized to stochastic processes as follows:\nLet formula_15 and formula_16 denote stochastic processes. Then the cross-covariance function of the processes formula_17 is defined by:\n\nwhere formula_18 and formula_19.\n\nIf the processes are complex stochastic processes, the second factor needs to be complex conjugated.\n\nIf formula_1 and formula_2 are a jointly wide-sense stationary, then the following are true:\n\nand\n\nBy setting formula_29 (the time lag, or the amount of time by which the signal has been shifted), we may define\n\nThe autocovariance function of a WSS process is therefore given by:\n\nwhich is equivalent to\n\nTwo stochastic processes formula_1 and formula_2 are called uncorrelated if their covariance formula_34 is zero for all times. Formally:\n\nThe cross-covariance is also relevant in signal processing where the cross-covariance between two wide-sense stationary random processes can be estimated by averaging the product of samples measured from one process and samples measured from the other (and its time shifts). The samples included in the average can be an arbitrary subset of all the samples in the signal (e.g., samples within a finite time window or a sub-sampling of one of the signals). For a large number of samples, the average converges to the true covariance.\n\nCross-covariance may also refer to a \"deterministic\" cross-covariance between two signals. This consists of summing over \"all\" time indices. \nFor example, for discrete-time signals formula_36 and formula_37 the cross-covariance is defined as\n\nwhere the line indicates that the complex conjugate is taken when the signals are complex-valued.\n\nFor continuous functions formula_39 and formula_40 the (deterministic) cross-covariance is defined as\n\nThe (deterministic) cross-covariance of two continuous signals is related to the convolution by\n\nand the (deterministic) cross-covariance of two discrete-time signals is related to the discrete convolution by\n\n", "related": "\n- Autocovariance\n- Autocorrelation\n- Correlation\n- Convolution\n- Cross-correlation\n\n- Cross Correlation from Mathworld\n- http://scribblethink.org/Work/nvisionInterface/nip.html\n- http://www.phys.ufl.edu/LIGO/stochastic/sign05.pdf\n- http://www.staff.ncl.ac.uk/oliver.hinton/eee305/Chapter6.pdf\n"}
{"id": "2338241", "url": "https://en.wikipedia.org/wiki?curid=2338241", "title": "Lanczos resampling", "text": "Lanczos resampling\n\nLanczos filtering and Lanczos resampling are two applications of a mathematical formula. It can be used as a low-pass filter or used to smoothly interpolate the value of a digital signal between its samples. In the latter case it maps each sample of the given signal to a translated and scaled copy of the Lanczos kernel, which is a sinc function windowed by the central lobe of a second, longer, sinc function. The sum of these translated and scaled kernels is then evaluated at the desired points.\n\nLanczos resampling is typically used to increase the sampling rate of a digital signal, or to shift it by a fraction of the sampling interval. It is often used also for multivariate interpolation, for example to resize or rotate a digital image. It has been considered the \"best compromise\" among several simple filters for this purpose.\n\nThe filter is named after its inventor, Cornelius Lanczos ().\n\nThe effect of each input sample on the interpolated values is defined by the filter's reconstruction kernel , called the Lanczos kernel. It is the normalized sinc function , windowed (multiplied) by the Lanczos window, or sinc window, which is the central lobe of a horizontally stretched sinc function for .\n\nEquivalently,\n\nThe parameter is a positive integer, typically 2 or 3, which determines the size of the kernel. The Lanczos kernel has lobes: a positive one at the center, and alternating negative and positive lobes on each side.\n\nGiven a one-dimensional signal with samples , for integer values of , the value interpolated at an arbitrary real argument is obtained by the discrete convolution of those samples with the Lanczos kernel:\nwhere is the filter size parameter, and formula_4 is the floor function. The bounds of this sum are such that the kernel is zero outside of them.\n\nAs long as the parameter is a positive integer, the Lanczos kernel is continuous everywhere, and its derivative is defined and continuous everywhere (even at  = , where both sinc functions go to zero). Therefore, the reconstructed signal too will be continuous, with continuous derivative.\n\nThe Lanczos kernel is zero at every integer argument , except at  = 0, where it has value 1. Therefore, the reconstructed signal exactly interpolates the given samples: we will have  =  for every integer argument  = .\n\nLanczos filter's kernel in two dimensions is\n\nThe theoretically optimal reconstruction filter for band-limited signals is the sinc filter, which has infinite support. The Lanczos filter is one of many practical (finitely supported) approximations of the sinc filter. Each interpolated value is the weighted sum of consecutive input samples. Thus, by varying the parameter one may trade computation speed for improved frequency response. The parameter also allows one to choose between a smoother interpolation or a preservation of sharp transients in the data. For image processing, the trade-off is between the reduction of aliasing artefacts and the preservation of sharp edges. Also as with any such processing, there are no results for the borders of the image. Increasing the length of the kernel increases the cropping of the edges of the image.\n\nThe Lanczos filter has been compared with other interpolation methods for discrete signals, particularly other windowed versions of the sinc filter. Turkowski and Gabriel claimed that the Lanczos filter (with = 2) the \"best compromise in terms of reduction of aliasing, sharpness, and minimal ringing\", compared with truncated sinc and the Bartlett, cosine-, and Hann-windowed sinc, for decimation and interpolation of 2-dimensional image data. According to Jim Blinn, the Lanczos kernel (with = 3) \"keeps low frequencies and rejects high frequencies better than any (achievable) filter we've seen so far.\"\n\nLanczos interpolation is a popular filter for \"upscaling\" videos in various media utilities, such as AviSynth and FFmpeg.\n\nSince the kernel assumes negative values for , the interpolated signal can be negative even if all samples are positive. More generally, the range of values of the interpolated signal may be wider than the range spanned by the discrete sample values. In particular, there may be ringing artifacts just before and after abrupt changes in the sample values, which may lead to clipping artifacts. However, these effects are reduced compared to the (non-windowed) sinc filter. For \"a\" = 2 (a three-lobed kernel) the ringing is < 1%.\n\nThe method is one of the interpolation options available in the free software GNU Image Manipulation Program (GIMP). One way to visualise the ringing effect is to rescale a black and white block graphic and select Lanczos interpolation. \n\nWhen using the Lanczos filter for image resampling, the ringing effect will create light and dark halos along any strong edges. While these bands may be visually annoying, they help increase the perceived sharpness, and therefore provide a form of edge enhancement. This may improve the subjective quality of the image, given the special role of edge sharpness in vision.\n\nIn some applications, the low-end clipping artifacts can be ameliorated by transforming the data to a logarithmic domain prior to filtering. In this case the interpolated values will be a weighted geometric mean, rather than an arithmetic mean, of the input samples.\n\nThe Lanczos kernel does not have the partition of unity property. That is, the sum formula_6 of all integer-translated copies of the kernel is not always 1. Therefore, the Lanczos interpolation of a discrete signal with constant samples does not yield a constant function. This defect is most evident when  = 1. Also, for  = 1 the interpolated signal has zero derivative at every integer argument. This is rather academic, since using a single-lobe kernel (\"a\" = 1) loses all the benefits of the Lanczos approach and provides a poor filter. There are many better single-lobe, bell-shaped windowing functions.\n\n", "related": "\n- Bicubic interpolation\n- Bilinear interpolation\n- Spline interpolation\n- Nearest-neighbor interpolation\n- Sinc filter\n\n- Anti-Grain Geometry examples: image_filters.cpp shows comparisons of repeatedly resampling an image with various kernels.\n- imageresampler: A public domain image resampling class in C++ with support for several windowed Lanczos filter kernels.\n"}
{"id": "2885946", "url": "https://en.wikipedia.org/wiki?curid=2885946", "title": "Median filter", "text": "Median filter\n\nThe Median Filter is a non-linear digital filtering technique, often used to remove noise from an image or signal. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image). Median filtering is very widely used in digital image processing because, under certain conditions, it preserves edges while removing noise (but see discussion below), also having applications in signal processing.\n\nThe main idea of the median filter is to run through the signal entry by entry, replacing each entry with the median of neighboring entries. The pattern of neighbors is called the \"window\", which slides, entry by entry, over the entire signal. For 1D signals, the most obvious window is just the first few preceding and following entries, whereas for 2D (or higher-dimensional) data the window must include all entries within a given radius or ellipsoidal region (i.e. the median filter is not a separable filter).\n\nTo demonstrate, using a window size of three with one entry immediately preceding and following each entry, a median filter will be applied to the following simple 1D signal:\n\nSo, the median filtered output signal \"y\" will be:\n\ni.e. \"y\" = (3, 6, 6, 3).\n\nIn the example above, because there is no entry preceding the first value, the first value is repeated, as with the last value, to obtain enough entries to fill the window. This is one way of handling missing window entries at the boundaries of the signal, but there are other schemes that have different properties that might be preferred in particular circumstances:\n\n- Avoid processing the boundaries, with or without cropping the signal or image boundary afterwards,\n- Fetching entries from other places in the signal. With images for example, entries from the far horizontal or vertical boundary might be selected,\n- Shrinking the window near the boundaries, so that every window is full.\n\nCode for a simple 2D median filter algorithm might look like this:\n\nThis algorithm:\n- Processes one color channel only,\n- Takes the \"not processing boundaries\" approach (see above discussion about boundary issues).\n\nTypically, by far the majority of the computational effort and time is spent on calculating the median of each window. Because the filter must process every entry in the signal, for large signals such as images, the efficiency of this median calculation is a critical factor in determining how fast the algorithm can run. The naïve implementation described above sorts every entry in the window to find the median; however, since only the middle value in a list of numbers is required, selection algorithms can be much more efficient. Furthermore, some types of signals (very often the case for images) use whole number representations: in these cases, histogram medians can be far more efficient because it is simple to update the histogram from window to window, and finding the median of a histogram is not particularly onerous.\n\nMedian filtering is one kind of smoothing technique, as is linear Gaussian filtering. All smoothing techniques are effective at removing noise in smooth patches or smooth regions of a signal, but adversely affect edges. Often though, at the same time as reducing the noise in a signal, it is important to preserve the edges. Edges are of critical importance to the visual appearance of images, for example. For small to moderate levels of Gaussian noise, the median filter is demonstrably better than Gaussian blur at removing noise whilst preserving edges for a given, fixed window size. However, its performance is not that much better than Gaussian blur for high levels of noise, whereas, for speckle noise and salt-and-pepper noise (impulsive noise), it is particularly effective. Because of this, median filtering is very widely used in digital image processing.\n\n", "related": "\n- Image noise\n- Weighted median\n- pseudo-median filter\n- Lulu smoothing\n- Bilateral filter\n- Average with limited data validity\n- Smoothing\n\n- Fast Matlab 1D median filter implementation\n- Mathematica MedianFilter function\n- Median filter\n- Fast 2D median filter\n- Implementation of 2D Median filter in constant time (GPL license) – the running time per pixel of this algorithm is proportional to the number of elements in a histogram (typically this is formula_1, where \"n\" is the number of bits per channel), even though this in turn is a constant.\n- Implementation written in different programming languages (on Rosetta Code)\n- Dr Dobbs article\n- 100+ Times Faster Weighted Median Filter\n"}
{"id": "3125808", "url": "https://en.wikipedia.org/wiki?curid=3125808", "title": "Instantaneous phase and frequency", "text": "Instantaneous phase and frequency\n\nInstantaneous phase and frequency are important concepts in signal processing that occur in the context of the representation and analysis of time-varying functions. The instantaneous phase (also known as local phase or simply phase) of a \"complex-valued\" function \"s\"(\"t\"), is the real-valued function:\nwhere arg is the complex argument function.\nThe instantaneous frequency is the temporal rate of the instantaneous phase.\n\nAnd for a \"real-valued\" function \"s\"(\"t\"), it is determined from the function's analytic representation, \"s\"(\"t\"):\n\nWhen \"φ\"(\"t\") is constrained to its principal value, either the interval (−, ] or [0, 2), it is called \"wrapped phase\". Otherwise it is called \"unwrapped phase\", which is a continuous function of argument \"t\", assuming \"s\"(\"t\") is a continuous function of \"t\". Unless otherwise indicated, the continuous form should be inferred.\n\nwhere \"ω\" > 0.\nIn this simple sinusoidal example, the constant \"θ\" is also commonly referred to as \"phase\" or \"phase offset\". \"φ\"(\"t\") is a function of time; \"θ\" is not. In the next example, we also see that the phase offset of a real-valued sinusoid is ambiguous unless a reference (sin or cos) is specified. \"φ\"(\"t\") is unambiguously defined.\n\nwhere \"ω\" > 0.\nIn both examples the local maxima of \"s\"(\"t\") correspond to \"φ\"(\"t\") = 2\"N\" for integer values of \"N\". This has applications in the field of computer vision.\n\nInstantaneous angular frequency is defined as:\nand instantaneous (ordinary) frequency is defined as:\nwhere \"φ\"(\"t\") must be the \"unwrapped\" instantaneous phase angle. If \"φ\"(\"t\") is wrapped, discontinuities in \"φ\"(\"t\") will result in Dirac delta impulses in \"f\"(\"t\").\n\nThe inverse operation, which always unwraps phase, is:\n\nThis instantaneous frequency, \"ω\"(\"t\"), can be derived directly from the real and imaginary parts of \"s\"(\"t\"), instead of the complex arg without concern of phase unwrapping.\n\n2\"m\" and \"m\" are the integer multiples of necessary to add to unwrap the phase. At values of time, \"t\", where there is no change to integer \"m\", the derivative of \"φ\"(\"t\") is\n\nThis representation is similar to the wrapped phase representation in that it does not distinguish between multiples of 2 in the phase, but similar to the unwrapped phase representation since it is continuous. A vector-average phase can be obtained as the arg of the sum of the complex numbers without concern about wrap-around.\n\n", "related": "\n- Analytic signal\n- Frequency modulation\n- Instantaneous amplitude\n\n"}
{"id": "3243253", "url": "https://en.wikipedia.org/wiki?curid=3243253", "title": "Comb generator", "text": "Comb generator\n\nA comb generator is a signal generator that produces multiple harmonics of its input signal. The appearance of the output at the spectrum analyzer screen, resembling teeth of a comb, gave the device its name.\n\nComb generators find wide range of uses in microwave technology. E.g., synchronous signals in wide frequency bandwidth can be produced by a comb generator. The most common use is in broadband frequency synthesizers, where the high frequency signals act as stable references correlated to the lower energy references; the outputs can be used directly, or to synchronize phase-locked loop oscillators. It may be also used to generate a complete set of substitution channels for testing, each of which carries the same baseband audio and video signal.\n\nComb generators are also used in RFI testing of consumer electronics, where their output is used as a simulated RF emissions, as it is a stable broadband noise source with repeatable output. It is also used during compliance testing to various government requirements for products such as medical devices (FDA), military electronics (MIL-STD-461), commercial avionics (Federal Aviation Administration), digital electronics (Federal Communications Commission), in the USA.\n\nAn optical comb generator can be used as generators of terahertz radiation. Internally, it is a resonant electro-optic modulator, with the capability of generating hundreds of sidebands with total span of at least 3 terahertz (limited by the optical dispersion of the lithium niobate crystal) and frequency spacing of 17 GHz. Other construction can be based on erbium-doped fiber laser or Ti-sapphire laser often in combination with carrier envelope offset control.\n\n", "related": "\n- Comb filter\n- Frequency comb\n\n- Yet another comb generator\n- Com-Power Corporation\n"}
{"id": "546120", "url": "https://en.wikipedia.org/wiki?curid=546120", "title": "Recurrence plot", "text": "Recurrence plot\n\nIn descriptive statistics and chaos theory, a recurrence plot (RP) is a plot showing, for each moment \"i\" in time, the times at which a phase space trajectory visits roughly the same area in the phase space as at time \"j\". In other words, it is a graph of\n\nshowing formula_2 on a horizontal axis and formula_3 on a vertical axis, where formula_4 is a phase space trajectory.\n\nNatural processes can have a distinct recurrent behaviour, e.g. periodicities (as seasonal or Milankovich cycles), but also irregular cyclicities (as El Niño Southern Oscillation). Moreover, the recurrence of states, in the meaning that states are again arbitrarily close after some time of divergence, is a fundamental property of deterministic dynamical systems and is typical for nonlinear or chaotic systems (cf. Poincaré recurrence theorem). The recurrence of states in nature has been known for a long time and has also been discussed in early work (e.g. Henri Poincaré 1890).\n\nEckmann et al. (1987) introduced recurrence plots, which provide a way to visualize the periodic nature of a trajectory through a phase space. Often, the phase space does not have a low enough dimension (two or three) to be pictured, since higher-dimensional phase spaces can only be visualized by projection into the two or three-dimensional sub-spaces. However, making a recurrence plot enables us to investigate certain aspects of the \"m\"-dimensional phase space trajectory through a two-dimensional representation.\n\nA recurrence is a time the trajectory returns to a location it has visited before. The recurrence plot depicts the collection of pairs of times at which the trajectory is at the same place, i.e. the set of formula_5 with formula_6. This can show many things: for instance, if the trajectory is strictly periodic with period formula_7, then all such pairs of times will be separated by a multiple of formula_7 and visible as diagonal lines. To make the plot, continuous time and continuous phase space are discretized, taking e.g. formula_9 as the location of the trajectory at time formula_10 and counting as a recurrence any time the trajectory gets sufficiently close (say, within ε) to a point it has been previously. Concretely then, recurrence/non-recurrence can be recorded by the binary function\n\nand the recurrence plot puts a (black) point at coordinates formula_5 if formula_13.\n\nCaused by characteristic behaviour of the phase space trajectory, a recurrence plot contains typical small-scale structures, as single dots, diagonal lines and vertical/horizontal lines (or a mixture of the latter, which combines to extended clusters). The large-scale structure, also called \"texture\", can be visually characterised by \"homogenous\", \"periodic\", \"drift\" or \"disrupted\". The visual appearance of an RP gives hints about the dynamics of the system.\n\nThe small-scale structures in RPs are used by the recurrence quantification analysis (Zbilut & Webber 1992; Marwan et al. 2002). This quantification allows to describe the RPs in a quantitative way, and to study transitions or nonlinear parameters of the system. In contrast to the heuristic approach of the recurrence quantification analysis, which depends on the choice of the embedding parameters, some dynamical invariants as correlation dimension, K2 entropy or mutual information, which are independent on the embedding, can also be derived from recurrence plots. The base for these dynamical invariants are the recurrence rate and the distribution of the lengths of the diagonal lines.\n\nClose returns plots are similar to recurrence plots. The difference is that the relative time between recurrences is used for the formula_14-axis (instead of absolute time).\n\nThe main advantage of recurrence plots is that they provide useful information even for short and non-stationary data, where other methods fail.\n\nMultivariate extensions of recurrence plots were developed as cross recurrence plots and joint recurrence plots.\n\nCross recurrence plots consider the phase space trajectories of two different systems in the same phase space (Marwan & Kurths 2002):\n\nThe dimension of both systems must be the same, but the number of considered states (i.e. data length) can be different. Cross recurrence plots compare the occurrences of \"similar states\" of two systems. They can be used in order to analyse the similarity of the dynamical evolution between two different systems, to look for similar matching patterns in two systems, or to study the time-relationship of two similar systems, whose time-scale differ (Marwan & Kurths 2005).\n\nJoint recurrence plots are the Hadamard product of the recurrence plots of the considered sub-systems (Romano et al. 2004), e.g. for two systems formula_4 and formula_17 the joint recurrence plot is\n\nIn contrast to cross recurrence plots, joint recurrence plots compare the simultaneous occurrence of \"recurrences\" in two (or more) systems. Moreover, the dimension of the considered phase spaces can be different, but the number of the considered states has to be the same for all the sub-systems. Joint recurrence plots can be used in order to detect phase synchronisation.\n\n", "related": "\n- Poincaré plot\n- Recurrence period density entropy, an information-theoretic method for summarising the recurrence properties of both deterministic and stochastic dynamical systems.\n- Recurrence quantification analysis, a heuristic approach to quantify recurrence plots.\n- Self-similarity matrix\n- Dot plot (bioinformatics)\n\n- http://www.recurrence-plot.tk/\n- http://www.scitopics.com/Recurrence_plot.html\n"}
{"id": "4119746", "url": "https://en.wikipedia.org/wiki?curid=4119746", "title": "Zero-crossing rate", "text": "Zero-crossing rate\n\nThe zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes \nfrom positive to zero to negative or \nfrom negative to zero to positive. \nThis feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.\n\nZCR is defined formally as\n\nwhere formula_2 is a signal of length formula_3 and formula_4 is an indicator function.\n\nIn some cases only the \"positive-going\" or \"negative-going\" crossings are counted, rather than all the crossings - since, logically, between a pair of adjacent positive zero-crossings there must be one and only one negative zero-crossing.\n\nFor monophonic tonal signals, the zero-crossing rate can be used as a primitive pitch detection algorithm.\n\nZero crossing rates are used for Voice activity detection (VAD), i.e., finding whether human speech is present in an audio segment or not.\n\n", "related": "\n- Zero crossing\n- Digital signal processing\n"}
{"id": "151474", "url": "https://en.wikipedia.org/wiki?curid=151474", "title": "Aliasing", "text": "Aliasing\n\nIn signal processing and related disciplines, aliasing is an effect that causes different signals to become indistinguishable (or \"aliases\" of one another) when sampled. It also often refers to the distortion or artifact that results when a signal reconstructed from samples is different from the original continuous signal.\n\nAliasing can occur in signals sampled in time, for instance digital audio, and is referred to as temporal aliasing. It can also occur in spatially sampled signals (e.g. moiré patterns in digital images); this type of aliasing is called spatial aliasing.\n\nAliasing is generally avoided by applying low pass filters or anti-aliasing filters (AAF) to the input signal before sampling and when converting a signal from a higher to a lower sampling rate. Suitable reconstruction filtering should then be used when restoring the sampled signal to the continuous domain or converting a signal from a lower to a higher sampling rate. For spatial anti-aliasing, the types of anti-aliasing include fast sample anti-aliasing (FSAA), multisample anti-aliasing, and supersampling.\n\nWhen a digital image is viewed, a reconstruction is performed by a display or printer device, and by the eyes and the brain. If the image data is processed in some way during sampling or reconstruction, the reconstructed image will differ from the original image, and an alias is seen.\n\nAn example of spatial aliasing is the moiré pattern observed in a poorly pixelized image of a brick wall. Spatial anti-aliasing techniques avoid such poor pixelizations. Aliasing can be caused either by the sampling stage or the reconstruction stage; these may be distinguished by calling sampling aliasing \"prealiasing\" and reconstruction aliasing \"postaliasing.\"\n\nTemporal aliasing is a major concern in the sampling of video and audio signals. Music, for instance, may contain high-frequency components that are inaudible to humans. If a piece of music is sampled at 32000 samples per second (Hz), any frequency components at or above 16000 Hz (the Nyquist frequency for this sampling rate) will cause aliasing when the music is reproduced by a digital-to-analog converter (DAC). To prevent this, an anti-aliasing filter is used to remove components above the Nyquist frequency prior to sampling.\n\nIn video or cinematography, temporal aliasing results from the limited frame rate, and causes the wagon-wheel effect, whereby a spoked wheel appears to rotate too slowly or even backwards. Aliasing has changed its apparent frequency of rotation. A reversal of direction can be described as a negative frequency. Temporal aliasing frequencies in video and cinematography are determined by the frame rate of the camera, but the relative intensity of the aliased frequencies is determined by the shutter timing (exposure time) or the use of a temporal aliasing reduction filter during filming.\n\nLike the video camera, most sampling schemes are periodic; that is, they have a characteristic sampling frequency in time or in space. Digital cameras provide a certain number of samples (pixels) per degree or per radian, or samples per mm in the focal plane of the camera. Audio signals are sampled (digitized) with an analog-to-digital converter, which produces a constant number of samples per second. Some of the most dramatic and subtle examples of aliasing occur when the signal being sampled also has periodic content.\n\nActual signals have a finite duration and their frequency content, as defined by the Fourier transform, has no upper bound. Some amount of aliasing always occurs when such functions are sampled. Functions whose frequency content is bounded (\"bandlimited\") have an infinite duration in the time domain. If sampled at a high enough rate, determined by the \"bandwidth\", the original function can, in theory, be perfectly reconstructed from the infinite set of samples.\n\nSometimes aliasing is used intentionally on signals with no low-frequency content, called \"bandpass\" signals. Undersampling, which creates low-frequency aliases, can produce the same result, with less effort, as frequency-shifting the signal to lower frequencies before sampling at the lower rate. Some digital channelizers\nexploit aliasing in this way for computational efficiency.\nSee Sampling (signal processing), Nyquist rate (relative to sampling), and Filter bank.\n\nSinusoids are an important type of periodic function, because realistic signals are often modeled as the summation of many sinusoids of different frequencies and different amplitudes (for example, with a Fourier series or transform). Understanding what aliasing does to the individual sinusoids is useful in understanding what happens to their sum.\n\nWhen sampling a function at frequency (intervals ), the following functions yield identical sets of samples: }. A frequency spectrum of the samples produces equally strong responses at all those frequencies. Without collateral information, the frequency of the original function is ambiguous. So the functions and their frequencies are said to be \"aliases\" of each other. Noting the trigonometric identity:\n\nwe can write all the alias frequencies as positive values:  .\n\nFor example, here a plot depicts a set of samples with parameter , and two different sinusoids that could have produced the samples. Nine cycles of the red sinusoid and one cycle of the blue sinusoid span an interval of 10 samples. The corresponding number of \"cycles per sample\" are    and  .  So the   alias of    is    (and vice versa).\n\nAliasing matters when one attempts to reconstruct the original waveform from its samples. The most common reconstruction technique produces the smallest of the    frequencies. So it is usually important that    be the unique minimum.  A necessary and sufficient condition for that is    where    is commonly called the Nyquist frequency of a system that samples at rate    In our example, the Nyquist condition is satisfied if the original signal is the blue sinusoid ().  But if    the usual reconstruction method will produce the blue sinusoid instead of the red one.\n\nIn the example above,    and    are symmetrical around the frequency    And in general, as    increases from 0 to       decreases from    to    Similarly, as    increases from    to       continues decreasing from    to 0.\n\nA graph of amplitude vs frequency for a single sinusoid at frequency    and some of its aliases at      and    would look like the 4 black dots in the first figure below. The red lines depict the paths (loci) of the 4 dots if we were to adjust the frequency and amplitude of the sinusoid along the solid red segment (between    and  ).  No matter what function we choose to change the amplitude vs frequency, the graph will exhibit symmetry between 0 and    This symmetry is commonly referred to as folding, and another name for    (the Nyquist frequency) is folding frequency. Folding is often observed in practice when viewing the frequency spectrum of real-valued samples, such as the second figure below.\n\nComplex sinusoids are waveforms whose samples are complex numbers, and the concept of negative frequency is necessary to distinguish them. In that case, the frequencies of the aliases are given by just:    Therefore, as    increases from    to       goes from    up to 0.  Consequently, complex sinusoids do not exhibit \"folding\". Complex samples of real-valued sinusoids have zero-valued imaginary parts and do exhibit folding.\n\nWhen the condition    is met for the highest frequency component of the original signal, then it is met for all the frequency components, a condition called the Nyquist criterion. That is typically approximated by filtering the original signal to attenuate high frequency components before it is sampled. These attenuated high frequency components still generate low-frequency aliases, but typically at low enough amplitudes that they do not cause problems. A filter chosen in anticipation of a certain sample frequency is called an anti-aliasing filter.\n\nThe filtered signal can subsequently be reconstructed, by interpolation algorithms, without significant additional distortion. Most sampled signals are not simply stored and reconstructed. But the fidelity of a theoretical reconstruction (via the Whittaker–Shannon interpolation formula) is a customary measure of the effectiveness of sampling.\n\nHistorically the term \"aliasing\" evolved from radio engineering because of the action of superheterodyne receivers. When the receiver shifts multiple signals down to lower frequencies, from RF to IF by heterodyning, an unwanted signal, from an RF frequency equally far from the local oscillator (LO) frequency as the desired signal, but on the wrong side of the LO, can end up at the same IF frequency as the wanted one. If it is strong enough it can interfere with reception of the desired signal. This unwanted signal is known as an \"image\" or \"alias\" of the desired signal.\n\nAliasing occurs whenever the use of discrete elements to capture or produce a continuous signal causes frequency ambiguity.\n\nSpatial aliasing, particular of angular frequency, can occur when reproducing a light field or sound field with discrete elements, as in 3D displays or wave field synthesis of sound.\n\nThis aliasing is visible in images such as posters with lenticular printing: if they have low angular resolution, then as one moves past them, say from left-to-right, the 2D image does not initially change (so it appears to move left), then as one moves to the next angular image, the image suddenly changes (so it jumps right) – and the frequency and amplitude of this side-to-side movement corresponds to the angular resolution of the image (and, for frequency, the speed of the viewer's lateral movement), which is the angular aliasing of the 4D light field.\n\nThe lack of parallax on viewer movement in 2D images and in 3-D film produced by stereoscopic glasses (in 3D films the effect is called \"yawing\", as the image appears to rotate on its axis) can similarly be seen as loss of angular resolution, all angular frequencies being aliased to 0 (constant).\n\nThe qualitative effects of aliasing can be heard in the following audio demonstration. Six sawtooth waves are played in succession, with the first two sawtooths having a fundamental frequency of 440 Hz (A4), the second two having fundamental frequency of 880 Hz (A5), and the final two at 1760 Hz (A6). The sawtooths alternate between bandlimited (non-aliased) sawtooths and aliased sawtooths and the sampling rate is 22.05 kHz. The bandlimited sawtooths are synthesized from the sawtooth waveform's Fourier series such that no harmonics above the Nyquist frequency are present.\n\nThe aliasing distortion in the lower frequencies is increasingly obvious with higher fundamental frequencies, and while the bandlimited sawtooth is still clear at 1760 Hz, the aliased sawtooth is degraded and harsh with a buzzing audible at frequencies lower than the fundamental.\n\nA form of spatial aliasing can also occur in antenna arrays or microphone arrays used to estimate the direction of arrival of a wave signal, as in geophysical exploration by seismic waves. Waves must be sampled more densely than two points per wavelength, or the wave arrival direction becomes ambiguous.\n\n", "related": "\n- Brillouin zone\n- Glossary of video terms\n- Jaggies\n- Kell factor\n- Sinc filter\n- Sinc function\n- Stroboscopic effect\n- Wagon-wheel effect\n- Nyquist–Shannon sampling theorem#Critical frequency\n\n- Pharr, Matt; Humphreys, Greg. (28 June 2010). \"Physically Based Rendering: From Theory to Implementation\". Morgan Kaufmann. . Chapter 7 (\"Sampling and reconstruction\"). Retrieved 3 March 2013.\n- by Tektronix Application Engineer\n- Anti-Aliasing Filter Primer by La Vida Leica, discusses its purpose and effect on recorded images\n- Interactive examples demonstrating the aliasing effect\n"}
{"id": "4358050", "url": "https://en.wikipedia.org/wiki?curid=4358050", "title": "Bandwidth expansion", "text": "Bandwidth expansion\n\nBandwidth expansion is a technique for widening the bandwidth or the resonances in an LPC filter. This is done by moving all the poles towards the origin by a constant factor formula_1. The bandwidth-expanded filter formula_2 can be easily derived from the original filter formula_3 by:\n\nLet formula_3 be expressed as:\n\nThe bandwidth-expanded filter can be expressed as:\n\nIn other words, each coefficient formula_8 in the original filter is simply multiplied by formula_9 in the bandwidth-expanded filter. The simplicity of this transformation makes it attractive, especially in CELP coding of speech, where it is often used for the perceptual noise weighting and/or to stabilize the LPC analysis. However, when it comes to stabilizing the LPC analysis, lag windowing is often preferred to bandwidth expansion.\n\nP. Kabal, \"Ill-Conditioning and Bandwidth Expansion in Linear Prediction of Speech\", \"Proc. IEEE Int. Conf. Acoustics, Speech, Signal Processing\", pp. I-824-I-827, 2003.\n", "related": "NONE"}
{"id": "527784", "url": "https://en.wikipedia.org/wiki?curid=527784", "title": "Pulse (signal processing)", "text": "Pulse (signal processing)\n\nA pulse in signal processing is a rapid, transient change in the amplitude of a signal from a baseline value to a higher or lower value, followed by a rapid return to the baseline value.\n\nPulse shapes can arise out of a process called pulse-shaping. Optimum pulse shape depends on the application.\n\nThese can be found in pulse waves, square waves, boxcar functions, and rectangular functions. In digital signals the up and down transitions between high and low levels are called the rising edge and the falling edge. In digital systems the detection of these sides or action taken in response is termed edge-triggered, rising or falling depending on which side of rectangular pulse. A digital timing diagram is an example of a well-ordered collection of rectangular pulses.\n\nA Nyquist pulse is one which meets the Nyquist ISI criterion and is important in data transmission. An example of a pulse which meets this condition is the sinc function. The sinc pulse is of some significance in signal-processing theory but cannot be produced by a real generator for reasons of causality.\n\nIn 2013, Nyquist pulses were produced in an effort to reduce the size of pulses in optical fibers, which enables them to be packed 10x more closely together, yielding a corresponding 10x increase in bandwidth. The pulses were more than 99 percent perfect and were produced using a simple laser and modulator.\n\nA Gaussian pulse is shaped as a Gaussian function and is produced by a Gaussian filter. It has the properties of maximum steepness of transition with no overshoot and minimum group delay.\n", "related": "NONE"}
{"id": "2894481", "url": "https://en.wikipedia.org/wiki?curid=2894481", "title": "Energy (signal processing)", "text": "Energy (signal processing)\n\nIn signal processing, the energy formula_1 of a continuous-time signal \"x\"(\"t\") is defined as the area under the squared magnitude of the considered signal i.e., mathematically\n\nAnd the energy formula_1 of a discrete-time signal \"x\"(\"n\") is defined mathematically as\n\nEnergy in this context is not, strictly speaking, the same as the conventional notion of energy in physics and the other sciences. The two concepts are, however, closely related, and it is possible to convert from one to the other:\n\nFor example, if \"x\"(\"t\") represents the potential (in volts) of an electrical signal propagating across a transmission line, then \"Z\" would represent the characteristic impedance (in ohms) of the transmission line. The units of measure for the signal energy formula_1 would appear as volt·seconds, which is \"not\" dimensionally correct for energy in the sense of the physical sciences. After dividing formula_1 by \"Z\", however, the dimensions of \"E\" would become volt·seconds per ohm,\n\nformula_9\n\nwhich is equivalent to joules, the SI unit for energy as defined in the physical sciences.\n\nSimilarly, the spectral energy density of signal x(t) is\n\nwhere \"X\"(\"f\") is the Fourier transform of \"x\"(\"t\").\n\nFor example, if \"x\"(\"t\") represents the magnitude of the electric field component (in volts per meter) of an optical signal propagating through free space, then the dimensions of \"X\"(\"f\") would become volt·seconds per meter and formula_11 would represent the signal's spectral energy density (in volts·second per meter) as a function of frequency \"f\" (in hertz). Again, these units of measure are not dimensionally correct in the true sense of energy density as defined in physics. Dividing formula_11 by \"Z\", the characteristic impedance of free space (in ohms), the dimensions become joule-seconds per meter or, equivalently, joules per meter per hertz, which is dimensionally correct in SI units for spectral energy density.\n\nAs a consequence of Parseval's theorem, one can prove that the signal energy is always equal to the summation across all frequency components of the signal's spectral energy density.\n\n", "related": "\n- Signal processing\n- Parseval's theorem\n- Spectral density\n- Inner product\n"}
{"id": "2824933", "url": "https://en.wikipedia.org/wiki?curid=2824933", "title": "Bit banging", "text": "Bit banging\n\nIn computer engineering and electrical engineering, bit banging is slang for any method of data transmission which employs software as a substitute for dedicated hardware to generate transmitted signals or process received signals. Software directly sets and samples the states of GPIOs (e.g., pins on a microcontroller), and is responsible for meeting all timing requirements and protocol sequencing of the signals. In contrast to bit banging, dedicated hardware (e.g., UART, SPI interface) satisfies these requirements and, if necessary, provides a data buffer to relax software timing requirements. Bit banging can be implemented at very low cost, and is commonly used in embedded systems.\n\nBit banging allows a device to implement different protocols with minimal or no hardware changes. In some cases, bit banging is made feasible by newer, faster processors because more recent hardware operates much more quickly than hardware did when standard communications protocols were created.\n\nSending a byte on an SPI bus.\n\nThe question whether to deploy bit banging or not is a trade-off between load, performance and reliability on the one hand, and the availability of a hardware alternative on the other. The software emulation process consumes more processing power than does supporting dedicated hardware. The microcontroller spends much of its time sending or receiving samples to and from the pins, at the expense of other tasks. The signal produced usually has more jitter or glitches, especially if the processor is also executing other tasks while communicating. However, if the bit-banging software is interrupt-driven by the signal, this may be of minor importance, especially if control signals such as RTS, CTS, or DCD are available. The implementation in software can be a solution when specific hardware support is not available or requires a more expensive microcontroller.\n\n", "related": "\n- Bit manipulation\n- Bit stream\n- Bit twiddler (disambiguation)\n- Bit-serial architecture\n- 1-bit architecture\n- FTDI, a series of USB to serial converter chips also supporting a bit bang mode\n- 2MGUI (a DOS driver by Ciriaco García de Celis utilizing bit-banging to support non-standard ultra-high capacity floppy disk formats \"bypassing\" the normal floppy controller logic, a similar program for Amiga floppies is Vincent Joguin's Disk2FDI)\n- Virtual machine (VM) (implementing virtual device drivers emulating actual hardware controllers sometimes involves utilizing programming techniques similar to bit banging)\n- Software-defined radio (SDR)\n\n- Asynchronous serial (RS-232)\n- Notes on bit-banging async serial\n- Bit banging for Async Serial Communication\n- Bit banging for RS-232\n\n- I²C bus\n- I2C on AVR using bit banging\n\n- SPI bus\n- Efficient bit-banged SPI for 8051 microcontroller\n"}
{"id": "4641000", "url": "https://en.wikipedia.org/wiki?curid=4641000", "title": "Fluctuation loss", "text": "Fluctuation loss\n\nFluctuation loss is an effect seen in radar systems as the target object moves or changes its orientation relative to the radar system. It was extensively studied during the 1950s by Peter Swerling, who introduced the Swerling models to allow the effect to be simulated. For this reason, it is sometimes known as Swerling loss or similar names.\n\nThe effect occurs when the target's physical size is within a key range of values relative to the wavelength of the radar signal. As the signal reflects off various parts of the target, they may interfere as they return to the radar receiver. At any single distance from the station, this will cause the signal to be amplified or diminished compared to the baseline signal one calculates from the radar equation. As the target moves, these patterns change. This causes the signal to fluctuate in strength and may cause it to disappear entirely at certain times.\n\nThe effect can be reduced or eliminated by operating on more than one frequency or using modulation techniques like pulse compression that change the frequency over the period of a pulse. In these cases, it is unlikely that the pattern of reflections from the target causes the same destructive interference at two different frequencies.\n\nSwerling modeled these effects in a famous 1954 paper introduced while working at RAND Corporation. Swerling's models considered the contribution of multiple small reflectors, or many small reflectors and a single large one. This offered the ability to model real-world objects like aircraft to understand the expected fluctuation loss effects.\n\nFor basic considerations of the strength of a signal returned by a given target, the radar equation models the target as a single point in space with a given radar cross-section (RCS). The RCS is difficult to estimate except for the most basic cases, like a perpendicular surface or a sphere. Before the introduction of detailed computer modeling, the RCS for real-world objects was generally measured instead of calculated from first principles.\n\nSuch models fail to account for real-world effects due to the radar signal reflecting off multiple points on the target. If the distance between these points is on the order of the wavelength of the radar signal, the reflections are subject to wave interference effects that can cause the signal to be amplified or diminished depending on the exact path lengths. As the target moves in relation to the radar, these distances change and create a constantly changing signal. On the radar display, this causes the signal to fade in and out, making target tracking difficult. This effect is identical to the fading that occurs in radio signals in a car as it moves about, which is caused by multipath propagation.\n\nOne way to reduce or eliminate this effect is to have two or more frequencies in the radar signal. Unless the distances between the aircraft parts are distributed at a multiple of both wavelengths, which can be eliminated by selecting suitable frequencies, one of the two signals will generally be free of this effect. This was used in the AN/FPS-24 radar, for instance. Multi-frequency signals of this sort also give the radar system frequency agility, which is useful for avoiding jamming from a carcinotron, so most radars of the 1960s had some capability to avoid fluctuation loss even if this was not an explicit design goal.\n\nThe Swerling target models address these issues by modeling the target as a number of individual radiators and considering the result using the chi-squared distribution:\n\nwhere formula_2 refers to the mean value of formula_3. This is not always easy to determine, as certain objects may be viewed the most frequently from a limited range of angles. For instance, a sea-based radar system is most likely to view a ship from the side, the front, and the back, but never the top or the bottom. formula_4 is the degree of freedom divided by 2. The degree of freedom used in the chi-squared probability density function is a positive number related to the target model. Values of formula_4 between 0.3 and 2 have been found to closely approximate certain simple shapes, such as cylinders or cylinders with fins.\n\nSince the ratio of the standard deviation to the mean value of the chi-squared distribution is equal to formula_4, larger values of formula_4 will result in smaller fluctuations. If formula_4 equals infinity, the target's RCS is non-fluctuating.\n\nThe difference between the models is largely to the degrees of freedom and the general layout of the target. The first four of these models were considered in Swerling's original paper, and are referred to as models I through IV. The V model, also referred to as the 0 model, is the degenerate case with an infinite number of degrees of freedom.\n\nA model where the RCS varies according to a chi-squared probability density function with two degrees of freedom (formula_9). This applies to a target that is made up of many independent scatterers of roughly equal areas. As few as half a dozen scattering surfaces can produce this distribution. This model is particularly useful for considering aircraft shapes.\n\nSwerling I describes the case in which the target's velocity is low compared to the observation time, and can thus be considered non-moving. This is the case for a scanning radar, which sweeps its signal past the target in a relatively short time, often on the order of milliseconds. The motion of the target is thus seen only from scan-to-scan, not intra-scan. In this case, the pdf reduces to:\n\nSimilar to Swerling I, except the RCS values change from pulse-to-pulse, instead of scan-to-scan. This is the case for very high-speed targets, or, more commonly, \"staring\" radars like fire-control radars or search radars that are locked-on to a single target.\n\nA model where the RCS varies according to a Chi-squared probability density function with four degrees of freedom (formula_11). This PDF approximates an object with one large scattering surface with several other small scattering surfaces. Examples include some helicopters and propeller-driven aircraft, as the propeller/rotor provides a strong constant signal. Model III is the analog of I, considering the case where the RCS is constant through a single scan. The pdf becomes:\n\nSimilar to Swerling III, but the RCS varies from pulse-to-pulse rather than from scan-to-scan.\n\nConstant RCS, corresponding to infinite degrees of freedom (formula_13).\n\n- Skolnik, M. Introduction to Radar Systems: Third Edition. McGraw-Hill, New York, 2001.\n- Swerling, P. Probability of Detection for Fluctuating Targets. ASTIA Document Number AD 80638. March 17, 1954.\n", "related": "NONE"}
{"id": "329898", "url": "https://en.wikipedia.org/wiki?curid=329898", "title": "Stationary process", "text": "Stationary process\n\nIn mathematics and statistics, a stationary process (or a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time.\n\nSince stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data are often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a unit root or of a deterministic trend. In the former case of a unit root, stochastic shocks have permanent effects, and the process is not mean-reverting. In the latter case of a deterministic trend, the process is called a trend stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving (non-constant) mean.\n\nA trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.\n\nFor many applications strict-sense stationarity is too restrictive. Other forms of stationarity such as wide-sense stationarity or N-th order stationarity are then employed. The definitions for different kinds of stationarity are not consistent among different authors (see Other terminology).\n\nFormally, let formula_1 be a stochastic process and let formula_2 represent the cumulative distribution function of the unconditional (i.e., with no reference to any particular starting value) joint distribution of formula_1 at times formula_4. Then, formula_1 is said to be strictly stationary, strongly stationary or strict-sense stationary if\n\nSince formula_6 does not affect formula_7, formula_8 is not a function of time.\n\nWhite noise is the simplest example of a stationary process. \n\nAn example of a discrete-time stationary process where the sample space is also discrete (so that the random variable may take one of \"N\" possible values) is a Bernoulli scheme. Other examples of a discrete-time stationary process with continuous sample space include some autoregressive and moving average processes which are both subsets of the autoregressive moving average model. Models with a non-trivial autoregressive component may be either stationary or non-stationary, depending on the parameter values, and important non-stationary special cases are where unit roots exist in the model.\n\nLet formula_9 be any scalar random variable, and define a time-series formula_1, by\nThen formula_1 is a stationary time series, for which realisations consist of a series of constant values, with a different constant value for each realisation. A law of large numbers does not apply on this case, as the limiting value of an average from a single realisation takes the random value determined by formula_9, rather than taking the expected value of formula_9.\n\nAs a further example of a stationary process for which any single realisation has an apparently noise-free structure, let formula_9 have a uniform distribution on formula_16 and define the time series formula_1 by\nThen formula_1 is strictly stationary.\n\nIn , the distribution of formula_20 samples of the stochastic process must be equal to the distribution of the samples shifted in time \"for all\" formula_20. N-th order stationarity is a weaker form of stationarity where this is only requested for all formula_20 up to a certain order formula_23. A random process formula_1 is said to be N-th order stationary if:\n\nA weaker form of stationarity commonly employed in signal processing is known as weak-sense stationarity, wide-sense stationarity (WSS), or covariance stationarity. WSS random processes only require that 1st moment (i.e. the mean) and autocovariance do not vary with respect to time and that the 2nd moment is finite for all times. Any strictly stationary process which has a defined mean and a covariance is also WSS.\n\nSo, a continuous time random process formula_1 which is WSS has the following restrictions on its mean function formula_26 and autocovariance function formula_27:\n\nThe first property implies that the mean function formula_28 must be constant. The second property implies that the covariance function depends only on the \"difference\" between formula_29 and formula_30 and only needs to be indexed by one variable rather than two variables. Thus, instead of writing,\n\nthe notation is often abbreviated by the substitution formula_32:\n\nThis also implies that the autocorrelation depends only on formula_32, that is\n\nThe third property says that the second moments must be finite for any time formula_36.\n\nThe main advantage of wide-sense stationarity is that it places the time-series in the context of Hilbert spaces. Let \"H\" be the Hilbert space generated by {\"x\"(\"t\")} (that is, the closure of the set of all linear combinations of these random variables in the Hilbert space of all square-integrable random variables on the given probability space). By the positive definiteness of the autocovariance function, it follows from Bochner's theorem that there exists a positive measure formula_37 on the real line such that \"H\" is isomorphic to the Hilbert subspace of \"L\"(\"μ\") generated by {\"e\"}. This then gives the following Fourier-type decomposition for a continuous time stationary stochastic process: there exists a stochastic process formula_38 with orthogonal increments such that, for all formula_36\n\nwhere the integral on the right-hand side is interpreted in a suitable (Riemann) sense. The same result holds for a discrete-time stationary process, with the spectral measure now defined on the unit circle.\n\nWhen processing WSS random signals with linear, time-invariant (LTI) filters, it is helpful to think of the correlation function as a linear operator. Since it is a circulant operator (depends only on the difference between the two arguments), its eigenfunctions are the Fourier complex exponentials. Additionally, since the eigenfunctions of LTI operators are also complex exponentials, LTI processing of WSS random signals is highly tractable—all computations can be performed in the frequency domain. Thus, the WSS assumption is widely employed in signal processing algorithms.\n\nIn the case where formula_1 is a complex stochastic process the autocovariance function is defined as formula_42 and, in addition to the requirements in , it is required that the pseudo-autocovariance function formula_43 depends only on the time lag. In formulas, formula_1 is WSS, if\n\nThe concept of stationarity may be extended to two stochastic processes.\nTwo stochastic processes formula_1 and formula_46 are called jointly strict-sense stationary if their joint cumulative distribution formula_47 remains unchanged under time shifts, i.e. if\n\nTwo random processes formula_1 and formula_46 is said to be jointly (M+N)-th order stationary if:\n\nTwo stochastic processes formula_1 and formula_46 are called jointly wide-sense stationary if they are both wide-sense stationary and their cross-covariance function formula_52 depends only on the time difference formula_32. This may be summarized as follows:\n\n- If a stochastic process is N-th order stationary, then it is also M-th Order Stationary for all formula_54.\n- If a stochastic process is second order stationary (formula_55) and has finite second moments, then it is also wide-sense stationary.\n- If a stochastic process is wide-sense stationary, it is not necessarily second order stationary.\n- If a stochastic process is strict-sense stationary and has finite second moments, it is wide-sense stationary.\n- If two stochastic processes are jointly (M+N)-th order stationary, this does not guarantee that the individual processes are M-th respectively N-th order order stationary.\n\nThe terminology used for types of stationarity other than strict stationarity can be rather mixed. Some examples follow.\n- Priestley uses stationary up to order \"m\" if conditions similar to those given here for wide sense stationarity apply relating to moments up to order \"m\". Thus wide sense stationarity would be equivalent to \"stationary to order 2\", which is different from the definition of second-order stationarity given here.\n- Honarkhah and Caers also use the assumption of stationarity in the context of multiple-point geostatistics, where higher n-point statistics are assumed to be stationary in the spatial domain.\n- Tahmasebi and Sahimi have presented an adaptive Shannon-based methodology that can be used for modeling of any non-stationary systems.\n\nOne way to make some time series stationary is to compute the differences between consecutive observations. This is known as differencing.\n\nTransformations such as logarithms can help to stabilize the variance of a time series. Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and so eliminating trend and seasonality.\n\nOne of the ways for identifying non-stationary times series is the ACF plot. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly.\n\n", "related": "\n- Lévy process\n- Stationary ergodic process\n- Wiener–Khinchin theorem\n- Ergodicity\n- Statistical regularity\n- Autocorrelation\n- Whittle likelihood\n\n- Hyndman, Athanasopoulos (2013). Forecasting: Principles and Practice. Otexts. https://www.otexts.org/fpp/8/1\n\n- Spectral decomposition of a random function (Springer)\n"}
{"id": "5251146", "url": "https://en.wikipedia.org/wiki?curid=5251146", "title": "Autocorrelator", "text": "Autocorrelator\n\nA real time interferometric autocorrelator is an electronic tool used to examine the autocorrelation of, among other things, optical beam intensity and spectral components through examination of variable beam path differences. \"See Optical autocorrelation.\"\n\nIn an interferometric autocorrelator, the input beam is split into a fixed path beam and a variable path beam using a standard beamsplitter. The fixed path beam travels a known and constant distance, whereas the variable path beam has its path length changed via rotating mirrors or other path changing mechanisms. At the end of the two paths, the beams are ideally parallel, but slightly separated, and using a correctly positioned lens, the two beams are crossed inside a second-harmonic generating (SHG) crystal. The autocorrelation term of the output is then passed into a photomultiplying tube (PMT) and measured.\n\nConsidering the input beam as a single pulse with envelope formula_1, the constant fixed path distance as formula_2, and the variable path distance as a function of time formula_3, the input to the SHG can be viewed as\nThis comes from formula_5 being the speed of light and formula_6 being the time for the beam to travel the given path. In general, SHG produces output proportional to the square of the input, which in this case is\nThe first two terms are based only on the fixed and variable paths respectively, but the third term is based on the difference between them, as is evident in\nThe PMT used is assumed to be much slower than the envelope function formula_9, so it effectively integrates the incoming signal\nSince both the fixed path and variable path terms are not dependent on each other, they would constitute a background \"noise\" in examination of the autocorrelation term and would ideally be removed first. This can be accomplished by examining the momentum vectors\nIf the fixed and variable momentum vectors are assumed to be of approximately equal magnitude, the second harmonic momentum vector will fall geometrically between them. Assuming enough space is given in the component setup, the PMT could be fitted with a slit to decrease the effect the divergent fixed and variable beams have on the autocorrelation measurement, without losing much of the autocorrelation term. formula_12 can then be assumed to be nearly equal to\nwhich gives the autocorrelation as a function of formula_14, the difference in path lengths.\n\n- \"Calibration Factor\" -- the factor to convert real-time to pulse delay time when viewing the output of the autocorrelator. One example of this would be 30 ps/ms in the Coherent Model FR-103 scanning autocorrelator, which suggests that a 30 ps pulse autocorrelation width would produce a 1 ms FWHM trace when viewed on an oscilloscope.\n- \"Time Resolution\" -- related to the time constant of the PMT, an estimate can be made by multiplying the time constant with the calibration factor.\n\n", "related": "\n- Autocorrelation technique\n- Optical autocorrelation\n\n- \"Coherent Mira Model 900 Laser Operator's Manual\"\n- \"Principles of Communications: Systems Modulation and Noise 5ed\", Rodger E. Ziemer and William H. Tranter\n- \"Elements of Optoelectronics and Fiber Optics\", Chin-Lin Chen\n"}
{"id": "5534001", "url": "https://en.wikipedia.org/wiki?curid=5534001", "title": "Matching pursuit", "text": "Matching pursuit\n\nMatching pursuit (MP) is a sparse approximation algorithm which finds the \"best matching\" projections of multidimensional data onto the span of an over-complete (i.e., redundant) dictionary formula_1. The basic idea is to approximately represent a signal formula_2 from Hilbert space formula_3 as a weighted sum of finitely many functions formula_4 (called atoms) taken from formula_1. An approximation with formula_6 atoms has the form\nwhere formula_4 is the formula_9th column of the matrix formula_1 and formula_11 is the scalar weighting factor (amplitude) for the atom formula_4. Normally, not every atom in formula_1 will be used in this sum. Instead, matching pursuit chooses the atoms one at a time in order to maximally (greedily) reduce the approximation error. This is achieved by finding the atom that has the highest inner product with the signal (assuming the atoms are normalized), subtracting from the signal an approximation that uses only that one atom, and repeating the process until the signal is satisfactorily decomposed, i.e., the norm of the residual is small,\nwhere the residual after calculating formula_14 and formula_15 is denoted by\nIf formula_17 converges quickly to zero, then only a few atoms are needed to get a good approximation to formula_2. Such sparse representations are desirable for signal coding and compression. More precisely, the sparsity problem that matching pursuit is intended to \"approximately\" solve is\nwhere formula_20 is the formula_21 pseudo-norm (i.e. the number of nonzero elements of formula_22). In the previous notation, the nonzero entries of formula_22 are formula_24. Solving the sparsity problem exactly is NP-hard, which is why approximation methods like MP are used.\n\nFor comparison, consider the Fourier transform representation of a signal - this can be described using the terms given above, where the dictionary is built from sinusoidal basis functions (the smallest possible complete dictionary). The main disadvantage of Fourier analysis in signal processing is that it extracts only the global features of the signals and does not adapt to the analysed signals formula_2. \nBy taking an extremely redundant dictionary, we can look in it for atoms (functions) that best match a signal formula_2.\n\nIf formula_1 contains a large number of vectors, searching for the \"most\" sparse representation of formula_2 is computationally unacceptable for practical applications.\nIn 1993, Mallat and Zhang proposed a greedy solution that they named \"Matching Pursuit.\"\nFor any signal formula_29 and any dictionary formula_1, the algorithm iteratively generates a sorted list of atom indices and weighting scalars, which form the sub-optimal solution to the problem of sparse signal representation.\nIn signal processing, the concept of matching pursuit is related to statistical projection pursuit, in which \"interesting\" projections are found; ones that deviate more from a normal distribution are considered to be more interesting.\n\n- The algorithm converges (i.e. formula_44) for any formula_2 that is in the space spanned by the dictionary.\n- The error formula_46 decreases monotonically.\n- As at each step, the residual is orthogonal to the selected filter, the energy conservation equation is satisfied for each formula_6:\n- In the case that the vectors in formula_1 are orthonormal, rather than being redundant, then MP is a form of principal component analysis\n\nMatching pursuit has been applied to signal, image and video coding, shape representation and recognition, 3D objects coding, and in interdisciplinary applications like structural health monitoring. It has been shown that it performs better than DCT based coding for low bit rates in both efficiency of coding and quality of image.\nThe main problem with matching pursuit is the computational complexity of the encoder. In the basic version of an algorithm, the large dictionary needs to be searched at each iteration. Improvements include the use of approximate dictionary representations and suboptimal ways of choosing the best match at each iteration (atom extraction).\nThe matching pursuit algorithm is used in MP/SOFT, a method of simulating quantum dynamics.\n\nMP is also used in dictionary learning. In this algorithm, atoms are learned from a database (in general, natural scenes such as usual images) and not chosen from generic dictionaries.\n\nA popular extension of Matching Pursuit (MP) is its orthogonal version: Orthogonal Matching Pursuit (OMP). The main difference from MP is that after every step, \"all\" the coefficients extracted so far are updated, by computing the orthogonal projection of the signal onto the subspace spanned by the set of atoms selected so far. This can lead to results better than standard MP, but requires more computation.\n\nExtensions such as Multichannel MP and Multichannel OMP allow one to process multicomponent signals. An obvious extension of Matching Pursuit is over multiple positions and scales, by augmenting the dictionary to be that of a wavelet basis. This can be done efficiently using the convolution operator without changing the core algorithm.\n\nMatching pursuit is related to the field of compressed sensing and has been extended by researchers in that community. Notable extensions are Orthogonal Matching Pursuit (OMP), Stagewise OMP (StOMP), compressive sampling matching pursuit (CoSaMP), Generalized OMP (gOMP), and Multipath Matching Pursuit (MMP).\n\n", "related": "\n- CLEAN algorithm\n- Principal component analysis (PCA)\n- Projection pursuit\n- Image processing\n- Signal processing\n- Sparse approximation\n"}
{"id": "5547122", "url": "https://en.wikipedia.org/wiki?curid=5547122", "title": "Prony's method", "text": "Prony's method\n\nProny analysis (Prony's method) was developed by Gaspard Riche de Prony in 1795. However, practical use of the method awaited the digital computer. Similar to the Fourier transform, Prony's method extracts valuable information from a uniformly sampled signal and builds a series of damped complex exponentials or sinusoids. This allows for the estimation of frequency, amplitude, phase and damping components of a signal.\n\nLet formula_1 be a signal consisting of formula_2 evenly spaced samples. Prony's method fits a function\n\nto the observed formula_1. After some manipulation utilizing Euler's formula, the following result is obtained. This allows more direct computation of terms.\n\nwhere:\n- formula_6 are the eigenvalues of the system,\n- formula_7 are the damping components,\n- formula_8 are the angular frequency components\n- formula_9 are the phase components,\n- formula_10 are the frequency components,\n- formula_11 are the amplitude components of the series, and\n- formula_12 is the imaginary unit (formula_13).\n\nProny's method is essentially a decomposition of a signal with formula_14 complex exponentials via the following process:\n\nRegularly sample formula_15 so that the formula_16-th of formula_2 samples may be written as\n\nIf formula_15 happens to consist of damped sinusoids, then there will be pairs of complex exponentials such that\nwhere\n\nBecause the summation of complex exponentials is the homogeneous solution to a linear difference equation, the following difference equation will exist:\n\nThe key to Prony's Method is that the coefficients in the difference equation are related to the following polynomial:\n\nThese facts lead to the following three steps to Prony's Method:\n\n1) Construct and solve the matrix equation for the formula_24 values:\n\nNote that if formula_26, a generalized matrix inverse may be needed to find the values formula_24.\n\n2) After finding the formula_24 values find the roots (numerically if necessary) of the polynomial\n\nThe formula_30-th root of this polynomial will be equal to formula_31.\n\n3) With the formula_31 values the formula_33 values are part of a system of linear equations that may be used to solve for the formula_34 values:\nwhere formula_14 unique values formula_37 are used. It is possible to use a generalized matrix inverse if more than formula_14 samples are used.\n\nNote that solving for formula_39 will yield ambiguities, since only formula_31 was solved for, and formula_41 for an integer formula_42. This leads to the same Nyquist sampling criteria that discrete Fourier transforms are subject to:\n\n", "related": "\n- Generalized pencil-of-function method\n"}
{"id": "5569055", "url": "https://en.wikipedia.org/wiki?curid=5569055", "title": "First-order hold", "text": "First-order hold\n\nFirst-order hold (FOH) is a mathematical model of the practical reconstruction of sampled signals that could be done by a conventional digital-to-analog converter (DAC) and an analog circuit called an integrator. For FOH, the signal is reconstructed as a piecewise linear approximation to the original signal that was sampled. A mathematical model such as FOH (or, more commonly, the zero-order hold) is necessary because, in the sampling and reconstruction theorem, a sequence of Dirac impulses, \"x\"(\"t\"), representing the discrete samples, \"x\"(\"nT\"), is low-pass filtered to recover the original signal that was sampled, \"x\"(\"t\"). However, outputting a sequence of Dirac impulses is impractical. Devices can be implemented, using a conventional DAC and some linear analog circuitry, to reconstruct the piecewise linear output for either predictive or delayed FOH.\n\nEven though this is \"not\" what is physically done, an identical output can be generated by applying the hypothetical sequence of Dirac impulses, \"x\"(\"t\"), to a linear time-invariant system, otherwise known as a linear filter with such characteristics (which, for an LTI system, are fully described by the impulse response) so that each input impulse results in the correct piecewise linear function in the output.\n\nFirst-order hold is the hypothetical filter or LTI system that converts the ideally sampled signal\n\nto the piecewise linear signal\n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThis is an acausal system in that the linear interpolation function moves toward the value of the next sample before such sample is applied to the hypothetical FOH filter.\n\nDelayed first-order hold, sometimes called causal first-order hold, is identical to FOH above except that its output is delayed by one sample period resulting in a delayed piecewise linear output signal\n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the delayed FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThe delayed output makes this a causal system. The impulse response of the delayed FOH does not respond before the input impulse.\n\nThis kind of delayed piecewise linear reconstruction is physically realizable by implementing a digital filter of gain \"H\"(\"z\") = 1 − \"z\", applying the output of that digital filter (which is simply \"x\"[\"n\"]−\"x\"[\"n\"−1]) to an ideal conventional digital-to-analog converter (that has an inherent zero-order hold as its model) and integrating (in continuous-time, \"H\"(\"s\") = 1/(\"sT\")) the DAC output.\n\nLastly, the predictive first-order hold is quite different. This is a \"causal\" hypothetical LTI system or filter that converts the ideally sampled signal\n\ninto a piecewise linear output such that the current sample and immediately previous sample are used to linearly extrapolate up to the next sampling instance. The output of such a filter would be \n\nresulting in an effective impulse response of\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the predictive FOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThis a causal system. The impulse response of the predictive FOH does not respond before the input impulse.\n\nThis kind of piecewise linear reconstruction is physically realizable by implementing a digital filter of gain \"H\"(\"z\") = 1 − \"z\", applying the output of that digital filter (which is simply \"x\"[\"n\"]−\"x\"[\"n\"−1]) to an ideal conventional digital-to-analog converter (that has an inherent zero-order hold as its model) and applying that DAC output to an analog filter with transfer function \"H\"(\"s\") = (1+\"sT\")/(\"sT\").\n\n", "related": "\n- Nyquist–Shannon sampling theorem\n- Zero-order hold\n- Bilinear interpolation\n\n- Zero order hold and first order hold based interpolation\n"}
{"id": "5860704", "url": "https://en.wikipedia.org/wiki?curid=5860704", "title": "MUSHRA", "text": "MUSHRA\n\nMUSHRA stands for MUltiple Stimuli with Hidden Reference and Anchor and is a methodology for conducting a codec listening test to evaluate the perceived quality of the output from lossy audio compression algorithms. It is defined by ITU-R recommendation BS.1534-3. The MUSHRA methodology is recommended for assessing \"intermediate audio quality\". For very small audio impairments, Recommendation ITU-R BS.1116-3 (ABC/HR) is recommended instead.\n\nThe main advantage over the mean opinion score (MOS) methodology (which serves a similar purpose) is that MUSHRA requires fewer participants to obtain statistically significant results. This is because all codecs are presented at the same time, on the same samples, so that a paired t-test or a repeated measures analysis of variance can be used for statistical analysis. Also, the 0–100 scale used by MUSHRA makes it possible to rate very small differences.\n\nIn MUSHRA, the listener is presented with the reference (labeled as such), a certain number of test samples, a hidden version of the reference and one or more anchors. The recommendation specifies that a low-range and a mid-range anchor should be included in the test signals. These are typically a 7 kHz and a 3.5 kHz low-pass version of the reference. The purpose of the anchors is to calibrate the scale so that minor artifacts are not unduly penalized. This is particularly important when comparing or pooling results from different labs.\n\nBoth, MUSHRA and ITU BS.1116 tests call for trained expert listeners who know what typical artifacts sound like and where they are likely to occur. Expert listeners also have a better internalization of the rating scale which leads to more repeatable results than with untrained listeners. Thus, with trained listeners, fewer listeners are needed to achieve statistically significant results.\n\nIt is assumed that preferences are similar for expert listeners and naive listeners and thus results of expert listeners are also predictive for consumers. In agreement with this assumption Schinkel-Bielefeld et al. found no differences in the rank order between expert listeners and untrained listeners when using test signals containing only timbre and no spatial artifacts. However, Rumsey et al. showed that for signals containing spatial artifacts, expert listeners weigh spatial artifacts slightly stronger than untrained listeners, who primarily focus on timbre artifacts.\n\nIn addition to this, it has been shown that expert listeners make more use of the option to listen to smaller sections of the signals under test repeatedly and perform more comparisons between the signals under test and the reference. In contrast to the naive listener who produce a preference rating, expert listeners therefore produce an audio quality rating, rating the differences between the signal under test and the uncompressed original, which is the actual goal of a MUSHRA-test.\n\nThe MUSHRA guideline mentions several possibilities to assess the reliability of a listener.\n\nThe easiest and most common is to disqualify listeners who rate the hidden reference below 90 MUSHRA points for more than 15 percent of all test items. The hidden reference should be rated with 100 MUSHRA points so this is obviously a mistake. While it can happen that the hidden reference and a high-quality signal are confused, a rating of lower than 90 should only be given when the listener is certain that the rated signal is different than the original reference.\n\nThe other possibility to assess a listener's performance is eGauge, a framework based on the analysis of variance. It computes \"agreement\", \"repeatability\" and \"discriminability\", though only the latter two are recommended for pre or post screening. \"Agreement\" analyses how well a listener agrees with the rest of the listeners. \"Repeatability\" looks at the variance when rating the same test signal again in comparison to the variance of the other test signals and \"discriminability\" analyses if listeners can distinguish between test signals of different conditions. As eGauge requires listening to every test signal twice, it is more effort to apply this than to post screen listeners based on the ratings of the hidden reference. However, if a listener has proven a reliable listener using eGauge, he or she can also be considered a reliable listener for future listening tests, provided the character of the test does not change; A reliable listener for stereo listening test is not necessarily equally good in perceiving artifacts in 5.1 or 22.2 format test items.\n\nIt is important to choose critical test items; items which are difficult to encode and are likely to produce artifacts. At the same time, the test items should be ecological valid; they should be representative of broadcast material and not some synthetic signals especially designed to be difficult to encode. A method to choose critical material is presented by Ekeroot et al. who propose a ranking by elimination procedure. While this is a good way to choose the most critical test items, it does not ensure inclusion a variety of test items prone to different artifacts.\n\nIdeally the character of a MUSHRA test item should not change too much for the whole duration of that item. Otherwise it can be difficult for the listener to decide on a rating if different parts of the items display different or stronger artifacts than others. Often shorter items lead to less variability than longer ones, as they are more stationary. However, even when trying to choose stationary items, ecologically valid stimuli will very often have sections that are slightly more critical than the rest of the signal. Thus, listeners who focus on different sections of the signal may evaluate it differently. In this case more critical listeners seem to be better in identifying the most critical regions of a stimulus than less critical listeners.\n\nWhile in ITU-T P.800 tests which are commonly used to evaluate telephone quality codecs the tested speech items should always be in the native language of the listeners, this is not necessary in MUSHRA tests. A study with Mandarin Chinese and German listeners found no significant difference between rating foreign language and native language test items. However, listeners needed more time and compared more when evaluating the foreign language items. So it seems that listeners compensate for any difficulties they may have in rating foreign language items. Such compensation is not possible in ITU-T P.800 ACR tests where items are heard only once and no comparison to the reference is possible. There, foreign language items are rated as being of lower quality when listeners' language proficiency is low.\n\n- RateIt: A GUI for performing MUSHRA experiments\n- MUSHRAM - A Matlab interface for MUSHRA listening tests\n- A Max/MSP interface for MUSHRA listening tests\n- A Browser Based Audio Evaluation Tool, for running many different tests including MUSHRA - No coding needed\n- BeaqleJS: HTML5 and JavaScript based framework for listening tests\n- mushraJS+Server: based on mushraJS with mochiweb server, which is erlang web server\n", "related": "NONE"}
{"id": "5839494", "url": "https://en.wikipedia.org/wiki?curid=5839494", "title": "Decorrelation", "text": "Decorrelation\n\nDecorrelation is a general term for any process that is used to reduce autocorrelation within a signal, or cross-correlation within a set of signals, while preserving other aspects of the signal. A frequently used method of decorrelation is the use of a matched linear filter to reduce the autocorrelation of a signal as far as possible. Since the minimum possible autocorrelation for a given signal energy is achieved by equalising the power spectrum of the signal to be similar to that of a white noise signal, this is often referred to as signal whitening. \n\nAlthough most decorrelation algorithms are linear, non-linear decorrelation algorithms also exist.\n\nMany data compression algorithms incorporate a decorrelation stage. For example, many transform coders first apply a fixed linear transformation that would, on average, have the effect of decorrelating a typical signal of the class to be coded, prior to any later processing. This is typically a Karhunen–Loève transform, or a simplified approximation such as the discrete cosine transform. \n\nBy comparison, sub-band coders do not generally have an explicit decorrelation step, but instead exploit the already-existing reduced correlation within each of the sub-bands of the signal, due to the relative flatness of each sub-band of the power spectrum in many classes of signals.\n\nLinear predictive coders can be modeled as an attempt to decorrelate signals by subtracting the best possible linear prediction from the input signal, leaving a whitened residual signal.\n\nDecorrelation techniques can also be used for many other purposes, such as reducing crosstalk in a multi-channel signal, or in the design of echo cancellers. \n\nIn image processing decorrelation techniques can be used to enhance or stretch, color differences found in each pixel of an image. This is generally termed as 'decorrelation stretching'.\n\nThe concept of decorrelation can be applied in many other fields.\nIn neuroscience, decorrelation is used in the analysis of the neural networks in the human visual system.\nIn cryptography, it is used in cipher design (see Decorrelation theory) and in the design of hardware random number generators.\n\n", "related": "\n- Equalization\n- Randomness extractor\n- Eigenvalue decomposition\n- Whitening transformation\n\n- Non-linear decorrelation algorithms\n- Associative Decorrelation Dynamics in Visual Cortex\n"}
{"id": "2242975", "url": "https://en.wikipedia.org/wiki?curid=2242975", "title": "Wavelet packet decomposition", "text": "Wavelet packet decomposition\n\nOriginally known as Optimal Subband Tree Structuring (SB-TS) also called Wavelet Packet Decomposition (WPD)\n(sometimes known as just Wavelet Packets or Subband Tree) is a wavelet transform where the discrete-time (sampled) signal is passed through more filters than the discrete wavelet transform (DWT).\n\nIn the DWT, each level is calculated by passing only the previous wavelet approximation coefficients (cA) through discrete-time low and high pass quadrature mirror filters. However, in the WPD, both the detail (cD (in the 1-D case), cH, cV, cD (in the 2-D case)) and approximation coefficients are decomposed to create the full binary tree.\n\nFor n levels of decomposition the WPD produces 2 different sets of coefficients (or nodes) as opposed to (3n + 1) sets for the DWT. However, due to the downsampling process the overall number of coefficients is still the same and there is no redundancy.\n\nFrom the point of view of compression, the standard wavelet transform may not produce the best result, since it is limited to wavelet bases that increase by a power of two towards the low frequencies. It could be that another combination of bases produce a more desirable representation for a particular signal. The best basis algorithm by Coifman and Wickerhauser finds a set of bases that provide the most desirable representation of the data relative to a particular cost function (e.g. entropy).\n\nThere were relevant studies in signal processing and communications fields to address the selection of subband trees (orthogonal basis) of various kinds, e.g. regular, dyadic, irregular, with respect to performance metrics of interest including energy compaction (entropy), subband correlations and others.\n\nDiscrete wavelet transform theory (continuous in the variable(s)) offers an approximation to transform discrete (sampled) signals. In contrast, the discrete subband transform theory provides a perfect representation of discrete signals.\n\nWavelet packets were successfully applied in preclinical diagnosis.\n\n- An implementation of wavelet packet decomposition can be found in MATLAB wavelet toolbox: .\n- An implementation for R can be found in the wavethresh package: .\n- An illustration and implementation of wavelet packets along with its code in C++ can be found at .\n- JWave: An implementation in Java for 1-D and 2-D wavelet packets using Haar, Daubechies, Coiflet, and Legendre wavelets.\n", "related": "NONE"}
{"id": "5895822", "url": "https://en.wikipedia.org/wiki?curid=5895822", "title": "Sensitivity index", "text": "Sensitivity index\n\nThe sensitivity index or \"d\"′ (pronounced 'dee-prime') is a statistic used in signal detection theory. It provides the separation between the means of the signal and the noise distributions, compared against the standard deviation of the signal or noise distribution. For normally distributed signal and noise with mean and standard deviations \"μ\" and \"σ\", and \"μ\" and \"σ\", respectively, \"d\"′ is defined as:\n\n\"d\"′ assumes that the standard deviations for signal and noise are equal. \"d\"′ can be estimated from the observed hit rate and false-alarm rate, as follows:\n\nwhere function \"Z\"(\"p\"), \"p\" ∈ [0,1], is the inverse of the cumulative distribution function of the Gaussian distribution.\n\n\"d\"′ can be related to the area under the receiver operating characteristic curve, or AUC, via:\n\n6\n\"d\"′ is a dimensionless statistic. A higher \"d\"′ indicates that the signal can be more readily detected.\n\n", "related": "\n- Receiver operating characteristic (ROC)\n- Summary statistics\n- Effect size\n\n\n- Interactive signal detection theory tutorial including calculation of \"d\"′.\n"}
{"id": "567723", "url": "https://en.wikipedia.org/wiki?curid=567723", "title": "Zero crossing", "text": "Zero crossing\n\nA zero-crossing is a point where the sign of a mathematical function changes (e.g. from positive to negative), represented by a intercept of the axis (zero value) in the graph of the function. It is a commonly used term in electronics, mathematics, acoustics, and image processing.\n\nIn alternating current, the zero-crossing is the instantaneous point at which there is no voltage present. In a sine wave or other simple waveform, this normally occurs twice during each cycle. It is a device for detecting the point where the voltage crosses zero in either direction\n\nThe zero-crossing is important for systems which send digital data over AC circuits, such as modems, X10 home automation control systems, and Digital Command Control type systems for Lionel and other AC model trains.\n\nCounting zero-crossings is also a method used in speech processing to estimate the fundamental frequency of speech.\n\nIn a system where an amplifier with digitally controlled gain is applied to an input signal, artifacts in the non-zero output signal occur when the gain of the amplifier is abruptly switched between its discrete gain settings. At audio frequencies, such as in modern consumer electronics like digital audio players, these effects are clearly audible, resulting in a 'zipping' sound when rapidly ramping the gain, or a soft 'click' when a single gain change is made. Artifacts are disconcerting and clearly not desirable. If changes are made only at zero-crossings of the input signal, then no matter how the amplifier gain setting changes, the output also remains at zero, thereby minimizing the change. (The instantaneous change in gain will still produce distortion, but will not produce a click.)\n\nIf electrical power is to be switched, no electrical interference is generated if switched at an instant when there is no current—a zero crossing. Early light dimmers and similar devices generated interference; later versions were designed to switch at the zero crossing.\n\nIn the field of Digital Image Processing, great emphasis is placed on operators which seek out edges within an image. They are called 'Edge Detection' or 'Gradient filters'. A gradient filter is a filter which seeks out areas of rapid change in pixel value. These points usually mark an edge or a boundary. A Laplace filter is a filter which fits in this family, though it sets about the task in a different way. It seeks out points in the signal stream where the digital signal of an image passes through a pre-set '0' value, and marks this out as a potential edge point. Because the signal has crossed through the point of zero, it is called a zero-crossing. An example can be found here, including the source in Java.\n\n", "related": "\n- Reconstruction from zero crossings\n- Zero-crossing rate\n- Root of a function\n- Sign function\n"}
{"id": "6959617", "url": "https://en.wikipedia.org/wiki?curid=6959617", "title": "Nominal level", "text": "Nominal level\n\nNominal level is the operating level at which an electronic signal processing device is designed to operate. The electronic circuits that make up such equipment are limited in the maximum signal they can handle and the low-level internally generated electronic noise they add to the signal. The difference between the internal noise and the maximum level is the device's dynamic range. The nominal level is the level that these devices were designed to operate at, for best dynamic range and adequate headroom. When a signal is chained with improper gain staging through many devices, the dynamic range of the signal is reduced. \n\nIn audio, a related measurement, signal-to-noise ratio, is usually defined as the difference between the nominal level and the noise floor, leaving the headroom as the difference between nominal and maximum output. It is important to realize that the measured level is a time average, meaning that the peaks of audio signals regularly exceed the measured average level. The headroom measurement defines how far the peak levels can stray from the nominal measured level before clipping. The difference between the peaks and the average for a given signal is the crest factor.\n\nVU meters are designed to represent the perceived loudness of a passage of music, or other audio content, measuring in volume units. Devices are designed so that the best signal quality is obtained when the meter rarely goes above nominal. The markings are often in dB instead of \"VU\", and the reference level should be defined in the device's manual. In most professional recording and sound reinforcement equipment, the nominal level is . In semi-professional and domestic equipment, the nominal level is usually −10 dBV. This difference is due to the cost required to create larger power supplies and output higher levels.\n\nIn broadcasting equipment, this is termed the Maximum Permitted Level, which is defined by European Broadcasting Union standards. These devices use peak programme meters instead of VU meters, which gives the reading a different meaning.\n\n\"Mic level\" is sometimes defined as −60 dBV, though levels from microphones vary widely.\n\nIn video systems, nominal levels are 1 V for synched systems, such as baseband composite video, and 0.7 V for systems without sync. Note that these levels are measured peak-to-peak, while audio levels are time averages.\n\n", "related": "\n- Alignment level\n- Programme levels\n- Transmission level point\n\n- Nominal Level — Sweetwater glossary\n- Level Headed — Nominal Level (explained) plus an SV-3700 modification\n"}
{"id": "545863", "url": "https://en.wikipedia.org/wiki?curid=545863", "title": "Step response", "text": "Step response\n\nThe step response of a system in a given initial state consists of the time evolution of its outputs when its control inputs are Heaviside step functions. In electronic engineering and control theory, step response is the time behaviour of the outputs of a general system when its inputs change from zero to one in a very short time. The concept can be extended to the abstract mathematical notion of a dynamical system using an evolution parameter.\n\nFrom a practical standpoint, knowing how the system responds to a sudden input is important because large and possibly fast deviations from the long term steady state may have extreme effects on the component itself and on other portions of the overall system dependent on this component. In addition, the overall system cannot act until the component's output settles down to some vicinity of its final state, delaying the overall system response. Formally, knowing the step response of a dynamical system gives information on the stability of such a system, and on its ability to reach one stationary state when starting from another.\n\nInstead of frequency response, system performance may be specified in terms of parameters describing time-dependence of response. The step response can be described by the following quantities related to its time behavior,\n\n- overshoot\n- rise time\n- settling time\n- ringing\n\nIn the case of linear dynamic systems, much can be inferred about the system from these characteristics. Below the step response of a simple two-pole amplifier is presented, and some of these terms are illustrated.\n\nThis section describes the step response of a simple negative feedback amplifier shown in Figure 1. The feedback amplifier consists of a main open-loop amplifier of gain \"A\" and a feedback loop governed by a feedback factor β. This feedback amplifier is analyzed to determine how its step response depends upon the time constants governing the response of the main amplifier, and upon the amount of feedback used.\n\nA negative-feedback amplifier has gain given by (see negative feedback amplifier):\n\nwhere \"A\" = open-loop gain, \"A\" = closed-loop gain (the gain with negative feedback present) and β = feedback factor.\n\nIn many cases, the forward amplifier can be sufficiently well modeled in terms of a single dominant pole of time constant τ, that it, as an open-loop gain given by:\n\nwith zero-frequency gain \"A\" and angular frequency ω = 2π\"f\". This forward amplifier has unit step response\n\nan exponential approach from 0 toward the new equilibrium value of \"A\".\n\nThe one-pole amplifier's transfer function leads to the closed-loop gain:\n\nThis closed-loop gain is of the same form as the open-loop gain: a one-pole filter. Its step response is of the same form: an exponential decay toward the new equilibrium value. But the time constant of the closed-loop step function is τ / (1 + β \"A\"), so it is faster than the forward amplifier's response by a factor of 1 + β \"A\":\n\nAs the feedback factor β is increased, the step response will get faster, until the original assumption of one dominant pole is no longer accurate. If there is a second pole, then as the closed-loop time constant approaches the time constant of the second pole, a two-pole analysis is needed.\n\nIn the case that the open-loop gain has two poles (two time constants, τ, τ), the step response is a bit more complicated. The open-loop gain is given by:\n\nwith zero-frequency gain \"A\" and angular frequency ω = 2π\"f\".\n\nThe two-pole amplifier's transfer function leads to the closed-loop gain:\n\nThe time dependence of the amplifier is easy to discover by switching variables to \"s\" = \"j\"ω, whereupon the gain becomes:\n\nThe poles of this expression (that is, the zeros of the denominator) occur at:\n\nwhich shows for large enough values of βA\" the square root becomes the square root of a negative number, that is the square root becomes imaginary, and the pole positions are complex conjugate numbers, either \"s\" or \"s\"; see Figure 2:\n\nwith\n\nand\nUsing polar coordinates with the magnitude of the radius to the roots given by |\"s\"| (Figure 2):\n\nand the angular coordinate φ is given by:\n\nTables of Laplace transforms show that the time response of such a system is composed of combinations of the two functions:\n\nwhich is to say, the solutions are damped oscillations in time. In particular, the unit step response of the system is:\n\nwhich simplifies to\n\nwhen \"A\" tends to infinity and the feedback factor β is one.\n\nNotice that the damping of the response is set by ρ, that is, by the time constants of the open-loop amplifier. In contrast, the frequency of oscillation is set by μ, that is, by the feedback parameter through β\"A\". Because ρ is a sum of reciprocals of time constants, it is interesting to notice that ρ is dominated by the \"shorter\" of the two.\n\nFigure 3 shows the time response to a unit step input for three values of the parameter μ. It can be seen that the frequency of oscillation increases with μ, but the oscillations are contained between the two asymptotes set by the exponentials [ 1 − exp (−ρt) ] and [ 1 + exp(−ρt) ]. These asymptotes are determined by ρ and therefore by the time constants of the open-loop amplifier, independent of feedback.\n\nThe phenomenon of oscillation about the final value is called ringing. The overshoot is the maximum swing above final value, and clearly increases with μ. Likewise, the undershoot is the minimum swing below final value, again increasing with μ. The settling time is the time for departures from final value to sink below some specified level, say 10% of final value.\n\nThe dependence of settling time upon μ is not obvious, and the approximation of a two-pole system probably is not accurate enough to make any real-world conclusions about feedback dependence of settling time. However, the asymptotes [ 1 − exp (−ρt) ] and [ 1 + exp (−ρt) ] clearly impact settling time, and they are controlled by the time constants of the open-loop amplifier, particularly the shorter of the two time constants. That suggests that a specification on settling time must be met by appropriate design of the open-loop amplifier.\n\nThe two major conclusions from this analysis are: \n1. Feedback controls the amplitude of oscillation about final value for a given open-loop amplifier and given values of open-loop time constants, τ and τ.\n2. The open-loop amplifier decides settling time. It sets the time scale of Figure 3, and the faster the open-loop amplifier, the faster this time scale.\n\nAs an aside, it may be noted that real-world departures from this linear two-pole model occur due to two major complications: first, real amplifiers have more than two poles, as well as zeros; and second, real amplifiers are nonlinear, so their step response changes with signal amplitude.\nHow overshoot may be controlled by appropriate parameter choices is discussed next.\n\nUsing the equations above, the amount of overshoot can be found by differentiating the step response and finding its maximum value. The result for maximum step response \"S\" is:\n\nThe final value of the step response is 1, so the exponential is the actual overshoot itself. It is clear the overshoot is zero if μ = 0, which is the condition:\n\nThis quadratic is solved for the ratio of time constants by setting \"x\" = ( τ / τ ) with the result\n\nBecause β \"A\" » 1, the 1 in the square root can be dropped, and the result is\n\nIn words, the first time constant must be much larger than the second. To be more adventurous than a design allowing for no overshoot we can introduce a factor α in the above relation:\n\nand let α be set by the amount of overshoot that is acceptable.\n\nFigure 4 illustrates the procedure. Comparing the top panel (α = 4) with the lower panel (α = 0.5) shows lower values for α increase the rate of response, but increase overshoot. The case α = 2 (center panel) is the \"maximally flat\" design that shows no peaking in the Bode gain vs. frequency plot. That design has the rule of thumb built-in safety margin to deal with non-ideal realities like multiple poles (or zeros), nonlinearity (signal amplitude dependence) and manufacturing variations, any of which can lead to too much overshoot. The adjustment of the pole separation (that is, setting α) is the subject of frequency compensation, and one such method is pole splitting.\n\nThe amplitude of ringing in the step response in Figure 3 is governed by the damping factor exp ( −ρ t ). That is, if we specify some acceptable step response deviation from final value, say Δ, that is:\n\nthis condition is satisfied regardless of the value of β \"A\" provided the time is longer than the settling time, say \"t\", given by:\n\nwhere the τ » τ is applicable because of the overshoot control condition, which makes τ = \"αβA\" τ. Often the settling time condition is referred to by saying the settling period is inversely proportional to the unity gain bandwidth, because 1/(2π τ) is close to this bandwidth for an amplifier with typical dominant pole compensation. However, this result is more precise than this rule of thumb. As an example of this formula, if Δ = 1/e = 1.8 %, the settling time condition is \"t\" = 8 τ.\n\nIn general, control of overshoot sets the time constant ratio, and settling time \"t\" sets τ.\n\nThis method uses significant points of the step response. There is no need to guess tangents to the measures Signal. The equations are derived using numerical simulations, determining some significant ratios and fitting parameters of nonlinear equations. See also .\n\nHere the steps:\n\n- Measure the system step-response formula_30of the system with an input step signal formula_31.\n- Determine the time-spans formula_32and formula_33where the step response reaches 25% and 75% of the steady state output value.\n- Determine the system steady-state gain formula_34with formula_35\n- Calculate\n\nformula_36\n\nformula_37\n\nformula_38\n- Determine the two time constants\n\nformula_39\n\nformula_40\n- Calculate the transfer function of the identified system within the Laplace-domain\n\nformula_41\n\nNext, the choice of pole ratio τ/τ is related to the phase margin of the feedback amplifier. The procedure outlined in the Bode plot article is followed. Figure 5 is the Bode gain plot for the two-pole amplifier in the range of frequencies up to the second pole position. The assumption behind Figure 5 is that the frequency \"f\" lies between the lowest pole at \"f\" = 1/(2πτ) and the second pole at \"f\" = 1/(2πτ). As indicated in Figure 5, this condition is satisfied for values of α ≥ 1.\n\nUsing Figure 5 the frequency (denoted by \"f\") is found where the loop gain β\"A\" satisfies the unity gain or 0 dB condition, as defined by:\n\nThe slope of the downward leg of the gain plot is (20 dB/decade); for every factor of ten increase in frequency, the gain drops by the same factor:\n\nThe phase margin is the departure of the phase at \"f\" from −180°. Thus, the margin is:\n\nBecause \"f\" / \"f\" = \"βA\" » 1, the term in \"f\" is 90°. That makes the phase margin:\n\nIn particular, for case α = 1, φ = 45°, and for α = 2, φ = 63.4°. Sansen recommends α = 3, φ = 71.6° as a \"good safety position to start with\".\n\nIf α is increased by shortening τ, the settling time \"t\" also is shortened. If α is increased by lengthening τ, the settling time \"t\" is little altered. More commonly, both τ \"and\" τ change, for example if the technique of pole splitting is used.\n\nAs an aside, for an amplifier with more than two poles, the diagram of Figure 5 still may be made to fit the Bode plots by making \"f\" a fitting parameter, referred to as an \"equivalent second pole\" position.\n\nThis section provides a formal mathematical definition of step response in terms of the abstract mathematical concept of a dynamical system formula_48: all notations and assumptions required for the following description are listed here. \n- formula_49 is the evolution parameter of the system, called \"time\" for the sake of simplicity,\n- formula_50 is the state of the system at time formula_51, called \"output\" for the sake of simplicity,\n- formula_52 is the dynamical system evolution function,\n- formula_53 is the dynamical system initial state,\n- formula_54 is the Heaviside step function\n\nFor a general dynamical system, the step response is defined as follows:\n\nIt is the evolution function when the control inputs (or source term, or forcing inputs) are Heaviside functions: the notation emphasizes this concept showing \"H\"(\"t\") as a subscript.\n\nFor a linear time-invariant black box, let formula_56 for notational convenience: the step response can be obtained by convolution of the Heaviside step function control and the impulse response \"h\"(\"t\") of the system itself\n\nwhich for an LTI system is equivalent to just integrating the latter. Conversely, for an LTI system, the derivative of the step response yields the impulse response:\n\nHowever, these simple relations are not true for a non-linear or time-variant system.\n\n", "related": "\n- Impulse response\n- Overshoot (signal)\n- Pole splitting\n- Rise time\n- Settling time\n- Time constant\n\n- Robert I. Demrow \"Settling time of operational amplifiers\"\n- Cezmi Kayabasi \"Settling time measurement techniques achieving high precision at high speeds\"\n- Vladimir Igorevic Arnol'd \"Ordinary differential equations\", various editions from MIT Press and from Springer Verlag, chapter 1 \"Fundamental concepts\"\n- Kuo power point slides; Chapter 7 especially\n"}
{"id": "2724", "url": "https://en.wikipedia.org/wiki?curid=2724", "title": "Autocorrelation", "text": "Autocorrelation\n\nAutocorrelation, also known as serial correlation, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.\n\nDifferent fields of study define autocorrelation differently, and not all of these definitions are equivalent. In some fields, the term is used interchangeably with autocovariance.\n\nUnit root processes, trend stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.\n\nIn statistics, the autocorrelation of a real or complex random process is the Pearson correlation between values of the process at different times, as a function of the two times or of the time lag . Let formula_1 be a random process, and formula_2 be any point in time (formula_2 may be an integer for a discrete-time process or a real number for a continuous-time process). Then formula_4 is the value (or realization) produced by a given run of the process at time formula_2. Suppose that the process has mean formula_6 and variance formula_7 at time formula_2, for each formula_2. Then the definition of the auto-correlation function between times formula_10 and formula_11 is\n\nwhere formula_12 is the expected value operator and the bar represents complex conjugation. Note that the expectation may not be well defined.\n\nSubtracting the mean before multiplication yields the auto-covariance function between times formula_10 and formula_11:\n\nNote that this expression is not well-defined for all-time series or processes, because the mean may not exist, or the variance may be zero (for a constant process) or infinite (for processes with distribution lacking well-behaved moments, such as certain types of power law).\n\nIf formula_1 is a wide-sense stationary process then the mean formula_16 and the variance formula_17 are time-independent, and further the autocovariance function depends only on the lag between formula_10 and formula_11: the autocovariance depends only on the time-distance between the pair of values but not on their position in time. This further implies that the autocovariance and auto-correlation can be expressed as a function of the time-lag, and that this would be an even function of the lag formula_20. This gives the more familiar forms for the auto-correlation function\n\nand the auto-covariance function:\n\nIt is common practice in some disciplines (e.g. statistics and time series analysis) to normalize the autocovariance function to get a time-dependent Pearson correlation coefficient. However, in other disciplines (e.g. engineering) the normalization is usually dropped and the terms \"autocorrelation\" and \"autocovariance\" are used interchangeably.\n\nThe definition of the auto-correlation coefficient of a stochastic process is\n\nIf the function formula_22 is well-defined, its value must lie in the range formula_23, with 1 indicating perfect correlation and −1 indicating perfect anti-correlation.\n\nFor a weak-sense stationarity, wide-sense stationarity (WSS) process, the definition is\n\nwhere\n\nThe normalization is important both because the interpretation of the autocorrelation as a correlation provides a scale-free measure of the strength of statistical dependence, and because the normalization has an effect on the statistical properties of the estimated autocorrelations.\n\nThe fact that the auto-correlation function formula_26 is an even function can be stated as\nRespectively for WSS a process:\n\nFor a WSS process:\nNotice that formula_30 is always real.\n\nThe Cauchy–Schwarz inequality, inequality for stochastic processes:\n\nThe autocorrelation of a continuous-time white noise signal will have a strong peak (represented by a Dirac delta function) at formula_32 and will be exactly 0 for all other formula_33.\n\nThe Wiener–Khinchin theorem relates the autocorrelation function formula_26 to the power spectral density formula_35 via the Fourier transform:\n\nFor real-valued functions, the symmetric autocorrelation function has a real symmetric transform, so the Wiener–Khinchin theorem can be re-expressed in terms of real cosines only:\n\nThe auto-correlation matrix (also called second moment) of a random vector formula_40 is an formula_41 matrix containing as elements the autocorrelations of all pairs of elements of the random vector formula_42. The autocorrelation matrix is used in various digital signal processing algorithms.\n\nFor a random vector formula_40 containing random elements whose expected value and variance exist, the auto-correlation matrix is defined by\n\nwhere formula_44 denotes transposition and has dimensions formula_41.\n\nWritten component-wise:\n\nIf formula_47 is a complex random vector, the autocorrelation matrix is instead defined by\n\nHere formula_49 denotes Hermitian transposition.\n\nFor example, if formula_50 is a random vectors, then formula_51 is a formula_52 matrix whose formula_53-th entry is formula_54.\n\n- The autocorrelation matrix is a Hermitian matrix for complex random vectors and a symmetric matrix for real random vectors.\n- The autocorrelation matrix is a positive semidefinite matrix, i.e. formula_55 for a real random vector respectively formula_56 in case of a complex random vector.\n- All eigenvalues of the autocorrelation matrix are real and non-negative.\n- The \"auto-covariance matrix\" is related to the autocorrelation matrix as follows:\n\nIn signal processing, the above definition is often used without the normalization, that is, without subtracting the mean and dividing by the variance. When the autocorrelation function is normalized by mean and variance, it is sometimes referred to as the autocorrelation coefficient or autocovariance function.\n\nGiven a signal formula_59, the continuous autocorrelation formula_60 is most often defined as the continuous cross-correlation integral of formula_59 with itself, at lag formula_33.\n\nwhere formula_63 represents the complex conjugate of formula_59. Note that the parameter formula_2 in the integral is a dummy variable and is only necessary to calculate the integral. It has no specific meaning.\n\nThe discrete autocorrelation formula_66 at lag formula_67 for a discrete-time signal formula_68 is\n\nThe above definitions work for signals that are square integrable, or square summable, that is, of finite energy. Signals that \"last forever\" are treated instead as random processes, in which case different definitions are needed, based on expected values. For wide-sense-stationary random processes, the autocorrelations are defined as\n\nFor processes that are not stationary, these will also be functions of formula_2, or formula_72.\n\nFor processes that are also ergodic, the expectation can be replaced by the limit of a time average. The autocorrelation of an ergodic process is sometimes defined as or equated to\n\nThese definitions have the advantage that they give sensible well-defined single-parameter results for periodic functions, even when those functions are not the output of stationary ergodic processes.\n\nAlternatively, signals that \"last forever\" can be treated by a short-time autocorrelation function analysis, using finite time integrals. (See short-time Fourier transform for a related process.)\n\nIf formula_75 is a continuous periodic functions of period formula_76, the integration from formula_77 to formula_78 is replaced by integration over any interval formula_79 of length formula_76:\n\nwhich is equivalent to\n\nIn the following, we will describe properties of one-dimensional autocorrelations only, since most properties are easily transferred from the one-dimensional case to the multi-dimensional cases. These properties hold for wide-sense stationary processes.\n\n- A fundamental property of the autocorrelation is symmetry, formula_83, which is easy to prove from the definition. In the continuous case,\n\n- The continuous autocorrelation function reaches its peak at the origin, where it takes a real value, i.e. for any delay formula_33, formula_89. This is a consequence of the rearrangement inequality. The same result holds in the discrete case.\n- The autocorrelation of a periodic function is, itself, periodic with the same period.\n- The autocorrelation of the sum of two completely uncorrelated functions (the cross-correlation is zero for all formula_33) is the sum of the autocorrelations of each function separately.\n- Since autocorrelation is a specific type of cross-correlation, it maintains all the properties of cross-correlation.\n- By using the symbol formula_91 to represent convolution and formula_92 is a function which manipulates the function formula_75 and is defined as formula_94, the definition for formula_60 may be written as:\n\nMulti-dimensional autocorrelation is defined similarly. For example, in three dimensions the autocorrelation of a square-summable discrete signal would be\n\nWhen mean values are subtracted from signals before computing an autocorrelation function, the resulting function is usually called an auto-covariance function.\n\nFor data expressed as a discrete sequence, it is frequently necessary to compute the autocorrelation with high computational efficiency. A brute force method based on the signal processing definition formula_98 can be used when the signal size is small. For example, to calculate the autocorrelation of the real signal sequence formula_99 (i.e. formula_100, and formula_101 for all other values of ) by hand, we first recognize that the definition just given is the same as the \"usual\" multiplication, but with right shifts, where each vertical addition gives the autocorrelation for particular lag values:\n\nThus the required autocorrelation sequence is formula_103, where formula_104 formula_105 and formula_106 the autocorrelation for other lag values being zero. In this calculation we do not perform the carry-over operation during addition as is usual in normal multiplication. Note that we can halve the number of operations required by exploiting the inherent symmetry of the autocorrelation. If the signal happens to be periodic, i.e. formula_107 then we get a circular autocorrelation (similar to circular convolution) where the left and right tails of the previous autocorrelation sequence will overlap and give formula_108 which has the same period as the signal sequence formula_109 The procedure can be regarded as an application of the convolution property of z-transform of a discrete signal.\n\nWhile the brute force algorithm is order , several efficient algorithms exist which can compute the autocorrelation in order . For example, the Wiener–Khinchin theorem allows computing the autocorrelation from the raw data with two fast Fourier transforms (FFT):\n\nwhere IFFT denotes the inverse fast Fourier transform. The asterisk denotes complex conjugate.\n\nAlternatively, a multiple correlation can be performed by using brute force calculation for low values, and then progressively binning the data with a logarithmic density to compute higher values, resulting in the same efficiency, but with lower memory requirements.\n\nFor a discrete process with known mean and variance for which we observe formula_72 observations formula_112, an estimate of the autocorrelation may be obtained as\n\nfor any positive integer formula_114. When the true mean formula_16 and variance formula_17 are known, this estimate is unbiased. If the true mean and variance of the process are not known there are a several possibilities:\n- If formula_16 and formula_17 are replaced by the standard formulae for sample mean and sample variance, then this is a biased estimate.\n- A periodogram-based estimate replaces formula_119 in the above formula with formula_72. This estimate is always biased; however, it usually has a smaller mean squared error.\n- Other possibilities derive from treating the two portions of data formula_121 and formula_122 separately and calculating separate sample means and/or sample variances for use in defining the estimate.\nThe advantage of estimates of the last type is that the set of estimated autocorrelations, as a function of formula_123, then form a function which is a valid autocorrelation in the sense that it is possible to define a theoretical process having exactly that autocorrelation. Other estimates can suffer from the problem that, if they are used to calculate the variance of a linear combination of the formula_124's, the variance calculated may turn out to be negative. \n\nIn regression analysis using time series data, autocorrelation in a variable of interest is typically modeled either with an autoregressive model (AR), a moving average model (MA), their combination as an autoregressive-moving-average model (ARMA), or an extension of the latter called an autoregressive integrated moving average model (ARIMA). With multiple interrelated data series, vector autoregression (VAR) or its extensions are used.\n\nIn ordinary least squares (OLS), the adequacy of a model specification can be checked in part by establishing whether there is autocorrelation of the regression residuals. \nProblematic autocorrelation of the errors, which themselves are unobserved, can generally be detected because it produces autocorrelation in the observable residuals. (Errors are also known as \"error terms\" in econometrics.) Autocorrelation of the errors violates the ordinary least squares assumption that the error terms are uncorrelated, meaning that the Gauss Markov theorem does not apply, and that OLS estimators are no longer the Best Linear Unbiased Estimators (BLUE). While it does not bias the OLS coefficient estimates, the standard errors tend to be underestimated (and the t-scores overestimated) when the autocorrelations of the errors at low lags are positive.\n\nThe traditional test for the presence of first-order autocorrelation is the Durbin–Watson statistic or, if the explanatory variables include a lagged dependent variable, Durbin's h statistic. The Durbin-Watson can be linearly mapped however to the Pearson correlation between values and their lags. A more flexible test, covering autocorrelation of higher orders and applicable whether or not the regressors include lags of the dependent variable, is the Breusch–Godfrey test. This involves an auxiliary regression, wherein the residuals obtained from estimating the model of interest are regressed on (a) the original regressors and (b) \"k\" lags of the residuals, where 'k' is the order of the test. The simplest version of the test statistic from this auxiliary regression is \"TR\", where \"T\" is the sample size and \"R\" is the coefficient of determination. Under the null hypothesis of no autocorrelation, this statistic is\nasymptotically distributed as formula_125 with \"k\" degrees of freedom.\n\nResponses to nonzero autocorrelation include generalized least squares and the Newey–West HAC estimator (Heteroskedasticity and Autocorrelation Consistent).\n\nIn the estimation of a moving average model (MA), the autocorrelation function is used to determine the appropriate number of lagged error terms to be included. This is based on the fact that for an MA process of order \"q\", we have formula_126, for formula_127, and formula_128, for formula_129.\n\n- Autocorrelation analysis is used heavily in fluorescence correlation spectroscopy to provide quantitative insight into molecular-level diffusion and chemical reactions.\n- Another application of autocorrelation is the measurement of optical spectra and the measurement of very-short-duration light pulses produced by lasers, both using optical autocorrelators.\n- Autocorrelation is used to analyze dynamic light scattering data, which notably enables determination of the particle size distributions of nanometer-sized particles or micelles suspended in a fluid. A laser shining into the mixture produces a speckle pattern that results from the motion of the particles. Autocorrelation of the signal can be analyzed in terms of the diffusion of the particles. From this, knowing the viscosity of the fluid, the sizes of the particles can be calculated.\n- The small-angle X-ray scattering intensity of a nanostructured system is the Fourier transform of the spatial autocorrelation function of the electron density.\n- In surface science and scanning probe microscopy, autocorrelation is used to establish a link between surface morphology and functional characteristics.\n- In optics, normalized autocorrelations and cross-correlations give the degree of coherence of an electromagnetic field.\n- In signal processing, autocorrelation can give information about repeating events like musical beats (for example, to determine tempo) or pulsar frequencies, though it cannot tell the position in time of the beat. It can also be used to estimate the pitch of a musical tone.\n- In music recording, autocorrelation is used as a pitch detection algorithm prior to vocal processing, as a distortion effect or to eliminate undesired mistakes and inaccuracies.\n\n- Autocorrelation in space rather than time, via the Patterson function, is used by X-ray diffractionists to help recover the \"Fourier phase information\" on atom positions not available through diffraction alone.\n- In statistics, spatial autocorrelation between sample locations also helps one estimate mean value uncertainties when sampling a heterogeneous population.\n- The SEQUEST algorithm for analyzing mass spectra makes use of autocorrelation in conjunction with cross-correlation to score the similarity of an observed spectrum to an idealized spectrum representing a peptide.\n- In astrophysics, autocorrelation is used to study and characterize the spatial distribution of galaxies in the universe and in multi-wavelength observations of low mass X-ray binaries.\n- In panel data, spatial autocorrelation refers to correlation of a variable with itself through space.\n- In analysis of Markov chain Monte Carlo data, autocorrelation must be taken into account for correct error determination.\n- In geosciences (specifically in geophysics) it can be used to compute an autocorrelation seismic attribute, out of a 3D seismic survey of the underground.\n- In medical ultrasound imaging, autocorrelation is used to visualize blood flow.\n- In intertemporal portfolio choice, the presence or absence of autocorrelation in an asset's rate of return can affect the optimal portion of the portfolio to hold in that asset.\n\nSerial dependence is closely linked to the notion of autocorrelation, but represents a distinct concept (see Correlation and dependence). In particular, it is possible to have serial dependence but no (linear) correlation. In some fields however, the two terms are used as synonyms.\n\nA time series of a random variable has serial dependence if the value at some time formula_2 in the series is statistically dependent on the value at another time formula_131. A series is serially independent if there is no dependence between any pair.\n\nIf a time series formula_1 is stationary, then statistical dependence between the pair formula_133 would imply that there is statistical dependence between all pairs of values at the same lag formula_134.\n\n", "related": "\n- Autocorrelation matrix\n- Autocorrelation technique\n- Autocorrelation of a formal word\n- Autocorrelator\n- Correlation function\n- Correlogram\n- Cross-correlation\n- Galton's problem\n- Partial autocorrelation function\n- Fluorescence correlation spectroscopy\n- Optical autocorrelation\n- Pitch detection algorithm\n- Triple correlation\n- CUSUM\n- Cochrane–Orcutt estimation (transformation for autocorrelated error terms)\n- Prais–Winsten transformation\n- Scaled Correlation\n- Unbiased estimation of standard deviation\n\n- Mojtaba Soltanalian, and Petre Stoica. \"Computational design of sequences with good correlation properties.\" IEEE Transactions on Signal Processing, 60.5 (2012): 2180–2193.\n- Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n- Klapetek, Petr (2018). \"Quantitative Data Processing in Scanning Probe Microscopy: SPM Applications for Nanometrology\"(Second ed.). Elsevier. pp. 108-112 .\n"}
{"id": "8433728", "url": "https://en.wikipedia.org/wiki?curid=8433728", "title": "Pulse compression", "text": "Pulse compression\n\nPulse compression is a signal processing technique commonly used by radar, sonar and echography to increase the range resolution as well as the signal to noise ratio. This is achieved by modulating the transmitted pulse and then correlating the received signal with the transmitted pulse.\n\nThe simplest signal a pulse radar can transmit is a sinusoidal-amplitude pulse, formula_1 and carrier frequency, formula_2, truncated by a rectangular function of width, formula_3. The pulse is transmitted periodically, but that is not the main topic of this article; we will consider only a single pulse, formula_4. If we assume the pulse to start at time formula_5, the signal can be written the following way, using the complex notation:\n\nLet us determine the range resolution which can be obtained with such a signal. The return signal, written formula_7, is an attenuated and time-shifted copy of the original transmitted signal (in reality, Doppler effect can play a role too, but this is not important here.) There is also noise in the incoming signal, both on the imaginary and the real channel, which we will assume to be white and Gaussian (this generally holds in reality); we write formula_8 to denote that noise. To detect the incoming signal, matched filtering is commonly used. This method is optimal when a known signal is to be detected among additive white Gaussian noise.\n\nIn other words, the cross-correlation of the received signal with the transmitted signal is computed. This is achieved by convolving the incoming signal with a conjugated and time-reversed version of the transmitted signal. This operation can be done either in software or with hardware. We write formula_9 for this cross-correlation. We have:\n\nIf the reflected signal comes back to the receiver at time formula_11 and is attenuated by factor formula_12, this yields:\n\nSince we know the transmitted signal, we obtain:\n\nwhere formula_15, is the result of the intercorrelation between the noise and the transmitted signal. Function formula_16 is the triangle function, its value is 0 on formula_17, it increases linearly on formula_18 where it reaches its maximum 1, and it decreases linearly on formula_19 until it reaches 0 again. Figures at the end of this paragraph show the shape of the intercorrelation for a sample signal (in red), in this case a real truncated sine, of duration formula_20 seconds, of unit amplitude, and frequency formula_21 hertz. Two echoes (in blue) come back with delays of 3 and 5 seconds and amplitudes equal to 0.5 and 0.3 times the amplitude of the transmitted pulse, respectively; these are just random values for the sake of the example. Since the signal is real, the intercorrelation is weighted by an additional factor.\n\nIf two pulses come back (nearly) at the same time, the intercorrelation is equal to the sum of the intercorrelations of the two elementary signals. To distinguish one \"triangular\" envelope from that of the other pulse, it is clearly visible that the times of arrival of the two pulses must be separated by at least formula_3 so that the maxima of both pulses can be separated. If this condition is not met, both triangles will be mixed together and impossible to separate.\n\nSince the distance travelled by a wave during formula_3 is formula_24 (where \"c\" is the speed of the wave in the medium), and since this distance corresponds to a round-trip time, we get:\n\nThe instantaneous power of the transmitted pulse is formula_25. The energy put into that signal is:\n\nSimilarly, the energy in the received pulse is formula_27. If formula_28 is the standard deviation of the noise, the signal-to-noise ratio (SNR) at the receiver is:\n\nThe SNR is proportional to pulse duration formula_30, if other parameters are held constant. This introduces a tradeoff: increasing formula_30 improves the SNR, but reduces the resolution, and vice versa.\n\nHow can one have a large enough pulse (to still have a good SNR at the receiver) without poor resolution? This is where pulse compression enters the picture. The basic principle is the following:\n- a signal is transmitted, with a long enough length so that the energy budget is correct\n- this signal is designed so that after matched filtering, the width of the intercorrelated signals is smaller than the width obtained by the standard sinusoidal pulse, as explained above (hence the name of the technique: pulse compression).\n\nIn radar or sonar applications, linear chirps are the most typically used signals to achieve pulse compression. The pulse being of finite length, the amplitude is a rectangle function. If the transmitted signal has a duration formula_3, begins at formula_33 and linearly sweeps the frequency band formula_34 centered on carrier formula_35, it can be written:\n\nThe chirp definition above means that the phase of the chirped signal (that is, the argument of the complex exponential), is the quadratic:\n\nthus the instantaneous frequency is (by definition):\n\nwhich is the intended linear ramp going from formula_39 at formula_33 to formula_41 at formula_42.\n\nThe relation of phase to frequency is often used in the other direction, starting with the desired formula_43 and writing the chirp phase via the integration of frequency:\n\nAs for the \"simple\" pulse, let us compute the cross-correlation between the transmitted and the received signal. To simplify things, we shall consider that the chirp is not written as it is given above, but in this alternate form (the final result will be the same):\n\nSince this cross-correlation is equal (save for the formula_46 attenuation factor), to the autocorrelation function of formula_47, this is what we consider:\n\nIt can be shown that the autocorrelation function of formula_49 is:\n\nThe maximum of the autocorrelation function of formula_47 is reached at 0. Around 0, this function behaves as the sinc (or cardinal sine) term, defined here as formula_52. The −3 dB temporal width of that cardinal sine is more or less equal to formula_53. Everything happens as if, after matched filtering, we had the resolution that would have been reached with a simple pulse of duration formula_54. For the common values of formula_34, formula_54 is smaller than formula_3, hence the \"pulse compression\" name.\n\nSince the cardinal sine can have annoying sidelobes, a common practice is to filter the result by a window (Hamming, Hann, etc.). In practice, this can be done at the same time as the adapted filtering by multiplying the reference chirp with the filter. The result will be a signal with a slightly lower maximum amplitude, but the sidelobes will be filtered out, which is more important.\n \n\nThe energy of the signal does not vary during pulse compression. However, it is now located in the main lobe of the cardinal sine, whose width is approximately formula_58. If formula_59 is the power of the signal before compression, and formula_60 the power of the signal after compression, we have:\n\nwhich yields:\n\nAs a consequence:\n\nWhile pulse compression can ensure good SNR and fine range resolution in the same time, digital signal processing in such a system can be difficult to implement because of the high instantaneous bandwidth of the waveform (formula_34 can be hundreds of megahertz or even exceed 1GHz.) \nStretch Processing is a technique for matched filtering of wideband chirping waveform and is suitable for applications seeking very fine range resolution over relatively short range intervals.\n\nPicture above shows the scenario for analyzing stretch processing. The central reference point(CRP) is in the middle of the range window of interest at range of formula_64, corresponding to a time delay of formula_65.\n\nIf the transmitted waveform is the chirp waveform:\nthen the echo from the target at distance formula_67can be expressed as:\nwhere formula_69 is proportional to the scatterer reflectivity.\nWe then multiply the echo by formula_70and the echo will become:\nwhere formula_72 is the wavelength of electromagnetic wave in air.\n\nAfter conducting sampling and discrete fourier transform on y(t) the sinusoid frequency formula_73 can be solved:\nand the differential range formula_75 can be obtained:\n\nTo show that the bandwidth of y(t) is less than the original signal bandwidth formula_34, we suppose that the range window is formula_78 long. If the target is at the lower bound of the range window, the echo will arrive formula_79 seconds after transmission; similarly, If the target is at the upper bound of the range window, the echo will arrive formula_80 seconds after transmission.\nThe differential arrive time formula_81 for each case is formula_82 and formula_83, respectively.\n\nWe can then obtain the bandwidth by considering the difference in sinusoid frequency for targets at the lower and upper bound of the range window:\nAs a consequence:\n\nTo demonstrate that stretch processing preserves range resolution, we need to understand that y(t) is actually an impulse train with pulse duration T and period formula_85, which is equal to the period of the transmitted impulse train. As a result, the fourier transform of y(t) is actually a sinc function with Rayleigh resolution formula_86. That is, the processor will be able to resolve scatterers whose formula_73 are at least formula_88 apart.\n\nConsequently, \nand,\nwhich is the same as the resolution of the original linear frequency modulation waveform.\n\nAlthough stretch processing can reduce the bandwidth of received baseband signal, all of the analog components in RF front-end circuitry still must be able to support an instantaneous bandwidth of formula_34. In addition, the effective wavelength of the electromagnetic wave changes during the frequency sweep of a chirp signal, and therefore the antenna look direction will be inevitably changed in a Phased array system.\n\nStepped-frequency waveforms are an alternative technique that can preserve fine range resolution and SNR of the received signal without large instantaneous bandwidth. Unlike the chirping waveform, which sweeps linearly across a total bandwidth of formula_34 in a single pulse, stepped-frequency waveform employs an impulse train where the frequency of each pulse is increased by formula_93 from the preceding pulse. The baseband signal can be expressed as:\nwhere formula_95 is a rectangular impulse of length formula_96 and M is the number of pulses in a single pulse train. The total bandwidth of the waveform is still equal to formula_97, but the analog components can be reset to support the frequency of the following pulse during the time between pulses. As a result, the problem mentioned above can be avoided.\n\nTo calculate the distance of the target corresponding to a delay formula_98, individual pulses are processed through the simple pulse matched filter:\nand the output of the matched filter is:\nwhere\nIf we sample formula_102 at formula_103, we can get:\nwhere l means the range bin l.\nConduct DTFT (m is served as time here) and we can get:\n,and the peak of the summation occurs when formula_106.\n\nConsequently, the DTFT of formula_107 provides a measure of the delay of the target relative to the range bin delay formula_108:\nand the differential range can be obtained:\nwhere c is the speed of light.\n\nTo demonstrate stepped-frequency waveform preserves range resolution, it should be noticed that formula_111 is a sinc-like function, and therefore it has a Rayleigh resolution of formula_112. As a result:\nand therefore the differential range resolution is :\nwhich is the same of the resolution of the original linear-frequency-modulation waveform.\n\nThere are other means to modulate the signal. Phase modulation is a commonly used technique; in this case, the pulse is divided in formula_115 time slots of duration formula_116 for which the phase at the origin is chosen according to a pre-established convention. For instance, it is possible to not change the phase for some time slots (which comes down to just leaving the signal as it is, in those slots) and de-phase the signal in the other slots by formula_117 (which is equivalent of changing the sign of the signal). The precise way of choosing the sequence of formula_118 phases is done according to a technique known as Barker codes. It is possible to code the sequence on more than two phases (polyphase coding). As with a linear chirp, pulse compression is achieved through intercorrelation.\n\nThe advantages of the Barker codes are their simplicity (as indicated above, a formula_117 de-phasing is a simple sign change), but the pulse compression ratio is lower than in the chirp case and the compression is very sensitive to frequency changes due to the Doppler effect if that change is larger than formula_86.\n\n- Nadav Levanon, and Eli Mozeson. Radar signals. Wiley. com, 2004.\n- Hao He, Jian Li, and Petre Stoica. Waveform design for active sensing systems: a computational approach. Cambridge University Press, 2012.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n- Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n- Fulvio Gini, Antonio De Maio, and Lee Patton, eds. Waveform design and diversity for advanced radar systems. Institution of engineering and technology, 2012.\n- John J. Benedetto, Ioannis Konstantinidis, and Muralidhar Rangaswamy. \"Phase-coded waveforms and their design.\" IEEE Signal Processing Magazine, 26.1 (2009): 22-31.\n- Ducoff, Michael R., and Byron W. Tietjen. \"Pulse compression radar.\" Radar Handbook (2008): 8-3.\n\n", "related": "\n- Spread spectrum\n- Chirp compression\n"}
{"id": "8799355", "url": "https://en.wikipedia.org/wiki?curid=8799355", "title": "Complementary sequences", "text": "Complementary sequences\n\nIn applied mathematics, complementary sequences (CS) are pairs of sequences with the useful property that their out-of-phase aperiodic autocorrelation coefficients sum to zero. Binary complementary sequences were first introduced by Marcel J. E. Golay in 1949. In 1961–1962 Golay gave several methods for constructing sequences of length 2 and gave examples of complementary sequences of lengths 10 and 26. In 1974 R. J. Turyn gave a method for constructing sequences of length \"mn\" from sequences of lengths \"m\" and \"n\" which allows the construction of sequences of any length of the form 21026.\n\nLater the theory of complementary sequences was generalized by other authors to polyphase complementary sequences, multilevel complementary sequences, and arbitrary complex complementary sequences. Complementary sets have also been considered; these can contain more than two sequences.\n\nLet (\"a\", \"a\", ..., \"a\") and (\"b\", \"b\", ..., \"b\") be a pair of bipolar sequences, meaning that \"a\"(\"k\") and \"b\"(\"k\") have values +1 or −1. Let the aperiodic autocorrelation function of the sequence x be defined by\n\nThen the pair of sequences \"a\" and \"b\" is complementary if:\n\nfor \"k\" = 0, and \n\nfor \"k\" = 1, ..., \"N\" − 1.\n\nOr using Kronecker delta we can write:\n\nSo we can say that the sum of autocorrelation functions of complementary sequences is a delta function, which is an ideal autocorrelation for many applications like radar pulse compression and spread spectrum telecommunications.\n\n- As the simplest example we have sequences of length 2: (+1, +1) and (+1, −1). Their autocorrelation functions are (2, 1) and (2, −1), which add up to (4, 0).\n- As the next example (sequences of length 4), we have (+1, +1, +1, −1) and (+1, +1, −1, +1). Their autocorrelation functions are (4, 1, 0, −1) and (4, −1, 0, 1), which add up to (8, 0, 0, 0).\n- One example of length 8 is (+1, +1, +1, −1, +1, +1, −1, +1) and (+1, +1, +1, −1, −1, −1, +1, −1). Their autocorrelation functions are (8, −1, 0, 3, 0, 1, 0, 1) and (8, 1, 0, −3, 0, −1, 0, −1).\n- An example of length 10 given by Golay is (+1, +1, −1, +1, −1, +1, −1, −1, +1, +1) and (+1, +1, −1, +1, +1, +1, +1, +1, −1, −1). Their autocorrelation functions are (10, −3, 0, −1, 0, 1,−2, −1, 2, 1) and (10, 3, 0, 1, 0, −1, 2, 1, −2, −1).\n\n- Complementary sequences have complementary spectra. As the autocorrelation function and the power spectra form a Fourier pair, complementary sequences also have complementary spectra. But as the Fourier transform of a delta function is a constant, we can write\n\n- CS spectra is upper bounded. As \"S\" and \"S\" are non-negative values we can write\n\n- If either of the sequences of the CS pair is inverted (multiplied by −1) they remain complementary. More generally if any of the sequences is multiplied by \"e\" they remain complementary;\n- If either of the sequences is reversed they remain complementary;\n- If either of the sequences is delayed they remain complementary;\n- If the sequences are interchanged they remain complementary;\n- If both sequences are multiplied by the same constant (real or complex) they remain complementary;\n- If both sequences are decimated in time by \"K\" they remain complementary. More precisely if from a complementary pair (\"a\"(\"k\"), \"b\"(\"k\")) we form a new pair (\"a\"(\"Nk\"), \"b\"(\"Nk\")) with skipped samples discarded then the new sequences are complementary.\n- If alternating bits of both sequences are inverted they remain complementary. In general for arbitrary complex sequences if both sequences are multiplied by \"e\" (where \"k\" is a constant and \"n\" is the time index) they remain complementary;\n- A new pair of complementary sequences can be formed as [\"a\" \"b\"] and [\"a\" −\"b\"] where [..] denotes concatenation and \"a\" and \"b\" are a pair of CS;\n- A new pair of sequences can be formed as {\"a\" \"b\"} and {\"a\" −\"b\"} where {..} denotes interleaving of sequences.\n- A new pair of sequences can be formed as \"a\" + \"b\" and \"a\" − \"b\".\n\nA complementary pair \"a\", \"b\" may be encoded as polynomials \"A\"(\"z\") = \"a\"(0) + \"a\"(1)\"z\" + ... + \"a\"(\"N\" − 1)\"z\" and similarly for \"B\"(\"z\"). The complementarity property of the sequences is equivalent to the condition\n\nfor all \"z\" on the unit circle, that is, |\"z\"| = 1. If so, \"A\" and \"B\" form a Golay pair of polynomials. Examples include the Shapiro polynomials, which give rise to complementary sequences of length a power of two.\n\n- Multislit spectrometry\n- Ultrasound measurements\n- Acoustic measurements\n- radar pulse compression\n- Wi-Fi networks,\n- 3G CDMA wireless networks\n- OFDM communication systems\n- Train wheel detection systems\n- Non-destructive tests (NDT)\n- Communications\n- coded aperture masks are designed using a 2-dimensional generalization of complementary sequences.\n\n", "related": "\n- Binary Golay code (Error-correcting code)\n- Gold sequences\n- Kasami sequences\n- Polyphase sequence\n- Pseudorandom binary sequences (also called maximum length sequences or M-sequences)\n- Ternary Golay code (Error-correcting code)\n- Walsh-Hadamard sequences\n- Zadoff–Chu sequence\n\n"}
{"id": "9532895", "url": "https://en.wikipedia.org/wiki?curid=9532895", "title": "Coherent sampling", "text": "Coherent sampling\n\nFast Fourier Transform (FFT) is a common tool to investigate performance of data converters and other sampled systems. Coherent sampling refers to a certain relationship between input frequency, formula_1, sampling frequency, formula_2, number of cycles in the sampled set, formula_3, and number of samples, formula_4. With coherent sampling one is assured that the signal power in an FFT is contained within one FFT bin, assuming single input frequency. \n\nThe condition for coherent sampling is given by \n\nWhere formula_4 value must be a power of 2 and formula_3 value must be an odd or prime number.\n\nIf we have formula_8 and formula_9 and we want an input frequency close to formula_10, let's say formula_11, then formula_12 which is close to an integer, so we could round it down to formula_13 and we would get formula_14. This is an input frequency that satisfies coherent sampling and makes sure that we get an integer number of cycles. \n\nThis integer number formula_15 can be chosen to be a Prime number (except for 2). This ensures that the same FFT samples will not be repeated across any input signal cycles of the sampled signal. For example, if N=32 and M=6 (non-prime) then there will be N/M=5.33 samples per cycle of the input frequency, so three cycles of the input will take exactly 16 samples, and so the first 16 samples will be identical to the second 16 samples. Usually, no additional information is gained by repeating with the same sampling points and so generally a non-prime M should not be used.\n\n", "related": "\n- Bin-centres\n\n- Coherent Sampling Application Note and Calculator\n"}
{"id": "9633614", "url": "https://en.wikipedia.org/wiki?curid=9633614", "title": "Delay equalization", "text": "Delay equalization\n\nIn signal processing, delay equalization corresponds to adjusting the relative phases of different frequencies to achieve a constant group delay, using by adding an all-pass filter in series with an uncompensated filter. Clever machine-learning techniques are now being applied to the design of such filters.\n", "related": "NONE"}
{"id": "10384478", "url": "https://en.wikipedia.org/wiki?curid=10384478", "title": "Triple correlation", "text": "Triple correlation\n\nThe triple correlation of an ordinary function on the real line is the integral of the product of that function with two independently shifted copies of itself:\n\nThe Fourier transform of triple correlation is the bispectrum. The triple correlation extends the concept of autocorrelation, which correlates a function with a single shifted copy of itself and thereby enhances its latent periodicities.\n\nThe theory of the triple correlation was first investigated by statisticians examining the cumulant structure of non-Gaussian random processes. It was also independently studied by physicists as a tool for spectroscopy of laser beams. Hideya Gamo in 1963 described an apparatus for measuring the triple correlation of a laser beam, and also showed how phase information can be recovered from the real part of the bispectrum—up to sign reversal and linear offset. However, Gamo's method implicitly requires the Fourier transform to never be zero at any frequency. This requirement was relaxed, and the class of functions which are known to be uniquely identified by their triple (and higher-order) correlations was considerably expanded, by the study of Yellott and Iverson (1992). Yellott & Iverson also pointed out the connection between triple correlations and the visual texture discrimination theory proposed by Bela Julesz.\n\nTriple correlation methods are frequently used in signal processing for treating signals that are corrupted by additive white Gaussian noise; in particular, triple correlation techniques are suitable when multiple observations of the signal are available and the signal may be translating in between the observations, e.g.,a sequence of images of an object translating on a noisy background. What makes the triple correlation particularly useful for such tasks are three properties: (1) it is invariant under translation of the underlying signal; (2) it is unbiased in additive Gaussian noise; and (3) it retains nearly all of the relevant phase information in the underlying signal. Properties (1)-(3) of the triple correlation extend in many cases to functions on an arbitrary locally compact group, in particular to the groups of rotations and rigid motions of euclidean space that arise in computer vision and signal processing.\n\nThe triple correlation may be defined for any locally compact group by using the group's left-invariant Haar measure. It is easily shown that the resulting object is invariant under left translation of the underlying function and unbiased in additive Gaussian noise. What is more interesting is the question of uniqueness : when two functions have the same triple correlation, how are the functions related? For many cases of practical interest, the triple correlation of a function on an abstract group uniquely identifies that function up to a single unknown group action. This uniqueness is a mathematical result that relies on the Pontryagin duality theorem, the Tannaka–Krein duality theorem, and related results of Iwahori-Sugiura, and Tatsuuma. Algorithms exist for recovering bandlimited functions from their triple correlation on euclidean space, as well as rotation groups in two and three dimensions. There is also an interesting link with Wiener's tauberian theorem: any function whose translates are dense in formula_2, where G is a locally compact abelian group, is also uniquely identified by its triple correlation.\n\n- K. Hasselman, W. Munk, and G. MacDonald (1963), \"Bispectra of ocean waves\", in \"Time Series Analysis\", M. Rosenblatt, Ed., New York: Wiley, 125-139.\n- R. Kakarala (1992) \"Triple correlation on groups\", Ph.D. Thesis, Department of Mathematics, University of California, Irvine.\n- R. Kondor (2007), \"A complete set of rotationally and translationally invariant features for images\",\n", "related": "NONE"}
{"id": "2697574", "url": "https://en.wikipedia.org/wiki?curid=2697574", "title": "Leveler", "text": "Leveler\n\nA leveler performs an audio process similar to compression, which is used to reduce the dynamic range of a signal, so that the quietest portion of the signal is loud enough to hear and the loudest portion is not too loud. \n\nLevelers work especially well with vocals, as there are huge dynamic differences in the human voice and levelers work in such a way as to sound very natural, letting the character of the sound change with the different levels but still maintaining a predictable and usable dynamic range.\n\nA leveler is different from a compressor in that the ratio and threshold are controlled with a single control.\n\n- Summit Audio TLA-100 Tube Levelling Amplifier\n", "related": "NONE"}
{"id": "4484195", "url": "https://en.wikipedia.org/wiki?curid=4484195", "title": "Hilbert spectrum", "text": "Hilbert spectrum\n\nThe Hilbert spectrum (sometimes referred to as the Hilbert amplitude spectrum), named after David Hilbert, is a statistical tool that can help in distinguishing among a mixture of moving signals. The spectrum itself is decomposed into its component sources using independent component analysis. The separation of the combined effects of unidentified sources (blind signal separation) has applications in climatology, seismology, and biomedical imaging.\n\nThe Hilbert spectrum is computed by way of a 2-step process consisting of:\n- Preprocessing a signal separate it into intrinsic mode functions using a mathematical decomposition such as singular value decomposition (SVD);\n- Applying the Hilbert transform to the results of the above step to obtain the instantaneous frequency spectrum of each of the components.\n\nThe Hilbert transform defines the imaginary part of the function to make it an analytic function (sometimes referred to as a progressive function), \"i.e.\" a function whose signal strength is zero for all frequency components less than zero.\n\nWith the Hilbert transform, the singular vectors give instantaneous frequencies that are functions of time, so that the result is an energy distribution over time and frequency.\n\nThe result is an ability to capture time-frequency localization to make the concept of instantaneous frequency and time relevant (the concept of instantaneous frequency is otherwise abstract or difficult to define for all but monocomponent signals).\n\nFor a given signal formula_1 decomposed (with for example Empirical Mode Decomposition) to\n\nformula_2\n\nwhere formula_3 is the number of intrinsic mode functions that formula_1 consist of and\n\nformula_5\n\nThe instantaneous angle frequency is then defined as\n\nformula_6\n\nFrom this, we can define the Hilbert Spectrum for formula_7 as\n\nformula_8\n\nThe Hilbert Spectrum of formula_1 is then given by\n\nformula_10\n\nA two dimensional representation of a Hilbert Spectrum, called Marginal Hilbert Spectrum, is defined as\n\nformula_11\n\nwhere formula_12 is the length of the sampled signal formula_1. The Marginal Hilbert Spectrum show the total energy that each frequency value contribute with.\n\nThe Hilbert spectrum has many practical applications. One example application pioneered by Professor Richard Cobbold, is the use of the Hilbert spectrum for the analysis of blood flow by pulse Doppler ultrasound. Other applications of the Hilbert spectrum include analysis of climatic features, water waves, and the like.\n\n", "related": "\n- Hilbert–Huang transform\n\n- Huang, et al., \"The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis\" \"Proc. R. Soc. Lond.\" (A) 1998\n"}
{"id": "3951487", "url": "https://en.wikipedia.org/wiki?curid=3951487", "title": "Gain compression", "text": "Gain compression\n\nGain compression is a reduction in \"differential\" or \"slope\" gain caused by nonlinearity of the transfer function of the amplifying device. This nonlinearity may be caused by heat due to power dissipation or by overdriving the active device beyond its linear region. It is a \"large-signal\" phenomenon of circuits.\n\nGain compression is relevant in any system with a wide dynamic range, such as audio or RF. It is more common in tube circuits than transistor circuits, due to topology differences, possibly causing the differences in audio performance called \"valve sound\". The front-end RF amps of radio receivers are particularly susceptible to this phenomenon when overloaded by a strong unwanted signal.\n\nA tube radio or tube amplifier will increase in volume to a point, and then as the input signal extends beyond the linear range of the device, the effective gain is reduced, altering the shape of the waveform. The effect is also present in transistor circuits. The extent of the effect depends on the topology of the amplifier.\n\nClipping, as a form of signal compression, differs from the operation of the typical studio audio level compressor, in which gain compression is not instantaneous (delayed in time via attack and release settings). \n\nClipping destroys any audio information which is over a certain threshold. Compression and limiting, change the shape of the entire waveform, not just the shape of the waveform above the threshold. This is why it is possible to limit and compress with very high ratios without causing distortion.\n\nGain is a linear operation. \"Gain compression\" is not linear and, as such, its effect is one of distortion, due to the nonlinearity of the transfer characteristic which also causes a loss of 'slope' or 'differential' gain. So the output is less than expected using the \"small signal\" gain of the amplifier. \n\nIn clipping, the signal is abruptly limited to a certain amplitude and is thereby distorted in keeping under that level. This creates extra harmonics that are not present in the original signal. \n\n\"Soft\" clipping or limiting means there isn't a sharp \"knee point\" in the transfer characteristic. A sine wave that has been softly clipped will become more like a square wave, with more rounded edges, but will still have many extra harmonics.\n\nCompression of gain is caused by non-linear characteristics of the device when run at large amplitudes. With any signal, as the input level is increased beyond the linear range of the amplifier, gain compression will occur. \n\nA transistor's operating point may move with temperature, so higher power output may lead to compression due to collector dissipation. But it's not a change in gain; it's non-linear distortion. The output level stays relatively the same as the input level goes higher. Once the non-linear portion of the transfer characteristic of any amplifier is reached, any increase in input will not be matched by a proportional increase in output. Thus there is compression of gain. Also, at this time because the transfer function is no longer linear, harmonic distortion will result.\n\nIn intentional compression (sometimes called automatic gain control or audio level compression) as used in devices called 'dynamic range compressors', the overall gain of the circuit is actively changed in response to the level of the input over time, so the transfer function remains linear over a short period of time. A sine wave into such a system will still look like a sine wave at the output, but the overall gain is varied, depending on the level of that sine wave. Above a certain input level, the output sine wave will always be the same amplitude. The output level of Intentional compression varies over time, in order to minimize non linear behavior. With gain compression, the opposite is true, its output is constant. In this respect intentional compression serves less of an artistic purpose.\n\n\"Gain compression\" in RF amplifiers is similar to soft clipping. However, in narrowband systems, the effect \"looks\" more like gain compression simply because the harmonics are filtered out after amplification. Many data sheets for RF amplifiers list gain compression rather than distortion figures because it's easier to measure and is more important than distortion figures in nonlinear RF amplifiers.\n\nIn wideband and low-frequency systems, the nonlinear effects are readily visible, e.g. the output is clipped. To see the same thing at 1 GHz, an oscilloscope with a bandwidth of at least 10 GHz is needed. Observing with a spectrum analyzer, the fundamental compressed and the harmonics picking up. \n\nA low-noise RF amplifier, if fed by a directional antenna to a consumer 900 MHz receiver, should improve the transmission range. It works, but the receiver may also pick up a couple of UHF stations around 700 MHz.\nFor example, if channel 54 is transmitting 6 MW of AM, FM, and PM, the RF front end, expecting −80 dBm, would be grossly overloaded and generate mixing products. This is a typical effect of gain compression.\n\nPower compression is form of gain compression that takes place in loudspeaker voice coils when they heat up and increase their resistance. This causes less power to be drawn from the amplifier and a reduction in sound pressure level.\n\n", "related": "\n- Third-order intercept point\n- Dynamic range compression\n"}
{"id": "1434057", "url": "https://en.wikipedia.org/wiki?curid=1434057", "title": "Phase vocoder", "text": "Phase vocoder\n\nA phase vocoder is a type of vocoder which can scale both the frequency and time domains of audio signals by using phase information. The computer algorithm allows frequency-domain modifications to a digital sound file (typically time expansion/compression and pitch shifting).\n\nAt the heart of the phase vocoder is the short-time Fourier transform (STFT), typically coded using fast Fourier transforms. The STFT converts a time domain representation of sound into a time-frequency representation (the \"analysis\" phase), allowing modifications to the amplitudes or phases of specific frequency components of the sound, before resynthesis of the time-frequency domain representation into the time domain by the inverse STFT. The time evolution of the resynthesized sound can be changed by means of modifying the time position of the STFT frames prior to the resynthesis operation\nallowing for time-scale modification of the original sound file.\n\nThe main problem that has to be solved for all cases of manipulation of the STFT is the fact that individual signal components (sinusoids, impulses) will be spread over multiple frames and multiple STFT frequency locations (bins). This is because the STFT analysis is done using overlapping analysis windows. The windowing results in spectral leakage such that the information of individual sinusoidal components is spread over adjacent STFT bins. To avoid border effects of tapering of the analysis windows, STFT analysis windows overlap in time. This time overlap results in the fact that adjacent STFT analyses are strongly correlated (a sinusoid present in analysis frame at time \"t\" will be present in the subsequent frames as well). The problem of signal transformation with the phase vocoder is related to the problem that all modifications that are done in the STFT representation need to preserve the appropriate correlation between adjacent frequency bins (vertical coherence) and time frames (horizontal coherence). Except in the case of extremely simple synthetic sounds, these appropriate correlations can be preserved only approximately, and since the invention of the phase vocoder research has been mainly concerned with finding algorithms that would preserve the vertical and horizontal coherence of the STFT representation after the modification. The phase coherence problem was investigated for quite a while before appropriate solutions emerged.\n\nThe phase vocoder was introduced in 1966 by Flanagan as an algorithm that would preserve horizontal coherence between the phases of bins that represent sinusoidal components. This original phase vocoder did not take into account the vertical coherence between adjacent frequency bins, and therefore, time stretching with this system did produce sound signals that were missing clarity.\n\nThe optimal reconstruction of the sound signal from STFT after amplitude modifications has been proposed by Griffin and Lim in 1984. This algorithm does not consider the problem of producing a coherent STFT, but it does allow finding the sound signal that has an STFT that is as close as possible to the modified STFT even if the modified STFT is not coherent (does not represent any signal).\n\nThe problem of the vertical coherence remained a major issue for the quality of time scaling operations until 1999 when Laroche and Dolson proposed a means to preserve phase consistency across spectral bins. The proposition of Laroche and Dolson has to be seen as a turning point in phase vocoder history. It has been shown that by means of ensuring vertical phase consistency very high quality time scaling transformations can be obtained.\n\nThe algorithm proposed by Laroche did not allow preservation of vertical phase coherence for sound onsets (note onsets). A solution for this problem has been proposed by Roebel.\n\nAn example of software implementation of phase vocoder based signal transformation using means similar to those described here to achieve high quality signal transformation is Ircam's SuperVP.\n\nBritish composer Trevor Wishart used phase vocoder analyses and transformations of a human voice as the basis for his composition \"Vox 5\" (part of his larger Vox Cycle). \"Transfigured Wind\" by American composer Roger Reynolds uses the phase vocoder to perform time-stretching of flute sounds. The music of JoAnn Kuchera-Morin makes some of the earliest and most extensive use of phase vocoder transformations, such as in \"Dreampaths\" (1989).\n\nThe proprietary Auto-Tune pitch-correcting software, widely used in commercial music production, is based on the phase vocoder principle.\n\n", "related": "\n- Auto tune\n- Audio timescale/pitch modification\n- Vocoder\n\n- The Phase Vocoder: A Tutorial - A good description of the phase vocoder\n- New Phase-Vocoder Techniques for Pitch-Shifting, Harmonizing and Other Exotic Effects\n- A new Approach to Transient Processing in the Phase Vocoder\n- Phase Vocoder - Phase vocoder description with figures and equations\n"}
{"id": "11351749", "url": "https://en.wikipedia.org/wiki?curid=11351749", "title": "Clipping (signal processing)", "text": "Clipping (signal processing)\n\nClipping is a form of distortion that limits a signal once it exceeds a threshold. Clipping may occur when a signal is recorded by a sensor that has constraints on the range of data it can measure, it can occur when a signal is digitized, or it can occur any other time an analog or digital signal is transformed, particularly in the presence of gain or overshoot and undershoot.\n\nClipping may be described as hard, in cases where the signal is strictly limited at the threshold, producing a flat cutoff; or it may be described as soft, in cases where the clipped signal continues to follow the original at a reduced gain. Hard clipping results in many high frequency harmonics; soft clipping results in fewer higher order harmonics and intermodulation distortion components.\n\nIn the audio domain, clipping may be heard as general distortion or as pops.\n\nBecause the clipped waveform has more area underneath it than the smaller unclipped waveform, the amplifier produces more power than its rated (sine wave) output when it is clipping. This extra power can damage any part of the loudspeaker, including the woofer, or the tweeter, by causing over-excursion, or by overheating the voice coil. It may cause damage to the amplifier's power supply or simply blow a fuse.\n\nIn the frequency domain, clipping produces strong harmonics in the high frequency range (as the clipped waveform comes closer to a squarewave). The extra high frequency weighting of the signal could make tweeter damage more likely than if the signal was not clipped. However most loudspeakers are designed to handle signals like cymbal crashes that have even more high frequency weighting than amplifier clipping produces, so damage attributable to this characteristic is rare.\n\nMany electric guitar players intentionally overdrive their amplifiers (or insert a \"fuzz box\") to cause clipping in order to get a desired sound (see guitar distortion). \n\nSome audiophiles believe that the clipping behavior of vacuum tubes with little or no negative feedback is superior to that of transistors, in that vacuum tubes clip more gradually than transistors (i.e. \"soft\" clipping, and mostly even harmonics), resulting in harmonic distortion that is generally less objectionable. In general though, the distortion associated with clipping is unwanted, and is visible on an oscilloscope even if it is inaudible. Even in a transistorised amplifier with hard clipping, the gain of the transistor will be reducing (leading to nonlinear distortion) as the output current increases and the voltage across the transistor reduces close to the saturation voltage (for bipolar transistors), and so \"full power\" for the purposes of measuring distortion in amplifiers is usually taken as a few percent \"below\" clipping.\n\nIn the image domain, clipping is seen as desaturated (washed-out) bright areas that turn to pure white if all color components clip.\n\nA circuit designer may intentionally use a clipper or clamper to keep a signal within a desired range.\n\nWhen an amplifier is pushed to create a signal with more power than it can support, it will amplify the signal only up to its maximum capacity, at which point the signal will be amplified no further.\n\n- An integrated circuit or discrete solid state amplifier cannot give an output voltage larger than the voltage it is powered by (commonly a 24- or 30-volt spread for operational amplifiers used in line level equipment).\n- A vacuum tube can only move a limited number of electrons in an amount of time, dependent on its size, temperature, and metals.\n- A transformer (most commonly used between stages in tube equipment) will clip when its ferromagnetic core becomes electromagnetically saturated.\n\nIn digital signal processing, clipping occurs when the signal is restricted by the range of a chosen representation. For example in a system using 16-bit signed integers, 32767 is the largest positive value that can be represented, and if during processing the amplitude of the signal is doubled, sample values of 32000 should become 64000, but instead they are truncated to the maximum, 32767. Clipping is preferable to the alternative in digital systems — wrapping — which occurs if the digital hardware is allowed to \"overflow\", ignoring the most significant bits of the magnitude, and sometimes even the sign of the sample value, resulting in gross distortion of the signal.\n\nThe incidence of clipping may be greatly reduced by using floating point numbers instead of integers. However, floating point numbers are usually less efficient to use, sometimes result in a loss of precision, and they can still clip if a number is extremely large or small.\n\nClipping can be detected by viewing the signal (on an oscilloscope, for example), and observing that the tops and bottoms of waves aren't smooth anymore. When working with images, some tools can highlight all pixels that are pure white, allowing the user to identify larger groups of white pixels and decide if too much clipping has occurred.\n\nTo avoid clipping, the signal can be dynamically reduced using a limiter. If not done carefully, this can still cause undesirable distortion, but it prevents any data from being completely lost.\n\nWhen clipping occurs, part of the original signal is lost, so perfect restoration is impossible. Thus, it is much preferable to avoid clipping in the first place. However, when repair is the only option, the goal is to make up a plausible replacement for the clipped part of the signal.\n\n", "related": "\n- Dynamic range\n- Dynamic range compression\n"}
{"id": "11612350", "url": "https://en.wikipedia.org/wiki?curid=11612350", "title": "Factorial code", "text": "Factorial code\n\nMost real world data sets consist of data vectors whose individual components are not statistically independent. In other words, knowing the value of an element will provide information about the value of elements in the data vector. When this occurs, it can be desirable to create a factorial code of the data, i. e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.\n\nLater supervised learning usually works much better when the raw input data is first translated into such a factorial code. For example, suppose the final goal is to classify images with highly redundant pixels. A naive Bayes classifier will assume the pixels are statistically independent random variables and therefore fail to produce good results. If the data are first encoded in a factorial way, however, then the naive Bayes classifier will achieve its optimal performance (compare Schmidhuber et al. 1996).\n\nTo create factorial codes, Horace Barlow and co-workers suggested to minimize the sum of the bit entropies of the code components of binary codes (1989). Jürgen Schmidhuber (1992) re-formulated the problem in terms of predictors and binary feature detectors, each receiving the raw data as an input. For each detector there is a predictor that sees the other detectors and learns to predict the output of its own detector in response to the various input vectors or images. But each detector uses a machine learning algorithm to become as unpredictable as possible. The global optimum of this objective function corresponds to a factorial code represented in a distributed fashion across the outputs of the feature detectors.\n\nPainsky, Rosset and Feder (2016, 2017) further studied this problem in the context of independent component analysis over finite alphabet sizes. Through a series of theorems they show that the factorial coding problem can be accurately solved with a branch and bound search tree algorithm, or tightly approximated with a series of linear problems. In addition, they introduce a simple transformation (namely, order permutation) which provides a greedy yet very effective approximation of the optimal solution. Practically, they show that with a careful implementation, the favorable properties of the order permutation may be achieved in an asymptotically optimal computational complexity. Importantly, they provide theoretical guarantees, showing that while not every random vector can be efficiently decomposed into independent components, the majority of vectors do decompose very well (that is, with a small constant cost), as the dimension increases. In addition, they demonstrate the use of factorial codes to data compression in multiple setups (2017).\n\n", "related": "\n- Blind signal separation (BSS)\n- Principal component analysis (PCA)\n- Factor analysis\n- Unsupervised learning\n- Image processing\n- Signal processing\n\n- Horace Barlow, T. P. Kaushal, and G. J. Mitchison. Finding minimum entropy codes. Neural Computation, 1:412-423, 1989.\n- Jürgen Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863-879, 1992\n- J. Schmidhuber and M. Eldracher and B. Foltin. Semilinear predictability minimization produces well-known feature detectors. Neural Computation, 8(4):773-786, 1996\n- A. Painsky, S. Rosset and M. Feder. Generalized independent component analysis over finite alphabets. IEEE Transactions on Information Theory, 62(2):1038-1053, 2016\n- A. Painsky, S. Rosset and M. Feder. Large Alphabet Source Coding using Independent Component Analysis. IEEE Transactions on Information Theory, 63(10):6514 - 6529, 2017\n"}
{"id": "12069013", "url": "https://en.wikipedia.org/wiki?curid=12069013", "title": "Recurrence period density entropy", "text": "Recurrence period density entropy\n\nRecurrence period density entropy (RPDE) is a method, in the fields of dynamical systems, stochastic processes, and time series analysis, for determining the periodicity, or repetitiveness of a signal.\n\nRecurrence period density entropy is useful for characterising the extent to which a time series repeats the same sequence, and is therefore similar to linear autocorrelation and time delayed mutual information, except that it measures repetitiveness in the phase space of the system, and is thus a more reliable measure based upon the dynamics of the underlying system that generated the signal. It has the advantage that it does not require the assumptions of linearity, Gaussianity or dynamical determinism. It has been successfully used to detect abnormalities in biomedical contexts such as speech signal.\n\nThe RPDE value formula_1 is a scalar in the range zero to one. For purely periodic signals, formula_2, whereas for purely i.i.d., uniform white noise, formula_3.\n\nThe RPDE method first requires the embedding of a time series in phase space, which, according to stochastic extensions to Taken's embedding theorems, can be carried out by forming time-delayed vectors:\n\nfor each value \"x\" in the time series, where \"M\" is the embedding dimension, and τ is the embedding delay. These parameters are obtained by systematic search for the optimal set (due to lack of practical embedding parameter techniques for stochastic systems) (Stark et al. 2003). Next, around each point formula_5 in the phase space, an formula_6-neighbourhood (an \"m\"-dimensional ball with this radius) is formed, and every time the time series returns to this ball, after having left it, the time difference \"T\" between successive returns is recorded in a histogram. This histogram is normalised to sum to unity, to form an estimate of the recurrence period density function \"P\"(\"T\"). The normalised entropy of this density:\n\nis the RPDE value, where formula_8 is the largest recurrence value (typically on the order of 1000 samples). Note that RPDE is intended to be applied to both deterministic and stochastic signals, therefore, strictly speaking, Taken's original embedding theorem does not apply, and needs some modification.\n\nRPDE has the ability to detect subtle changes in natural biological time series such as the breakdown of regular periodic oscillation in abnormal cardiac function which are hard to detect using classical signal processing tools such as the Fourier transform or linear prediction. The recurrence period density is a sparse representation for nonlinear, non-Gaussian and nondeterministic signals, whereas the Fourier transform is only sparse for purely periodic signals.\n\n", "related": "\n- Recurrence plot, a powerful visualisation tool of recurrences in dynamical (and other) systems.\n- Recurrence quantification analysis, another approach to quantify recurrence properties.\n\n- Fast MATLAB code for calculating the RPDE value.\n- http://www.recurrence-plot.tk/\n"}
{"id": "1565926", "url": "https://en.wikipedia.org/wiki?curid=1565926", "title": "Estimation theory", "text": "Estimation theory\n\nEstimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.\n\nIn estimation theory, two approaches are generally considered.\n\n- The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest\n- The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.\n\nFor example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters. Alternatively, it is desired to estimate the probability of a voter voting for a particular candidate, based on some demographic features, such as age.\n\nOr, for example, in radar the aim is to find the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated.\n\nAs another example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal.\n\nFor a given model, several statistical \"ingredients\" are needed so the estimator can be implemented. The first is a statistical sample – a set of data points taken from a random vector (RV) of size \"N\". Put into a vector,\n\nSecondly, there are \"M\" parameters\n\nwhose values are to be estimated. Third, the continuous probability density function (pdf) or its discrete counterpart, the probability mass function (pmf), of the underlying distribution that generated the data must be stated conditional on the values of the parameters:\n\nIt is also possible for the parameters themselves to have a probability distribution (e.g., Bayesian statistics). It is then necessary to define the Bayesian probability\n\nAfter the model is formed, the goal is to estimate the parameters, with the estimates commonly denoted formula_5, where the \"hat\" indicates the estimate.\n\nOne common estimator is the minimum mean squared error (MMSE) estimator, which utilizes the error between the estimated parameters and the actual value of the parameters\n\nas the basis for optimality. This error term is then squared and the expected value of this squared value is minimized for the MMSE estimator.\n\nCommonly used estimators (estimation methods) and topics related to them include:\n- Maximum likelihood estimators\n- Bayes estimators\n- Method of moments estimators\n- Cramér–Rao bound\n- Least squares\n- Minimum mean squared error (MMSE), also known as Bayes least squared error (BLSE)\n- Maximum a posteriori (MAP)\n- Minimum variance unbiased estimator (MVUE)\n- Nonlinear system identification\n- Best linear unbiased estimator (BLUE)\n- Unbiased estimators — see estimator bias.\n- Particle filter\n- Markov chain Monte Carlo (MCMC)\n- Kalman filter, and its various derivatives\n- Wiener filter\n\nConsider a received discrete signal, formula_7, of formula_8 independent samples that consists of an unknown constant formula_9 with additive white Gaussian noise (AWGN) formula_10 with known variance formula_11 (\"i.e.\", formula_12).\nSince the variance is known then the only unknown parameter is formula_9.\n\nThe model for the signal is then\n\nTwo possible (of many) estimators for the parameter formula_9 are:\n- formula_16\n- formula_17 which is the sample mean\n\nBoth of these estimators have a mean of formula_9, which can be shown through taking the expected value of each estimator\n\nand\n\nAt this point, these two estimators would appear to perform the same.\nHowever, the difference between them becomes apparent when comparing the variances.\n\nand\n\nIt would seem that the sample mean is a better estimator since its variance is lower for every \"N\" > 1.\n\nContinuing the example using the maximum likelihood estimator, the probability density function (pdf) of the noise for one sample formula_10 is\n\nand the probability of formula_7 becomes (formula_7 can be thought of a formula_27)\n\nBy independence, the probability of formula_29 becomes\n\nTaking the natural logarithm of the pdf\n\nand the maximum likelihood estimator is\n\nTaking the first derivative of the log-likelihood function\n\nand setting it to zero\n\nThis results in the maximum likelihood estimator\n\nwhich is simply the sample mean.\nFrom this example, it was found that the sample mean is the maximum likelihood estimator for formula_8 samples of a fixed, unknown parameter corrupted by AWGN.\n\nTo find the Cramér–Rao lower bound (CRLB) of the sample mean estimator, it is first necessary to find the Fisher information number\n\nand copying from above\n\nTaking the second derivative\n\nand finding the negative expected value is trivial since it is now a deterministic constant\nformula_40\n\nFinally, putting the Fisher information into\n\nresults in\n\nComparing this to the variance of the sample mean (determined previously) shows that the sample mean is \"equal to\" the Cramér–Rao lower bound for all values of formula_8 and formula_9.\nIn other words, the sample mean is the (necessarily unique) efficient estimator, and thus also the minimum variance unbiased estimator (MVUE), in addition to being the maximum likelihood estimator.\n\nOne of the simplest non-trivial examples of estimation is the estimation of the maximum of a uniform distribution. It is used as a hands-on classroom exercise and to illustrate basic principles of estimation theory. Further, in the case of estimation based on a single sample, it demonstrates philosophical issues and possible misunderstandings in the use of maximum likelihood estimators and likelihood functions.\n\nGiven a discrete uniform distribution formula_45 with unknown maximum, the UMVU estimator for the maximum is given by\nwhere \"m\" is the sample maximum and \"k\" is the sample size, sampling without replacement. This problem is commonly known as the German tank problem, due to application of maximum estimation to estimates of German tank production during World War II.\n\nThe formula may be understood intuitively as;\nthe gap being added to compensate for the negative bias of the sample maximum as an estimator for the population maximum.\n\nThis has a variance of\nso a standard deviation of approximately formula_48, the (population) average size of a gap between samples; compare formula_49 above. This can be seen as a very simple case of maximum spacing estimation.\n\nThe sample maximum is the maximum likelihood estimator for the population maximum, but, as discussed above, it is biased.\n\nNumerous fields require the use of estimation theory.\nSome of these fields include (but are by no means limited to):\n- Interpretation of scientific experiments\n- Signal processing\n- Clinical trials\n- Opinion polls\n- Quality control\n- Telecommunications\n- Project management\n- Software engineering\n- Control theory (in particular Adaptive control)\n- Network intrusion detection system\n- Orbit determination\n\nMeasured data are likely to be subject to noise or uncertainty and it is through statistical probability that optimal solutions are sought to extract as much information from the data as possible.\n\n", "related": "\n- Best linear unbiased estimator (BLUE)\n- Chebyshev center\n- Completeness (statistics)\n- Cramér–Rao bound\n- Detection theory\n- Efficiency (statistics)\n- Estimator, Estimator bias\n- Expectation-maximization algorithm (EM algorithm)\n- Fermi problem\n- Grey box model\n- Information theory\n- Kalman filter\n- Least-squares spectral analysis\n- Markov chain Monte Carlo (MCMC)\n- Matched filter\n- Maximum a posteriori (MAP)\n- Maximum likelihood\n- Maximum entropy spectral estimation\n- Method of moments, generalized method of moments\n- Minimum mean squared error (MMSE)\n- Minimum variance unbiased estimator (MVUE)\n- Nonlinear system identification\n- Nuisance parameter\n- Parametric equation\n- Pareto principle\n- Particle filter\n- Rao–Blackwell theorem\n- Rule of three (statistics)\n- Spectral density, Spectral density estimation\n- Statistical signal processing\n- Sufficiency (statistics)\n- Wiener filter\n\n- \"Theory of Point Estimation\" by E.L. Lehmann and G. Casella. ()\n- \"Systems Cost Engineering\" by Dale Shermon. ()\n- \"Mathematical Statistics and Data Analysis\" by John Rice. ()\n- \"Fundamentals of Statistical Signal Processing: Estimation Theory\" by Steven M. Kay ()\n- \"An Introduction to Signal Detection and Estimation\" by H. Vincent Poor ()\n- \"Detection, Estimation, and Modulation Theory, Part 1\" by Harry L. Van Trees (; website)\n- \"Optimal State Estimation: Kalman, H-infinity, and Nonlinear Approaches\" by Dan Simon website\n- Ali H. Sayed, Adaptive Filters, Wiley, NJ, 2008, .\n- Ali H. Sayed, Fundamentals of Adaptive Filtering, Wiley, NJ, 2003, .\n- Thomas Kailath, Ali H. Sayed, and Babak Hassibi, Linear Estimation, Prentice-Hall, NJ, 2000, .\n- Babak Hassibi, Ali H. Sayed, and Thomas Kailath, Indefinite Quadratic Estimation and Control: A Unified Approach to H and H Theories, Society for Industrial & Applied Mathematics (SIAM), PA, 1999, .\n- V.G.Voinov, M.S.Nikulin, \"Unbiased estimators and their applications. Vol.1: Univariate case\", Kluwer Academic Publishers, 1993, .\n- V.G.Voinov, M.S.Nikulin, \"Unbiased estimators and their applications. Vol.2: Multivariate case\", Kluwer Academic Publishers, 1996, .\n"}
{"id": "65635", "url": "https://en.wikipedia.org/wiki?curid=65635", "title": "Chirp", "text": "Chirp\n\nA chirp is a signal in which the frequency increases (\"up-chirp\") or decreases (\"down-chirp\") with time. In some sources, the term \"chirp\" is used interchangeably with sweep signal. It is commonly used in sonar, radar, and laser, but has other applications, such as in spread-spectrum communications. \n\nIn spread-spectrum usage, surface acoustic wave (SAW) devices are often used to generate and demodulate the chirped signals. In optics, ultrashort laser pulses also exhibit chirp, which, in optical transmission systems, interacts with the dispersion properties of the materials, increasing or decreasing total pulse dispersion as the signal propagates. The name is a reference to the chirping sound made by birds; see bird vocalization.\n\nIf a waveform is defined as:\n\nthen the instantaneous angular frequency, \"ω\", is defined as the phase rate as given by the first derivative of phase,\nwith the instantaneous ordinary frequency, \"f\", being its normalized version:\n\nFinally, the instantaneous angular chirpyness, \"γ\", is defined to be the second derivative of instantaneous phase or the first derivative of instantaneous angular frequency, \nwith the instantaneous ordinary chirpyness, \"c\", being its normalized version:\nThus chirpyness is the rate of change of the instantaneous frequency.\n\nIn a linear-frequency chirp or simply linear chirp, the instantaneous frequency formula_4 varies exactly linearly with time:\n\nwhere formula_6 is the starting frequency (at time formula_7), and formula_8 is the chirpyness, assumed constant:\n\nwhere formula_10 is the final frequency; formula_11 is the time it takes to sweep from formula_12 to formula_10.\n\nThe corresponding time-domain function for the phase of any oscillating signal is the integral of the frequency function, as one expects the phase to grow like formula_14, i.e., that the derivative of the phase is the angular frequency formula_15.\n\nFor the linear chirp, this results in:\n\nwhere formula_17 is the initial phase (at time formula_7). Thus this is also called a quadratic-phase signal.\n\nThe corresponding time-domain function for a sinusoidal linear chirp is the sine of the phase in radians:\n\nIn a geometric chirp, also called an exponential chirp, the frequency of the signal varies with a geometric relationship over time. In other words, if two points in the waveform are chosen, formula_20 and formula_21, and the time interval between them formula_22 is kept constant, the frequency ratio formula_23 will also be constant.\n\nIn an exponential chirp, the frequency of the signal varies exponentially as a function of time:\n\nwhere formula_6 is the starting frequency (at formula_7), and formula_27 is the rate of exponential change in frequency.\nUnlike the linear chirp, which has a constant chirpyness, an exponential chirp has an exponentially increasing frequency rate.\n\nThe corresponding time-domain function for the phase of an exponential chirp is the integral of the frequency:\n\nwhere formula_17 is the initial phase (at formula_7).\n\nThe corresponding time-domain function for a sinusoidal exponential chirp is the sine of the phase in radians:\n\nAs was the case for the Linear Chirp, the instantaneous frequency of the Exponential Chirp consists of the fundamental frequency formula_24 accompanied by additional harmonics.\n\nA chirp signal can be generated with analog circuitry via a voltage-controlled oscillator (VCO), and a linearly or exponentially ramping control voltage. It can also be generated digitally by a digital signal processor (DSP) and digital to analog converter (DAC), using a direct digital synthesizer (DDS) and by varying the step in the numerically controlled oscillator. It can also be generated by a YIG oscillator.\nA chirp signal shares the same spectral content with an impulse signal. However, unlike in the impulse signal, spectral components of the chirp signal have different phases, i.e., their power spectra are alike but the phase spectra are distinct. Dispersion of a signal propagation medium may result in unintentional conversion of impulse signals into chirps. On the other hand, many practical applications, such as chirped pulse amplifiers or echolocation systems, use chirp signals instead of impulses because of their inherently lower PAPR.\n\nChirp modulation, or linear frequency modulation for digital communication, was patented by Sidney Darlington in 1954 with significant later work performed by Winkler in 1962. This type of modulation employs sinusoidal waveforms whose instantaneous frequency increases or decreases linearly over time. These waveforms are commonly referred to as linear chirps or simply chirps.\n\nHence the rate at which their frequency changes is called the \"chirp rate\". In binary chirp modulation, binary data is transmitted by mapping the bits into chirps of opposite chirp rates. For instance, over one bit period \"1\" is assigned a chirp with positive rate \"a\" and \"0\" a chirp with negative rate \"−a\". Chirps have been heavily used in radar applications and as a result advanced sources for transmission and matched filters for reception of linear chirps are available.\n\nAnother kind of chirp is the projective chirp, of the form:\n\nhaving the three parameters \"a\" (scale), \"b\" (translation), and \"c\" (chirpiness). The projective chirp is ideally suited to image processing, and forms the basis for the projective chirplet transform.\n\nA change in frequency of Morse code from the desired frequency, due to poor stability in the RF oscillator, is known as chirp, and in the R-S-T system is given an appended letter 'C'.\n\n", "related": "\n- Chirp spectrum - Analysis of the frequency spectrum of chirp signals\n- Chirp compression - Further information on compression techniques\n- Chirp spread spectrum - A part of the wireless telecommunications standard IEEE 802.15.4a CSS\n- Chirped mirror\n- Chirped pulse amplification\n- Chirplet transform - A signal representation based on a family of localized chirp functions.\n- Continuous-wave radar\n- Dispersion (optics)\n- Pulse compression\n- Radio propagation\n\n- Online Chirp Tone Generator (wav file output)\n- CHIRP Sonar on FishFinder\n"}
{"id": "275626", "url": "https://en.wikipedia.org/wiki?curid=275626", "title": "Deconvolution", "text": "Deconvolution\n\nIn mathematics, deconvolution is an algorithm-based process used to enhance signals from recorded data. Where the recorded data can be modeled as a pure signal that is distorted by a filter (a process known as convolution), deconvolution can be used to restore the original signal. The concept of deconvolution is widely used in the techniques of signal processing and image processing. \n\nThe foundations for deconvolution and time-series analysis were largely laid by Norbert Wiener of the Massachusetts Institute of Technology in his book \"Extrapolation, Interpolation, and Smoothing of Stationary Time Series\" (1949). The book was based on work Wiener had done during World War II but that had been classified at the time. Some of the early attempts to apply these theories were in the fields of weather forecasting and economics.\n\nIn general, the objective of deconvolution is to find the solution of a convolution equation of the form:\n\nUsually, \"h\" is some recorded signal, and \"f\" is some signal that we wish to recover, but has been convolved with a filter or distortion function \"g\", before we recorded it. The function \"g\" might represent the transfer function of an instrument or a driving force that was applied to a physical system. If we know \"g\", or at least know the form of \"g\", then we can perform deterministic deconvolution. However, if we do not know \"g\" in advance, then we need to estimate it. This is most often done using methods of statistical estimation.\n\nIn physical measurements, the situation is usually closer to\n\nIn this case \"ε\" is noise that has entered our recorded signal. If we assume that a noisy signal or image is noiseless when we try to make a statistical estimate of \"g\", our estimate will be incorrect. In turn, our estimate of \"ƒ\" will also be incorrect. The lower the signal-to-noise ratio, the worse our estimate of the deconvolved signal will be. That is the reason why inverse filtering the signal is usually not a good solution. However, if we have at least some knowledge of the type of noise in the data (for example, white noise), we may be able to improve the estimate of \"ƒ\" through techniques such as Wiener deconvolution.\n\nDeconvolution is usually performed by computing the Fourier transform of the recorded signal \"h\" and the distortion function (in general terms, it is known as a transfer function) \"g\". Deconvolution is then performed in the frequency domain (in the absence of noise) using:\n\nwhere \"F\", \"G\", and \"H\" are the Fourier transforms of \"f\", \"g\", and \"h\" respectively. Finally, the inverse Fourier transform of the function \"F\" is taken to find the estimated deconvolved signal \"f\".\n\nThe concept of deconvolution had an early application in reflection seismology. In 1950, Enders Robinson was a graduate student at MIT. He worked with others at MIT, such as Norbert Wiener, Norman Levinson, and economist Paul Samuelson, to develop the \"convolutional model\" of a reflection seismogram. This model assumes that the recorded seismogram \"s\"(\"t\") is the convolution of an Earth-reflectivity function \"e\"(\"t\") and a seismic wavelet \"w\"(\"t\") from a point source, where \"t\" represents recording time. Thus, our convolution equation is\n\nThe seismologist is interested in \"e\", which contains information about the Earth's structure. By the convolution theorem, this equation may be Fourier transformed to\n\nin the frequency domain. By assuming that the reflectivity is white, we can assume that the power spectrum of the reflectivity is constant, and that the power spectrum of the seismogram is the spectrum of the wavelet multiplied by that constant. Thus,\n\nIf we assume that the wavelet is minimum phase, we can recover it by calculating the minimum phase equivalent of the power spectrum we just found. The reflectivity may be recovered by designing and applying a Wiener filter that shapes the estimated wavelet to a Dirac delta function (i.e., a spike). The result may be seen as a series of scaled, shifted delta functions (although this is not mathematically rigorous):\n\nwhere \"N\" is the number of reflection events, \"τ\" \"τ\" are the reflection times of each event, and \"r\" are the reflection coefficients.\n\nIn practice, since we are dealing with noisy, finite bandwidth, finite length, discretely sampled datasets, the above procedure only yields an approximation of the filter required to deconvolve the data. However, by formulating the problem as the solution of a Toeplitz matrix and using Levinson recursion, we can relatively quickly estimate a filter with the smallest mean squared error possible. We can also do deconvolution directly in the frequency domain and get similar results. The technique is closely related to linear prediction.\n\nIn optics and imaging, the term \"deconvolution\" is specifically used to refer to the process of reversing the optical distortion that takes place in an optical microscope, electron microscope, telescope, or other imaging instrument, thus creating clearer images. It is usually done in the digital domain by a software algorithm, as part of a suite of microscope image processing techniques. Deconvolution is also practical to sharpen images that suffer from fast motion or jiggles during capturing. Early Hubble Space Telescope images were distorted by a flawed mirror and were sharpened by deconvolution.\n\nThe usual method is to assume that the optical path through the instrument is optically perfect, convolved with a point spread function (PSF), that is, a mathematical function that describes the distortion in terms of the pathway a theoretical point source of light (or other waves) takes through the instrument. Usually, such a point source contributes a small area of fuzziness to the final image. If this function can be determined, it is then a matter of computing its inverse or complementary function, and convolving the acquired image with that. The result is the original, undistorted image.\n\nIn practice, finding the true PSF is impossible, and usually an approximation of it is used, theoretically calculated or based on some experimental estimation by using known probes. Real optics may also have different PSFs at different focal and spatial locations, and the PSF may be non-linear. The accuracy of the approximation of the PSF will dictate the final result. Different algorithms can be employed to give better results, at the price of being more computationally intensive. Since the original convolution discards data, some algorithms use additional data acquired at nearby focal points to make up some of the lost information. Regularization in iterative algorithms (as in expectation-maximization algorithms) can be applied to avoid unrealistic solutions.\n\nWhen the PSF is unknown, it may be possible to deduce it by systematically trying different possible PSFs and assessing whether the image has improved. This procedure is called \"blind deconvolution\". Blind deconvolution is a well-established image restoration technique in astronomy, where the point nature of the objects photographed exposes the PSF thus making it more feasible. It is also used in fluorescence microscopy for image restoration, and in fluorescence spectral imaging for spectral separation of multiple unknown fluorophores. The most common iterative algorithm for the purpose is the Richardson–Lucy deconvolution algorithm; the Wiener deconvolution (and approximations) are the most common non-iterative algorithms.\n\nFor some specific imaging systems such as laser pulsed terahertz systems, PSF can be modeled mathematically. As a result, as shown in the figure, deconvolution of the modeled PSF and the terahertz image can give a higher resolution representation of the terahertz image.\n\nWhen performing image synthesis in radio interferometry, a specific kind of radio astronomy, one step consists of deconvolving the produced image with the \"dirty beam\", which is a different name for the point spread function. A commonly used method is the CLEAN algorithm.\n\nDeconvolution maps to division in the Fourier co-domain. This allows deconvolution to be easily applied with experimental data that are subject to a Fourier transform. An example is NMR spectroscopy where the data are recorded in the time domain, but analyzed in the frequency domain. Division of the time-domain data by an exponential function has the effect of reducing the width of Lorenzian lines in the frequency domain.\n\nDeconvolution has been applied extensively to absorption spectra. The (in German) may be used.\n\n", "related": "\n- Convolution\n- Bit plane\n- Digital filter\n- Filter (signal processing)\n- Filter design\n- Minimum phase\n- Independent component analysis\n- Wiener deconvolution\n- Richardson–Lucy deconvolution\n- Digital room correction\n- Free deconvolution\n- Point spread function\n- Deblurring\n- Unsharp masking\n"}
{"id": "7132704", "url": "https://en.wikipedia.org/wiki?curid=7132704", "title": "Fibre multi-object spectrograph", "text": "Fibre multi-object spectrograph\n\nFibre multi-object spectrograph (FMOS) is facility instrument for the Subaru telescope on Mauna Kea in Hawaii. The instrument consists of a complex fibre-optic positioning system mounted at the prime focus of the telescope. Fibres are then fed to a pair of large spectrographs, each weighing nearly 3000 kg. The instrument will be used to look at the light from up to 400 stars or galaxies simultaneously over a field of view of 30 arcminutes (about the size of the full moon on the sky). The instrument will be used for a number of key programmes, including galaxy formation and evolution and dark energy via a measurement of the rate at which the universe is expanding.\n\nIt is currently being built by a consortium of institutes led by Kyoto University and Oxford University with parts also being manufactured by the Rutherford Appleton Laboratory, Durham University and the Anglo-Australian Observatory. The instrument is scheduled for engineering first-light in late 2008.\n\nThe spectrographs use a technique called OH-suppression to increase the sensitivity of the observations: The incoming light from the fibres is dispersed to a relatively high resolution and this spectrum forms an image on a pair of spherical mirrors which have been etched at the positions corresponding to the bright OH-lines. This spectrum is then re-imaged through a second diffraction grating to allow the full spectrum (without the OH lines) to be imaged onto a single infrared detector.\n\n- FMOS\n- FMOS Project\n", "related": "NONE"}
{"id": "14699765", "url": "https://en.wikipedia.org/wiki?curid=14699765", "title": "Argument (complex analysis)", "text": "Argument (complex analysis)\n\nIn mathematics, the argument is a multi-valued function operating on the nonzero complex numbers. With complex numbers \"z\" visualized as a point in the complex plane, the argument of \"z\" is the angle between the positive real axis and the line joining the point to the origin, shown as in figure 1 and denoted arg \"z\". To define a single-valued function, the principal value of the argument (sometimes denoted Arg \"z\") is used. It is chosen to be the unique value of the argument that lies within the interval (–π, π].\n\nAn argument of the complex number , denoted , is defined in two equivalent ways:\n1. Geometrically, in the complex plane, as the 2D polar angle from the positive real axis to the vector representing . The numeric value is given by the angle in radians and is positive if measured counterclockwise.\n2. Algebraically, as any real quantity such that\n\nThe names \"magnitude,\" for the modulus, and \"phase\", for the argument, are sometimes used equivalently.\n\nUnder both definitions, it can be seen that the argument of any non-zero complex number has many possible values: firstly, as a geometrical angle, it is clear that whole circle rotations do not change the point, so angles differing by an integer multiple of radians (a complete circle) are the same, as reflected by figure 2 on the right. Similarly, from the periodicity of and , the second definition also has this property. The argument of zero is usually left undefined.\n\nBecause a complete rotation around the origin leaves a complex number unchanged, there are many choices which could be made for by circling the origin any number of times. This is shown in figure 2, a representation of the multi-valued (set-valued) function formula_3, where a vertical line (not shown in the figure) cuts the surface at heights representing all the possible choices of angle for that point.\n\nWhen a well-defined function is required then the usual choice, known as the \"principal value\", is the value in the open-closed interval , that is from to radians, excluding rad itself (equivalently from −180 to +180 degrees, excluding −180° itself). This represents an angle of up to half a complete circle from the positive real axis in either direction.\n\nSome authors define the range of the principal value as being in the closed-open interval .\n\nThe principal value sometimes has the initial letter capitalized as in , especially when a general version of the argument is also being considered. Note that notation varies, so and may be interchanged in different texts.\n\nThe set of all possible values of the argument can be written in terms of as:\n\nLikewise\n\nIf a complex number is known in terms of its real and imaginary parts, then the function that calculates the principal value is called the two-argument arctangent function atan2:\nThe atan2 function (also called arctan2 or other synonyms) is available in the math libraries of many programming languages, and usually returns a value in the range .\n\nMany texts say the value is given by , as is slope, and converts slope to angle. This is correct only when , so the quotient is defined and the angle lies between and , but extending this definition to cases where is not positive is relatively involved. Specifically, one may define the principal value of the argument separately on the two half-planes and (separated into two quadrants if one wishes a branch cut on the negative -axis), , , and then patch together.\n\nA compact expression with 4 overlapping half-planes is\n\nFor the variant where is defined to lie in the interval , the value can be found by adding to the value above when it is negative.\n\nAlternatively, the principal value can be calculated in a uniform way using the tangent half-angle formula, the function being defined over the complex plane but excluding the origin:\nThis is based on a parametrization of the circle (except for the negative -axis) by rational functions. This version of is not stable enough for floating point computational use (it may overflow near the region ) but can be used in symbolic calculation.\n\nA variant of the last formula which avoids overflow is sometimes used in high precision computation:\n\nOne of the main motivations for defining the principal value is to be able to write complex numbers in modulus-argument form. Hence for any complex number ,\n\nThis is only really valid if is non-zero but can be considered as valid also for if is considered as being an indeterminate form rather than as being undefined.\n\nSome further identities follow. If and are two non-zero complex numbers, then\n\nIf and is any integer, then\n\nds//arg(z)=tan'|y/x|\n\nFrom formula_16, it easily follows that formula_17. This is useful when one has the complex logarithm available.\n\n- \"Argument\" at Encyclopedia of Mathematics.\n", "related": "NONE"}
{"id": "11206152", "url": "https://en.wikipedia.org/wiki?curid=11206152", "title": "Digital down converter", "text": "Digital down converter\n\nIn digital signal processing, a digital down-converter (DDC) converts a digitized, band limited signal to a lower frequency signal at a lower sampling rate in order to simplify the subsequent radio stages. The process preserves all the information in the original signal less that which is lost to rounding errors in the mathematical processes. The input and output signals can be real or complex samples. Often the DDC converts from the raw radio frequency or intermediate frequency down to a complex baseband signal.\n\nA DDC consists of three subcomponents: a direct digital synthesizer (DDS), a low-pass filter (LPF), and a downsampler (which may be integrated into the low-pass filter).\n\nThe DDS generates a complex sinusoid at the intermediate frequency (IF). Multiplication of the intermediate frequency with the input signal creates images centered at the sum and difference frequency (which follows from the frequency shifting properties of the Fourier transform). The lowpass filters pass the difference (i.e. baseband) frequency while rejecting the sum frequency image, resulting in a complex baseband representation of the original signal. Assuming judicious choice of IF and LPF bandwidth, the complex baseband signal is mathematically equivalent to the original signal. In its new form, it can readily be downsampled and is more convenient to many DSP algorithms.\n\nAny suitable low-pass filter can be used including FIR, IIR and CIC filters. The most common choice is a FIR filter for low amounts of decimation (less than ten) or a CIC filter followed by a FIR filter for larger downsampling ratios.\n\nSeveral variations on the DDC are useful, including many that input a feedback signal into the DDS. These include:\n- Decision directed carrier recovery phase locked loops in which the I and Q are compared to the nearest ideal constellation point of a PSK signal, and the resulting error signal is filtered and fed back into the DDS\n- A Costas loop in which the I and Q are multiplied and low pass filtered as part of a BPSK/QPSK carrier recovery loop\n\nDDCs are most commonly implemented in logic in field-programmable gate arrays or application-specific integrated circuits. While software implementations are also possible, operations in the DDS, multipliers and input stages of the lowpass filters all run at the sampling rate of the input data. This data is commonly taken directly from analog to digital converters (ADCs) sampling at tens or hundreds of MHz.\n\nCORDICs are an alternative to the use of multipliers in the\nimplementation of digital down converters.\n\n- National Instruments RF Resources\n- Xilinx DDC Documentation\n- Altera Designing Digital Down Conversion Systems\n- MATLAB/MathWorks Digital down converter\n- Analog Devices AD6636 DDC Datasheet\n- TI GC5018 8 ch DDC Product Page\n- T.Hollis R.Weir\n", "related": "NONE"}
{"id": "14941864", "url": "https://en.wikipedia.org/wiki?curid=14941864", "title": "Reverberation mapping", "text": "Reverberation mapping\n\nReverberation mapping is an astrophysical technique for measuring the structure of the broad emission-line region (BLR) around a supermassive black hole at the center of an active galaxy, and thus estimating the hole's mass. It is considered a \"primary\" mass estimation technique, i.e., the mass is measured directly from the motion that its gravitational force induces in the nearby gas.\n\nNewton's law of gravity defines a direct relation between the mass of a central object and the speed of a smaller object in orbit around the central mass. Thus, for matter orbiting a black hole, the black hole mass formula_1 is related by the formula\n\nto the RMS velocity Δ\"V\" of gas moving near the black hole in the broad emission-line region, measured from the Doppler broadening of the gaseous emission lines. In that formula, \"R\" is the radius of the broad-line region; \"G\" is the constant of gravitation; and \"f\" is a poorly known \"form factor\" that depends on the shape of the BLR.\n\nWhile Δ\"V\" can be measured directly using spectroscopy, the necessary determination of \"R\" is much less straightforward. This is where reverberation mapping comes into play. It utilizes the fact that the emission-line fluxes vary strongly in response to changes in the continuum, i.e., the light from the accretion disk near the black hole. Put simply, if the brightness of the accretion disk varies, the emission lines, which are excited in response to the accretion disk's light, will \"reverberate\", that is, vary in response. But it will take some time for light from the accretion disk to reach the broad-line region. Thus, the emission-line response is delayed with respect to changes in the continuum. Assuming that this delay is solely due to light travel times, the distance traveled by the light, corresponding to the radius of the broad emission-line region, can be measured.\n\nOnly a small handful of AGN (less than 40) have been accurately \"mapped\" in this way. An alternative approach is to use an empirical correlation between \"R\" and the continuum luminosity.\n\nAnother uncertainty is the value of \"f\". In principle, the response of the BLR to variations in the continuum could be used to map out the three-dimensional structure of the BLR. In practice, the amount and quality of data required to carry out such a deconvolution is prohibitive. Until about 2004, \"f\" was estimated ab initio based on simple models for the structure of the BLR. More recently, the value of \"f\" has been determined so as to bring the M-sigma relation for active galaxies into the best possible agreement with the M–sigma relation for quiescent galaxies. When \"f\" is determined in this way, reverberation mapping becomes a \"secondary\", rather than \"primary,\" mass estimation technique.\n\n", "related": "\n- Supermassive black holes\n\n- Reverberation Mapping ppt-presentation (2005)\n"}
{"id": "15175696", "url": "https://en.wikipedia.org/wiki?curid=15175696", "title": "Signal-flow graph", "text": "Signal-flow graph\n\nA signal-flow graph or signal-flowgraph (SFG), invented by Claude Shannon, but often called a Mason graph after Samuel Jefferson Mason who coined the term, is a specialized flow graph, a directed graph in which nodes represent system variables, and branches (edges, arcs, or arrows) represent functional connections between pairs of nodes. Thus, signal-flow graph theory builds on that of directed graphs (also called digraphs), which includes as well that of oriented graphs. This mathematical theory of digraphs exists, of course, quite apart from its applications.\n\nSFGs are most commonly used to represent signal flow in a physical system and its controller(s), forming a cyber-physical system. Among their other uses are the representation of signal flow in various electronic networks and amplifiers, digital filters, state-variable filters and some other types of analog filters. In nearly all literature, a signal-flow graph is associated with a set of linear equations.\n\nWai-Kai Chen wrote: \"The concept of a signal-flow graph was originally worked out by Shannon [1942] \nin dealing with analog computers. The greatest credit for the formulation of signal-flow graphs is normally extended to Mason [1953], [1956]. He showed how to use the signal-flow graph technique to solve some difficult electronic problems in a relatively simple manner. The term signal flow graph was used because of its original application to electronic problems and the association with electronic signals and flowcharts of the systems under study.\"\n\nLorens wrote: \"Previous to Mason's work, C. E. Shannon worked out a number of the properties of what are now known as flow graphs. Unfortunately, the paper originally had a restricted classification and very few people had access to the material.\"\n\n\"The rules for the evaluation of the graph determinant of a Mason Graph were first given and proven by Shannon [1942] using mathematical induction. His work remained essentially unknown even after Mason published his classical work in 1953. Three years later, Mason [1956] rediscovered the rules and proved them by considering the value of a determinant and how it changes as variables are added to the graph. [...]\"\n\nRobichaud \"et al.\" identify the domain of application of SFGs as follows:\n1. The finite lumped system is composed of a number of simple parts, each of which has known dynamical properties which can be defined by equations using two types of scalar variables and parameters of the system. Variables of the first type represent quantities which can be measured, at least conceptually, by attaching an indicating instrument to two connection points of the element. Variables of the second type characterize quantities which can be measured by connecting a meter in series with the element. Relative velocities and positions, pressure differentials and voltages are typical quantities of the first class, whereas electric currents, forces, rates of heat flow, are variables of the second type. Firestone has been the first to distinguish these two types of variables with the names \"across variables\" and \"through variables\".\n2. Variables of the first type must obey a mesh law, analogous to Kirchhoff's voltage law, whereas variables of the second type must satisfy an incidence law analogous to Kirchhoff's current law.\n3. Physical dimensions of appropriate products of the variables of the two types must be consistent. For the systems in which these conditions are satisfied, it is possible to draw a linear graph isomorphic with the dynamical properties of the system as described by the chosen variables. The techniques [...] can be applied directly to these linear graphs as well as to electrical networks, to obtain a signal flow graph of the system.\"\n\nThe following illustration and its meaning were introduced by Mason to illustrate basic concepts:\n\nIn the simple flow graphs of the figure, a functional dependence of a node is indicated by an incoming arrow, the node originating this influence is the beginning of this arrow, and in its most general form the signal flow graph indicates by incoming arrows only those nodes that influence the processing at the receiving node, and at each node, \"i\", the incoming variables are processed according to a function associated with that node, say \"F\". The flowgraph in (a) represents a set of explicit relationships:\nNode \"x\" is an isolated node because no arrow is incoming; the equations for \"x\" and \"x\" have the graphs shown in parts (b) and (c) of the figure.\n\nThese relationships define for every node a function that processes the input signals it receives. Each non-source node combines the input signals in some manner, and broadcasts a resulting signal along each outgoing branch. \"A flow graph, as defined originally by Mason, implies a set of functional relations, linear or not.\"\n\nHowever, the commonly used Mason graph is more restricted, assuming that each node simply sums its incoming arrows, and that each branch involves only the initiating node involved. Thus, in this more restrictive approach, the node \"x\" is unaffected while:\n\nand now the functions \"f\" can be associated with the signal-flow branches \"ij\" joining the pair of nodes \"x, x\", rather than having general relationships associated with each node. A contribution by a node to itself like \"f\" for \"x\" is called a \"self-loop\". Frequently these functions are simply multiplicative factors (often called \"transmittances\" or \"gains\"), for example, \"f(x)=cx\", where \"c\" is a scalar, but possibly a function of some parameter like the Laplace transform variable \"s\". Signal-flow graphs are very often used with Laplace-transformed signals, and in this case the transmittance, \"c(s)\", often is called a transfer function.\n\nRobichaud et al. wrote: \"The signal flow graph contains the same information as the equations from which it is derived; but there does not exist a one-to-one correspondence between the graph and the system of equations. One system will give different graphs according to the order in which the equations are used to define the variable written on the left-hand side.\" If all equations relate all dependent variables, then there are \"n!\" possible SFGs to choose from.\n\nLinear signal-flow graph (SFG) methods only apply to linear time-invariant systems, as studied by their associated theory. When modeling a system of interest, the first step is often to determine the equations representing the system's operation without assigning causes and effects (this is called acausal modeling). A SFG is then derived from this system of equations.\n\nA linear SFG consists of nodes indicated by dots and weighted directional branches indicated by arrows. The nodes are the variables of the equations and the branch weights are the coefficients. Signals may only traverse a branch in the direction indicated by its arrow. The elements of a SFG can only represent the operations of multiplication by a coefficient and addition, which are sufficient to represent the constrained equations. When a signal traverses a branch in its indicated direction, the signal is multiplied the weight of the branch. When two or more branches direct into the same node, their outputs are added.\n\nFor systems described by linear algebraic or differential equations, the signal-flow graph is mathematically equivalent to the system of equations describing the system, and the equations governing the nodes are discovered for each node by summing incoming branches to that node. These incoming branches convey the contributions of the other nodes, expressed as the connected node value multiplied by the weight of the connecting branch, usually a real number or function of some parameter (for example a Laplace transform variable \"s\").\n\nFor linear active networks, Choma writes: \"By a 'signal flow representation' [or 'graph', as it is commonly referred to] we mean a diagram that, by displaying the algebraic relationships among relevant branch variables of network, paints an unambiguous picture of the way an applied input signal ‘flows’ from input-to-output ... ports.\"\n\nA motivation for a SFG analysis is described by Chen:\n\nA linear signal flow graph is related to a system of linear equations of the following form:\n\nThe figure to the right depicts various elements and constructs of a signal flow graph (SFG).\n\nTerms used in linear SFG theory also include:\n\n- Path. A path is a continuous set of branches traversed in the direction indicated by the branch arrows.\n- Open path. If no node is re-visited, the path is open.\n- Forward path. A path from an input node (source) to an output node (sink) that does not re-visit any node.\n- Path gain: the product of the gains of all the branches in the path.\n- Loop. A closed path. (it originates and ends on the same node, and no node is touched more than once).\n- Loop gain: the product of the gains of all the branches in the loop.\n- Non-touching loops. Non-touching loops have no common nodes.\n- Graph reduction. Removal of one or more nodes from a graph using graph transformations.\n- Residual node. In any contemplated process of graph reduction, the nodes to be retained in the new graph are called residual nodes.\n- Splitting a node. Splitting a node corresponds to splitting a node into two half nodes, one being a sink and the other a source.\n- Index: The index of a graph is the minimum number of nodes which have to be split in order to remove all the loops in a graph.\n- Index node. The nodes that are split to determine the index of a graph are called \"index\" nodes, and in general they are not unique.\n\nA signal-flow graph may be simplified by graph transformation rules. These simplification rules are also referred to as \"signal-flow graph algebra\".\nThe purpose of this reduction is to relate the dependent variables of interest (residual nodes, sinks) to its independent variables (sources).\n\nThe systematic reduction of a linear signal-flow graph is a graphical method equivalent to the Gauss-Jordan elimination method for solving linear equations.\n\nThe rules presented below may be applied over and over until the signal flow graph is reduced to its \"minimal residual form\". Further reduction can require loop elimination or the use of a \"reduction formula\" with the goal to directly connect sink nodes representing the dependent variables to the source nodes representing the independent variables. By these means, any signal-flow graph can be simplified by successively removing internal nodes until only the input and output and index nodes remain. Robichaud described this process of systematic flow-graph reduction:\n\nFor digitally reducing a flow graph using an algorithm, Robichaud extends the notion of a simple flow graph to a \"generalized\" flow graph:\nThe definition of an elementary transformation varies from author to author:\n- Some authors only consider as elementary transformations the summation of parallel-edge gains and the multiplication of series-edge gains, but not the elimination of self-loops\n- Other authors consider the elimination of a self-loop as an elementary transformation\n\nParallel edges. Replace parallel edges with a single edge having a gain equal to the sum of original gains.\n\nThe graph on the left has parallel edges between nodes. On the right, these parallel edges have been replaced with a single edge having a gain equal to the sum of the gains on each original edge.\nThe equations corresponding to the reduction between N and node I are:\n\nOutflowing edges. Replace outflowing edges with edges directly flowing from the node's sources.\n\nThe graph on the left has an intermediate node N between nodes from which it has inflows, and nodes to which it flows out.\nThe graph on the right shows direct flows between these node sets, without transiting via N.\nFor the sake of simplicity, N and its inflows are not represented. The outflows from N are eliminated.\nThe equations corresponding to the reduction directly relating N's input signals to its output signals are:\n\nZero-signal nodes.\nEliminate outflowing edges from a node determined to have a value of zero.\n\nIf the value of a node is zero, its outflowing edges can be eliminated.\n\nNodes without outflows.\nEliminate a node without outflows.\n\nIn this case, N is not a variable of interest, and it has no outgoing edges; therefore, N, and its inflowing edges, can be eliminated.\n\nSelf-looping edge. Replace looping edges by adjusting the gains on the incoming edges.\nThe graph on the left has a looping edge at node N, with a gain of g. On the right, the looping edge has been eliminated, and all inflowing edges have their gain divided by (1-g).The equations corresponding to the reduction between N and all its input signals are:\n\nThe above procedure for building the SFG from an acausal system of equations and for solving the SFG's gains have been implemented as an add-on to MATHLAB 68, an on-line system providing machine aid for the mechanical symbolic processes encountered in analysis.\n\nSignal flow graphs can be used to solve sets of simultaneous linear equations. The set of equations must be consistent and all equations must be linearly independent.\n\nFor M equations with N unknowns where each y is a known value and each x is an unknown value, there is equation for each known of the following form.\n\nAlthough it is feasible, particularly for simple cases, to establish a signal flow graph using the equations in this form, some rearrangement allows a general procedure that works easily for any set of equations, as now is presented. To proceed, first the equations are rewritten as\n\nand further rewritten as\n\nand finally rewritten as \n\nThe signal-flow graph is now arranged by selecting one of these equations and addressing the node on the right-hand side. This is the node for which the node connects to itself with the branch of weight including a '+1', making a \"self-loop\" in the flow graph. The other terms in that equation connect this node first to the source in this equation and then to all the other branches incident on this node. Every equation is treated this way, and then each incident branch is joined to its respective emanating node. For example, the case of three variables is shown in the figure, and the first equation is:\n\nwhere the right side of this equation is the sum of the weighted arrows incident on node \"x\".\n\nAs there is a basic symmetry in the treatment of every node, a simple starting point is an arrangement of nodes with each node at one vertex of a regular polygon. When expressed using the general coefficients {\"c\"}, the environment of each node is then just like all the rest apart from a permutation of indices. Such an implementation for a set of three simultaneous equations is seen in the figure.\n\nOften the known values, y are taken as the primary causes and the unknowns values, x to be effects, but regardless of this interpretation, the last form for the set of equations can be represented as a signal-flow graph. This point is discussed further in the subsection Interpreting 'causality'.\n\nIn the most general case, the values for all the x variables can be calculated by computing Mason's gain formula for the path from each y to each x and using superposition.\n\nIn general, there are N-1 paths from y to variable x so the computational effort to calculated G is proportional to N-1.\nSince there are M values of y, G must be computed M times for a single value of x. The computational effort to calculate a single x variable is proportional to (N-1)(M). The effort to compute all the x variables is proportional to (N)(N-1)(M). If there are N equations and N unknowns, then the computation effort is on the order of N.\n\nFor some authors, a linear signal-flow graph is more constrained than a block diagram, in that the SFG rigorously describes linear algebraic equations represented by a directed graph.\n\nFor other authors, linear block diagrams and linear signal-flow graphs are equivalent ways of depicting a system, and either can be used to solve the gain.\n\nA tabulation of the comparison between block diagrams and signal-flow graphs is provided by Bakshi & Bakshi, and another tabulation by Kumar. According to Barker \"et al.\":\n\nIn the figure, a simple block diagram for a feedback system is shown with two possible interpretations as a signal-flow graph. The input \"R(s)\" is the Laplace-transformed input signal; it is shown as a source node in the signal-flow graph (a source node has no input edges). The output signal \"C(s)\" is the Laplace-transformed output variable. It is represented as a sink node in the flow diagram (a sink has no output edges). \"G(s)\" and \"H(s)\" are transfer functions, with \"H(s)\" serving to feed back a modified version of the output to the input, \"B(s)\". The two flow graph representations are equivalent.\n\nThe term \"cause and effect\" was applied by Mason to SFGs: \n\nand has been repeated by many later authors:\n\nHowever, Mason's paper is concerned to show in great detail how a \"set of equations\" is connected to an SFG, an emphasis unrelated to intuitive notions of \"cause and effect\". Intuitions can be helpful for arriving at an SFG or for gaining insight from an SFG, but are inessential to the SFG. The essential connection of the SFG is to its own set of equations, as described, for example, by Ogata:\n\nThere is no reference to \"cause and effect\" here, and as said by Barutsky:\n\nThe term \"cause and effect\" may be misinterpreted as it applies to the SFG, and taken incorrectly to suggest a system view of causality, rather than a \"computationally\" based meaning. To keep discussion clear, it may be advisable to use the term \"computational causality\", as is suggested for bond graphs:\n\nThe term \"computational causality\" is explained using the example of current and voltage in a resistor:\n\nA computer program or algorithm can be arranged to solve a set of equations using various strategies. They differ in how they prioritize finding some of the variables in terms of the others, and these algorithmic decisions, which are simply about solution strategy, then set up the variables expressed as dependent variables earlier in the solution to be \"effects\", determined by the remaining variables that now are \"causes\", in the sense of \"computational causality\".\n\nUsing this terminology, it is \"computational\" causality, not \"system\" causality, that is relevant to the SFG. There exists a wide-ranging philosophical debate, not concerned specifically with the SFG, over connections between computational causality and system causality.\n\nSignal-flow graphs can be used for analysis, that is for understanding a model of an existing system, or for synthesis, that is for determining the properties of a design alternative.\n\nWhen building a model of a dynamic system, a list of steps is provided by Dorf & Bishop:\n- Define the system and its components.\n- Formulate the mathematical model and list the needed assumptions.\n- Write the differential equations describing the model.\n- Solve the equations for the desired output variables.\n- Examine the solutions and the assumptions.\n- If needed, reanalyze or redesign the system.\nIn this workflow, equations of the physical system's mathematical model are used to derive the signal-flow graph equations.\n\nSignal-flow graphs have been used in Design Space Exploration (DSE), as an intermediate representation towards a physical implementation. The DSE process seeks a suitable solution among different alternatives. In contrast with the typical analysis workflow, where a system of interest is first modeled with the physical equations of its components, the specification for synthesizing a design could be a desired transfer function. For example, different strategies would create different signal-flow graphs, from which implementations are derived.\nAnother example uses an annotated SFG as an expression of the continuous-time behavior, as input to an architecture generator\n\nShannon's formula is an analytic expression for calculating the gain of an interconnected set of amplifiers in an analog computer. During World War II, while investigating the functional operation of an analog computer, Claude Shannon developed his formula. Because of wartime restrictions, Shannon's work was not published at that time, and, in 1952, Mason rediscovered the same formula.\n\nHapp generalized the Shannon formula for topologically closed systems. The Shannon-Happ formula can be used for deriving transfer functions, sensitivities, and error functions.\n\nFor a consistent set of linear unilateral relations, the Shannon-Happ formula expresses the solution using direct substitution (non-iterative).\n\nNASA's electrical circuit software NASAP is based on the Shannon-Happ formula.\n\nThe amplification of a signal \"V\" by an amplifier with gain \"a\" is described mathematically by\n\nThis relationship represented by the signal-flow graph of Figure 1. is that V is dependent on V but it implies no dependency of V on V. See Kou page 57.\n\nA possible SFG for the asymptotic gain model for a negative feedback amplifier is shown in Figure 3, and leads to the equation for the gain of this amplifier as\n\nThe interpretation of the parameters is as follows: \"T\" = return ratio, \"G\" = direct amplifier gain, \"G\" = feedforward (indicating the possible bilateral nature of the feedback, possibly deliberate as in the case of feedforward compensation). Figure 3 has the interesting aspect that it resembles Figure 2 for the two-port network with the addition of the extra \"feedback relation\" \"x = T y\".\n\nFrom this gain expression an interpretation of the parameters \"G\" and \"G\" is evident, namely:\n\nThere are many possible SFG's associated with any particular gain relation. Figure 4 shows another SFG for the asymptotic gain model that can be easier to interpret in terms of a circuit. In this graph, parameter β is interpreted as a feedback factor and \"A\" as a \"control parameter\", possibly related to a dependent source in the circuit. Using this graph, the gain is\n\nTo connect to the asymptotic gain model, parameters \"A\" and β cannot be arbitrary circuit parameters, but must relate to the return ratio \"T\" by:\n\nand to the asymptotic gain as:\n\nSubstituting these results into the gain expression,\n\nwhich is the formula of the asymptotic gain model.\n\nThe figure to the right depicts a circuit that contains a \"y\"-parameter two-port network. V is the input of the circuit and V is the output. The two-port equations impose a set of linear constraints between its port voltages and currents. The terminal equations impose other constraints. All these constraints are represented in the SFG (Signal Flow Graph) below the circuit. There is only one path from input to output which is shown in a different color and has a (voltage) gain of -Ry. There are also three loops: -Ry, -Ry, RyRy. Sometimes a loop indicates intentional feedback but it can also indicate a constraint on the relationship of two variables. For example, the equation that describes a resistor says that the ratio of the voltage across the resistor to the current through the resistor is a constant which is called the resistance. This can be interpreted as the voltage is the input and the current is the output, or the current is the input and the voltage is the output, or merely that the voltage and current have a linear relationship. Virtually all passive two terminal devices in a circuit will show up in the SFG as a loop.\n\nThe SFG and the schematic depict the same circuit, but the schematic also suggests the circuit's purpose. Compared to the schematic, the SFG is awkward but it does have the advantage that the input to output gain can be written down by inspection using Mason's rule.\n\nThis example is representative of a SFG (signal-flow graph) used to represent a servo control system and illustrates several features of SFGs. Some of the loops (loop 3, loop 4 and loop 5) are extrinsic intentionally designed feedback loops. These are shown with dotted lines. There are also intrinsic loops (loop 0, loop1, loop2) that are not intentional feedback loops, although they can be analyzed as though they were. These loops are shown with solid lines. Loop 3 and loop 4 are also known as minor loops because they are inside a larger loop.\n\n- The forward path begins with θ, the desired position command. This is multiplied by K which could be a constant or a function of frequency. K incorporates the conversion gain of the DAC and any filtering on the DAC output. The output of K is the velocity command V which is multiplied by K which can be a constant or a function of frequency. The output of K is the current command, V which is multiplied by K which can be a constant or a function of frequency. The output of K is the amplifier output voltage, V. The current, I, though the motor winding is the integral of the voltage applied to the inductance. The motor produces a torque, T, proportional to I. Permanent magnet motors tend to have a linear current to torque function. The conversion constant of current to torque is K. The torque, T, divided by the load moment of inertia, M, is the acceleration, α, which is integrated to give the load velocity ω which is integrated to produce the load position, θ.\n- The forward path of loop 0 asserts that acceleration is proportional to torque and the velocity is the time integral of acceleration. The backward path says that as the speed increases there is a friction or drag that counteracts the torque. Torque on the load decreases proportionately to the load velocity until the point is reached that all the torque is used to overcome friction and the acceleration drops to zero. Loop 0 is intrinsic.\n- Loop1 represents the interaction of an inductor's current with its internal and external series resistance. The current through an inductance is the time integral of the voltage across the inductance. When a voltage is first applied, all of it appears across the inductor. This is shown by the forward path through formula_41. As the current increases, voltage is dropped across the inductor internal resistance R and the external resistance R. This reduces the voltage across the inductor and is represented by the feedback path -(R + R). The current continues to increase but at a steadily decreasing rate until the current reaches the point at which all the voltage is dropped across (R + R). Loop 1 is intrinsic.\n- Loop2 expresses the effect of the motor back EMF. Whenever a permanent magnet motor rotates, it acts like a generator and produces a voltage in its windings. It does not matter whether the rotation is caused by a torque applied to the drive shaft or by current applied to the windings. This voltage is referred to as back EMF. The conversion gain of rotational velocity to back EMF is G. The polarity of the back EMF is such that it diminishes the voltage across the winding inductance. Loop 2 is intrinsic.\n- Loop 3 is extrinsic. The current in the motor winding passes through a sense resister. The voltage, V, developed across the sense resister is fed back to the negative terminal of the power amplifier K. This feedback causes the voltage amplifier to act like a voltage controlled current source. Since the motor torque is proportional to motor current, the sub-system V to the output torque acts like a voltage controlled torque source. This sub-system may be referred to as the \"current loop\" or \"torque loop\". Loop 3 effectively diminishes the effects of loop 1 and loop 2.\n- Loop 4 is extrinsic. A tachometer (actually a low power dc generator) produces an output voltage V that is proportional to is angular velocity. This voltage is fed to the negative input of K. This feedback causes the sub-system from V to the load angular velocity to act like a voltage to velocity source. This sub-system may be referred to as the \"velocity loop\". Loop 4 effectively diminishes the effects of loop 0 and loop 3.\n- Loop 5 is extrinsic. This is the overall position feedback loop. The feedback comes from an angle encoder that produces a digital output. The output position is subtracted from the desired position by digital hardware which drives a DAC which drives K. In the SFG, the conversion gain of the DAC is incorporated into K.\n\nSee Mason's rule for development of Mason's Gain Formula for this example.\n\nThere is some confusion in literature about what a signal-flow graph is; Henry Paynter, inventor of bond graphs, writes: \"But much of the decline of signal-flow graphs [...] is due in part to the mistaken notion that the branches must be linear and the nodes must be summative. Neither assumption was embraced by Mason, himself !\"\n\n- IEEE Std 155-1960, IEEE Standards on Circuits: Definitions of Terms for Linear Signal Flow Graphs, 1960.\n\nA state transition SFG or state diagram is a simulation diagram for a system of equations, including the initial conditions of the states.\n\n Closed flowgraphs describe closed systems and have been utilized to provide a rigorous theoretical basis for topological techniques of circuit analysis.\n- Terminology for closed flowgraph theory includes:\n- Contributive node. Summing point for two or more incoming signals resulting in only one outgoing signal.\n- Distributive node. Sampling point for two or more outgoing signals resulting from only one incoming signal.\n- Compound node. Contraction of a contributive node and a distributive node.\n- Strictly dependent & strictly independent node. A strictly independent node represent s an independent source; a strictly dependent node represents a meter.\n- Open & Closed Flowgraphs. An open flowgraph contains strictly dependent or strictly independent nodes; otherwise it is a closed flowgraph.\n\nMason introduced both nonlinear and linear flow graphs. To clarify this point, Mason wrote : \"A linear flow graph is one whose associated equations are linear.\"\n\nIt we denote by x the signal at node j, the following are examples of node functions that do not pertain to a linear time-invariant system:\n\n- Although they generally can't be transformed between time domain and frequency domain representations for classical control theory analysis, nonlinear signal-flow graphs can be found in electrical engineering literature.\n- Nonlinear signal-flow graphs can also be found in life sciences, for example, Dr Arthur Guyton's model of the cardiovascular system.\n\n- Electronic circuits\n- Characterizing sequential circuits of the Moore and Mealy type, obtaining regular expressions from state diagrams.\n- Synthesis of non-linear data converters\n- Control and network theory\n- Stochastic signal processing.\n- Reliability of electronic systems\n- Physiology and biophysics\n- Cardiac output regulation\n- Simulation\n- Simulation on analog computers\n\n", "related": "\n- Asymptotic gain model\n- Bond graphs\n- Coates graph\n- in the Control Systems Wikibook\n- Flow graph (mathematics)\n- Leapfrog filter for an example of filter design using a signal flow graph\n- Mason's gain formula\n- Minor loop feedback\n- Noncommutative signal-flow graph\n\n- Book almost entirely devoted to this topic.\n- © Copyright by Khoman Phang 2001\n\n- Chapter 3 for the essentials, but applications are scattered throughout the book.\n- Compares Mason and Coates graph approaches with Maxwell's k-tree approach.\n- A comparison of the utility of the Coates flow graph and the Mason flow graph.\n\n- M. L. Edwards: \"S-parameters, signal flow graphs, and other matrix representations\" All Rights Reserved\n- H Schmid: \"Signal-Flow Graphs in 12 Short Lessons\"\n"}
{"id": "15197395", "url": "https://en.wikipedia.org/wiki?curid=15197395", "title": "Return ratio", "text": "Return ratio\n\nThe return ratio of a dependent source in a linear electrical circuit is the \"negative\" of the ratio of \"the current (voltage) returned to the site of the dependent source\" to \"the current (voltage) of a replacement independent source\". The terms \"loop gain\" and \"return ratio\" are often used interchangeably; however, they are necessarily equivalent only in the case of a single feedback loop system with unilateral blocks.\n\nThe steps for calculating the return ratio of a source are as follows:\n1. Set all independent sources to zero.\n2. Select the dependent source for which the return ratio is sought.\n3. Place an independent source of the same type (voltage or current) and polarity in parallel with the selected dependent source.\n4. Move the dependent source to the side of the inserted source and cut the two leads joining the dependent source to the independent source.\n5. For a voltage source the return ratio is minus the ratio of the voltage across the dependent source divided by the voltage of the independent replacement source.\n6. For a current source, short-circuit the broken leads of the dependent source. The return ratio is minus the ratio of the resulting short-circuit current to the current of the independent replacement source.\n\nThese steps may not be feasible when the dependent sources inside the devices are not directly accessible, for example when using built-in \"black box\" SPICE models or when measuring the return ratio experimentally.\nFor SPICE simulations, one potential workaround is to manually replace non-linear devices by their small-signal equivalent model, with exposed dependent sources. However this will have to be redone if the bias point changes.\n\nA result by Rosenstark shows that return ratio can be calculated by breaking the loop at any unilateral point in the circuit. The problem is now finding how to break the loop without affecting the bias point and altering the results. Middlebrook and Rosenstark have proposed several methods for experimental evaluation of return ratio (loosely referred to by these authors as simply \"loop gain\"), and similar methods have been adapted for use in SPICE by Hurst. See Spectrum user note or Roberts, or Sedra, and especially Tuinenga.\n\nFigure 1 (top right) shows a bipolar amplifier with feedback bias resistor \"R\" driven by a Norton signal source. Figure 2 (left panel) shows the corresponding small-signal circuit obtained by replacing the transistor with its hybrid-pi model. The objective is to find the return ratio of the dependent current source in this amplifier. To reach the objective, the steps outlined above are followed. Figure 2 (center panel) shows the application of these steps up to Step 4, with the dependent source moved to the left of the inserted source of value \"i\", and the leads targeted for cutting marked with an \"x\". Figure 2 (right panel) shows the circuit set up for calculation of the return ratio \"T\", which is\n\nThe return current is\n\nThe feedback current in \"R\" is found by current division to be:\n\nThe base-emitter voltage \"v\" is then, from Ohm's law:\n\nConsequently,\n\nThe overall transresistance gain of this amplifier can be shown to be:\n\nwith \"R = R || r\" and \"R = R || r\".\n\nThis expression can be rewritten in the form used by the asymptotic gain model, which expresses the overall gain of a feedback amplifier in terms of several independent factors that are often more easily derived separately than the overall gain itself, and that often provide insight into the circuit. This form is:\n\nwhere the so-called asymptotic gain \"G\" is the gain at infinite \"g\", namely:\n\nand the so-called feed forward or direct feedthrough \"G\" is the gain for zero \"g\", namely:\n\nFor additional applications of this method, see asymptotic gain model and Blackman's theorem.\n\n", "related": "\n- Asymptotic gain model\n- Blackman's theorem\n- Extra element theorem\n"}
{"id": "997021", "url": "https://en.wikipedia.org/wiki?curid=997021", "title": "Asymptotic gain model", "text": "Asymptotic gain model\n\nThe asymptotic gain model (also known as the Rosenstark method) is a representation of the gain of negative feedback amplifiers given by the asymptotic gain relation:\nwhere formula_2 is the return ratio with the input source disabled (equal to the negative of the loop gain in the case of a single-loop system composed of unilateral blocks), \"G\" is the asymptotic gain and \"G\" is the direct transmission term. This form for the gain can provide intuitive insight into the circuit and often is easier to derive than a direct attack on the gain.\n\nFigure 1 shows a block diagram that leads to the asymptotic gain expression. The asymptotic gain relation also can be expressed as a signal flow graph. See Figure 2. The asymptotic gain model is a special case of the extra element theorem.\nAs follows directly from limiting cases of the gain expression, the asymptotic gain \"G\" is simply the gain of the system when the return ratio approaches infinity:\n\nwhile the direct transmission term \"G\" is the gain of the system when the return ratio is zero:\n\n- This model is useful because it completely characterizes feedback amplifiers, including loading effects and the bilateral properties of amplifiers and feedback networks.\n- Often feedback amplifiers are designed such that the return ratio \"T\" is much greater than unity. In this case, and assuming the direct transmission term \"G\" is small (as it often is), the gain \"G\" of the system is approximately equal to the asymptotic gain \"G\".\n- The asymptotic gain is (usually) only a function of passive elements in a circuit, and can often be found by inspection.\n- The feedback topology (series-series, series-shunt, etc.) need not be identified beforehand as the analysis is the same in all cases.\n\nDirect application of the model involves these steps:\n1. Select a dependent source in the circuit.\n2. Find the return ratio for that source.\n3. Find the gain \"G\" directly from the circuit by replacing the circuit with one corresponding to \"T\" = ∞.\n4. Find the gain \" G\" directly from the circuit by replacing the circuit with one corresponding to \"T\" = 0.\n5. Substitute the values for \"T, G\" and \" G\" into the asymptotic gain formula.\n\nThese steps can be implemented directly in SPICE using the small-signal circuit of hand analysis. In this approach the dependent sources of the devices are readily accessed. In contrast, for experimental measurements using real devices or SPICE simulations using numerically generated device models with inaccessible dependent sources, evaluating the return ratio requires special methods.\n\nClassical feedback theory neglects feedforward (\"G\"). If feedforward is dropped, the gain from the asymptotic gain model becomes\n\nwhile in classical feedback theory, in terms of the open loop gain \"A\", the gain with feedback (closed loop gain) is:\n\nComparison of the two expressions indicates the feedback factor \"β\" is:\n\nwhile the open-loop gain is:\n\nIf the accuracy is adequate (usually it is), these formulas suggest an alternative evaluation of \"T\": evaluate the open-loop gain and \"G\" and use these expressions to find \"T\". Often these two evaluations are easier than evaluation of \"T\" directly.\n\nThe steps in deriving the gain using the asymptotic gain formula are outlined below for two negative feedback amplifiers. The single transistor example shows how the method works in principle for a transconductance amplifier, while the second two-transistor example shows the approach to more complex cases using a current amplifier.\n\nConsider the simple FET feedback amplifier in Figure 3. The aim is to find the low-frequency, open-circuit, transresistance gain of this circuit \"G\" = \"v\" / \"i\" using the asymptotic gain model.\n\nThe small-signal equivalent circuit is shown in Figure 4, where the transistor is replaced by its hybrid-pi model.\n\nIt is most straightforward to begin by finding the return ratio \"T\", because \"G\" and \"G\" are defined as limiting forms of the gain as \"T\" tends to either zero or infinity. To take these limits, it is necessary to know what parameters \"T\" depends upon. There is only one dependent source in this circuit, so as a starting point the return ratio related to this source is determined as outlined in the article on return ratio.\n\nThe return ratio is found using Figure 5. In Figure 5, the input current source is set to zero, By cutting the dependent source out of the output side of the circuit, and short-circuiting its terminals, the output side of the circuit is isolated from the input and the feedback loop is broken. A test current \"i\" replaces the dependent source. Then the return current generated in the dependent source by the test current is found. The return ratio is then \"T\" = −\"i / i\". Using this method, and noticing that \"R\" is in parallel with \"r\", \"T\" is determined as:\nwhere the approximation is accurate in the common case where \"r\" » \"R\". With this relationship it is clear that the limits \"T\" → 0, or ∞ are realized if we let transconductance \"g\" → 0, or ∞.\n\nFinding the asymptotic gain \"G\" provides insight, and usually can be done by inspection. To find \"G\" we let \"g\" → ∞ and find the resulting gain. The drain current, \"i\" = \"g\" \"v\", must be finite. Hence, as \"g\" approaches infinity, \"v\" also must approach zero. As the source is grounded, \"v\" = 0 implies \"v\" = 0 as well. With \"v\" = 0 and the fact that all the input current flows through \"R\" (as the FET has an infinite input impedance), the output voltage is simply −\"i\" \"R\". Hence\n\nAlternatively \"G\" is the gain found by replacing the transistor by an ideal amplifier with infinite gain - a nullor.\n\nTo find the direct feedthrough formula_11 we simply let \"g\" → 0 and compute the resulting gain. The currents through \"R\" and the parallel combination of \"R\" || \"r\" must therefore be the same and equal to \"i\". The output voltage is therefore \"i\" \"(R\" \"|| r\"\")\".\n\nHence\n\nwhere the approximation is accurate in the common case where \"r\" » \"R\".\n\nThe overall transresistance gain of this amplifier is therefore:\n\nExamining this equation, it appears to be advantageous to make \"R\" large in order make the overall gain approach the asymptotic gain, which makes the gain insensitive to amplifier parameters (\"g\" and \"R\"). In addition, a large first term reduces the importance of the direct feedthrough factor, which degrades the amplifier. One way to increase \"R\" is to replace this resistor by an active load, for example, a current mirror.\n\nFigure 6 shows a two-transistor amplifier with a feedback resistor \"R\". This amplifier is often referred to as a \"shunt-series feedback\" amplifier, and analyzed on the basis that resistor \"R\" is in series with the output and samples output current, while \"R\" is in shunt (parallel) with the input and subtracts from the input current. See the article on negative feedback amplifier and references by Meyer or Sedra. That is, the amplifier uses current feedback. It frequently is ambiguous just what type of feedback is involved in an amplifier, and the asymptotic gain approach has the advantage/disadvantage that it works whether or not you understand the circuit.\n\nFigure 6 indicates the output node, but does not indicate the choice of output variable. In what follows, the output variable is selected as the short-circuit current of the amplifier, that is, the collector current of the output transistor. Other choices for output are discussed later.\n\nTo implement the asymptotic gain model, the dependent source associated with either transistor can be used. Here the first transistor is chosen.\n\nThe circuit to determine the return ratio is shown in the top panel of Figure 7. Labels show the currents in the various branches as found using a combination of Ohm's law and Kirchhoff's laws. Resistor \"R\" \"= R\" \"// r\" and \"R\" \"= R\" \"// R\". KVL from the ground of \"R\" to the ground of \"R\" provides:\n\nKVL provides the collector voltage at the top of \"R\" as\n\nFinally, KCL at this collector provides\n\nSubstituting the first equation into the second and the second into the third, the return ratio is found as\n\nThe circuit to determine \"G\" is shown in the center panel of Figure 7. In Figure 7, the output variable is the output current β\"i\" (the short-circuit load current), which leads to the short-circuit current gain of the amplifier, namely β\"i\" / \"i\":\n\nUsing Ohm's law, the voltage at the top of \"R\" is found as\n\nor, rearranging terms,\n\nUsing KCL at the top of \"R\":\n\nEmitter voltage \"v\" already is known in terms of \"i\" from the diagram of Figure 7. Substituting the second equation in the first, \"i\" is determined in terms of \"i\" alone, and \"G\" becomes:\n\nGain \"G\" represents feedforward through the feedback network, and commonly is negligible.\n\nThe circuit to determine \"G\" is shown in the bottom panel of Figure 7. The introduction of the ideal op amp (a nullor) in this circuit is explained as follows. When \"T \"→ ∞, the gain of the amplifier goes to infinity as well, and in such a case the differential voltage driving the amplifier (the voltage across the input transistor \"r\") is driven to zero and (according to Ohm's law when there is no voltage) it draws no input current. On the other hand, the output current and output voltage are whatever the circuit demands. This behavior is like a nullor, so a nullor can be introduced to represent the infinite gain transistor.\n\nThe current gain is read directly off the schematic:\n\nUsing the classical model, the feed-forward is neglected and the feedback factor β is (assuming transistor β » 1):\n\nand the open-loop gain \"A\" is:\n\nThe above expressions can be substituted into the asymptotic gain model equation to find the overall gain G. The resulting gain is the \"current\" gain of the amplifier with a short-circuit load.\n\nIn the amplifier of Figure 6, \"R\" and \"R\" are in parallel.\nTo obtain the transresistance gain, say \"A\", that is, the gain using voltage as output variable, the short-circuit current gain \"G\" is multiplied by \"R // R\" in accordance with Ohm's law:\n\nThe \"open-circuit\" voltage gain is found from \"A\" by setting \"R\" → ∞.\n\nTo obtain the current gain when load current \"i\" in load resistor \"R\" is the output variable, say \"A\", the formula for current division is used: \"i = i × R / ( R + R )\" and the short-circuit current gain \"G\" is multiplied by this loading factor:\n\nOf course, the short-circuit current gain is recovered by setting \"R\" = 0 Ω.\n\n", "related": "\n- Blackman's theorem\n- Extra element theorem\n- Mason's gain formula\n- Feedback amplifiers\n- Return ratio\n- Signal-flow graph\n\n- Lecture notes on the asymptotic gain model\n"}
{"id": "11178898", "url": "https://en.wikipedia.org/wiki?curid=11178898", "title": "Nullator", "text": "Nullator\n\nIn electronics, a nullator is a theoretical linear, time-invariant one-port \"defined\" as having zero current and voltage across its terminals. Nullators are strange in the sense that they simultaneously have properties of both a short (zero voltage) and an open circuit (zero current). They are neither current nor voltage sources, yet both at the same time.\n\nInserting a nullator in a circuit schematic imposes a \"mathematical constraint\" on how that circuit must behave, forcing the circuit itself to adopt whatever arrangements needed to meet the condition. For example, the inputs of an ideal operational amplifier (with negative feedback) behave like a nullator, as they draw no current and have no voltage across them, and these conditions are used to analyze the circuitry surrounding the operational amplifier.\n\nA nullator is normally paired with a norator to form a nullor.\n\nTwo trivial cases are worth noting: A nullator in parallel with a norator is equivalent to a short (zero voltage any current) and a nullator in series with a norator is an open circuit (zero current, any voltage).\n\n- Nullator article from Analog Insydes reference\n", "related": "NONE"}
{"id": "11179042", "url": "https://en.wikipedia.org/wiki?curid=11179042", "title": "Norator", "text": "Norator\n\nIn electronics, a norator is a theoretical linear, time-invariant one-port which can have an arbitrary current and voltage between its terminals. A norator represents a controlled voltage or current source with infinite gain.\n\nInserting a norator in a circuit schematic provides whatever current and voltage the outside circuit demands, in particular, the demands of Kirchhoff's circuit laws. For example, the output of an ideal opamp behaves as a norator, producing nonzero output voltage and current that meet circuit requirements despite a zero input.\n\nA norator is often paired with a nullator to form a nullor.\n\nTwo trivial cases are worth noting: A nullator in parallel with a norator is equivalent to a short (zero voltage any current) and a nullator in series with a norator is an open circuit (zero current, any voltage).\n\n- Norator article from Analog Insydes reference\n", "related": "NONE"}
{"id": "15165446", "url": "https://en.wikipedia.org/wiki?curid=15165446", "title": "Nullor", "text": "Nullor\n\nA nullor is a theoretical two-port network consisting of a nullator at its input and a norator at its output. Nullors represent an ideal amplifier, having infinite current, voltage, transconductance and transimpedance gain. Its transmission parameters are all zero, that is, its input–output behavior is summarized with the matrix equation\nIn negative-feedback circuits, the circuit surrounding the nullor determines the nullor output in such a way as to force the nullor input to zero.\n\nInserting a nullor in a circuit schematic imposes mathematical constraints on how that circuit must behave, forcing the circuit itself to adopt whatever arrangements are needed to meet the conditions. For example, an ideal operational amplifier can be modeled using a nullor, and the textbook analysis of a feedback circuit using an ideal op-amp uses the mathematical conditions imposed by the nullor to analyze the circuit surrounding the op-amp.\n\nFigure 1 shows a voltage-controlled current sink. The sink is intended to draw the same current \"i\" regardless of the applied voltage \"V\" at the output. The value of current drawn is to be set by the input voltage \"v\". Here the sink is to be analyzed by idealizing the op amp as a nullor.\n\nUsing properties of the input nullator portion of the nullor, the input voltage across the op amp input terminals is zero. Consequently, the voltage across reference resistor \"R\" is the applied voltage \"v\", making the current in \"R\" simply \"v\"/\"R\". Again using the nullator properties, the input current to the nullor is zero. Consequently, Kirchhoff's current law at the emitter provides an emitter current of \"v\"/\"R\". Using properties of the norator output portion of the nullor, the nullor provides whatever current is demanded of it, regardless of the voltage at its output. In this case, it provides the transistor base current \"i\". Thus, Kirchhoff's current law applied to the transistor as a whole provides the output current drawn through resistor \"R\" as\n\nwhere the base current of the bipolar transistor \"i\" is normally negligible provided the transistor remains in active mode. That is, based upon the idealization of a nullor, the output current is controlled by the user-applied input voltage \"v\" and the designer's choice for the reference resistor \"R\".\n\nThe purpose of the transistor in the circuit is to reduce the portion of the current in \"R\" supplied by the op-amp. Without the transistor, the current through \"R\" would be \"i\" = (\"V\" − \"v\")/\"R\", which interferes with the design goal of independence of \"i\" from \"V\". Another practical advantage of the transistor is that the op amp must deliver only the small transistor base current, which is unlikely to tax the op amp's current delivery capability. Of course, only real op amps are current-limited, not nullors.\n\nThe remaining variation of the current with the voltage \"V\" is due to the Early effect, which causes the β of the transistor to change with its collector-to-base voltage \"V\" according to the relation β = β(1 + \"V\"/\"V\"), where \"V\" is the so-called Early voltage. Analysis based upon a nullor leads to the output resistance of this current sink as \"R\" = \"r\"(β + 1) + \"R\", where \"r\" is the small-signal transistor output resistance given by \"r\" = (\"V\" + \"V\")/\"i\". See current mirror for the analysis.\n\nUse of the nullor idealization allows design of the circuitry around the op-amp. The practical problem remains of designing an op-amp that behaves like a nullor.\n", "related": "NONE"}
{"id": "15602155", "url": "https://en.wikipedia.org/wiki?curid=15602155", "title": "Log-spectral distance", "text": "Log-spectral distance\n\nThe log-spectral distance (LSD), also referred to as log-spectral distortion or root mean square log-spectral distance, is a distance measure (expressed in dB) between two spectra. The log-spectral distance between spectra formula_1 and formula_2 is defined as:\n\nwhere formula_1 and formula_2 are power spectra.\nUnlike the Itakura–Saito distance, the log-spectral distance is symmetric.\n\nIn speech coding, log spectral distortion for a given frame is defined as the root mean square difference between the original LPC log power spectrum and the quantized or interpolated LPC log power spectrum. Usually the average of spectral distortion over a large number of frames is calculated and that is used as the measure of performance of quantization or interpolation.\n\n", "related": "\n- Itakura–Saito distance\n"}
{"id": "14780124", "url": "https://en.wikipedia.org/wiki?curid=14780124", "title": "Rasta filtering", "text": "Rasta filtering\n\nRASTA-filtering and Mean Subtraction was introduced to support Perceptual Linear Prediction\n(PLP) preprocessing. It uses bandpass filtering in the log spectral domain. Rasta filtering then removes slow channel variations. It has also been applied to cepstrum feature-based preprocessing with both log spectral and cepstral domain filtering. \n\nIn general a RASTA filter is defined by\n\nformula_1\n\nThe numerator is a regression filter with N being the order (must be odd) and the denominator is an integrator with time decay. The pole controls the lower limit of frequency and is normally around 0.9. RASTA-filtering can be changed to use mean subtraction, implementing a moving average filter. Filtering is normally performed in the cepstral domain. The mean becomes the long term cepstrum and is typically computed on the speech part for each separate utterance. A silence is necessary to detect each utterance.\n\n- https://labrosa.ee.columbia.edu/~dpwe/papers/HermM94-rasta.pdf\n", "related": "NONE"}
{"id": "4110803", "url": "https://en.wikipedia.org/wiki?curid=4110803", "title": "Wigner distribution function", "text": "Wigner distribution function\n\nThe Wigner distribution function (WDF) is used in signal processing as a transform in time-frequency analysis.\n\nThe WDF was first proposed in physics to account for quantum corrections to classical statistical mechanics in 1932 by Eugene Wigner, and it is of importance in quantum mechanics in phase space (see, by way of comparison: \"Wigner quasi-probability distribution\", also called the \"Wigner function\" or the \"Wigner–Ville distribution\").\n\nGiven the shared algebraic structure between position-momentum and time-frequency conjugate pairs, it also usefully serves in signal processing, as a transform in time-frequency analysis, the subject of this article. Compared to a short-time Fourier transform, such as the Gabor transform, the Wigner distribution function provides the highest possible temporal vs frequency resolution which is mathematically possible within the limitations of uncertainty in quantum wave theory.\n\nWDF spectrograms are visually distinctly different than FFT spectrograms. WDF spectrograms are too slow for streaming audio compared to FFT ones: they take about 50 times longer to compute. WDF is a better choice than FFT when studying audio in one detail, where the highest quality TF graph is required, e.g. for a neural network; WDF is computationally too expensive for streaming audio, e.g. speech recognition. To generate a sample-accurate (1024 band) WDF spectrogram in real time would require about 16 cores of a modern desktop PC. \n\nThere are several different definitions for the Wigner distribution function. The definition given here is specific to time-frequency analysis. Given the time series formula_1, its non-stationary autocorrelation function is given by\n\nwhere formula_3 denotes the average over all possible realizations of the process and formula_4 is the mean, which may or may not be a function of time. The Wigner function formula_5 is then given by first expressing the autocorrelation function in terms of the average time formula_6 and time lag formula_7, and then Fourier transforming the lag.\n\nSo for a single (mean-zero) time series, the Wigner function is simply given by\n\nThe motivation for the Wigner function is that it reduces to the spectral density function at all times formula_10 for stationary processes, yet it is fully equivalent to the non-stationary autocorrelation function. Therefore, the Wigner function tells us (roughly) how the spectral density changes in time.\n\nHere are some examples illustrating how the WDF is used in time-frequency analysis.\n\nWhen the input signal is constant, its time-frequency distribution is a horizontal line along the time axis. For example, if \"x\"(\"t\") = 1, then\n\nWhen the input signal is a sinusoidal function, its time-frequency distribution is a horizontal line parallel to the time axis, displaced from it by the sinusoidal signal's frequency. For example, if , then\n\nWhen the input signal is a linear chirp function, the instantaneous frequency is a linear function. This means that the time frequency distribution should be a straight line. For example, if \n\nthen its instantaneous frequency is \n\nand its WDF\n\nWhen the input signal is a delta function, since it is only non-zero at t=0 and contains infinite frequency components, its time-frequency distribution should be a vertical line across the origin. This means that the time frequency distribution of the delta function should also be a delta function. By WDF\n\nThe Wigner distribution function is best suited for time-frequency analysis when the input signal's phase is 2nd order or lower. For those signals, WDF can exactly generate the time frequency distribution of the input signal.\n\nthe rectangular function   ⇒ \n\nThe Wigner distribution function is not a linear transform. A cross term (\"time beats\") occurs when there is more than one component in the input signal, analogous in time to frequency beats. In the ancestral physics Wigner quasi-probability distribution, this term has important and useful physics consequences, required for faithful expectation values. By contrast, the short-time Fourier transform does not have this feature. Negative features of the WDF are reflective of the Gabor limit of the classical signal and physically unrelated to any possible underlay of quantum structure.\n\nThe following are some examples that exhibit the cross-term feature of the Wigner distribution function.\n- formula_19\n- formula_20\n\nIn order to reduce the cross-term difficulty, several approaches have been proposed in the literature, some of them leading to new transforms as the modified Wigner distribution function, the Gabor–Wigner transform, the Choi-Williams distribution function and Cohen's class distribution.\n\nThe Wigner distribution function has several evident properties listed in the following table.\n\n- Projection property\n- Energy property\n- Recovery property\n- Mean condition frequency and mean condition time\n- Moment properties\n- Real properties\n- Region properties\n- Multiplication theorem\n- Convolution theorem\n- Correlation theorem\n- Time-shifting covariance\n- Modulation covariance\n- Scale covariance\n\n", "related": "\n- Time-frequency representation\n- Short-time Fourier transform\n- Spectrogram\n- Gabor transform\n- Autocorrelation\n- Gabor–Wigner transform\n- Modified Wigner distribution function\n- Optical equivalence theorem\n- Polynomial Wigner–Ville distribution\n- Cohen's class distribution function\n- Wigner quasi-probability distribution\n- Transformation between distributions in time-frequency analysis\n- Bilinear time–frequency distribution\n\n- , 1948. \"Théorie et Applications de la Notion de Signal Analytique\", \"Câbles et Transmission\", 2, 61–74 .\n- T. A. C. M. Classen and W. F. G. Mecklenbrauker, 1980. “The Wigner distribution-a tool for time-frequency signal analysis; Part I,” Philips J. Res., vol. 35, pp. 217–250.\n- L. Cohen (1989): \"Proceedings of the IEEE\" 77 pp. 941–981, Time-frequency distributions---a review\n- L. Cohen, \"Time-Frequency Analysis\", Prentice-Hall, New York, 1995.\n- S. Qian and D. Chen, \"Joint Time-Frequency Analysis: Methods and Applications\", Chap. 5, Prentice Hall, N.J., 1996.\n- B. Boashash, \"Note on the Use of the Wigner Distribution for Time Frequency Signal Analysis\", \"IEEE Transactions on Acoustics, Speech, and Signal Processing\", Vol. 36, No. 9, pp. 1518–1521, Sept. 1988. . B. Boashash, editor,\"Time-Frequency Signal Analysis and Processing – A Comprehensive Reference\", Elsevier Science, Oxford, 2003, .\n- F. Hlawatsch, G. F. Boudreaux-Bartels: “Linear and quadratic time-frequency signal representation,” IEEE Signal Processing Magazine, pp. 21–67, Apr. 1992.\n- R. L. Allen and D. W. Mills, \"Signal Analysis: Time, Frequency, Scale, and Structure\", Wiley- Interscience, NJ, 2004.\n- R.B. Pachori and A. Nishad, Cross-terms reduction in Wigner-Ville distribution using tunable-Q wavelet transform, Signal Processing, vol. 120, pp. 288–304, 2016.\n- Jian-Jiun Ding, Time frequency analysis and wavelet transform class notes, the Department of Electrical Engineering, National Taiwan University (NTU), Taipei, Taiwan, 2015.\n- Kakofengitis, D., & Steuernagel, O. (2017). \"Wigner's quantum phase space current in weakly anharmonic weakly excited two-state systems\" \"European Physical Journal Plus\" 14.07.2017\n- R.R. Sharma and R.B. Pachori, Improved eigenvalue decomposition-based approach for reducing cross-terms in Wigner-Ville distribution, Circuits, Systems, and Signal Processing, 2018.\n"}
{"id": "15231991", "url": "https://en.wikipedia.org/wiki?curid=15231991", "title": "Modified Wigner distribution function", "text": "Modified Wigner distribution function\n\nA Modified Wigner distribution function is a variation of the Wigner distribution function (WD) with reduced or removed cross-terms.\n\nThe Wigner distribution (WD) was first proposed for corrections to classical statistical mechanics in 1932 by Eugene Wigner. The Wigner distribution function, or Wigner–Ville distribution (WVD) for analytic signals, also has applications in time frequency analysis. The Wigner distribution gives better auto term localisation compared to the smeared out spectrogram (SP). However, when applied to a signal with multi frequency components, cross terms appear due to its quadratic nature. Several methods have been proposed to reduce the cross terms. For example, in 1994 L. Stankovic proposed a novel technique, now mostly referred to as S-method, resulting in the reduction or removal of cross terms. The concept of the S-method is a combination between the spectrogram and the Pseudo Wigner Distribution (PWD), the windowed version of the WD.\n\nThe original WD, the spectrogram, and the modified WDs all belong to the Cohen's class of bilinear time-frequency representations :\nwhere formula_2 is Cohen's kernel function, which is often a low-pass function, and normally serves to mask out the interference in the original Wigner representation.\n\n- Wigner distribution\nCohen's kernel function : formula_4\n\n- Spectrogram\nwhere formula_6 is the short-time Fourier transform of formula_7. \n\n- Modified form I\n\nformula_10\n\nCan't solve the cross term problem, however it can solve the problem of 2 components time difference larger than window size B.\n\n- Modified form II\n\nformula_11\n\n- Modified form III (Pseudo L-Wigner Distribution)\n\nformula_12\n\nWhere L is any integer greater than 0\n\nIncrease L can reduce the influence of cross term (however it can't eliminate completely )\n\nFor example, for L=2, the dominant third term is divided by 4 ( which is equivalent to 12dB ).\n\nThis gives a significant improvement over the Wigner Distribution.\n\nProperties of L-Wigner Distribution:\n\n1. The L-Wigner Distribution is always real.\n2. If the signal is time shifted formula_13\",\" then its LWD is time shifted as well, formula_14\n3. The LWD of a modulated signal formula_15is shifted in frequency formula_16\n4. Is the signal formula_17is time limited, i.e.,formula_18 formula_19 then the L-Wigner distribution is time limited, formula_20 formula_21\n5. If the signal formula_17is band limited with formula_23(formula_24formula_25), then formula_26is limited in the frequency domain by formula_23as well.\n6. Integral of L-Wigner distribution over frequency is equal to the generalized signal power: formula_28\n7. Integral of formula_26over time and frequency is equal to the formula_30power of the formula_30norm of signal formula_17:\n\nformula_33\n1. The integral over time is:\n\nformula_34\n1. For a large value of formula_35We may neglect all values of formula_26, Comparing them to the one at the points formula_37, where the distribution reaches its essential supremum:\n\nformula_38 \n\n- Modified form IV (Polynomial Wigner Distribution Function)\n\nformula_39\n\nWhen formula_40 and formula_41, it becomes the original Wigner distribution function.\n\nIt can avoid the cross term when the order of phase of the exponential function is no larger than formula_42\n\nHowever the cross term between two components cannot be removed.\n\nformula_43should be chosen properly such that\n\nformula_44\n\nformula_45\n\nformula_46\n\nIf formula_47\n\nwhen formula_48 , formula_49\n\nformula_50\n\nformula_51\n\nformula_52\n\n- Pseudo Wigner distribution\nCohen's kernel function : formula_54 which is concentred on the frequency axis.\n\nNote that the pseudo Wigner can also be written as the Fourier transform of the “spectral-correlation” of the STFT\n\n- Smoothed pseudo Wigner distribution :\nIn the pseudo Wigner the time windowing acts as a frequency direction smoothing. Therefore, it suppresses the Wigner distribution interference components that oscillate in the frequency direction. Time direction smoothing can be implemented by a time-convolution of the PWD with a lowpass function formula_56 :\nCohen's kernel function : formula_58 where formula_59 is the Fourier transform of the window formula_60.\n\nThus the kernel corresponding to the smoothed pseudo Wigner distribution has a separable form. Note that even if the SPWD and the S-Method both smoothes the WD in the time domain, they are not equivalent in general.\n\n- S-method\n\nCohen's kernel function : formula_62\n\nThe S-method limits the range of the integral of the PWD with a low-pass windowing function formula_63 of Fourier transform formula_64. This results in the cross-term removal, without blurring the auto-terms that are well-concentred along the frequency axis.\nThe S-method strikes a balance in smoothing between the pseudo-Wigner distribution formula_65 [formula_66] and the power spectrogram formula_67 [formula_68].\n\nNote that in the original 1994 paper, Stankovic defines the S-methode with a modulated version of the short-time Fourier transform : \n\nwhere\n\nEven in this case we still have\n\n", "related": "\n- Time-frequency representation\n- Wigner distribution function\n- Bilinear time–frequency distribution\n- Short-time Fourier transform\n- Gabor transform\n\n- P. Gonçalves and R. Baraniuk, “Pseudo Affine Wigner Distributions : Definition and Kernel Formulation”, IEEE Transactions on Signal Processing, vol. 46, no. 6, Jun. 1998\n- L. Stankovic, “A Method for Time-Frequency Signal Analysis”, IEEE Transactions on Signal Processing, vol. 42, no. 1, Jan. 1994\n- L. J. Stankovic, S. Stankovic, and E. Fakultet, “An analysis of instantaneous frequency representation using time frequency distributions-generalized Wigner distribution,” \"IEEE Trans. on Signal Processing,\" pp. 549-552, vol. 43, no. 2, Feb. 1995\n"}
{"id": "3259068", "url": "https://en.wikipedia.org/wiki?curid=3259068", "title": "Fast folding algorithm", "text": "Fast folding algorithm\n\nIn signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. It computes superpositions of the signal modulo various window sizes simultaneously.\n\nThe FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse.\n\n", "related": "\n- Pulsar\n\n- David H. Staelin. Fast Folding Algorithm for Detection of Periodic Pulse Trains. \"Proceedings of the IEEE\", 57 (1969).\n- R. V. E. Lovelace, J. M. Sutton and E. E. Salpeter. Digital Search Methods for Pulsars. \"Nature\" 222, 231–233 (1969).\n\n- The search for unknown pulsars\n"}
{"id": "166891", "url": "https://en.wikipedia.org/wiki?curid=166891", "title": "Equivalent rectangular bandwidth", "text": "Equivalent rectangular bandwidth\n\nThe equivalent rectangular bandwidth or ERB is a measure used in psychoacoustics, which gives an approximation to the bandwidths of the filters in human hearing, using the unrealistic but convenient simplification of modeling the filters as rectangular band-pass filters.\n\nFor moderate sound levels and young listeners, the bandwidth of human auditory filters can be approximated by the polynomial equation:\n\nwhere \"f\" is the center frequency of the filter in kHz and ERB(\"f\") is the bandwidth of the filter in Hz. The approximation is based on the results of a number of published simultaneous masking experiments and is valid from 0.1 to 6.5 kHz.\n\nThe above approximation was given in 1983 by Moore and Glasberg, who in 1990 published another (linear) approximation:\n\nwhere \"f\" is in kHz and ERB(\"f\") is in Hz. The approximation is applicable at moderate sound levels and for values of \"f\" between 0.1 and 10 kHz.\n\nThe ERB-rate scale, or ERB-number scale, can be defined as a function ERBS(\"f\") which returns the number of equivalent rectangular bandwidths below the given frequency \"f\". The units of the ERB-number scale are Cams. The scale can be constructed by solving the following differential system of equations:\n\nThe solution for ERBS(\"f\") is the integral of the reciprocal of ERB(\"f\") with the constant of integration set in such a way that ERBS(0) = 0.\n\nUsing the second order polynomial approximation () for ERB(\"f\") yields:\n\nwhere \"f\" is in kHz. The VOICEBOX speech processing toolbox for MATLAB implements the conversion and its inverse as:\n\nwhere \"f\" is in Hz.\n\nUsing the linear approximation () for ERB(\"f\") yields:\n\nwhere \"f\" is in Hz.\n\n", "related": "\n- Critical bands\n- Bark scale\n\n- https://web.archive.org/web/20110427105916/http://www.ling.su.se/staff/hartmut/bark.htm\n"}
{"id": "3054565", "url": "https://en.wikipedia.org/wiki?curid=3054565", "title": "Apodization", "text": "Apodization\n\nApodization is an optical filtering technique. Its literal translation is \"removing the foot\". It is the technical term for changing the shape of a mathematical function, an electrical signal, an optical transmission or a mechanical structure. In optics, it is primarily used to remove Airy disks caused by diffraction around an intensity peak, improving the focus.\n\nThe term apodization is used frequently in publications on Fourier-transform infrared (FTIR) signal processing. An example of apodization is the use of the Hann window in the fast Fourier transform analyzer to smooth the discontinuities at the beginning and end of the sampled time record.\n\nAn apodizing filter can be used in digital audio processing instead of the more common brickwall filters, in order to avoid the pre-ringing that the latter introduces.\n\nDuring oscillation within an Orbitrap, ion transient signal may not be stable until the ions settle into their oscillations. Toward the end, subtle ion collisions have added up to cause noticeable dephasing. This presents a problem for the Fourier transformation, as it averages the oscillatory signal across the length of the time-domain measurement. Software allows “apodization”, the removal of the front and back section of the transient signal from consideration in the FT calculation. Thus, apodization improves the resolution of the resulting mass spectrum. Another way to improve the quality of the transient is to wait to collect data until ions have settled into stable oscillatory motion within the trap.\n\nIn optical design jargon, an \"apodization\" function is used to purposely change the input intensity profile of an optical system, and may be a complicated function to tailor the system to certain properties. Usually it refers to a non-uniform illumination\nor transmission profile that approaches zero at the edges.\n\nSince side lobes of the Airy disk are responsible for degrading the image, techniques for suppressing them are utilized. In case the imaging beam has Gaussian distribution, when the truncation ratio (the ratio of the diameter of the Gaussian beam to the diameter of the truncating aperture) is set to 1, the side-lobes become negligible and the beam profile becomes purely Gaussian. The measured beam profile of such imaging system is shown and compared to the modeled beam profile in the Figure on the right.\n\nMost camera lenses contain diaphragms which decrease the amount of light coming into the camera. These are not strictly an example of apodization, since the diaphragm does not produce a smooth transition to zero intensity, nor does it provide shaping of the intensity profile (beyond the obvious all-or-nothing, \"top hat\" transmission of its aperture). \n\nSome lenses use other methods to reduce the amount of light let in. For example, the Minolta/Sony STF 135mm f/2.8 T4.5 lens however, has a special design introduced in 1999, which accomplishes this by utilizing a concave neutral-gray tinted lens element as an apodization filter, thereby producing a pleasant bokeh. The same optical effect can be achieved combining depth-of-field bracketing with multi exposure, as implemented in the Minolta Maxxum 7's STF function. In 2014, Fujifilm announced a lens utilizing a similar apodization filter in the Fujinon XF 56mm F1.2 R APD lens. In 2017, Sony introduced the E-mount full-frame lens Sony FE 100mm F2.8 STF GM OSS (SEL-100F28GM) based on the same optical Smooth Trans Focus principle.\n\nSimulation of a Gaussian laser beam input profile is also an example of apodization.\n\nPhoton sieves provide a relatively easy way to achieve tailored optical apodization.\n\nApodization is used in telescope optics in order to improve the dynamic range of the image. For example, stars with low intensity in the close vicinity of very bright stars can be made visible using this technique, and even images of planets can be obtained when otherwise obscured by the bright atmosphere of the star they orbit. Generally, apodization reduces the resolution of an optical image; however, because it reduces diffraction edge effects, it can actually enhance certain small details. In fact the notion of resolution, as it is commonly defined with the Rayleigh criterion, is in this case partially irrelevant. One has to understand that the image formed in the focal plane of a lens (or a mirror) is modelled through the Fresnel diffraction formalism. The classical diffraction pattern, the Airy disk, is connected to a circular pupil, without any obstruction and with a uniform transmission. Any change in the shape of the pupil (for example a square instead of a circle), or in its transmission, results in an alteration in the associated diffraction pattern.\n\n", "related": "\n- Apodization function\n"}
{"id": "263317", "url": "https://en.wikipedia.org/wiki?curid=263317", "title": "Spectrogram", "text": "Spectrogram\n\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. \nWhen applied to an audio signal, spectrograms are sometimes called sonographs, voiceprints, or voicegrams. \nWhen the data is represented in a 3D plot they may be called waterfalls.\n\nSpectrograms are used extensively in the fields of music, sonar, radar, and speech processing, seismology, and others. Spectrograms of audio can be used to identify spoken words phonetically, and to analyse the various calls of animals.\n\nA spectrogram can be generated by an optical spectrometer, a bank of band-pass filters, by Fourier transform or by a wavelet transform (in which case it is also known as a scaleogram).\nA spectrogram is usually depicted as a heat map, i.e., as an image with the intensity shown by varying the colour or brightness.\n\nA common format is a graph with two geometric dimensions: one axis represents time, and the other axis represents frequency; a third dimension indicating the amplitude of a particular frequency at a particular time is represented by the intensity or color of each point in the image.\n\nThere are many variations of format: sometimes the vertical and horizontal axes are switched, so time runs up and down; sometimes as a waterfall plot where the amplitude is represented by height of a 3D surface instead of color or intensity. The frequency and amplitude axes can be either linear or logarithmic, depending on what the graph is being used for. Audio would usually be represented with a logarithmic amplitude axis (probably in decibels, or dB), and frequency would be linear to emphasize harmonic relationships, or logarithmic to emphasize musical, tonal relationships.\n\nSpectrograms of light may be created directly using an optical spectrometer over time.\n\nSpectrograms may be created from a time-domain signal in one of two ways: approximated as a filterbank that results from a series of band-pass filters (this was the only way before the advent of modern digital signal processing), or calculated from the time signal using the Fourier transform. These two methods actually form two different time–frequency representations, but are equivalent under some conditions.\n\nThe bandpass filters method usually uses analog processing to divide the input signal into frequency bands; the magnitude of each filter's output controls a transducer that records the spectrogram as an image on paper.\n\nCreating a spectrogram using the FFT is a digital process. Digitally sampled data, in the time domain, is broken up into chunks, which usually overlap, and Fourier transformed to calculate the magnitude of the frequency spectrum for each chunk. Each chunk then corresponds to a vertical line in the image; a measurement of magnitude versus frequency for a specific moment in time (the midpoint of the chunk). These spectrums or time plots are then \"laid side by side\" to form the image or a three-dimensional surface, or slightly overlapped in various ways, i.e. windowing. This process essentially corresponds to computing the squared magnitude of the short-time Fourier transform (STFT) of the signal formula_1 — that is, for a window width formula_2, formula_3.\n\nFrom the formula above, it appears that a spectrogram contains no information about the exact, or even approximate, phase of the signal that it represents. For this reason, it is not possible to reverse the process and generate a copy of the original signal from a spectrogram, though in situations where the exact initial phase is unimportant it may be possible to generate a useful approximation of the original signal. The Analysis & Resynthesis Sound Spectrograph is an example of a computer program that attempts to do this. The Pattern Playback was an early speech synthesizer, designed at Haskins Laboratories in the late 1940s, that converted pictures of the acoustic patterns of speech (spectrograms) back into sound.\n\nIn fact, there is some phase information in the spectrogram, but it appears in another form, as time delay (or group delay) which is the dual of the Instantaneous Frequency .\n\nThe size and shape of the analysis window can be varied. A smaller (shorter) window will produce more accurate results in timing, at the expense of precision of frequency representation. A larger (longer) window will provide a more precise frequency representation, at the expense of precision in timing representation. This is an instance of the Heisenberg uncertainty principle, that the product of the precision in two conjugate variables is greater than or equal to a constant (B*T>=1 in the usual notation).\n\n- Early analog spectrograms were applied to a wide range of areas including the study of bird calls (such as that of the great tit), with current research continuing using modern digital equipment and applied to all animal sounds. Contemporary use of the digital spectrogram is especially useful for studying frequency modulation (FM) in animal calls. Specifically, the distinguishing characteristics of FM chirps, broadband clicks, and social harmonizing are most easily visualized with the spectrogram.\n- Spectrograms are useful in assisting in overcoming speech deficits and in speech training for the portion of the population that is profoundly deaf\n- The studies of phonetics and speech synthesis are often facilitated through the use of spectrograms.\n- In deep learning-based speech synthesis, spectrogram (or spectrogram in mel scale) is first predicted by a seq2seq model, then the spectrogram is fed to a neural vocoder to derive the synthesized raw waveform.\n- By reversing the process of producing a spectrogram, it is possible to create a signal whose spectrogram is an arbitrary image. This technique can be used to hide a picture in a piece of audio and has been employed by several electronic music artists. See also steganography.\n- Some modern music is created using spectrograms as an intermediate medium; changing the intensity of different frequencies over time, or even creating new ones, by drawing them and then inverse transforming. See Audio timescale-pitch modification and Phase vocoder.\n- Spectrograms can be used to analyze the results of passing a test signal through a signal processor such as a filter in order to check its performance.\n- High definition spectrograms are used in the development of RF and microwave systems\n- Spectrograms are now used to display scattering parameters measured with vector network analyzers\n- The US Geological Survey and the IRIS Consortium provide near real-time spectrogram displays for monitoring seismic stations\n- Spectrograms can be used with recurrent neural networks for speech recognition.\n\n", "related": "\n- Acoustic signature\n- Chromagram\n- Generalized spectrogram\n- List of unexplained sounds\n- Reassignment method\n- Short-time Fourier transform\n- Spectral music\n- Spectrometer\n- Spectrum\n- Strobe tuner\n- Time-frequency representation\n- Waterfall plot\n- Wavelet transform\n- See an online spectrogram of speech or other sounds captured by your computer's microphone.\n- Generating a tone sequence whose spectrogram matches an arbitrary text, online\n- Further information on creating a signal whose spectrogram is an arbitrary image\n- Article describing the development of a software spectrogram\n- History of spectrograms & development of instrumentation\n- How to identify the words in a spectrogram from a linguistic professor's \"Monthly Mystery Spectrogram\" publication.\n"}
{"id": "17854576", "url": "https://en.wikipedia.org/wiki?curid=17854576", "title": "Adjacent channel power ratio", "text": "Adjacent channel power ratio\n\nAdjacent Channel Power Ratio (ACPR) is ratio between the total power of adjacent channel (intermodulation signal) to the main channel's power (useful signal).\n\nThe ratio between the total power adjacent channel (intermodulation signal) to the main channel's power (useful signal). There are two ways of measuring ACPR. The first way is by finding 10*log of the ratio of the total output power to the power in adjacent channel. The second (and much more popular method) is to find the ratio of the output power in a smaller bandwidth around the center of carrier to the power in the adjacent channel. The smaller bandwidth is equal to the bandwidth of the adjacent channel signal. Second way is more popular, because it can be measured easily.\n\nACPR is desired to be as low as possible. A high ACPR indicates that significant spectral spreading has occurred.\n\n", "related": "\n- Spectral leakage\n- Spread spectrum\n"}
{"id": "17960231", "url": "https://en.wikipedia.org/wiki?curid=17960231", "title": "Algebraic signal processing", "text": "Algebraic signal processing\n\nIn the algebraic theory of linear signal processing, a set of filters is treated as an algebra and a set of signals is treated as a module and the z-transform is generalized to linear maps.\n\n- Smart Project: Algebraic Theory of Signal Processing at the Department of Electrical and Computer Engineering at Carnegie Mellon University.\n", "related": "NONE"}
{"id": "18112690", "url": "https://en.wikipedia.org/wiki?curid=18112690", "title": "Hilbert spectral analysis", "text": "Hilbert spectral analysis\n\nHilbert spectral analysis is a signal analysis method applying the Hilbert transform to compute the instantaneous frequency of signals according to\n\nAfter performing the Hilbert transform on each signal, we can express the data in the following form:\n\nThis equation gives both the amplitude and the frequency of each component as functions of time. It also enables us to represent the amplitude and the instantaneous frequency as functions of time in a three-dimensional plot, in which the amplitude can be contoured on the frequency-time plane. This frequency-time distribution of the amplitude is designated as the Hilbert amplitude spectrum, or simply Hilbert spectrum.\n\nHilbert spectral analysis method is an important part of Hilbert–Huang transform.\n\n- Alan V. Oppenheim and Ronald W. Schafer, \"\"Discrete-Time Signal Processing\",\" Prentice-Hall Signal Processing Series, 2 ed., 1999.\n- Huang, et al. \"The empirical mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series analysis.\" \"Proc. R. Soc. Lond. A\" (1998) 454, 903–995\n", "related": "NONE"}
{"id": "10368082", "url": "https://en.wikipedia.org/wiki?curid=10368082", "title": "Spectral edge frequency", "text": "Spectral edge frequency\n\nThe spectral edge frequency or SEF is a measure used in signal processing. It is usually expressed as \"SEF \"x\"\", which stands for the frequency below which \"x\" percent of the total power of a given signal are located. (typically \"x\" is in the range 75 to 95.)\n\nIt is more particularly a popular measure used in EEG monitoring, in which case SEF has variously been used to estimate the depth of anesthesia and stages of sleep.\n\n", "related": "\n- Bispectral index\n\n- Drummond JC, Brann CA, Perkins DE, Wolfe DE: \"A comparison of median frequency, spectral edge frequency, a frequency band power ratio, total power, and dominance shift in the determination of depth of anesthesia,\" \"Acta Anaesthesiol Scand.\" 1991 Nov;35(8):693-9.\n\n- Specific\n"}
{"id": "1145733", "url": "https://en.wikipedia.org/wiki?curid=1145733", "title": "BIBO stability", "text": "BIBO stability\n\nIn signal processing, specifically control theory, bounded-input, bounded-output (BIBO) output will be bounded for every input to the system that is bounded.\n\nA signal is bounded if there is a finite value formula_1 such that the signal magnitude never exceeds formula_2, that is\n\nFor a continuous time linear time-invariant (LTI) system, the condition for BIBO stability is that the impulse response be absolutely integrable, i.e., its L norm exists.\n\nFor a discrete time LTI system, the condition for BIBO stability is that the impulse response be absolutely summable, i.e., its formula_6 norm exists.\n\nGiven a discrete time LTI system with impulse response formula_8 the relationship between the input formula_9 and the output formula_10 is\n\nwhere formula_12 denotes convolution. Then it follows by the definition of convolution\n\nLet formula_14 be the maximum value of formula_15, i.e., the formula_16-norm.\n\nIf formula_20 is absolutely summable, then formula_21 and\n\nSo if formula_20 is absolutely summable and formula_24 is bounded, then formula_25 is bounded as well because formula_26\n\nThe proof for continuous-time follows the same arguments.\n\nFor a rational and continuous-time system, the condition for stability is that the region of convergence (ROC) of the Laplace transform includes the imaginary axis. When the system is causal, the ROC is the open region to the right of a vertical line whose abscissa is the real part of the \"largest pole\", or the pole that has the greatest real part of any pole in the system. The real part of the largest pole defining the ROC is called the abscissa of convergence. Therefore, all poles of the system must be in the strict left half of the s-plane for BIBO stability.\n\nThis stability condition can be derived from the above time-domain condition as follows:\n\nwhere formula_28 and formula_29\n\nThe region of convergence must therefore include the imaginary axis.\n\nFor a rational and discrete time system, the condition for stability is that the region of convergence (ROC) of the z-transform includes the unit circle. When the system is causal, the ROC is the open region outside a circle whose radius is the magnitude of the pole with largest magnitude. Therefore, all poles of the system must be inside the unit circle in the z-plane for BIBO stability.\n\nThis stability condition can be derived in a similar fashion to the continuous-time derivation:\n\nwhere formula_31 and formula_32.\n\nThe region of convergence must therefore include the unit circle.\n\n", "related": "\n- LTI system theory\n- Finite impulse response (FIR) filter\n- Infinite impulse response (IIR) filter\n- Nyquist plot\n- Routh–Hurwitz stability criterion\n- Bode plot\n- Phase margin\n- Root locus method\n\n- Gordon E. Carlson \"Signal and Linear Systems Analysis with Matlab\" second edition, Wiley, 1998,\n- John G. Proakis and Dimitris G. Manolakis \"Digital Signal Processing Principals, Algorithms and Applications\" third edition, Prentice Hall, 1996,\n- D. Ronald Fannin, William H. Tranter, and Rodger E. Ziemer \"Signals & Systems Continuous and Discrete\" fourth edition, Prentice Hall, 1998,\n- Proof of the necessary conditions for BIBO stability.\n- Christophe Basso \"Designing Control Loops for Linear and Switching Power Supplies: A Tutorial Guide\" first edition, Artech House, 2012, 978-1608075577\n"}
{"id": "18651732", "url": "https://en.wikipedia.org/wiki?curid=18651732", "title": "Coherence (signal processing)", "text": "Coherence (signal processing)\n\nIn signal processing, the coherence is a statistic that can be used to examine the relation between two signals or data sets. It is commonly used to estimate the power transfer between input and output of a linear system. If the signals are ergodic, and the system function is linear, it can be used to estimate the causality between the input and output.\n\nThe coherence (sometimes called magnitude-squared coherence) between two signals x(t) and y(t) is a real-valued function that is defined as:\n\nwhere G(f) is the Cross-spectral density between x and y, and G(f) and G(f) the autospectral density of x and y respectively. The magnitude of the spectral density is denoted as |G|. Given the restrictions noted above (ergodicity, linearity) the coherence function estimates the extent to which y(t) may be predicted from x(t) by an optimum linear least squares function.\n\nValues of coherence will always satisfy formula_2. For an \"ideal\" constant parameter linear system with a single input x(t) and single output y(t), the coherence will be equal to one. To see this, consider a linear system with an impulse response h(t) defined as: formula_3, where * denotes convolution. In the Fourier domain this equation becomes formula_4, where Y(f) is the Fourier transform of y(t) and H(f) is the linear system transfer function. Since, for an ideal linear system: formula_5 and formula_6, and since formula_7 is real, the following identity holds,\n\nHowever, in the physical world an ideal linear system is rarely realized, noise is an inherent component of system measurement, and it is likely that a single input, single output linear system is insufficient to capture the complete system dynamics. In cases where the ideal linear system assumptions are insufficient, the Cauchy–Schwarz inequality guarantees a value of formula_9.\n\nIf C is less than one but greater than zero it is an indication that either: noise is entering the measurements, that the assumed function relating x(t) and y(t) is not linear, or that y(t) is producing output due to input x(t) as well as other inputs. If the coherence is equal to zero, it is an indication that x(t) and y(t) are completely unrelated, given the constraints mentioned above.\n\nThe coherence of a linear system therefore represents the fractional part of the output signal power that is produced by the input at that frequency. We can also view the quantity formula_10 as an estimate of the fractional power of the output that is not contributed by the input at a particular frequency. This leads naturally to definition of the coherent output spectrum:\n\nformula_12 provides a spectral quantification of the output power that is uncorrelated with noise or other inputs.\n\nHere we illustrate the computation of coherence (denoted as formula_13) as shown in figure 1.\nConsider the two signals shown in the lower portion of figure 2.\nThere appears to be a close relationship between the ocean surface water levels and the groundwater well levels. It is also clear that the barometric pressure has an effect on both the ocean water levels and groundwater levels.\n\nFigure 3 shows the autospectral density of ocean water level over a long period of time.As expected, most of the energy is centered on the well-known tidal frequencies. Likewise, the autospectral density of groundwater well levels are shown in figure 4.It is clear that variation of the groundwater levels have significant power at the ocean tidal frequencies. To estimate the extent at which the groundwater levels are influenced by the ocean surface levels, we compute the coherence between them. Let us assume that there is a linear relationship between the ocean surface height and the groundwater levels. We further assume that the ocean surface height controls the groundwater levels so that we take the ocean surface height as the input variable, and the groundwater well height as the output variable.\n\nThe computed coherence (figure 1) indicates that at most of the major ocean tidal frequencies the variation of groundwater level at this particular site is over 90% due to the forcing of the ocean tides. However, one must exercise caution in attributing causality. If the relation (transfer function) between the input and output is nonlinear, then values of the coherence can be erroneous. Another common mistake is to assume a causal input/output relation between observed variables, when in fact the causative mechanism is not in the system model. For example, it is clear that the atmospheric barometric pressure induces a variation in both the ocean water levels and the groundwater levels, but the barometric pressure is not included in the system model as an input variable. We have also assumed that the ocean water levels drive or control the groundwater levels. In reality it is a combination of hydrological forcing from the ocean water levels and the tidal potential that are driving both the observed input and output signals. Additionally, noise introduced in the measurement process, or by the spectral signal processing can contribute to or corrupt the coherence.\n\nIf the signals are non-stationary, (and therefore not ergodic), the above formulations may not be appropriate. For such signals, the concept of coherence has been extended by using the concept of time-frequency distributions to represent the time-varying spectral variations of non-stationary signals in lieu of traditional spectra. For more details, see.\n\nCoherence has been found a great application to find dynamic functional connectivity in the brain networks. Studies show that the coherence between different brain regions can be changed during different mental or perceptual states . The brain coherence during the rest state can be affected by disorders and diseases . \n\n", "related": "\n- Bicoherence\n- Scaled Correlation\n- Normalized cross-correlation\n"}
{"id": "19270266", "url": "https://en.wikipedia.org/wiki?curid=19270266", "title": "Spark (mathematics)", "text": "Spark (mathematics)\n\nIn mathematics, specifically in linear algebra, the spark of a formula_1 matrix formula_2 is the smallest number formula_3 such that there exists a set of formula_3 columns in formula_2 which are linearly dependent. Formally,\n\nwhere formula_6 is a nonzero vector and formula_7 denotes its number of nonzero coefficients.\n\nIf all the columns are linearly independent, formula_8 is usually defined to be formula_9.\n\nBy contrast, the rank of a matrix is the largest number formula_3 such that some set of formula_3 columns of formula_2 is linearly independent.\n\nConsider the following matrix formula_2.\n\nformula_14\n\nThe spark of this matrix equals 3 because:\n- There is no set of 1 column of formula_2 which are linearly dependent.\n- There is no set of 2 columns of formula_2 which are linearly dependent.\n- But there is a set of 3 columns of formula_2 which are linearly dependent.\nThe first three columns are linearly dependent because formula_18.\n\nIf formula_19, the following simple properties hold for the spark of a formula_1 matrix formula_2:\n\n- formula_22 (the spark equals formula_23 if and only if the matrix has full rank)\n- formula_24 if and only if the matrix has a zero column\n- If formula_25, then formula_26\n\nThe spark yields a simple criterion for uniqueness of sparse solutions of linear equation systems.\nGiven a linear equation system formula_27. If this system has a solution formula_28 that satisfies formula_29, then this solution is the sparsest possible solution. Here formula_30 denotes the number of nonzero entries of the vector formula_28.\n\nIf the columns of the matrix formula_2 are normalized to unit norm, we can lower bound the spark of the matrix in terms of the dictionary coherence:\n\nThe dictionary coherence formula_34 is defined as the maximum correlation between any two columns, i.e.\n\nThe minimum distance of a linear code equals the spark of its parity-check matrix.\n\nThe concept of the spark is also of use in the theory of compressive sensing, where requirements on the spark of the measurement matrix are used to ensure stability and consistency of various estimation techniques. It is also known in matroid theory as the girth of the vector matroid associated with the columns of the matrix. The spark of a matrix is NP-hard to compute.\n", "related": "NONE"}
{"id": "564695", "url": "https://en.wikipedia.org/wiki?curid=564695", "title": "Discrete system", "text": "Discrete system\n\nA discrete system is a system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models.\n\nA computer is a finite state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals.\n\n", "related": "\n- Digital control\n- Finite state machine\n- Frequency spectrum\n- Mathematical model\n- Sample and hold\n- Sample rate\n- Sample time\n- Z-transform\n\n"}
{"id": "19371277", "url": "https://en.wikipedia.org/wiki?curid=19371277", "title": "Spectral concentration problem", "text": "Spectral concentration problem\n\nThe spectral concentration problem in Fourier analysis refers to finding a time sequence of a given length whose discrete Fourier transform is maximally localized on a given frequency interval, as measured by the spectral concentration.\n\nThe discrete-time Fourier transform (DTFT) \"U\"(\"f\") of a finite series formula_1, formula_2 is defined as \n\nIn the following, the sampling interval will be taken as Δ\"t\" = 1, and hence the frequency interval as \n\"f\" ∈ [-½,½]. \"U\"(\"f\") is a periodic function with a period 1.\n\nFor a given frequency \"W\" such that 0<\"W\"<½, the spectral concentration formula_4 of \"U\"(\"f\") on the interval [-\"W\",\"W\"] is defined as the ratio of power of \"U\"(\"f\") contained in the frequency band [-\"W\",\"W\"] to the power of \"U\"(\"f\") contained in the entire frequency band [-½,½]. That is,\n\nIt can be shown that \"U\"(\"f\") has only isolated zeros and hence formula_6 (see [1]). Thus, the spectral concentration is strictly less than one, and there is no finite sequence formula_1 for which the DTFT can be confined to a band [-\"W\",\"W\"] and made to vanish outside this band.\n\nAmong all sequences formula_8 for a given \"T\" and \"W\", is there a sequence for which the spectral concentration is maximum? In other words, is there a sequence for which the sidelobe energy outside a frequency band [-\"W\",\"W\"] is minimum?\n\nThe answer is yes; such a sequence indeed exists and can be found by optimizing formula_4. Thus maximising the power \n\nsubject to the constraint that the total power is fixed, say \n\nleads to the following equation satisfied by the optimal sequence formula_1:\n\nThis is an eigenvalue equation for a symmetric matrix given by\n\nIt can be shown that this matrix is positive-definite, hence all the eigenvalues of\nthis matrix lie between 0 and 1. The largest eigenvalue of the above equation corresponds to the largest possible spectral concentration; the corresponding eigenvector is the required optimal sequence formula_1. This sequence is called a 0–order Slepian sequence (also known as a discrete prolate spheroidal sequence, or DPSS), which is a unique taper with maximally suppressed sidelobes.\n\nIt turns out that the number of dominant eigenvalues of the matrix \"M\" that are close to 1, corresponds to \"N=2WT\" called as Shannon number. If the eigenvalues formula_16 are arranged in decreasing order (i.e., formula_17), then the eigenvector corresponding to formula_18 is called \"n\"–order Slepian sequence (DPSS) (0≤\"n\"≤\"N\"-1). This \"n\"–order taper also offers the best sidelobe suppression and is pairwise orthogonal to the Slepian sequences of previous orders formula_19. These lower order Slepian sequences form\nthe basis for spectral estimation by multitaper method.\n\nNot limited to time series, the spectral concentration problem can be reformulated to apply on the surface of the sphere by using spherical harmonics, for applications in geophysics and cosmology among others.\n\n", "related": "\n- Multitaper\n- Fourier transform\n- Discrete Fourier transform\n\n- Partha Mitra and Hemant Bokil. \"Observed Brain Dynamics\", Oxford University Press, USA (2007), Link for book\n- Donald. B. Percival and Andrew. T. Walden. \"Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques\", Cambridge University Press, UK (2002).\n- Partha Mitra and B. Pesaran, \"Analysis of Dynamic Brain Imaging Data.\" The Biophysical Journal, Volume 76 (1999), 691-708, arxiv.org/abs/q-bio/0309028\n- F. J. Simons, M. A. Wieczorek and F. A. Dahlen. \"Spatiospectral concentration on a sphere\". SIAM Review, 2006,\n"}
{"id": "19480072", "url": "https://en.wikipedia.org/wiki?curid=19480072", "title": "Chronux", "text": "Chronux\n\nChronux is an open-source software package developed for the loading, visualization and analysis of a variety of modalities / formats of neurobiological time series data. Usage of this tool enables neuroscientists to perform a variety of analysis on multichannel electrophysiological data such as LFP (local field potentials), EEG, MEG, Neuronal spike times and also on spatiotemporal data such as FMRI and dynamic optical imaging data. The software consists of a set of MATLAB routines interfaced with C libraries that can be used to perform the tasks that constitute a typical study of neurobiological data. These include local regression and smoothing, spike sorting and spectral analysis - including multitaper spectral analysis, a powerful nonparametric method to estimate power spectrum. The package also includes some GUIs for time series visualization and analysis. Chronux is GNU GPL v2 licensed (and MATLAB is proprietary).\n\nThe most recent version of Chronux is version 2.12.\n\nFrom 1996 to 2001, the Marine Biological Laboratory (MBL) at Woods Hole, Massachusetts, USA hosted a workshop on the analysis of neural data. This workshop then evolved into the special topics course on neuroinformatics which is held at the MBL in the last two weeks of August every year. The popularity of these pedagogical efforts and the need for wider dissemination of sophisticated time-series analysis tools in the wider neuroscience community led the Mitra Lab at Cold Spring Harbor Laboratory to initiate an NIH funded effort to develop software tools for neural data analysis in the form of the Chronux package. Chronux is the result of efforts of a number of people, the chief among\nwhom are Hemant Bokil, Peter Andrews, Samar Mehta, Ken Harris, Catherine Loader, Partha Mitra, Hiren Maniar, Ravi Shukla, Ramesh Yadav, Hariharan Nalatore and Sumanjit Kaur. Important contributions were also made by Murray Jarvis, Bijan Pesaran and S.Gopinath. Chronux welcome contributions from interested individuals.\n\nChronux is organized into a number of distinct toolboxes. These include the spectral analysis toolbox, the local regression and likelihood toolbox, and the spike-sorting toolbox. In addition, a number of domain-specific GUIs are part of the Chronux package and more are envisaged. Much of Chronux is written in MATLAB with certain intensive computations being coded in C with a MEX interface to MATLAB. The methods employed are state-of-the-art: For example, the spectral analysis toolbox implements the multitaper spectral estimation method and the local regression and Likelihood toolbox (Locfit) implements a set of highly flexible methods for fitting functions and probability distributions to data. Chronux provides robust estimates of the confidence intervals on computed quantities. Thus, the computation of a spectrum can be augmented by a computation of both asymptotic and jackknife based confidence intervals and the same is true of most quantities in the spectral analysis toolbox. Similarly, the local regression and likelihood toolbox is a MEX front-end to the Locfit package which provides a comprehensive set of tools for model testing and validation.\n\nThe GUI can be invoked from the MATLAB prompt by typing ndb – short for the Neuro Data Browser (NDB) – which provides a standard user interface for loading, visualizing and analyzing neurobiological time series data. The data can be in different formats such as EEG, MEG, FMRI etc. A standard UI for selecting and visualizing relevant portions (samples/channels/trials) of the time series is used so that it is possible to view, store and analyze the data for a typical study – which can be of the order of several Gb's – from multiple modalities / formats on a single platform. The GUI also provides the facility to view a summary of all the data objects that have been added to the system pool. Currently there are two views of the summarized data – by patient name and by modality/format.\n\nAt a basic level, the GUI enables users, to load data, analyze them and visualize the results within the Browser framework without a need to write separate MATLAB codes. For advanced users, it also provides a command line interface, so that data can be directly loaded and visualized for analysis. The usage of XML based plugin-architecture allows for extending support to other modalities and formats and also serves to integrate any other MATLAB toolbox with minimal changes in the plugin XML.\n\nThe M2HTML documentation is an archive of online help for all MATLAB routines incorporated in Chronux. This consists of function descriptions and dependency graphs.\n\n- Partha Mitra and B. Pesaran, \"Analysis of Dynamic Brain Imaging Data.\" The Biophysical Journal, Volume 76 (1999), 691–708, arxiv.org/abs/q-bio/0309028.\n- Partha Mitra and Hemant Bokil. \"Observed Brain Dynamics\", Oxford University Press, USA (2007), Link connecting the Book\n- Donald. B. Percival and Andrew. T. Walden. \"Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques\", Cambridge University Press, UK (2002).\n- Peter Stoica and Randolph. L. Moses. \"Introduction to Spectral Analysis\", Prentice Hall, USA (1997).\n- Richard Shiavi. \"Introduction to Applied Statistical Signal Analysis\", Academic Press, (1999).\n\n- Welcome to Chronux\n", "related": "NONE"}
{"id": "16577694", "url": "https://en.wikipedia.org/wiki?curid=16577694", "title": "Alpha beta filter", "text": "Alpha beta filter\n\nAn alpha beta filter (also called alpha-beta filter, f-g filter or g-h filter) is a simplified form of observer for estimation, data smoothing and control applications. It is closely related to Kalman filters and to linear state observers used in control theory. Its principal advantage is that it does not require a detailed system model.\n\nAn alpha beta filter presumes that a system is adequately approximated by a model having two internal states, where the first state is obtained by integrating the value of the second state over time. Measured system output values correspond to observations of the first model state, plus disturbances. This very low order approximation is adequate for many simple systems, for example, mechanical systems where position is obtained as the time integral of velocity. Based on a mechanical system analogy, the two states can be called \"position x\" and \"velocity v\". Assuming that velocity remains approximately constant over the small time interval \"ΔT\" between measurements, the position state is projected forward to predict its value at the next sampling time using equation 1.\n\nSince velocity variable \"v\" is presumed constant, its projected value at the next sampling time equals the current value.\n\nIf additional information is known about how a driving function will change the \"v\" state during each time interval, equation 2 can be modified to include it.\n\nThe output measurement is expected to deviate from the prediction because of noise and dynamic effects not included in the simplified dynamic model. This prediction error \"r\" is also called the \"residual\" or \"innovation\", based on statistical or Kalman filtering interpretations\n\nSuppose that residual \"r\" is positive. This could result because the previous \"x\" estimate was low, the previous \"v\" was low, or some combination of the two. The alpha beta filter takes selected \"alpha\" and \"beta\" constants (from which the filter gets its name), uses \"alpha\" times the deviation \"r\" to correct the position estimate, and uses \"beta\" times the deviation \"r\" to correct the velocity estimate. An extra \"ΔT\" factor conventionally serves to normalize magnitudes of the multipliers.\n\nThe corrections can be considered small steps along an estimate of the gradient direction. As these adjustments accumulate, error in the state estimates is reduced. For convergence and stability, the values of the \"alpha\" and \"beta\" multipliers should be positive and small:\n\nNoise is suppressed only if formula_9, otherwise the noise is amplified.\n\nValues of \"alpha\" and \"beta\" typically are adjusted experimentally. In general, larger \"alpha\" and \"beta\" gains tend to produce faster response for tracking transient changes, while smaller \"alpha\" and \"beta\" gains reduce the level of noise in the state estimates. If a good balance between accurate tracking and noise reduction is found, and the algorithm is effective, filtered estimates are more accurate than the direct measurements. This motivates calling the alpha-beta process a \"filter\".\n\nInitialize. \n- Set the initial values of state estimates \"x\" and \"v\", using prior information or additional measurements; otherwise, set the initial state values to zero.\n- Select values of the \"alpha\" and \"beta\" correction gains.\n\nUpdate. Repeat for each time step ΔT:\n\nAlpha Beta filter can be implemented in C as follows:\n1. include <stdio.h>\n2. include <stdlib.h>\n\nint main()\n\n} \nThe following images depict the outcome of the above program in graphical format. In each image, the blue trace is the input signal; the output is red in the first image, yellow in the second, and green in the third. For the first two images, the output signal is visibly smoother than the input signal and lacks extreme spikes seen in the input. Also, the output moves in an estimate of gradient direction of input.\n\nThe higher the alpha parameter, the higher is the effect of input and the less damping is seen. A low value of beta is effective in controlling sudden surges in velocity. Also, as alpha increases beyond unity, the output becomes rougher and more uneven than the input.\n\nMore general state observers, such as the Luenberger observer for linear control systems, use a rigorous system model. Linear observers use a gain matrix to determine state estimate corrections from multiple deviations between measured variables and predicted outputs that are linear combinations of state variables. In the case of alpha beta filters, this gain matrix reduces to two terms. There is no general theory for determining the best observer gain terms, and typically gains are adjusted experimentally for both.\n\nThe linear Luenberger observer equations reduce to the alpha beta filter by applying the following specializations and simplifications.\n\n- The discrete state transition matrix A is a square matrix of dimension 2, with all main diagonal terms equal to 1, and the first super-diagonal terms equal to \"ΔT\".\n- The observation equation matrix C has one row that selects the value of the first state variable for output.\n- The filter correction gain matrix L has one column containing the alpha and beta gain values.\n- Any known driving signal for the second state term is represented as part of the input signal vector u, otherwise the u vector is set to zero.\n- Input coupling matrix B has a non-zero gain term as its last element if vector u is non-zero.\n\nA Kalman filter estimates the values of state variables and corrects them in a manner similar to an alpha beta filter or a state observer. However, a Kalman filter does this in a much more formal and rigorous manner. The principal differences between Kalman filters and alpha beta filters are the following.\n\n- Like state observers, Kalman filters use a detailed dynamic system model that is not restricted to two states.\n- Like state observers, Kalman filters in general use multiple observed variables to correct state variable estimates, and these do not have to be direct measurements of individual system states.\n- A Kalman filter uses covariance noise models for states and observations. Using these, a time-dependent estimate of state covariance is updated automatically, and from this the Kalman gain matrix terms are calculated. Alpha beta filter gains are manually selected and static.\n- For certain classes of problems, a Kalman filter is Wiener optimal, while alpha beta filtering is in general suboptimal.\n\nA Kalman filter designed to track a moving object using a constant-velocity target dynamics (process) model (i.e., constant velocity between measurement updates) with process noise covariance and measurement covariance held constant will converge to the same structure as an alpha-beta filter. However, a Kalman filter's gain is computed recursively at each time step using the assumed process and measurement error statistics, whereas the alpha-beta's gain is computed ad hoc.\n\nThe alpha-beta filter becomes a steady-state Kalman filter if filter parameters are calculated from the sampling interval formula_10, the process variance formula_11 and the noise variance formula_12 like this\n\nThis choice of filter parameters minimizes the mean square error.\n\nThe steady state innovation variance formula_17 can be expressed as:\n\nA simpler member of this family of filters is the alpha filter which observes only one state:\n\nwith the optimal parameter calculated like this:\n\nThis calculation is identical for a moving average and a low-pass filter.\n\nWhen the second state variable varies quickly, i.e. when the acceleration of the first state is large, it can be useful to extend the states of the alpha beta filter by one level. In this extension, the second state variable \"v\" is obtained from integrating a third \"acceleration\" state, analogous to the way that the first state is obtained by integrating the second. An equation for the \"a\" state is added to the equation system. A third multiplier, \"gamma\", is selected for applying corrections to the new \"a\" state estimates. This yields the \"alpha beta gamma\" update equations.\n\nSimilar extensions to additional higher orders are possible, but most systems of higher order tend to have significant interactions among the multiple states, so approximating the system dynamics as a simple integrator chain is less likely to prove useful.\n\nCalculating optimal parameters for the alpha-beta-gamma filter is a bit more involved than for the alpha-beta filter:\n\n", "related": "\n- Kalman filter\n- Control theory\n- State space (controls)\n- Moving average\n\n- Sources\n- \"The Alpha-Beta Filter\", Penoyer, Robert; C Users Journal, July 1993\n- \"Engine Speed Monitoring: The Alpha-Beta Filter\", Microstar Laboratories\n- \"Reconciling steady-state Kalman and alpha-beta filter design\", by Painter, J.H.; Kerstetter, D.; Jowers, S. IEEE Transactions on Aerospace and Electronic Systems, Volume 26, Issue 6, Nov. 1990, pp. 986–991\n- Fixed-lag alpha-beta filter for target trajectory smoothing Ogle, T.L.; Blair, W.D. Aerospace and Electronic Systems, IEEE Transactions on Volume 40, Issue 4, Oct. 2004, pp. 1417–1421\n- \"Description of an alpha-beta Filter in Cartesian Coordinates\", by Cantrell, Ben H., NAVAL RESEARCH LAB WASHINGTON DC, March 21, 1973\n- \"Comparison of Four Filtering Options for a Radar Tracking Problem\", by Lawton, John A. ; Jesionowski, Robert J. ; Zarchan, Paul. NAVAL SURFACE WARFARE CENTER DAHLGREN DIV VA, 1979.\n- \"Mathematical Techniques in Multisensor Data Fusion\", By David Lee Hall, Sonya A. H., Artech House, 2004, , section 4.4.4\n\n- Alpha-Beta C# source code sample\n- Alpha-Beta C source code sample\n- Tracking performance of the alpha-beta tracker versus the Kalman filter\n"}
{"id": "20641486", "url": "https://en.wikipedia.org/wiki?curid=20641486", "title": "Mutual coherence (linear algebra)", "text": "Mutual coherence (linear algebra)\n\nIn linear algebra, the coherence or mutual coherence of a matrix \"A\" is defined as the maximum absolute value of the cross-correlations between the columns of \"A\".\n\nFormally, let formula_1 be the columns of the matrix \"A\", which are assumed to be normalized such that formula_2 The mutual coherence of \"A\" is then defined as\n\nA lower bound is \nA deterministic matrix with the mutual coherence almost meeting the lower bound can be constructed by Weil's theorem.\n\nThis concept was reintroduced by David Donoho and Michael Elad in the context of sparse representations. A special case of this definition for the two-ortho case appeared earlier in the paper by Donoho and Huo. The mutual coherence has since been used extensively in the field of sparse representations of signals. In particular, it is used as a measure of the ability of suboptimal algorithms such as matching pursuit and basis pursuit to correctly identify the true representation of a sparse signal.\n\n", "related": "\n- Compressed sensing\n- Restricted isometry property\n\n- Mutual coherence\n- R1magic : R package providing mutual coherence computation.\n"}
{"id": "21311709", "url": "https://en.wikipedia.org/wiki?curid=21311709", "title": "Noiselet", "text": "Noiselet\n\nNoiselets are a family of functions which are related to wavelets, analogously to the way that the Fourier basis is related to a time-domain signal. In other words, if a signal is compact in the wavelet domain, then it will be spread out in the noiselet domain, and conversely.\n\nThe complementarity of wavelets and noiselets means that noiselets can be used in compressed sensing to reconstruct a signal (such as an image) which has a compact representation in wavelets. MRI data can be acquired in noiselet domain, and, subsequently, images can be reconstructed from undersampled data using compressive-sensing reconstruction.\n", "related": "NONE"}
{"id": "4415205", "url": "https://en.wikipedia.org/wiki?curid=4415205", "title": "Linear canonical transformation", "text": "Linear canonical transformation\n\nIn Hamiltonian mechanics, the linear canonical transformation (LCT) is a family of integral transforms that generalizes many classical transforms. It has 4 parameters and 1 constraint, so it is a 3-dimensional family, and can be visualized as the action of the special linear group SL(R) on the time–frequency plane (domain).\n\nThe LCT generalizes the Fourier, fractional Fourier, Laplace, Gauss–Weierstrass, Bargmann and the Fresnel transforms as particular cases. The name \"linear canonical transformation\" is from canonical transformation, a map that preserves the symplectic structure, as SL(R) can also be interpreted as the symplectic group Sp, and thus LCTs are the linear maps of the time–frequency domain which preserve the symplectic form.\n\nThe basic properties of the transformations mentioned above, such as scaling, shift, coordinate multiplication are considered. Any linear canonical transformation is related to affine transformations in phase space, defined by time-frequency or position-momentum coordinates.\n\nThe LCT can be represented in several ways; most easily, it can be parameterized by a 2×2 matrix with determinant 1, i.e., an element of the special linear group SL(C). Then for any such matrix formula_1 with \"ad\" − \"bc\" = 1, the corresponding integral transform from a function formula_2 to formula_3 is defined as\n\nMany classical transforms are special cases of the linear canonical transform:\n\n- The Fourier transform corresponds to rotation by 90°, represented by the matrix:\n\n- The fractional Fourier transform corresponds to rotation by an arbitrary angle; they are the elliptic elements of SL(R), represented by the matrices:\n\n- The Fresnel transform corresponds to shearing, and are a family of parabolic elements, represented by the matrices:\n\n- The Laplace transform corresponds to rotation by 90° into the complex domain, and can be represented by the matrix:\n\n- The Fractional Laplace transform corresponds to rotation by an arbitrary angle into the complex domain, and can be represented by the matrix:\n\nComposition of LCTs corresponds to multiplication of the corresponding matrices; this is also known as the \"additivity property of the WDF\".\n\nIn detail, if the LCT is denoted by \"O\", i.e.\n\nthen\n\nwhere\n\nIf formula_12is the formula_13, where formula_13is the LCT of formula_15, then\n\nLCT is equal to the twisting operation for the WDF and the Cohen's class distribution also has the twisting operation.\n\nWe can freely use the LCT to transform the parallelogram whose center is at (0,0) to another parallelogram which has the same same area and the same center\nFrom this picture we know that the point (-1,2) transform to the point (0,1) and the point (1,2) transform to the point (4,3). As the result, we can write down the equations below\n\nformula_17\n\nwe can solve the equations and get (a,b,c,d) is equal to (2,1,1,1)\n\nFrom the following picture, we summarize the LCT with other transform or properties\n\nParaxial optical systems implemented entirely with thin lenses and propagation through free space and/or graded index (GRIN) media, are quadratic phase systems (QPS); these were known before Moshinsky and Quesne (1974) called attention to their significance in connection with canonical transformations in quantum mechanics. The effect of any arbitrary QPS on an input wavefield can be described using the linear canonical transform, a particular case of which was developed by Segal (1963) and Bargmann (1961) in order to formalize Fock's (1928) boson calculus.\n\nIn Quantum mechanics, linear canonical transformations can be defined as the linear transformations mixing the Momentum operator with the Position operator and leaving invariant the Canonical commutation relations.\n\nCanonical transforms are used to analyze differential equations. These include diffusion, the Schrödinger free particle, the linear potential (free-fall), and the attractive and repulsive oscillator equations. It also includes a few others such as the Fokker–Planck equation. Although this class is far from universal, the ease with which solutions and properties are found makes canonical transforms an attractive tool for problems such as these.\n\nWave propagation through air, a lens, and between satellite dishes are discussed here. All of the computations can be reduced to 2×2 matrix algebra. This is the spirit of LCT.\n\nAssuming the system looks like as depicted in the figure, the wave travels from plane \"x\", \"y\" to the plane of \"x\" and \"y\".\nThe Fresnel transform is used to describe electromagnetic wave propagation in air:\n\nwith\n\nThis is equivalent to LCT (shearing), when\n\nWhen the travel distance (\"z\") is larger, the shearing effect is larger.\n\nWith the lens as depicted in the figure, and the refractive index denoted as \"n\", the result is:\n\nwith \"f\" the focal length and \"Δ\" the thickness of the lens.\n\nThe distortion passing through the lens is similar to LCT, when\n\nThis is also a shearing effect: when the focal length is smaller, the shearing effect is larger.\n\nThe spherical mirror—e.g., a satellite dish—can be described as a LCT, with\n\nThis is very similar to lens, except focal length is replaced by the radius of the dish. Therefore, if the radius is smaller, the shearing effect is larger.\n\nThe relation between the input and output we can use LCT to represent\n\nformula_23\n\n(1) If z1 = z2 = 2f, it is reverse real image\n\n(2) If z1 = z2 = f, it is Fourier transform+scaling\n\n(3) if z1=z2, it is fractional Fourier transform+scaling\n\nIn this part, we show the basic properties of LCT\n\nWith the two-dimension column vector r defined as r =formula_24, we show some basic properties (result) for the specific input below\nThe system considered is depicted in the figure to the right: two dishes – one being the emitter and the other one the receiver – and a signal travelling between them over a distance \"D\".\nFirst, for dish A (emitter), the LCT matrix looks like this:\n\nThen, for dish B (receiver), the LCT matrix similarly becomes:\n\nLast, for the propagation of the signal in air, the LCT matrix is:\n\nPutting all three components together, the LCT of the system is:\n\nIt was shown that a relation can be established between some properties of the elementary Fermion in the Standard Model of Particle physics and Spin representation of some particular linear canonical transformations. In this approach, the Electric charge, Weak hypercharge and Weak isospin of the particles are expressed as linear combinations of some operators defined from the generators of the Clifford algebra corresponding to the spinorial representation of linear canonical transformations.\n\n", "related": "\n- Segal–Shale–Weil distribution, a metaplectic group of operators related to the chirplet transform\n\n- Other time–frequency transforms:\n\n- Fractional Fourier transform\n- Continuous Fourier transform\n- Chirplet transform\n\n- Applications:\n\n- Focus recovery based on the linear canonical transform\n- Ray transfer matrix analysis\n\n- J.J. Healy, M.A. Kutay, H.M. Ozaktas and J.T. Sheridan, \"\"Linear Canonical Transforms: Theory and Applications\", Springer, New York 2016.\n- J.J. Ding, \"Time–frequency analysis and wavelet transform course note\", the Department of Electrical Engineering, National Taiwan University (NTU), Taipei, Taiwan, 2007.\n- K.B. Wolf, \"Integral Transforms in Science and Engineering\"\", Ch. 9&10, New York, Plenum Press, 1979.\n- S.A. Collins, \"Lens-system diffraction integral written in terms of matrix optics,\" \"J. Opt. Soc. Amer.\" 60, 1168–1177 (1970).\n- M. Moshinsky and C. Quesne, \"Linear canonical transformations and their unitary representations,\" \"J. Math. Phys.\" 12, 8, 1772–1783, (1971).\n- B.M. Hennelly and J.T. Sheridan, \"Fast Numerical Algorithm for the Linear Canonical Transform\", \"J. Opt. Soc. Am. A\" 22, 5, 928–937 (2005).\n- H.M. Ozaktas, A. Koç, I. Sari, and M.A. Kutay, \"Efficient computation of quadratic-phase integrals in optics\", \"Opt. Let.\" 31, 35–37, (2006).\n- Bing-Zhao Li, Ran Tao, Yue Wang, \"New sampling formulae related to the linear canonical transform\", \"Signal Processing\" '87', 983–990, (2007).\n- A. Koç, H.M. Ozaktas, C. Candan, and M.A. Kutay, \"Digital computation of linear canonical transforms\", \"IEEE Trans. Signal Process.\", vol. 56, no. 6, 2383–2394, (2008).\n- Ran Tao, Bing-Zhao Li, Yue Wang, \"On sampling of bandlimited signals associated with the linear canonical transform\", \"IEEE Transactions on Signal Processing\", vol. 56, no. 11, 5454–5464, (2008).\n- D. Stoler, \"Operator methods in Physical Optics\", \"26th Annual Technical Symposium\". International Society for Optics and Photonics, 1982.\n- Tian-Zhou Xu, Bing-Zhao Li, \" \"Linear Canonical Transform and Its Applications \"\", Beijing, Science Press, 2013.\n- Raoelina Andriambololona, R. T. Ranaivoson, H.D.E Randriamisy, R. Hanitriarivo, \"Dispersion Operators Algebra and Linear Canonical Transformations\", \"arXiv:1608.02268 [quant-ph]\", \"International Journal of Theoretical Physics\", Volume 56, Issue 4, pp 1258–1273, Springer, 2017\n- R.T. Ranaivoson, Raoelina Andriambololona, R. Hanitriarivo, \"Properties of Elementary Fermions of the Standard Model deduced from Linear Canonical Transformations\",\"arXiv:1806.07228 [physics.gen-ph]\", 2018.\n- Tatiana Alieva., Martin J. Bastiaans. (2016) The Linear Canonical Transformations: Definition and Properties. In: Healy J., Alper Kutay M., Ozaktas H., Sheridan J. (eds) Linear Canonical Transforms. Springer Series in Optical Sciences, vol 198. Springer, New York, NY\n"}
{"id": "22200477", "url": "https://en.wikipedia.org/wiki?curid=22200477", "title": "Ringing artifacts", "text": "Ringing artifacts\n\nIn signal processing, particularly digital image processing, ringing artifacts are artifacts that appear as spurious signals near sharp transitions in a signal. Visually, they appear as bands or \"ghosts\" near edges; audibly, they appear as \"echos\" near transients, particularly sounds from percussion instruments; most noticeable are the pre-echos. The term \"ringing\" is because the output signal oscillates at a fading rate around a sharp transition in the input, similar to a bell after being struck. As with other artifacts, their minimization is a criterion in filter design.\n\nThe main cause of ringing artifacts is due to a signal being bandlimited (specifically, not having high frequencies) or passed through a low-pass filter; this is the frequency domain description.\nIn terms of the time domain, the cause of this type of ringing is the ripples in the sinc function, which is the impulse response (time domain representation) of a perfect low-pass filter. Mathematically, this is called the Gibbs phenomenon.\n\nOne may distinguish overshoot (and undershoot), which occurs when transitions are accentuated – the output is higher than the input – from ringing, where \"after\" an overshoot, the signal overcorrects and is now below the target value; these phenomena often occur together, and are thus often conflated and jointly referred to as \"ringing\".\n\nThe term \"ringing\" is most often used for ripples in the \"time\" domain, though it is also sometimes used for \"frequency\" domain effects:\nwindowing a filter in the time domain by a rectangular function causes ripples in the \"frequency\" domain for the same reason as a brick-wall low pass filter (rectangular function in the \"frequency\" domain) causes ripples in the \"time\" domain, in each case the Fourier transform of the rectangular function being the sinc function.\n\nThere are related artifacts caused by other frequency domain effects,\nand similar artifacts due to unrelated causes.\n\nBy definition, ringing occurs when a non-oscillating input yields an oscillating output: formally, when an input signal which is monotonic on an interval has output response which is not monotonic. This occurs most severely when the impulse response or step response of a filter has oscillations – less formally, if for a spike input, respectively a step input (a sharp transition), the output has bumps. Ringing most commonly refers to step ringing, and that will be the focus.\n\nRinging is closely related to overshoot and undershoot, which is when the output takes on values higher than the maximum (respectively, lower than the minimum) input value: one can have one without the other, but in important cases, such as a low-pass filter, one first has overshoot, then the response bounces back below the steady-state level, causing the first ring, and then oscillates back and forth above and below the steady-state level. Thus overshoot is the first step of the phenomenon, while ringing is the second and subsequent steps. Due to this close connection, the terms are often conflated, with \"ringing\" referring to both the initial overshoot and the subsequent rings.\n\nIf one has a linear time invariant (LTI) filter, then one can understand the filter and ringing in terms of the impulse response (the time domain view), or in terms of its Fourier transform, the frequency response (the frequency domain view). Ringing is a \"time\" domain artifact, and in filter design is traded off with desired frequency domain characteristics: the desired frequency response may cause ringing, while reducing or eliminating ringing may worsen the frequency response.\n\nThe central example, and often what is meant by \"ringing artifacts\", is the ideal (brick-wall) low-pass filter, the sinc filter. This has an oscillatory impulse response function, as illustrated above, and the step response – its integral, the sine integral – thus also features oscillations, as illustrated at right.\n\nThese ringing artifacts are not results of imperfect implementation or windowing: the ideal low-pass filter, while possessing the desired frequency response, necessarily causes ringing artifacts in the \"time\" domain.\n\nIn terms of impulse response, the correspondence between these artifacts and the behavior of the function is as follows:\n- impulse undershoot is equivalent to the impulse response having negative values,\n- impulse ringing (ringing near a point) is precisely equivalent to the impulse response having oscillations, which is equivalent to the derivative of the impulse response alternating between negative and positive values,\n- and there is no notion of impulse overshoot, as the unit impulse is assumed to have infinite height (and integral 1 – a Dirac delta function), and thus cannot be overshot.\n\nTurning to step response,\nthe step response is the integral of the impulse response; formally, the value of the step response at time \"a\" is the integral formula_1 of the impulse response. Thus values of the step response can be understood in terms of \"tail\" integrals of the impulse response.\n\nAssume that the overall integral of the impulse response is 1, so it sends constant input to the same constant as output – otherwise the filter has gain, and scaling by gain gives an integral of 1.\n- Step undershoot is equivalent to a tail integral being negative, in which case the magnitude of the undershoot is the value of the tail integral.\n- Step overshoot is equivalent to a tail integral being greater than 1, in which case the magnitude of the overshoot is the amount by which the tail integral exceeds 1 – or equivalently the value of the tail in the other direction, formula_2 since these add up to 1.\n- Step ringing is equivalent to tail integrals alternating between increasing and decreasing – taking derivatives, this is equivalent to the impulse response alternating between positive and negative values. Regions where an impulse response are below or above the \"x\"-axis (formally, regions between zeros) are called lobes, and the magnitude of an oscillation (from peak to trough) equals the integral of the corresponding lobe.\n\nThe impulse response may have many negative lobes, and thus many oscillations, each yielding a ring, though these decay for practical filters, and thus one generally only sees a few rings, with the first generally being most pronounced.\n\nNote that if the impulse response has small negative lobes and larger positive lobes, then it will exhibit ringing but not undershoot or overshoot: the tail integral will always be between 0 and 1, but will oscillate down at each negative lobe. However, in the sinc filter, the lobes monotonically decrease in magnitude and alternate in sign, as in the alternating harmonic series, and thus tail integrals alternate in sign as well, so it exhibits overshoot as well as ringing.\n\nConversely, if the impulse response is always nonnegative, so it has no negative lobes – the function is a probability distribution – then the step response will exhibit neither ringing nor overshoot or undershoot – it will be a monotonic function growing from 0 to 1, like a cumulative distribution function. Thus the basic solution from the time domain perspective is to use filters with nonnegative impulse response.\n\nThe frequency domain perspective is that ringing is caused by the sharp cut-off in the rectangular passband in the frequency domain, and thus is reduced by smoother roll-off, as discussed below.\n\nSolutions depend on the parameters of the problem: if the cause is a low-pass filter, one may choose a different filter design, which reduces artifacts at the expense of worse frequency domain performance. On the other hand, if the cause is a band-limited signal, as in JPEG, one cannot simply replace a filter, and ringing artifacts may prove hard to fix – they are present in JPEG 2000 and many audio compression codecs (in the form of pre-echo), as discussed in the examples.\n\nIf the cause is the use of a brick-wall low-pass filter, one may replace the filter with one that reduces the time domain artifacts, at the cost of frequency domain performance. This can be analyzed from the time domain or frequency domain perspective.\n\nIn the time domain, the cause is an impulse response that oscillates, assuming negative values. This can be resolved by using a filter whose impulse response is non-negative and does not oscillate, but shares desired traits. For example, for a low-pass filter, the Gaussian filter is non-negative and non-oscillatory, hence causes no ringing. However, it is not as good as a low-pass filter: it rolls off in the passband, and leaks in the stopband: in image terms, a Gaussian filter \"blurs\" the signal, which reflects the attenuation of desired higher frequency signals in the passband.\n\nA general solution is to use a window function on the sinc filter, which cuts off or reduces the negative lobes: these respectively eliminate and reduce overshoot and ringing. Note that truncating some but not all of the lobes eliminates the ringing beyond that point, but does not reduce the amplitude of the ringing that is not truncated (because this is determined by the size of the lobe), and increases the magnitude of the overshoot if the last non-cut lobe is negative, since the magnitude of the overshoot is the integral of the \"tail,\" which is no longer canceled by positive lobes.\n\nFurther, in practical implementations one at least truncates sinc, otherwise one must use infinitely many data points (or rather, all points of the signal) to compute every point of the output – truncation corresponds to a rectangular window, and makes the filter practically implementable, but the frequency response is no longer perfect.\nIn fact, if one takes a brick wall low-pass filter (sinc in time domain, rectangular in frequency domain) and truncates it (multiplies with a rectangular function in the time domain), this convolves the frequency domain with sinc (Fourier transform of the rectangular function) and causes ringing in the \"frequency\" domain, which is referred to as \"ripple.\" In symbols, formula_3 The frequency ringing in the stopband is also referred to as side lobes. Flat response in the passband is desirable, so one windows with functions whose Fourier transform has fewer oscillations, so the frequency domain behavior is better.\n\nMultiplication in the time domain corresponds to convolution in the frequency domain, so multiplying a filter by a window function corresponds to convolving the Fourier transform of the original filter by the Fourier transform of the window, which has a smoothing effect – thus windowing in the time domain corresponds to smoothing in the frequency domain, and reduces or eliminates overshoot and ringing.\n\nIn the frequency domain, the cause can be interpreted as due to the sharp (brick-wall) cut-off, and ringing reduced by using a filter with smoother roll-off. This is the case for the Gaussian filter, whose magnitude Bode plot is a downward opening parabola (quadratic roll-off), as its Fourier transform is again a Gaussian, hence (up to scale) formula_4 – taking logarithms yields formula_5\nIn electronic filters, the trade-off between frequency domain response and time domain ringing artifacts is well-illustrated by the Butterworth filter: the frequency response of a Butterworth filter slopes down linearly on the log scale, with a first-order filter having slope of −6 dB per octave, a second-order filter –12 dB per octave, and an \"n\"th order filter having slope of formula_6 dB per octave – in the limit, this approaches a brick-wall filter. Thus, among these the, first-order filter rolls off slowest, and hence exhibits the fewest time domain artifacts, but leaks the most in the stopband, while as order increases, the leakage decreases, but artifacts increase.\n\nWhile ringing artifacts are generally considered undesirable, the initial overshoot (haloing) at transitions increases acutance (apparent sharpness) by increasing the derivative across the transition, and thus can be considered as an enhancement.\n\nAnother artifact is overshoot (and undershoot), which manifests itself not as rings, but as an increased jump at the transition. It is related to ringing, and often occurs in combination with it.\n\nOvershoot and undershoot are caused by a negative tail – in the sinc, the integral from the first zero to infinity, including the first negative lobe. While ringing is caused by a following \"positive\" tail – in sinc, the integral from the second zero to infinity, including the first non-central positive lobe.\nThus overshoot is \"necessary\" for ringing, but can occur separately: for example, the 2-lobed Lanczos filter has only a single negative lobe on each side, with no following positive lobe, and thus exhibits overshoot but no ringing, while the 3-lobed Lanczos filter exhibits both overshoot and ringing, though the windowing reduces this compared to the sinc filter or the truncated sinc filter.\n\nSimilarly, the convolution kernel used in bicubic interpolation is similar to a 2-lobe windowed sinc, taking on negative values, and thus produces overshoot artifacts, which appear as halos at transitions.\n\nFollowing from overshoot and undershoot is clipping.\nIf the signal is bounded, for instance an 8-bit or 16-bit integer, this overshoot and undershoot can exceed the range of permissible values, thus causing clipping.\n\nStrictly speaking, the clipping is caused by the combination of overshoot and limited numerical accuracy, but it is closely associated with ringing, and often occurs in combination with it.\n\nClipping can also occur for unrelated reasons, from a signal simply exceeding the range of a channel.\n\nOn the other hand, clipping can be exploited to conceal ringing in images. Some modern JPEG codecs, such as mozjpeg and ISO libjpeg, use such a trick to reduce ringing by deliberately causing overshoots in the IDCT results. This idea originated in a mozjpeg patch.\n\nIn signal processing and related fields, the general phenomenon of time domain oscillation is called ringing, while frequency domain oscillations are generally called ripple, though generally not \"rippling\".\n\nA key source of ripple in digital signal processing is the use of window functions: if one takes an infinite impulse response (IIR) filter, such as the sinc filter, and windows it to make it have finite impulse response, as in the window design method, then the frequency response of the resulting filter is the convolution of the frequency response of the IIR filter with the frequency response of the window function. Notably, the frequency response of the rectangular filter is the sinc function (the rectangular function and the sinc function are Fourier dual to each other), and thus truncation of a filter in the time domain corresponds to multiplication by the rectangular filter, thus convolution by the sinc filter in the frequency domain, causing ripple. In symbols, the frequency response of formula_7 is formula_8 In particular, truncating the sinc function itself yields formula_9 in the time domain, and formula_10 in the frequency domain, so just as low-pass filtering (truncating in the frequency domain) causes \"ringing\" in the time domain, truncating in the time domain (windowing by a rectangular filter) causes \"ripple\" in the frequency domain.\n\nJPEG compression can introduce ringing artifacts at sharp transitions, which are particularly visible in text.\n\nThis is a due to loss of high frequency components, as in step response ringing.\nJPEG uses 8×8 blocks, on which the discrete cosine transform (DCT) is performed. The DCT is a Fourier-related transform, and ringing occurs because of loss of high frequency components or loss of precision in high frequency components.\n\nThey can also occur at the edge of an image: since JPEG splits images into 8×8 blocks, if an image is not an integer number of blocks, the edge cannot easily be encoded, and solutions such as filling with a black border create a sharp transition in the source, hence ringing artifacts in the encoded image.\n\nRinging also occurs in the wavelet-based JPEG 2000.\n\nJPEG and JPEG 2000 have other artifacts, as illustrated above, such as blocking (\"jaggies\") and edge busyness (\"mosquito noise\"), though these are due to specifics of the formats, and are not ringing as discussed here.\n\nSome illustrations:\n- Baseline JPEG and JPEG2000 Artifacts Illustrated\nIn audio signal processing, ringing can cause echoes to occur before and after transients, such as the impulsive sound from percussion instruments, such as cymbals (this is \"impulse\" ringing). The (causal) echo after the transient is not heard, because it is masked by the \ntransient, an effect called temporal masking. Thus only the (anti-causal) echo before the transient is heard, and the phenomenon is called pre-echo.\n\nThis phenomenon occurs as a compression artifact in audio compression algorithms that use Fourier-related transforms, such as MP3, AAC, and Vorbis.\n\nOther phenomena have similar symptoms to ringing, but are otherwise distinct in their causes. In cases where these cause circular artifacts around point sources, these may be referred to as \"rings\" due to the round shape (formally, an annulus), which is unrelated to the \"ringing\" (oscillatory decay) frequency phenomenon discussed on this page.\n\nEdge enhancement, which aims to increase edges, may cause ringing phenomena, particularly under repeated application, such as by a DVD player followed by a television. This may be done by \"high\"-pass filtering, rather than low-pass filtering.\n\nMany special functions exhibit oscillatory decay, and thus convolving with such a function yields ringing in the output; one may consider these ringing, or restrict the term to unintended artifacts in frequency domain signal processing.\n\nFraunhofer diffraction yields the Airy disk as point spread function, which has a ringing pattern.\nThe Bessel function of the first kind, formula_11 which is related to the Airy function, exhibits such decay.\nIn cameras, a combination of defocus and spherical aberration can yield circular artifacts (\"ring\" patterns). However, the pattern of these artifacts need not be similar to ringing (as discussed on this page) – they may exhibit oscillatory decay (circles of decreasing intensity), or other intensity patterns, such as a single bright band.\n\nGhosting is a form of television interference where an image is repeated. Though this is not ringing, it can be interpreted as convolution with a function, which is 1 at the origin and ε (the intensity of the ghost) at some distance, which is formally similar to the above functions (a single discrete peak, rather than continuous oscillation).\n\nIn photography, lens flare is a defect where various circles can appear around highlights, and with ghosts throughout a photo, due to undesired light, such as reflection and scattering off elements in the lens.\n\nVisual illusions can occur at transitions, as in Mach bands, which perceptually exhibit a similar undershoot/overshoot to the Gibbs phenomenon.\n\n", "related": "\n- Artifact (error)\n- Digital artifact\n- sinc filter\n- Brick-wall filter\n- Chromatic aberration\n- Ghosting (television)\n- Gibbs phenomenon\n- Low-pass filter\n- Pre-echo\n- Purple fringing\n"}
{"id": "22223374", "url": "https://en.wikipedia.org/wiki?curid=22223374", "title": "Multiscale geometric analysis", "text": "Multiscale geometric analysis\n\nMultiscale geometric analysis or geometric multiscale analysis is an emerging area of high-dimensional signal processing and data analysis.\n\n", "related": "\n- Wavelet\n- Scale space\n- Multi-scale approaches\n- Multiresolution analysis\n- Singular value decomposition\n- Compressed sensing\n\n"}
{"id": "20269925", "url": "https://en.wikipedia.org/wiki?curid=20269925", "title": "Free convolution", "text": "Free convolution\n\nFree convolution is the free probability analog of the classical notion of convolution of probability measures. Due to the non-commutative nature of free probability theory, one has to talk separately about additive and multiplicative free convolution, which arise from addition and multiplication of free random variables (see below; in the classical case, what would be the analog of free multiplicative convolution can be reduced to additive convolution by passing to logarithms of random variables). These operations have some interpretations in terms of empirical spectral measures of random matrices.\n\nThe notion of free convolution was introduced by Voiculescu.\n\nLet formula_1 and formula_2 be two probability measures on the real line, and assume that formula_3 is a random variable in a non commutative probability space with law formula_1 and formula_5 is a random variable in the same non commutative probability space with law formula_2. Assume finally that formula_3 and formula_5 are freely independent. Then the free additive convolution formula_9 is the law of formula_10. Random matrices interpretation: if formula_11 and formula_12 are some independent formula_13 by formula_13 Hermitian (resp. real symmetric) random matrices such that at least one of them is invariant, in law, under conjugation by any unitary (resp. orthogonal) matrix and such that the empirical spectral measures of formula_11 and formula_12 tend respectively to formula_1 and formula_2 as formula_13 tends to infinity, then the empirical spectral measure of formula_20 tends to formula_9.\n\nIn many cases, it is possible to compute the probability measure formula_9 explicitly by using complex-analytic techniques and the R-transform of the measures formula_1 and formula_2.\n\nThe rectangular free additive convolution (with ratio formula_25) formula_26 has also been defined in the non commutative probability framework by Benaych-Georges and admits the following random matrices interpretation. For formula_27, for formula_11 and formula_12 are some independent formula_13 by formula_31 complex (resp. real) random matrices such that at least one of them is invariant, in law, under multiplication on the left and on the right by any unitary (resp. orthogonal) matrix and such that the empirical singular values distribution of formula_11 and formula_12 tend respectively to formula_1 and formula_2 as formula_13 and formula_31 tend to infinity in such a way that formula_38 tends to formula_25, then the empirical singular values distribution of formula_20 tends to formula_41.\n\nIn many cases, it is possible to compute the probability measure formula_41 explicitly by using complex-analytic techniques and the rectangular R-transform with ratio formula_25 of the measures formula_1 and formula_2.\n\nLet formula_1 and formula_2 be two probability measures on the interval formula_48, and assume that formula_3 is a random variable in a non commutative probability space with law formula_1 and formula_5 is a random variable in the same non commutative probability space with law formula_2. Assume finally that formula_3 and formula_5 are freely independent. Then the free multiplicative convolution formula_55 is the law of formula_56 (or, equivalently, the law of formula_57. Random matrices interpretation: if formula_11 and formula_12 are some independent formula_13 by formula_13 non negative Hermitian (resp. real symmetric) random matrices such that at least one of them is invariant, in law, under conjugation by any unitary (resp. orthogonal) matrix and such that the empirical spectral measures of formula_11 and formula_12 tend respectively to formula_1 and formula_2 as formula_13 tends to infinity, then the empirical spectral measure of formula_67 tends to formula_55.\n\nA similar definition can be made in the case of laws formula_69 supported on the unit circle formula_70, with an orthogonal or unitary random matrices interpretation.\n\nExplicit computations of multiplicative free convolution can be carried out using complex-analytic techniques and the S-transform.\n\n- Free convolution can be used to give a proof of the free central limit theorem.\n- Free convolution can be used to compute the laws and spectra of sums or products of random variables which are free. Such examples include: random walk operators on free groups (Kesten measures); and asymptotic distribution of eigenvalues of sums or products of independent random matrices.\n\nThrough its applications to random matrices, free convolution has some strong connections with other works on G-estimation of Girko.\n\nThe applications in wireless communications, finance and biology have provided a useful framework when the number of observations is of the same order as the dimensions of the system.\n\n", "related": "\n- Convolution\n- Free probability\n- Random matrix\n\n- \"Free Deconvolution for Signal Processing Applications\", O. Ryan and M. Debbah, ISIT 2007, pp. 1846–1850\n- James A. Mingo, Roland Speicher: Free Probability and Random Matrices. Fields Institute Monographs, Vol. 35, Springer, New York, 2017.\n- D.-V. Voiculescu, N. Stammeier, M. Weber (eds.): Free Probability and Operator Algebras, Münster Lectures in Mathematics, EMS, 2016\n- Alcatel Lucent Chair on Flexible Radio\n- http://www.cmapx.polytechnique.fr/~benaych\n- http://folk.uio.no/oyvindry\n- survey articles of Roland Speicher on free probability.\n"}
{"id": "326462", "url": "https://en.wikipedia.org/wiki?curid=326462", "title": "Undersampling", "text": "Undersampling\n\nIn signal processing, undersampling or bandpass sampling is a technique where one samples a bandpass-filtered signal at a sample rate below its Nyquist rate (twice the upper cutoff frequency), but is still able to reconstruct the signal.\n\nWhen one undersamples a bandpass signal, the samples are indistinguishable from the samples of a low-frequency alias of the high-frequency signal. Such sampling is also known as bandpass sampling, harmonic sampling, IF sampling, and direct IF-to-digital conversion.\n\nThe Fourier transforms of real-valued functions are symmetrical around the 0 Hz axis. After sampling, only a periodic summation of the Fourier transform (called discrete-time Fourier transform) is still available. The individual frequency-shifted copies of the original transform are called \"aliases\". The frequency offset between adjacent aliases is the sampling-rate, denoted by \"f\". When the aliases are mutually exclusive (spectrally), the original transform and the original continuous function, or a frequency-shifted version of it (if desired), can be recovered from the samples. The first and third graphs of Figure 1 depict a baseband spectrum before and after being sampled at a rate that completely separates the aliases.\n\nThe second graph of Figure 1 depicts the frequency profile of a bandpass function occupying the band (\"A\", \"A\"+\"B\") (shaded blue) and its mirror image (shaded beige). The condition for a non-destructive sample rate is that the aliases of both bands do not overlap when shifted by all integer multiples of \"f\". The fourth graph depicts the spectral result of sampling at the same rate as the baseband function. The rate was chosen by finding the lowest rate that is an integer sub-multiple of \"A\" and also satisfies the baseband Nyquist criterion: \"f\" > 2\"B\".  Consequently, the bandpass function has effectively been converted to baseband. All the other rates that avoid overlap are given by these more general criteria, where \"A\" and \"A\"+\"B\" are replaced by \"f\" and \"f\", respectively:\n\nThe highest \"n\" for which the condition is satisfied leads to the lowest possible sampling rates.\n\nImportant signals of this sort include a radio's intermediate-frequency (IF), radio-frequency (RF) signal, and the individual \"channels\" of a filter bank.\n\nIf \"n\" > 1, then the conditions result in what is sometimes referred to as \"undersampling\", \"bandpass sampling\", or using a sampling rate less than the Nyquist rate (2\"f\"). For the case of a given sampling frequency, simpler formulae for the constraints on the signal's spectral band are given below.\n\nAs we have seen, the normal baseband condition for reversible sampling is that \"X\"(\"f\") = 0 outside the interval:\n  formula_6\n\nand the reconstructive interpolation function, or lowpass filter impulse response, is  formula_7\n\nTo accommodate undersampling, the bandpass condition is that \"X\"(\"f\") = 0 outside the union of open positive and negative frequency bands\n\nThe corresponding interpolation function is the bandpass filter given by this difference of lowpass impulse responses:\n\nOn the other hand, reconstruction is not usually the goal with sampled IF or RF signals. Rather, the sample sequence can be treated as ordinary samples of the signal frequency-shifted to near baseband, and digital demodulation can proceed on that basis, recognizing the spectrum mirroring when \"n\" is even.\n\nFurther generalizations of undersampling for the case of signals with multiple bands are possible, and signals over multidimensional domains (space or space-time) and have been worked out in detail by Igor Kluvánek.\n\n", "related": "\n- Drizzle (image processing)\n"}
{"id": "1326932", "url": "https://en.wikipedia.org/wiki?curid=1326932", "title": "Beamforming", "text": "Beamforming\n\nBeamforming or spatial filtering is a signal processing technique used in sensor arrays for directional signal transmission or reception. This is achieved by combining elements in an antenna array in such a way that signals at particular angles experience constructive interference while others experience destructive interference. Beamforming can be used at both the transmitting and receiving ends in order to achieve spatial selectivity. The improvement compared with omnidirectional reception/transmission is known as the directivity of the array.\n\nBeamforming can be used for radio or sound waves. It has found numerous applications in radar, sonar, seismology, wireless communications, radio astronomy, acoustics and biomedicine. Adaptive beamforming is used to detect and estimate the signal of interest at the output of a sensor array by means of optimal (e.g. least-squares) spatial filtering and interference rejection.\n\nTo change the directionality of the array when transmitting, a beamformer controls the phase and relative amplitude of the signal at each transmitter, in order to create a pattern of constructive and destructive interference in the wavefront. When receiving, information from different sensors is combined in a way where the expected pattern of radiation is preferentially observed.\n\nFor example, in sonar, to send a sharp pulse of underwater sound towards a ship in the distance, simply simultaneously transmitting that sharp pulse from every sonar projector in an array fails because the ship will first hear the pulse from the speaker that happens to be nearest the ship, then later pulses from speakers that happen to be further from the ship. The beamforming technique involves sending the pulse from each projector at slightly different times (the projector closest to the ship last), so that every pulse hits the ship at exactly the same time, producing the effect of a single strong pulse from a single powerful projector. The same technique can be carried out in air using loudspeakers, or in radar/radio using antennas.\n\nIn passive sonar, and in reception in active sonar, the beamforming technique involves combining delayed signals from each hydrophone at slightly different times (the hydrophone closest to the target will be combined after the longest delay), so that every signal reaches the output at exactly the same time, making one loud signal, as if the signal came from a single, very sensitive hydrophone. Receive beamforming can also be used with microphones or radar antennas.\n\nWith narrow-band systems the time delay is equivalent to a \"phase shift\", so in this case the array of antennas, each one shifted a slightly different amount, is called a phased array. A narrow band system, typical of radars, is one where the bandwidth is only a small fraction of the center frequency. With wide band systems this approximation no longer holds, which is typical in sonars.\n\nIn the receive beamformer the signal from each antenna may be amplified by a different \"weight.\" Different weighting patterns (e.g., Dolph-Chebyshev) can be used to achieve the desired sensitivity patterns. A main lobe is produced together with nulls and sidelobes. As well as controlling the main lobe width (beamwidth) and the sidelobe levels, the position of a null can be controlled. This is useful to ignore noise or jammers in one particular direction, while listening for events in other directions. A similar result can be obtained on transmission.\n\nFor the full mathematics on directing beams using amplitude and phase shifts, see the mathematical section in phased array.\n\nBeamforming techniques can be broadly divided into two categories:\n- conventional (fixed or switched beam) beamformers\n- adaptive beamformers or phased array\n- Desired signal maximization mode\n- Interference signal minimization or cancellation mode\n\nConventional beamformers, such as the Butler matrix, use a fixed set of weightings and time-delays (or phasings) to combine the signals from the sensors in the array, primarily using only information about the location of the sensors in space and the wave directions of interest. In contrast, adaptive beamforming techniques (e.g., MUSIC, SAMV) generally combine this information with properties of the signals actually received by the array, typically to improve rejection of unwanted signals from other directions. This process may be carried out in either the time or the frequency domain.\n\nAs the name indicates, an adaptive beamformer is able to automatically adapt its response to different situations. Some criterion has to be set up to allow the adaptation to proceed such as minimizing the total noise output. Because of the variation of noise with frequency, in wide band systems it may be desirable to carry out the process in the frequency domain.\n\nBeamforming can be computationally intensive. Sonar phased array has a data rate low enough that it can be processed in real-time in software, which is flexible enough to transmit or receive in several directions at once. In contrast, radar phased array has a data rate so high that it usually requires dedicated hardware processing, which is hard-wired to transmit or receive in only one direction at a time. However, newer field programmable gate arrays are fast enough to handle radar data in real-time, and can be quickly re-programmed like software, blurring the hardware/software distinction.\n\nSonar beamforming utilizes a similar technique to electromagnetic beamforming, but varies considerably in implementation details. Sonar applications vary from 1 Hz to as high as 2 MHz, and array elements may be few and large, or number in the hundreds yet very small. This will shift sonar beamforming design efforts significantly between demands of such system components as the \"front end\" (transducers, pre-amplifiers and digitizers) and the actual beamformer computational hardware downstream. High frequency, focused beam, multi-element imaging-search sonars and acoustic cameras often implement fifth-order spatial processing that places strains equivalent to Aegis radar demands on the processors.\n\nMany sonar systems, such as on torpedoes, are made up of arrays of up to 100 elements that must accomplish beam steering over a 100 degree field of view and work in both active and passive modes.\n\nSonar arrays are used both actively and passively in 1-, 2-, and 3-dimensional arrays.\n\n- 1-dimensional \"line\" arrays are usually in multi-element passive systems towed behind ships and in single- or multi-element side-scan sonar.\n- 2-dimensional \"planar\" arrays are common in active/passive ship hull mounted sonars and some side-scan sonar.\n- 3-dimensional spherical and cylindrical arrays are used in 'sonar domes' in the modern submarine and ships.\n\nSonar differs from radar in that in some applications such as wide-area-search all directions often need to be listened to, and in some applications broadcast to, simultaneously. Thus a multibeam system is needed. In a narrowband sonar receiver the phases for each beam can be manipulated entirely by signal processing software, as compared to present radar systems that use hardware to 'listen' in a single direction at a time.\n\nSonar also uses beamforming to compensate for the significant problem of the slower propagation speed of sound as compared to that of electromagnetic radiation. In side-look-sonars, the speed of the towing system or vehicle carrying the sonar is moving at sufficient speed to move the sonar out of the field of the returning sound \"ping\". In addition to focusing algorithms intended to improve reception, many side scan sonars also employ beam steering to look forward and backward to \"catch\" incoming pulses that would have been missed by a single sidelooking beam.\n\n- A conventional beamformer can be a simple beamformer also known as delay-and-sum beamformer. All the weights of the antenna elements can have equal magnitudes. The beamformer is steered to a specified direction only by selecting appropriate phases for each antenna. If the noise is uncorrelated and there are no directional interferences, the signal-to-noise ratio of a beamformer with formula_1 antennas receiving a signal of power formula_2, (where formula_3 is Noise variance or Noise power), is: formula_4\n- Null-steering beamformer\n- Frequency domain beamformer\n\nBeamforming techniques used in cellular phone standards have advanced through the generations to make use of more complex systems to achieve higher density cells, with higher throughput.\n- Passive mode: (almost) non-standardized solutions\n- Wideband Code Division Multiple Access (WCDMA) supports direction of arrival (DOA) based beamforming\n\n- Active mode: mandatory standardized solutions\n- 2G — Transmit antenna selection as an elementary beamforming\n- 3G — WCDMA: Transmit antenna array (TxAA) beamforming\n- 3G evolution — LTE/UMB: Multiple-input multiple-output (MIMO) precoding based beamforming with partial Space-Division Multiple Access (SDMA)\n- Beyond 3G (4G, 5G, …) — More advanced beamforming solutions to support Space-division multiple access (SDMA) such as closed loop beamforming and multi-dimensional beamforming are expected\n\nAn increasing number of consumer 802.11ac Wi-Fi devices with MIMO capability can support beamforming to boost data communication rates.\n\nFor receive (but not transmit), there is a distinction between analog and digital beamforming. For example, if there are 100 sensor elements, the \"digital beamforming\" approach entails that each of the 100 signals passes through an analog-to-digital converter to create 100 digital data streams. Then these data streams are added up digitally, with appropriate scale-factors or phase-shifts, to get the composite signals. By contrast, the \"analog beamforming\" approach entails taking the 100 analog signals, scaling or phase-shifting them using analog methods, summing them, and then usually digitizing the \"single\" output data stream.\n\nDigital beamforming has the advantage that the digital data streams (100 in this example) can be manipulated and combined in many possible ways in parallel, to get many different output signals in parallel. The signals from every direction can be measured simultaneously, and the signals can be integrated for a longer time when studying far-off objects and simultaneously integrated for a shorter time to study fast-moving close objects, and so on. This cannot be done as effectively for analog beamforming, not only because each parallel signal combination requires its own circuitry, but more fundamentally because digital data can be copied perfectly but analog data cannot. (There is only so much analog power available, and amplification adds noise.) Therefore, if the received analog signal is split up and sent into a large number of different signal combination circuits, it can reduce the signal-to-noise ratio of each.\n\nIn MIMO communication systems with large number of antennas, so called massive MIMO systems, the beamforming algorithms executed at the digital baseband can get very complex.\nIn addition, if all beamforming is done at baseband, each antenna needs its own RF feed. At high frequencies and with large number of antenna elements, this can be very costly, and increase loss and complexity in the system. To remedy these issues, hybrid beamforming has been suggested where some of the beamforming is done using analog components and not digital.\n\nThere are many possible different functions that can be performed using analog components instead of at the digital baseband.\n\nBeamforming can be used to try to extract sound sources in a room, such as multiple speakers in the cocktail party problem. This requires the locations of the speakers to be known in advance, for example by using the time of arrival from the sources to mics in the array, and inferring the locations from the distances.\n\nCompared to carrier-wave telecommunications, natural audio contains a variety of frequencies. It is advantageous to separate frequency bands prior to beamforming because different frequencies have different optimal beamform filters (and hence can be treated as separate problems, in parallel, and then recombined afterward). Properly isolating these bands involves specialized non-standard filter banks. In contrast, for example, the standard fast Fourier transform (FFT) band-filters implicitly assume that the only frequencies present in the signal are exact harmonics; frequencies which lie between these harmonics will typically activate all of the FFT channels (which is not what is wanted in a beamform analysis). Instead, filters can be designed in which only local frequencies are detected by each channel (while retaining the recombination property to be able to reconstruct the original signal), and these are typically non-orthogonal unlike the FFT basis.\n\n", "related": "\n- Three-dimensional beamforming\n- Aperture synthesis\n- Inverse synthetic aperture radar (ISAR)\n- Synthetic aperture radar\n- Synthetic aperture sonar\n- Thinned array curse\n- Window function\n- Synthetic-aperture magnetometry (SAM)\n- Microphone array\n- Zero-forcing precoding\n- Multibeam echosounder\n- Pencil (optics)\n- Periodogram\n- MUSIC\n- SAMV\n- Spatial multiplexing\n- Antenna diversity\n- Channel state information\n- Space–time code\n- Space–time block code\n- Dirty paper coding (DPC)\n- Smart antenna\n- WSDMA (Wideband Space Division Multiple Access)\n- Golomb ruler\n- Audio Surveillance\n- Reconfigurable antenna\n- Sensor array\n\n- Louay M. A. Jalloul and Sam. P. Alex, \"Evaluation Methodology and Performance of an IEEE 802.16e System\", Presented to the IEEE Communications and Signal Processing Society, Orange County Joint Chapter (ComSig), December 7, 2006. Available at: https://web.archive.org/web/20110414143801/http://chapters.comsoc.org/comsig/meet.html\n- H. L. Van Trees, Optimum Array Processing, Wiley, NY, 2002.\n- Jian Li, and Petre Stoica, eds. Robust adaptive beamforming. New Jersey: John Wiley, 2006.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n- \"A Primer on Digital Beamforming\" by Toby Haynes, March 26, 1998\n- \"What Is Beamforming?\", an introduction to sonar beamforming by Greg Allen.\n- \"Dolph–Chebyshev Weights\" \"antenna-theory.com\"\n- A collection of pages providing a simple introduction to microphone array beamforming\n- MU-MIMO Beamforming by Constructive Interference, Wolfram Demonstrations Project\n"}
{"id": "300730", "url": "https://en.wikipedia.org/wiki?curid=300730", "title": "Mel-frequency cepstrum", "text": "Mel-frequency cepstrum\n\nIn sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n\nMel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. This frequency warping can allow for better representation of sound, for example, in audio compression.\n\nMFCCs are commonly derived as follows:\n\n1. Take the Fourier transform of (a windowed excerpt of) a signal.\n2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n3. Take the logs of the powers at each of the mel frequencies.\n4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n5. The MFCCs are the amplitudes of the resulting spectrum.\n\nThere can be variations on this process, for example: differences in the shape or spacing of the windows used to map the scale, or addition of dynamics features such as \"delta\" and \"delta-delta\" (first- and second-order frame-to-frame difference) coefficients.\n\nThe European Telecommunications Standards Institute in the early 2000s defined a standardised MFCC algorithm to be used in mobile phones.\n\nMFCCs are commonly used as features in speech recognition systems, such as the systems which can automatically recognize numbers spoken into a telephone.\n\nMFCCs are also increasingly finding uses in music information retrieval applications such as genre classification, audio similarity measures, etc.\n\nMFCC values are not very robust in the presence of additive noise, and so it is common to normalise their values in speech recognition systems to lessen the influence of noise. Some researchers propose modifications to the basic MFCC algorithm to improve robustness, such as by raising the log-mel-amplitudes to a suitable power (around 2 or 3) before taking the DCT (Discrete Cosine Transform), which reduces the influence of low-energy components.\n\nPaul Mermelstein is typically credited with the development of the MFC. Mermelstein credits Bridle and Brown for the idea:\nBridle and Brown used a set of 19 weighted spectrum-shape coefficients given by the cosine transform of the outputs of a set of nonuniformly spaced bandpass filters. The filter spacing is chosen to be logarithmic above 1 kHz and the filter bandwidths are increased there as well. We will, therefore, call these the mel-based cepstral parameters.\nSometimes both early originators are cited.\n\nMany authors, including Davis and Mermelstein, have commented that the spectral basis functions of the cosine transform in the MFC are very similar to the principal components of the log spectra, which were applied to speech representation and recognition much earlier by Pols and his colleagues.\n\n", "related": "\n- Gammatone filter\n- Psychoacoustics\n\n- MATLAB Codes for MFCC and Other Speech Features\n- A tutorial on MFCCs for Automatic Speech Recognition\n"}
{"id": "213328", "url": "https://en.wikipedia.org/wiki?curid=213328", "title": "Negative feedback", "text": "Negative feedback\n\nNegative feedback (or balancing feedback) occurs when some function of the output of a system, process, or mechanism is fed back in a manner that tends to reduce the fluctuations in the output, whether caused by changes in the input or by other disturbances.\n\nWhereas positive feedback tends to lead to instability via exponential growth, oscillation or chaotic behavior, negative feedback generally promotes stability. Negative feedback tends to promote a settling to equilibrium, and reduces the effects of perturbations. Negative feedback loops in which just the right amount of correction is applied with optimum timing can be very stable, accurate, and responsive.\n\nNegative feedback is widely used in mechanical and electronic engineering, and also within living organisms, and can be seen in many other fields from chemistry and economics to physical systems such as the climate. General negative feedback systems are studied in control systems engineering.\n\nNegative feedback loops also play an integral role in maintaining the atmospheric balance in various systems on Earth. One such feedback system is the interaction between solar radiation, cloud cover, and planet temperature.\n\n- Mercury thermostats (circa 1600) using expansion and contraction of columns of mercury in response to temperature changes were used in negative feedback systems to control vents in furnaces, maintaining a steady internal temperature.\n- In the invisible hand of the market metaphor of economic theory (1776), reactions to price movements provide a feedback mechanism to match supply and demand.\n- In centrifugal governors (1788), negative feedback is used to maintain a near-constant speed of an engine, irrespective of the load or fuel-supply conditions.\n- In a steering engine (1866), power assistance is applied to the rudder with a feedback loop, to maintain the direction set by the steersman.\n- In servomechanisms, the speed or position of an output, as determined by a sensor, is compared to a set value, and any error is reduced by negative feedback to the input.\n- In audio amplifiers, negative feedback reduces distortion, minimises the effect of manufacturing variations in component parameters, and compensates for changes in characteristics due to temperature change.\n- In analog computing feedback around operational amplifiers is used to generate mathematical functions such as addition, subtraction, integration, differentiation, logarithm, and antilog functions.\n- In a phase locked loop (1932) feedback is used to maintain a generated alternating waveform in a constant phase to a reference signal. In many implementations the generated waveform is the output, but when used as a demodulator in an FM radio receiver, the error feedback voltage serves as the demodulated output signal. If there is a frequency divider between the generated waveform and the phase comparator, the device acts as a frequency multiplier.\n- In organisms, feedback enables various measures (e.g. body temperature, or blood sugar level) to be maintained within a desired range by homeostatic processes.\n\nNegative feedback as a control technique may be seen in the refinements of the water clock introduced by Ktesibios of Alexandria in the 3rd century BCE. Self-regulating mechanisms have existed since antiquity, and were used to maintain a constant level in the reservoirs of water clocks as early as 200 BCE.\nNegative feedback was implemented in the 17th Century. Cornelius Drebbel had built thermostatically-controlled incubators and ovens in the early 1600s, and centrifugal governors were used to regulate the distance and pressure between millstones in windmills. James Watt patented a form of governor in 1788 to control the speed of his steam engine, and James Clerk Maxwell in 1868 described \"component motions\" associated with these governors that lead to a decrease in a disturbance or the amplitude of an oscillation.\n\nThe term \"feedback\" was well established by the 1920s, in reference to a means of boosting the gain of an electronic amplifier. Friis and Jensen described this action as \"positive feedback\" and made passing mention of a contrasting \"negative feed-back action\" in 1924. Harold Stephen Black came up with the idea of using negative feedback in electronic amplifiers in 1927, submitted a patent application in 1928, and detailed its use in his paper of 1934, where he defined negative feedback as a type of coupling that \"reduced\" the gain of the amplifier, in the process greatly increasing its stability and bandwidth.\n\nKarl Küpfmüller published papers on a negative-feedback-based automatic gain control system and a feedback system stability criterion in 1928.\n\nNyquist and Bode built on Black's work to develop a theory of amplifier stability.\n\nEarly researchers in the area of cybernetics subsequently generalized the idea of negative feedback to cover any goal-seeking or purposeful behavior.\n\nCybernetics pioneer Norbert Wiener helped to formalize the concepts of feedback control, defining feedback in general as \"the chain of the transmission and return of information\", and negative feedback as the case when:\n\nWhile the view of feedback as any \"circularity of action\" helped to keep the theory simple and consistent, Ashby pointed out that, while it may clash with definitions that require a \"materially evident\" connection, \"the exact definition of feedback is nowhere important\". Ashby pointed out the limitations of the concept of \"feedback\":\nTo reduce confusion, later authors have suggested alternative terms such as \"degenerative\", \"self-correcting\", \"balancing\", or \"discrepancy-reducing\" in place of \"negative\".\n\nIn many physical and biological systems, qualitatively different influences can oppose each other. For example, in biochemistry, one set of chemicals drives the system in a given direction, whereas another set of chemicals drives it in an opposing direction. If one or both of these opposing influences are non-linear, equilibrium point(s) result.\n\nIn biology, this process (in general, biochemical) is often referred to as homeostasis; whereas in mechanics, the more common term is equilibrium.\n\nIn engineering, mathematics and the physical, and biological sciences, common terms for the points around which the system gravitates include: attractors, stable states, eigenstates/eigenfunctions, equilibrium points, and setpoints.\n\nIn control theory, \"negative\" refers to the sign of the multiplier in mathematical models for feedback. In delta notation, −Δoutput is added to or mixed into the input. In multivariate systems, vectors help to illustrate how several influences can both partially complement and partially oppose each other.\n\nSome authors, in particular with respect to modelling business systems, use \"negative\" to refer to the reduction in difference between the desired and actual behavior of a system. In a psychology context, on the other hand, \"negative\" refers to the valence of the feedback – attractive versus aversive, or praise versus criticism.\n\nIn contrast, positive feedback is feedback in which the system responds so as to increase the magnitude of any particular perturbation, resulting in amplification of the original signal instead of stabilization. Any system in which there is positive feedback together with a gain greater than one will result in a runaway situation. Both positive and negative feedback require a feedback loop to operate.\n\nHowever, negative feedback systems can still be subject to oscillations. This is caused by the slight delays around any loop. Due to these delays the feedback signal of some frequencies can arrive one half cycle later which will have a similar effect to positive feedback and these frequencies can reinforce themselves and grow over time. This problem is often dealt with by attenuating or changing the phase of the problematic frequencies. Unless the system naturally has sufficient damping, many negative feedback systems have low pass filters or dampers fitted.\n\nThere are a large number of different examples of negative feedback and some are discussed below.\n\nOne use of feedback is to make a system (say \"T\") self-regulating to minimize the effect of a disturbance (say \"D\"). Using a negative feedback loop, a measurement of some variable (for example, a process variable, say \"E\") is subtracted from a required value (the 'set point') to estimate an operational error in system status, which is then used by a regulator (say \"R\") to reduce the gap between the measurement and the required value. The regulator modifies the input to the system \"T\" according to its interpretation of the error in the status of the system. This error may be introduced by a variety of possible disturbances or 'upsets', some slow and some rapid. The regulation in such systems can range from a simple 'on-off' control to a more complex processing of the error signal.\n\nIt may be noted that the physical form of the signals in the system may change from point to point. So, for example, a change in weather may cause a disturbance to the \"heat\" input to a house (as an example of the system \"T\") that is monitored by a thermometer as a change in \"temperature\" (as an example of an 'essential variable' \"E\"), converted by the thermostat (a 'comparator') into an \"electrical\" error in status compared to the 'set point' \"S\", and subsequently used by the regulator (containing a 'controller' that commands \"gas\" control valves and an ignitor) ultimately to change the \"heat\" provided by a furnace (an 'effector') to counter the initial weather-related disturbance in heat input to the house.\n\nError controlled regulation is typically carried out using a Proportional-Integral-Derivative Controller (PID controller). The regulator signal is derived from a weighted sum of the error signal, integral of the error signal, and derivative of the error signal. The weights of the respective components depend on the application.\n\nMathematically, the regulator signal is given by:\nwhere\n\nThe negative feedback amplifier was invented by Harold Stephen Black at Bell Laboratories in 1927, and granted a patent in 1937 (US Patent 2,102,671 \"a continuation of application Serial No. 298,155, filed August 8, 1928 ...\").\n\nThere are many advantages to feedback in amplifiers. In design, the type of feedback and amount of feedback are carefully selected to weigh and optimize these various benefits.\n\nAdvantages of negative voltage feedback in amplifiers are-\n1. It reduces non linear distortion that is it has higher fidelity.\n2. It increases circuit stability that is the gain remains stable though there are variations in ambient temperature, frequency and signal amplitude.\n3. It increases bandwidth that is the frequency response is improved.\n4. It is possible to modify the input and output impedances.\n5. The harmonic distortion, phase distortion are less.\n6. The amplitude and frequency distortion are less.\n7. Noise is reduced considerably.\n8. An important advantage of negative feedback is that it stabilizes the gain.\n\nThough negative feedback has many advantages, amplifiers with feedback can oscillate. See the article on step response. They may even exhibit instability. Harry Nyquist of Bell Laboratories proposed the Nyquist stability criterion and the Nyquist plot that identify stable feedback systems, including amplifiers and control systems.\nThe figure shows a simplified block diagram of a negative feedback amplifier.\n\nThe feedback sets the overall (closed-loop) amplifier gain at a value:\n\nwhere the approximate value assumes β\"A \" » 1. This expression shows that a gain greater than one requires β < 1. Because the approximate gain 1/β is independent of the open-loop gain \"A\", the feedback is said to 'desensitize' the closed-loop gain to variations in \"A \" (for example, due to manufacturing variations between units, or temperature effects upon components), provided only that the gain \"A\" is sufficiently large. In this context, the factor (1+β\"A\") is often called the 'desensitivity factor', and in the broader context of feedback effects that include other matters like electrical impedance and bandwidth, the 'improvement factor'.\n\nIf the disturbance \"D\" is included, the amplifier output becomes:\n\nwhich shows that the feedback reduces the effect of the disturbance by the 'improvement factor' (1+β \"A\"). The disturbance \"D\" might arise from fluctuations in the amplifier output due to noise and nonlinearity (distortion) within this amplifier, or from other noise sources such as power supplies.\n\nThe difference signal \"I\"–β\"O\" at the amplifier input is sometimes called the \"error signal\". According to the diagram, the error signal is:\n\nFrom this expression, it can be seen that a large 'improvement factor' (or a large loop gain β\"A\") tends to keep this error signal small.\n\nAlthough the diagram illustrates the principles of the negative feedback amplifier, modeling a real amplifier as a unilateral forward amplification block and a unilateral feedback block has significant limitations. For methods of analysis that do not make these idealizations, see the article Negative feedback amplifier.\n\nThe operational amplifier was originally developed as a building block for the construction of analog computers, but is now used almost universally in all kinds of applications including audio equipment and control systems.\n\nOperational amplifier circuits typically employ negative feedback to get a predictable transfer function. Since the open-loop gain of an op-amp is extremely large, a small differential input signal would drive the output of the amplifier to one rail or the other in the absence of negative feedback. A simple example of the use of feedback is the op-amp voltage amplifier shown in the figure.\n\nThe idealized model of an operational amplifier assumes that the gain is infinite, the input impedance is infinite, output resistance is zero, and input offset currents and voltages are zero. Such an ideal amplifier draws no current from the resistor divider. \nIgnoring dynamics (transient effects and propagation delay), the infinite gain of the ideal op-amp means this feedback circuit drives the voltage difference between the two op-amp inputs to zero. Consequently, the voltage gain of the circuit in the diagram, assuming an ideal op amp, is the reciprocal of feedback voltage division ratio β:\n\nA real op-amp has a high but finite gain \"A\" at low frequencies, decreasing gradually at higher frequencies. In addition, it exhibits a finite input impedance and a non-zero output impedance. Although practical op-amps are not ideal, the model of an ideal op-amp often suffices to understand circuit operation at low enough frequencies. \nAs discussed in the previous section, the feedback circuit stabilizes the closed-loop gain and desensitizes the output to fluctuations generated inside the amplifier itself.\n\nAn example of the use of negative feedback control is the ballcock control of water level (see diagram), or a pressure regulator. In modern engineering, negative feedback loops are found in engine governors, fuel injection systems and carburettors. Similar control mechanisms are used in heating and cooling systems, such as those involving air conditioners, refrigerators, or freezers.\n\nSome biological systems exhibit negative feedback such as the baroreflex in blood pressure regulation and erythropoiesis. Many biological process (e.g., in the human anatomy) use negative feedback. Examples of this are numerous, from the regulating of body temperature, to the regulating of blood glucose levels. The disruption of feedback loops can lead to undesirable results: in the case of blood glucose levels, if negative feedback fails, the glucose levels in the blood may begin to rise dramatically, thus resulting in diabetes.\n\nFor hormone secretion regulated by the negative feedback loop: when gland X releases hormone X, this stimulates target cells to release hormone Y. When there is an excess of hormone Y, gland X \"senses\" this and inhibits its release of hormone X. As shown in the figure, most endocrine hormones are controlled by a physiologic negative feedback inhibition loop, such as the glucocorticoids secreted by the adrenal cortex. The hypothalamus secretes corticotropin-releasing hormone (CRH), which directs the anterior pituitary gland to secrete adrenocorticotropic hormone (ACTH). In turn, ACTH directs the adrenal cortex to secrete glucocorticoids, such as cortisol. Glucocorticoids not only perform their respective functions throughout the body but also negatively affect the release of further stimulating secretions of both the hypothalamus and the pituitary gland, effectively reducing the output of glucocorticoids once a sufficient amount has been released.\n\nClosed systems containing substances undergoing a reversible chemical reaction can also exhibit negative feedback in accordance with Le Chatelier's principle which shift the chemical equilibrium to the opposite side of the reaction in order to reduce a stress. For example, in the reaction \nIf a mixture of the reactants and products exists at equilibrium in a sealed container and nitrogen gas is added to this system, then the equilibrium will shift toward the product side in response. If the temperature is raised, then the equilibrium will shift toward the reactant side which, because the reverse reaction is endothermic, will partially reduces the temperature.\n\nSelf-organization is the capability of certain systems \"of organizing their own behavior or structure\". There are many possible factors contributing to this capacity, and most often positive feedback is identified as a possible contributor. However, negative feedback also can play a role.\n\nIn economics, automatic stabilisers are government programs that are intended to work as negative feedback to dampen fluctuations in real GDP.\n\nMainstream economics asserts that the market pricing mechanism operates to match supply and demand, because mismatches between them feed back into the decision-making of suppliers and demanders of goods, altering prices and thereby reducing any discrepancy. However Norbert Wiener wrote in 1948:\n\nThe notion of economic equilibrium being maintained in this fashion by market forces has also been questioned by numerous heterodox economists such as financier George Soros and leading ecological economist and steady-state theorist Herman Daly, who was with the World Bank in 1988–1994.\n\nA basic and common example of a negative feedback system in the environment is the interaction among cloud cover, plant growth, solar radiation, and planet temperature. As incoming solar radiation increases, planet temperature increases. As the temperature increases, the amount of plant life that can grow increases. This plant life can then make products such as sulfur which produce more cloud cover. An increase in cloud cover leads to higher albedo, or surface reflectivity, of the Earth. As albedo increases, however, the amount of solar radiation decreases. This, in turn, affects the rest of the cycle.\n\nCloud cover, and in turn planet albedo and temperature, is also influenced by the hydrological cycle. As planet temperature increases, more water vapor is produced, creating more clouds. The clouds then block incoming solar radiation, lowering the temperature of the planet. This interaction produces less water vapor and therefore less cloud cover. The cycle then repeats in a negative feedback loop. In this way, negative feedback loops in the environment have a stabilizing effect.\n", "related": "NONE"}
{"id": "20805702", "url": "https://en.wikipedia.org/wiki?curid=20805702", "title": "Zero-forcing precoding", "text": "Zero-forcing precoding\n\nZero-forcing (or null-steering) precoding is a method of spatial signal processing by which the multiple antenna transmitter can null multiuser interference signals in wireless communications. Regularized zero-forcing precoding is enhanced processing to consider the impact on a background noise and unknown user interference, where the background noise and the unknown user interference can be emphasized in the result of (known) interference signal nulling.\n\nIn particular, null-steering is a method of beamforming for narrowband signals where we want to have a simple way of compensating delays of receiving signals from a specific source at different elements of the antenna array. In general to make better use of the antenna arrays, we sum and average the signals coming to different elements, but this is only possible when delays are equal. Otherwise, we first need to compensate the delays and then sum them up. To reach this goal, we may only add the weighted version of the signals with appropriate weight values. We do this in such a way that the frequency domain output of this weighted sum produces a zero result. This method is called null steering. The generated weights are of course related to each other and this relation is a function of delay and central working frequency of the source.\n\nIf the transmitter knows the downlink channel state information (CSI) perfectly, ZF-precoding can achieve almost the system capacity when the number of users is large. On the other hand, with limited channel state information at the transmitter (CSIT) the performance of ZF-precoding decreases depending on the accuracy of CSIT. ZF-precoding requires the significant feedback overhead with respect to signal-to-noise-ratio (SNR) so as to achieve the full multiplexing gain. Inaccurate CSIT results in the significant throughput loss because of residual multiuser interferences. Multiuser interferences remain since they can not be nulled with beams generated by imperfect CSIT.\n\nIn a multiple antenna downlink system which comprises an formula_1 transmit antenna access point (AP) and formula_2 single receive antenna users, the received signal of user formula_3 is described as\n\nwhere formula_5 is the formula_6 vector of transmitted symbols, formula_7 is the noise signal, formula_8 is the formula_6 channel vector and formula_10 is the formula_6 linear precoding vector. From the fact that each beam generated by ZF-precoding is orthogonal to all the other user channel vectors, one can rewrite the received signal as\n\nFor comparison purpose, we describe the received signal model for multiple antenna uplink systems. In the uplink system with a formula_13 receiver antenna AP and formula_2 K single transmit antenna user, the received signal at the AP is described as\nwhere formula_16 is the transmitted signal of user formula_17, formula_18 is the formula_19 noise vector, formula_8 is the formula_19 channel vector.\n\nQuantify the amount of the feedback resource required to maintain at least a given throughput performance gap between zero-forcing with perfect feedback and with limited feedback, i.e.,\n\nJindal showed that the required feedback bits of a spatially uncorrelated channel should be scaled according to SNR of the downlink channel, which is given by:\n\nwhere \"M\" is the number of transmit antennas and formula_24 is the SNR of the downlink channel.\n\nTo feed back \"B\" bits though the uplink channel, the throughput performance of the uplink channel should be larger than or equal to 'B'\n\nwhere formula_26 is the feedback resource consisted by multiplying the feedback frequency resource and the frequency temporal resource subsequently and formula_27 is SNR of the feedback channel. Then, the required feedback resource to satisfy formula_28 is \nNote that differently from the feedback bits case, the required feedback resource is a function of both downlink and uplink channel conditions. It is reasonable to include the uplink channel status in the calculation of the feedback resource since the uplink channel status determines the capacity, i.e., bits/second per unit frequency band (Hz), of the feedback link. Consider a case when SNR of the downlink and uplink are proportion such that formula_30 is constant and both SNRs are sufficiently high. Then, the feedback resource will be only proportional to the number of transmit antennas\n\nIt follows from the above equation that the feedback resource (formula_32) is not necessary to scale according to SNR of the downlink channel, which is almost contradict to the case of the feedback bits. One, hence, sees that the whole systematic analysis can reverse the facts resulted from each reductioned situation.\n\n", "related": "\n- Channel state information\n- Precoding\n- MIMO\n\n- Schelkunoff Polynomial Method (Null-Steering) www.antenna-theory.com\n"}
{"id": "23390068", "url": "https://en.wikipedia.org/wiki?curid=23390068", "title": "Regressive discrete Fourier series", "text": "Regressive discrete Fourier series\n\nIn applied mathematics, the regressive discrete Fourier series (RDFS) is a generalization of the discrete Fourier transform where the Fourier series coefficients are computed in a least squares sense and the period is arbitrary, i.e., not necessarily equal to the length of the data. It was first proposed by Arruda (1992a,1992b). It can be used to smooth data in one or more dimensions and to compute derivatives from the smoothed curve, surface, or hypersurface.\n\nThe one-dimensional RDFS proposed by Arruda (1992a) can be formulated in a very straightforward way. Given a sampled data vector (signal) formula_1, one can write the algebraic expression:\n\nTypically formula_3, but this is not necessary.\n\nThe above equation can be written in matrix form as\n\nThe least squares solution of the above linear system of equations can be written as:\n\nwhere formula_6 is the conjugate transpose of formula_7, and the smoothed signal is obtained from:\n\nThe first derivative of the smoothed signal formula_9 can be obtained from:\n\nThe two-dimensional, or bidimensional RDFS proposed by Arruda (1992b) can also be formulated in a straightforward way. Here the equally spaced data case will be treated for the sake of simplicity. The general non-equally-spaced and arbitrary grid cases are given in the reference (Arruda,1992b). Given a sampled data matrix (bi dimensional signal) formula_11 one can write the algebraic expression:\n\nThe above equation can be written in matrix form for a rectangular grid. For the equally spaced sampling case :formula_13 we have:\n\nThe least squares solution may be shown to be:\n\nand the smoothed bidimensional surface is given by:\n\nwhere formula_6 is the conjugate, and formula_18 is the transpose of formula_7.\n\nDifferentiation with respect to formula_20 can be easily implemented analogously to the one-dimensional case (Arruda, 1992b).\n\n- Spatially dense data condensation applications: Arruda, J.R.F. [1993] applied the RDFS to condense spatially dense spatial measurements made with a laser Doppler vibrometer prior to applying modal analysis parameter estimation methods. More recently, Vanherzeele et al. (2006,2008a) proposed a generalized and an optimized RDFS for the same kind of application. A review of optical measurement processing using the RDFS was published by Vanherzeele et al. (2009).\n- Spatial derivative applications: Batista et al. [2009] applied RDFS to obtain spatial derivatives of bi dimensional measured vibration data to identify material properties from transverse modes of rectangular plates.\n- SHM applications: Vanherzeele et al. [2009] applied a generalized version of the RDFS to tomography reconstruction.\n\nRecently, a package that includes one and two-dimensional RDFS was developed in order to make easier its use in the free and open source software R:\n- A R package for RDFS at Github\n\n", "related": "\n- Discrete Fourier transform\n- Fourier series\n\n- Arruda, J.R.F., 1992a: Analysis of non-equally spaced data using a Regressive discrete Fourier series. Journal of Sound and Vibration, 156(3), 571–574.\n- Arruda, J.R.F., 1992b: Surface smoothing and partial spatial derivatives using a regressive discrete Fourier series. Mechanical Systems and Signal Processing, 6(1), 41–50.\n- Arruda, J.R.F., 1993: Spatial domain modal analysis of lightly-damped structures using laser velocimeters. Journal of Vibration and Acoustics, 115, 225–231.\n- Batista, F.B., Albuquerque, E.L., Arruda, J.R.F., Dias Jr., M., 2009: Identification of the bending stiffness of symmetric laminates using regressive discrete Fourier series and finite differences. Journal of Sound and Vibration, 320, 793–807.\n- Vanherzeele, J., Guillaume, P., Vanlanduit, S., Verboten, P., 2006: Data reduction using a generalized regressive discrete Fourier series, Journal of Sound and Vibration, 298, 1–11.\n- Vanherzeele, J., Vanlanduit, S., Guillaume, P., 2008a: Reducing spatial data using an optimized regressive discrete Fourier series, Journal of Sound and Vibration, 309, 858–867.\n- Vanherzeele, J., Longo, R., Vanlanduit, S., Guillaume, P., 2008b: Tomographic reconstruction using a generalized regressive discrete Fourier series, Mechanical Systems and Signal Processing, 22, 1237–1247.\n- Vanherzeele, J., Vanlanduit, S., Guillaume, P., 2009: Processing optical measurements using a regressive discrete Fourier series, Optical and lasers in engineering, 47, 461–472.\n"}
{"id": "23452012", "url": "https://en.wikipedia.org/wiki?curid=23452012", "title": "Restricted isometry property", "text": "Restricted isometry property\n\nIn linear algebra, the restricted isometry property (RIP) characterizes matrices which are nearly orthonormal, at least when operating on sparse vectors. The concept was introduced by Emmanuel Candès and Terence Tao and is used to prove many theorems in the field of compressed sensing. There are no known large matrices with bounded restricted isometry constants (computing these constants is strongly NP-hard, and is hard to approximate as well), but many random matrices have been shown to remain bounded. In particular, it has been shown that with exponentially high probability, random Gaussian, Bernoulli, and partial Fourier matrices satisfy the RIP with number of measurements nearly linear in the sparsity level. The current smallest upper bounds for any large rectangular matrices are for those of Gaussian matrices. Web forms to evaluate bounds for the Gaussian ensemble are available at the Edinburgh Compressed Sensing RIC page.\n\nLet \"A\" be an \"m\" × \"p\" matrix and let \"1\" ≤ \"s\" ≤ \"p\" be an integer. Suppose that there exists a constant formula_1 such that, for every \"m\" × \"s\" submatrix \"A\" of \"A\" and for every \"s\"-dimensional vector \"y\",\n\nThen, the matrix \"A\" is said to satisfy the \"s\"-restricted isometry property with restricted isometry constant formula_3.\n\nThis is equivalent to\n\nwhere formula_5 is the identity matrix and formula_6 is the operator norm. See for example for a proof.\n\nFinally this is equivalent to stating that all eigenvalues of formula_7 are in the interval formula_8.\n\nThe RIC Constant is defined as the infimum of all possible formula_9 for a given formula_10.\n\nIt is denoted as formula_12.\n\nFor any matrix that satisfies the RIP property with a RIC of formula_12, the following condition holds:\n\nThe tightest upper bound on the RIC can be computed for Gaussian matrices. This can be achieved by computing the exact probability that all the eigenvalues of Wishart matrices lie within an interval.\n\n", "related": "\n- Compressed sensing\n- Mutual coherence (linear algebra)\n- Terence Tao's website on compressed sensing lists several related conditions, such as the 'Exact reconstruction principle' (ERP) and 'Uniform uncertainty principle' (UUP)\n- Nullspace property, another sufficient condition for sparse recovery\n- Generalized restricted isometry property, a generalized sufficient condition for sparse recovery, where mutual coherence and restricted isometry property are both its special forms.\n"}
{"id": "2322", "url": "https://en.wikipedia.org/wiki?curid=2322", "title": "Audio signal processing", "text": "Audio signal processing\n\nAudio signal processing is a subfield of signal processing that is concerned with the electronic manipulation of audio signals. Audio signals are electronic representations of sound waves—longitudinal waves which travel through air, consisting of compressions and rarefactions. The energy contained in audio signals is typically measured in decibels. As audio signals may be represented in either digital or analog format, processing may occur in either domain. Analog processors operate directly on the electrical signal, while digital processors operate mathematically on its digital representation.\n\nThe motivation for audio signal processing began at the beginning of the 20th century with inventions like the telephone, phonograph, and radio that allowed for the transmission and storage of audio signals. Audio processing was necessary for early radio broadcasting, as there were many problems with studio-to-transmitter links. The theory of signal processing and its application to audio was largely developed at Bell Labs in the mid 20th century. Claude Shannon and Harry Nyquist's early work on communication theory, sampling theory, and Pulse-code modulation laid the foundations for the field. In 1957, Max Mathews became the first person to synthesize audio from a computer, giving birth to computer music.\n\nAn analog audio signal is a continuous signal represented by an electrical voltage or current that is “analogous” to the sound waves in the air. Analog signal processing then involves physically altering the continuous signal by changing the voltage or current or charge via electrical circuits.\n\nHistorically, before the advent of widespread digital technology, analog was the only method by which to manipulate a signal. Since that time, as computers and software have become more capable and affordable and digital signal processing has become the method of choice. However, in music applications, analog technology is often still desirable as it often produces nonlinear responses that are difficult to replicate with digital filters.\n\nA digital representation expresses the audio waveform as a sequence of symbols, usually binary numbers. This permits signal processing using digital circuits such as digital signal processors, microprocessors and general-purpose computers. Most modern audio systems use a digital approach as the techniques of digital signal processing are much more powerful and efficient than analog domain signal processing.\n\nProcessing methods and application areas include storage, data compression, music information retrieval, speech processing, localization, acoustic detection, transmission, noise cancellation, acoustic fingerprinting, sound recognition, synthesis, and enhancement (e.g. equalization, filtering, level compression, echo and reverb removal or addition, etc.).\n\nAudio signal processing is used when broadcasting audio signals in order to enhance their fidelity or optimize for bandwidth or latency. In this domain, the most important audio processing takes place just before the transmitter. The audio processor here must prevent or minimize overmodulation, compensate for non-linear transmitters (a potential issue with medium wave and shortwave broadcasting), and adjust overall loudness to desired level.\n\nActive noise control is a technique designed to reduce unwanted sound. By creating a signal that is identical to the unwanted noise but with the opposite polarity, the two signals cancel out due to destructive interference.\n\nAudio synthesis is the electronic generation of audio signals. A musical instrument that accomplishes this is called a synthesizer. Synthesizers can either imitate sounds or generate new ones. Audio synthesis is also used to generate human speech using speech synthesis.\n\nAudio effects are systems designed to alter how an audio signal sounds. Unprocessed audio is metaphorically referred to as \"dry\", while processed audio is referred to as \"wet\".\n\n- \"delay\" or echo - To simulate the effect of reverberation in a large hall or cavern, one or several delayed signals are added to the original signal. To be perceived as echo, the delay has to be of order 35 milliseconds or above. Short of actually playing a sound in the desired environment, the effect of echo can be implemented using either digital or analog methods. Analog echo effects are implemented using tape delays or bucket-brigade devices. When large numbers of delayed signals are mixed a reverberation effect is produced; The resulting sound has the effect of being presented in a large room.\n- \"flanger\" - to create an unusual sound, a delayed signal is added to the original signal with a continuously variable delay (usually smaller than 10 ms). This effect is now done electronically using DSP, but originally the effect was created by playing the same recording on two synchronized tape players, and then mixing the signals together. As long as the machines were synchronized, the mix would sound more-or-less normal, but if the operator placed his finger on the flange of one of the players (hence \"flanger\"), that machine would slow down and its signal would fall out-of-phase with its partner, producing a phasing comb filter effect. Once the operator took his finger off, the player would speed up until it was back in phase with the master, and as this happened, the phasing effect would appear to slide up the frequency spectrum. This phasing up-and-down the register can be performed rhythmically.\n- \"phaser\" - another way of creating an unusual sound; the signal is split, a portion is filtered with a variable all-pass filter to produce a phase-shift, and then the unfiltered and filtered signals are mixed to produce a comb filter. The phaser effect was originally a simpler implementation of the flanger effect since delays were difficult to implement with analog equipment.\n- \"chorus\" - a delayed version of the signal is added to the original signal. The delay has to be short in order not to be perceived as echo, but above 5 ms to be audible. If the delay is too short, it will destructively interfere with the un-delayed signal and create a flanging effect. Often, the delayed signals will be slightly pitch shifted to more realistically convey the effect of multiple voices.\n- \"equalization\" - frequency response is adjusted using audio filter(s) to produce desired spectral characteristics. Frequency ranges can be emphasized or attenuated using low-pass, high-pass, band-pass or band-stop filters. Moderate use of equalization can be used to fine-tune the tonal quality of a recording; extreme use of equalization, such as heavily cutting a certain frequency can create more unusual effects. Band-pass filtering of voice can simulate the effect of a telephone because telephones use band-pass filters.\n- \"overdrive\" effects can be used to produce distorted sounds, and increase loudness. The most basic overdrive effect involves clipping the signal when its absolute value exceeds a certain threshold.\n- \"timescale-pitch modification\" - this effect shifts a signal up or down in pitch. For example, a signal may be shifted an octave up or down. Blending the original signal with shifted duplicate(s) can create harmonization. Another application of pitch shifting is pitch correction where a musical signal is adjusted to improve intonation. The complement of pitch shift is timescale modification, that is, the process of changing the speed of an audio signal without affecting its pitch.\n- \"resonators\" - emphasize harmonic frequency content on specified frequencies. These may be created from parametric equation or from delay-based comb-filters.\n- \"robotic voice effects\" are used to make an actor's voice sound like a synthesized human voice.\n- \"ring modulation\" is an effect made famous by Doctor Who's Daleks and commonly used throughout sci-fi.\n- \"dynamic range compression\" - the control of the dynamic range of a sound to avoid unintentional or undesirable fluctuation in level. Dynamic range compression is not to be confused with audio data compression, where the amount of data is reduced without affecting the amplitude of the sound it represents.\n- \"3D audio effects\" - placement of sounds outside the spatial range available through stereo imaging.\n- \"reverse echo\" - a swelling effect created by reversing an audio signal and recording echo and/or delay while the signal runs in reverse. When played back forward the last echos are heard before the effected sound creating a rush like swell preceding and during playback. Jimmy Page of Led Zeppelin used this effect in the bridge of \"Whole Lotta Love\".\n- \"wave field synthesis\" - a spatial audio rendering technique for the creation of virtual acoustic environments\n- De-esser - Some sounds in recorded speech such as \"s\", \"z\", \"ch\", \"j\" and \"sh\" can be louder than vowels. This is known as sibilance and there are several time and frequency based algorithms that can reduce sibilance or de-ess the sound. Time domain based approaches, such as band pass filters, are more suited to real time applications such as live radio due to less constraint on Digital signal processor. Playback or offline applications incorporate Fast Fourier Transform (FFT) based methods.\n\n", "related": "\n- Sound card\n- Sound effect\n\n"}
{"id": "23660201", "url": "https://en.wikipedia.org/wiki?curid=23660201", "title": "Sub-band coding", "text": "Sub-band coding\n\nIn signal processing, sub-band coding (SBC) is any form of transform coding that breaks a signal into a number of different frequency bands, typically by using a fast Fourier transform, and encodes each one independently. This decomposition is often the first step in data compression for audio and video signals.\n\nSBC is the core technique used in many popular lossy audio compression algorithms including MP3.\n\nThe simplest way to digitally encode audio signals is pulse-code modulation (PCM), which is used on audio CDs, DAT recordings, and so on. Digitization transforms continuous signals into discrete ones by sampling a signal's amplitude at uniform intervals and rounding to the nearest value representable with the available number of bits. This process is fundamentally inexact, and involves two errors: \"discretization error,\" from sampling at intervals, and \"quantization error,\" from rounding.\n\nThe more bits used to represent each sample, the finer the granularity in the digital representation, and thus the smaller the quantization error. Such \"quantization errors\" may be thought of as a type of noise, because they are effectively the difference between the original source and its binary representation. With PCM, the audible effects of these errors can be mitigated with dither and by using enough bits to ensure that the noise is low enough to be masked either by the signal itself or by other sources of noise. A high quality signal is possible, but at the cost of a high bitrate (e.g., over 700 kbit/s for one channel of CD audio). In effect, many bits are wasted in encoding masked portions of the signal because PCM makes no assumptions about how the human ear hears.\n\nCoding techniques reduce bitrate by exploiting known characteristics of the auditory system. A classic method is nonlinear PCM, such as the μ-law algorithm. Small signals are digitized with finer granularity than are large ones; the effect is to add noise that is proportional to the signal strength. Sun's Au file format for sound is a popular example of mu-law encoding. Using 8-bit mu-law encoding would cut the per-channel bitrate of CD audio down to about 350 kbit/s, half the standard rate. Because this simple method only minimally exploits masking effects, it produces results that are often audibly inferior compared to the original.\n\nThe utility of SBC is perhaps best illustrated with a specific example. When used for audio compression, SBC exploits auditory masking in the auditory system. Human ears are normally sensitive to a wide range of frequencies, but when a sufficiently loud signal is present at one frequency, the ear will not hear weaker signals at nearby frequencies. We say that the louder signal masks the softer ones.\n\nThe basic idea of SBC is to enable a data reduction by discarding information about frequencies which are masked. The result differs from the original signal, but if the discarded information is chosen carefully, the difference will not be noticeable, or more importantly, objectionable.\n\nFirst, a digital filter bank divides the input signal spectrum into some number (e.g., 32) of subbands. The psychoacoustic model looks at the energy in each of these subbands, as well as in the original signal, and computes masking thresholds using psychoacoustic information. Each of the subband samples is quantized and encoded so as to keep the quantization noise below the dynamically computed masking threshold. The final step is to format all these quantized samples into groups of data called frames, to facilitate eventual playback by a decoder.\n\nDecoding is much easier than encoding, since no psychoacoustic model is involved. The frames are unpacked, subband samples are decoded, and a frequency-time mapping reconstructs an output audio signal.\n\nBeginning in the late 1980s, a standardization body, the Moving Picture Experts Group (MPEG), developed standards for coding of both audio and video. Subband coding resides at the heart of the popular MP3 format (more properly known as MPEG-1 Audio Layer III), for example.\n\nSub-band coding is used in the G.722 codec which uses sub-band adaptive differential pulse code modulation (SB-ADPCM) within a bit rate of 64 kbit/s. In the SB-ADPCM technique, the frequency band is split into two sub-bands (higher and lower) and the signals in each sub-band are encoded using ADPCM.\n\n- Sub-Band Coding Tutorial\n", "related": "NONE"}
{"id": "16187387", "url": "https://en.wikipedia.org/wiki?curid=16187387", "title": "Derivation of the Routh array", "text": "Derivation of the Routh array\n\nThe Routh array is a tabular method permitting one to establish the stability of a system using only the coefficients of the characteristic polynomial. Central to the field of control systems design, the Routh–Hurwitz theorem and Routh array emerge by using the Euclidean algorithm and Sturm's theorem in evaluating Cauchy indices.\n\nGiven the system:\nAssuming no roots of formula_2 lie on the imaginary axis, and letting\nthen we have\nExpressing formula_8 in polar form, we have\nwhere\nand \nfrom (2) note that\nwhere\nNow if the i root of formula_2 has a positive real part, then ()\nand\nand\nSimilarly, if the i root of formula_18 has a negative real part,\nand\nand\nFrom (9) to (11) we find that formula_22 when the i root of formula_8 has a positive real part, and from (12) to (14) we find that formula_24 when the i root of formula_8 has a negative real part. Thus,\nSo, if we define\nthen we have the relationship\nand combining (3) and (17) gives us\nTherefore, given an equation of formula_8 of degree formula_32 we need only evaluate this function formula_33 to determine formula_3, the number of roots with negative real parts and formula_5, the number of roots with positive real parts.\n\nIn accordance with (6) and Figure 1, the graph of formula_36 vs formula_37, varying formula_38 over an interval (a,b) where formula_39 and formula_40 are integer multiples of formula_41, this variation causing the function formula_42 to have increased by formula_41, indicates that in the course of travelling from point a to point b, formula_37 has \"jumped\" from formula_45 to formula_46 one more time than it has jumped from formula_46 to formula_45. Similarly, if we vary formula_38 over an interval (a,b) this variation causing formula_42 to have decreased by formula_41, where again formula_37 is a multiple of formula_41 at both formula_54 and formula_55, implies that formula_56 has jumped from formula_46 to formula_45 one more time than it has jumped from formula_45 to formula_46 as formula_38 was varied over the said interval.\nThus, formula_62 is formula_41 times the difference between the number of points at which formula_64 jumps from formula_46 to formula_45 and the number of points at which formula_64 jumps from formula_45 to formula_46 as formula_38 ranges over the interval formula_71 provided that at formula_72, formula_73 is defined.\n\nIn the case where the starting point is on an incongruity (i.e. formula_74, \"i\" = 0, 1, 2, ...) the ending point will be on an incongruity as well, by equation (17) (since formula_3 is an integer and formula_5 is an integer, formula_33 will be an integer). In this case, we can achieve this same index (difference in positive and negative jumps) by shifting the axes of the tangent function by formula_78, through adding formula_78 to formula_37. Thus, our index is now fully defined for any combination of coefficients in formula_8 by evaluating formula_82 over the interval (a,b) = formula_83 when our starting (and thus ending) point is not an incongruity, and by evaluating\nover said interval when our starting point is at an incongruity.\nThis difference, formula_33, of negative and positive jumping incongruities encountered while traversing formula_38 from formula_87 to formula_88 is called the Cauchy Index of the tangent of the phase angle, the phase angle being formula_42 or formula_90, depending as formula_91 is an integer multiple of formula_41 or not.\n\nTo derive Routh's criterion, first we'll use a different notation to differentiate between the even and odd terms of formula_8:\nNow we have: \nTherefore, if formula_32 is even, \nand if formula_32 is odd:\nNow observe that if formula_32 is an odd integer, then by (3) formula_101 is odd. If formula_101 is an odd integer, then formula_103 is odd as well. Similarly, this same argument shows that when formula_32 is even, formula_103 will be even. Equation (15) shows that if formula_103 is even, formula_37 is an integer multiple of formula_41. Therefore, formula_36 is defined for formula_32 even, and is thus the proper index to use when n is even, and similarly formula_111 is defined for formula_32 odd, making it the proper index in this latter case.\nThus, from (6) and (23), for formula_32 even:\nand from (19) and (24), for formula_32 odd:\nLo and behold we are evaluating the same Cauchy index for both:\nformula_117\n\nSturm gives us a method for evaluating formula_118. His theorem states as follows:\nGiven a sequence of polynomials formula_119 where:\n1) If formula_120 then formula_121, formula_122, and formula_123\n2) formula_124 for formula_125\nand we define formula_126 as the number of changes of sign in the sequence formula_119 for a fixed value of formula_38, then:\nA sequence satisfying these requirements is obtained using the Euclidean algorithm, which is as follows:\nStarting with formula_130 and formula_131, and denoting the remainder of formula_132 by formula_133 and similarly denoting the remainder of formula_134 by formula_135, and so on, we obtain the relationships:\nor in general \nwhere the last non-zero remainder, formula_138 will therefore be the highest common factor of formula_139. It can be observed that the sequence so constructed will satisfy the conditions of Sturm's theorem, and thus an algorithm for determining the stated index has been developed.\nIt is in applying Sturm's theorem (28) to (29), through the use of the Euclidean algorithm above that the Routh matrix is formed.\nWe get\nand identifying the coefficients of this remainder by formula_141, formula_142, formula_143, formula_144, and so forth, makes our formed remainder \nwhere\nContinuing with the Euclidean algorithm on these new coefficients gives us\nwhere we again denote the coefficients of the remainder formula_148 by formula_149, formula_150, formula_151, formula_152,\nmaking our formed remainder \nand giving us\nThe rows of the Routh array are determined exactly by this algorithm when applied to the coefficients of (20). An observation worthy of note is that in the regular case the polynomials formula_155 and formula_156 have as the highest common factor formula_157 and thus there will be formula_32 polynomials in the chain formula_119.\nNote now, that in determining the signs of the members of the sequence of polynomials formula_160 that at formula_161 the dominating power of formula_162 will be the first term of each of these polynomials, and thus only these coefficients corresponding to the highest powers of formula_162 in formula_164, and formula_138, which are formula_166, formula_167, formula_141, formula_149, ... determine the signs of formula_130, formula_131, ..., formula_138 at formula_173.\nSo we get formula_174 that is, formula_175 is the number of changes of sign in the sequence formula_176, formula_177, formula_178, ... which is the number of sign changes in the sequence formula_166, formula_167, formula_141, formula_149, ... and formula_183; that is formula_184 is the number of changes of sign in the sequence formula_185, formula_186, formula_187, ... which is the number of sign changes in the sequence formula_166, formula_189, formula_141, formula_191, ... \nSince our chain formula_166, formula_167, formula_141, formula_149, ... will have formula_32 members it is clear that formula_197 since within formula_198 if going from formula_166 to formula_167 a sign change has not occurred, within \nformula_201 going from formula_166 to formula_189 one has, and likewise for all formula_32 transitions (there will be no terms equal to zero) giving us formula_32 total sign changes.\nAs formula_206 and formula_207, and from (18) formula_208, we have that formula_209 and have derived Routh's theorem -\nThe number of roots of a real polynomial formula_210 which lie in the right half plane formula_211 is equal to the number of changes of sign in the first column of the Routh scheme.\nAnd for the stable case where formula_212 then formula_213 by which we have Routh's famous criterion:\nIn order for all the roots of the polynomial formula_210 to have negative real parts, it is necessary and sufficient that all of the elements in the first column of the Routh scheme be different from zero and of the same sign.\n\n- Hurwitz, A., \"On the Conditions under which an Equation has only Roots with Negative Real Parts\", Rpt. in Selected Papers on Mathematical Trends in Control Theory, Ed. R. T. Ballman et al. New York: Dover 1964\n- Routh, E. J., A Treatise on the Stability of a Given State of Motion. London: Macmillan, 1877. Rpt. in Stability of Motion, Ed. A. T. Fuller. London: Taylor & Francis, 1975\n- Felix Gantmacher (J.L. Brenner translator) (1959) \"Applications of the Theory of Matrices\", pp 177–80, New York: Interscience.\n", "related": "NONE"}
{"id": "5978424", "url": "https://en.wikipedia.org/wiki?curid=5978424", "title": "Kernel principal component analysis", "text": "Kernel principal component analysis\n\nIn the field of multivariate statistics, kernel principal component analysis (kernel PCA) \n\nis an extension of principal component analysis (PCA) using techniques of kernel methods. Using a kernel, the originally linear operations of PCA are performed in a reproducing kernel Hilbert space.\n\nRecall that conventional PCA operates on zero-centered data; that is, \nwhere formula_2 is the vector of one of the formula_3 multivariate observations.\nIt operates by diagonalizing the covariance matrix,\nin other words, it gives an eigendecomposition of the covariance matrix:\nwhich can be rewritten as\n\nTo understand the utility of kernel PCA, particularly for clustering, observe that, while \"N\" points cannot, in general, be linearly separated in formula_7 dimensions, they can almost always be linearly separated in formula_8 dimensions. That is, given \"N\" points, formula_2, if we map them to an \"N\"-dimensional space with\nit is easy to construct a hyperplane that divides the points into arbitrary clusters. Of course, this formula_12 creates linearly independent vectors, so there is no covariance on which to perform eigendecomposition \"explicitly\" as we would in linear PCA.\n\nInstead, in kernel PCA, a non-trivial, arbitrary formula_12 function is 'chosen' that is never calculated explicitly, allowing the possibility to use very-high-dimensional formula_12's if we never have to actually evaluate the data in that space. Since we generally try to avoid working in the formula_12-space, which we will call the 'feature space', we can create the N-by-N kernel\n\nwhich represents the inner product space (see Gramian matrix) of the otherwise intractable feature space. The dual form that arises in the creation of a kernel allows us to mathematically formulate a version of PCA in which we never actually solve the eigenvectors and eigenvalues of the covariance matrix in the formula_17-space (see Kernel trick). The N-elements in each column of \"K\" represent the dot product of one point of the transformed data with respect to all the transformed points (N points). Some well-known kernels are shown in the example below.\n\nBecause we are never working directly in the feature space, the kernel-formulation of PCA is restricted in that it computes not the principal components themselves, but the projections of our data onto those components. To evaluate the projection from a point in the feature space formula_17 onto the kth principal component formula_19 (where superscript k means the component k, not powers of k)\n\nWe note that formula_21 denotes dot product, which is simply the elements of the kernel formula_22. It seems all that's left is to calculate and normalize the formula_23, which can be done by solving the eigenvector equation\n\nwhere N is the number of data points in the set, and formula_25 and formula_26 are the eigenvalues and eigenvectors of K. Then to normalize the eigenvectors formula_27's, we require that\n\nCare must be taken regarding the fact that, whether or not formula_29 has zero-mean in its original space, it is not guaranteed to be centered in the feature space (which we never compute explicitly). Since centered data is required to perform an effective principal component analysis, we 'centralize' K to become formula_30\n\nwhere formula_32 denotes a N-by-N matrix for which each element takes value formula_33. We use formula_30 to perform the kernel PCA algorithm described above.\n\nOne caveat of kernel PCA should be illustrated here. In linear PCA, we can use the eigenvalues to rank the eigenvectors based on how much of the variation of the data is captured by each principal component. This is useful for data dimensionality reduction and it could also be applied to KPCA. However, in practice there are cases that all variations of the data are same. This is typically caused by a wrong choice of kernel scale.\n\nIn practice, a large data set leads to a large K, and storing K may become a problem. One way to deal with this is to perform clustering on the dataset, and populate the kernel with the means of those clusters. Since even this method may yield a relatively large K, it is common to compute only the top P eigenvalues and eigenvectors of the eigenvalues are calculated in this way.\n\nConsider three concentric clouds of points (shown); we wish to use kernel PCA to identify these groups. The color of the points does not represent information involved in the algorithm, but only shows how the transformation relocates the data points.\n\nFirst, consider the kernel\n\nApplying this to kernel PCA yields the next image.\n\nNow consider a Gaussian kernel:\n\nThat is, this kernel is a measure of closeness, equal to 1 when the points coincide and equal to 0 at infinity.\n\nNote in particular that the first principal component is enough to distinguish the three different groups, which is impossible using only linear PCA, because linear PCA operates only in the given (in this case two-dimensional) space, in which these concentric point clouds are not linearly separable.\n\nKernel PCA has been demonstrated to be useful for novelty detection and image de-noising.\n\n", "related": "\n- Cluster analysis\n- Kernel trick\n- Multilinear PCA\n- Multilinear subspace learning\n- Nonlinear dimensionality reduction\n- Spectral clustering\n"}
{"id": "22950824", "url": "https://en.wikipedia.org/wiki?curid=22950824", "title": "Channel Division Multiple Access (ChDMA)", "text": "Channel Division Multiple Access (ChDMA)\n\nChannel Division Multiple Access (ChDMA) is a simple multiple access scheme developed for low duty cycle UWB networks. The idea lies in the fact that different users have different channel impulse response (CSI) and the signal at the receiver can be rebuilt based on this information.\n\n", "related": "\n- Blind equalization\n- Multiplexing\n"}
{"id": "23983507", "url": "https://en.wikipedia.org/wiki?curid=23983507", "title": "WSDMA", "text": "WSDMA\n\nWSDMA (Wideband Space Division Multiple Access) is a high bandwidth channel access method, developed for multi-transceiver systems such as active array antennas. WSDMA is a beamforming technique suitable for overlay on the latest air-interface protocols including WCDMA and OFDM. WSDMA enabled systems can determine the angle of arrival (AoA) of received signals to spatially divide a cell sector into many sub-sectors. This spatial awareness provides information necessary to maximise Carrier to Noise+Interference Ratio (CNIR) link budget, through a range of digital processing routines. WSDMA facilitates a flexible approach to how uplink and downlink beamforming is performed and is capable of spatial filtering known interference generating locations.\n\n- Transmit and receive beam shaping and steering\n- Multiple sub-sector path processing\n- Spatial interference filtering\n- Sector activity scan\n\nActive Panel Antenna systems, comprising a planar array of micro-radios and associated antenna element, rely upon a comprehensive calibration scheme which is able to correct inter-path signal mismatches in phase, amplitude and latency. This facilitates precise control of the uplink and downlink RF beam pattern and avoids distortion effects that occur in the absence of calibration.\n\nBy dividing the cell sector into a number of sub-sector beams, WSDMA provides the network with spatially filtered signals, maximising link budget through improved antenna gain and interference mitigation. This allows for mobile users in the cell to reduce their uplink power transmission, thereby further reducing interference and minimising both base station and UE power consumption. WSDMA provides simultaneous sector-wide and sub-sector beam processing to improve link performance in multipath environments. Sub-sector beam processing can optimise changing user demographics within the cell sector.\n\nDownlink WSDMA provides an optimised RF beam pattern, reducing interference in overlap regions of adjacent cell sectors. Long term statistical based adjustment can optimise cell patterns depending on the user population density per spatial region serviced by the cell.\n\n", "related": "\n- WCDMA\n- OFDM\n- Smart Antennas\n- Beamforming\n- 3G MIMO\n\n- \"Beamforming: A versatile approach to spatial filtering\" B. D. V. Veen and K. M. Buckley. . IEEE ASSP Magazine, Apr. 1988.\n- W-CDMA and cdma2000 for 3G mobile networks. M. R. Karim, M. Sarraf\n- L.Lengierand R.Farrell. Amplitude and Phase Mismatch Calibration Testbed for 2x2 Tower-Top Antenna Array System. China-Ireland Conference on Information and Communications Technologies 2007.\n"}
{"id": "23434533", "url": "https://en.wikipedia.org/wiki?curid=23434533", "title": "Filter (signal processing)", "text": "Filter (signal processing)\n\nIn signal processing, a filter is a device or process that removes some unwanted components or features from a signal. Filtering is a class of signal processing, the defining feature of filters being the complete or partial suppression of some aspect of the signal. Most often, this means removing some frequencies or frequency bands. However, filters do not exclusively act in the frequency domain; especially in the field of image processing many other targets for filtering exist. Correlations can be removed for certain frequency components and not for others without having to act in the frequency domain. Filters are widely used in electronics and telecommunication, in radio, television, audio recording, radar, control systems, music synthesis, image processing, and computer graphics.\n\nThere are many different bases of classifying filters and these overlap in many different ways; there is no simple hierarchical classification. Filters may be:\n- non-linear or linear\n- time-variant or time-invariant , also known as shift invariance. If the filter operates in a spatial domain then the characterization is space invariance.\n- causal or not-causal: A filter is non-causal if its present output depends on future input. Filters processing time-domain signals in real time must be causal, but not filters acting on spatial domain signals or deferred-time processing of time-domain signals.\n- analog or digital\n- discrete-time (sampled) or continuous-time\n- passive or active type of continuous-time filter\n- infinite impulse response (IIR) or finite impulse response (FIR) type of discrete-time or digital filter.\n\nLinear continuous-time circuit is perhaps the most common meaning for filter in the signal processing world, and simply \"filter\" is often taken to be synonymous. These circuits are generally designed to remove certain frequencies and allow others to pass. Circuits that perform this function are generally linear in their response, or at least approximately so. Any nonlinearity would potentially result in the output signal containing frequency components not present in the input signal.\n\nThe modern design methodology for linear continuous-time filters is called network synthesis. Some important filter families designed in this way are:\n- Chebyshev filter, has the best approximation to the ideal response of any filter for a specified order and ripple.\n- Butterworth filter, has a maximally flat frequency response.\n- Bessel filter, has a maximally flat phase delay.\n- Elliptic filter, has the steepest cutoff of any filter for a specified order and ripple.\n\nThe difference between these filter families is that they all use a different polynomial function to approximate to the ideal filter response. This results in each having a different transfer function.\n\nAnother older, less-used methodology is the image parameter method. Filters designed by this methodology are archaically called \"wave filters\". Some important filters designed by this method are:\n- Constant k filter, the original and simplest form of wave filter.\n- m-derived filter, a modification of the constant k with improved cutoff steepness and impedance matching.\n\nSome terms used to describe and classify linear filters:\n\n- The frequency response can be classified into a number of different bandforms describing which frequency bands the filter passes (the passband) and which it rejects (the stopband):\n- Low-pass filter – low frequencies are passed, high frequencies are attenuated.\n- High-pass filter – high frequencies are passed, low frequencies are attenuated.\n- Band-pass filter – only frequencies in a frequency band are passed.\n- Band-stop filter or band-reject filter – only frequencies in a frequency band are attenuated.\n- Notch filter – rejects just one specific frequency - an extreme band-stop filter.\n- Comb filter – has multiple regularly spaced narrow passbands giving the bandform the appearance of a comb.\n- All-pass filter – all frequencies are passed, but the phase of the output is modified.\n- Cutoff frequency is the frequency beyond which the filter will not pass signals. It is usually measured at a specific attenuation such as 3 dB.\n- Roll-off is the rate at which attenuation increases beyond the cut-off frequency.\n- Transition band, the (usually narrow) band of frequencies between a passband and stopband.\n- Ripple is the variation of the filter's insertion loss in the passband.\n- The order of a filter is the degree of the approximating polynomial and in passive filters corresponds to the number of elements required to build it. Increasing order increases roll-off and brings the filter closer to the ideal response.\n\nOne important application of filters is in telecommunication.\nMany telecommunication systems use frequency-division multiplexing, where the system designers divide a wide frequency band into many narrower frequency bands called \"slots\" or \"channels\", and each stream of information is allocated one of those channels.\nThe people who design the filters at each transmitter and each receiver try to balance passing the desired signal through as accurately as possible, keeping interference to and from other cooperating transmitters and noise sources outside the system as low as possible, at reasonable cost.\n\nMultilevel and multiphase digital modulation systems require filters that have flat phase delay—are linear phase in the passband—to preserve pulse integrity in the time domain,\ngiving less intersymbol interference than other kinds of filters.\n\nOn the other hand, analog audio systems using analog transmission can tolerate much larger ripples in phase delay, and so designers of such systems often deliberately sacrifice linear phase to get filters that are better in other ways—better stop-band rejection, lower passband amplitude ripple, lower cost, etc.\n\nFilters can be built in a number of different technologies. The same transfer function can be realised in several different ways, that is the mathematical properties of the filter are the same but the physical properties are quite different. Often the components in different technologies are directly analogous to each other and fulfill the same role in their respective filters. For instance, the resistors, inductors and capacitors of electronics correspond respectively to dampers, masses and springs in mechanics. Likewise, there are corresponding components in distributed-element filters.\n\n- Electronic filters were originally entirely passive consisting of resistance, inductance and capacitance. Active technology makes design easier and opens up new possibilities in filter specifications.\n- Digital filters operate on signals represented in digital form. The essence of a digital filter is that it directly implements a mathematical algorithm, corresponding to the desired filter transfer function, in its programming or microcode.\n- Mechanical filters are built out of mechanical components. In the vast majority of cases they are used to process an electronic signal and transducers are provided to convert this to and from a mechanical vibration. However, examples do exist of filters that have been designed for operation entirely in the mechanical domain.\n- Distributed-element filters are constructed out of components made from small pieces of transmission line or other distributed elements. There are structures in distributed-element filters that directly correspond to the lumped elements of electronic filters, and others that are unique to this class of technology.\n- Waveguide filters consist of waveguide components or components inserted in the waveguide. Waveguides are a class of transmission line and many structures of distributed-element filters, for instance the stub, can also be implemented in waveguides.\n- Optical filters were originally developed for purposes other than signal processing such as lighting and photography. With the rise of optical fiber technology, however, optical filters increasingly find signal processing applications and signal processing filter terminology, such as longpass and shortpass, are entering the field.\n- Transversal filter, or delay line filter, works by summing copies of the input after various time delays. This can be implemented with various technologies including analog delay lines, active circuitry, CCD delay lines, or entirely in the digital domain.\n\nDigital signal processing allows the inexpensive construction of a wide variety of filters. The signal is sampled and an analog-to-digital converter turns the signal into a stream of numbers. A computer program running on a CPU or a specialized DSP (or less often running on a hardware implementation of the algorithm) calculates an output number stream. This output can be converted to a signal by passing it through a digital-to-analog converter. There are problems with noise introduced by the conversions, but these can be controlled and limited for many useful filters. Due to the sampling involved, the input signal must be of limited frequency content or aliasing will occur.\n\nIn the late 1930s, engineers realized that small mechanical systems made of rigid materials such as quartz would acoustically resonate at radio frequencies, i.e. from audible frequencies (sound) up to several hundred megahertz. Some early resonators were made of steel, but quartz quickly became favored. The biggest advantage of quartz is that it is piezoelectric. This means that quartz resonators can directly convert their own mechanical motion into electrical signals. Quartz also has a very low coefficient of thermal expansion which means that quartz resonators can produce stable frequencies over a wide temperature range. Quartz crystal filters have much higher quality factors than LCR filters. When higher stabilities are required, the crystals and their driving circuits may be mounted in a \"crystal oven\" to control the temperature. For very narrow band filters, sometimes several crystals are operated in series.\n\nA large number of crystals can be collapsed into a single component, by mounting comb-shaped evaporations of metal on a quartz crystal. In this scheme, a \"tapped delay line\" reinforces the desired frequencies as the sound waves flow across the surface of the quartz crystal. The tapped delay line has become a general scheme of making high-\"Q\" filters in many different ways.\n\nSAW (surface acoustic wave) filters are electromechanical devices commonly used in radio frequency applications. Electrical signals are converted to a mechanical wave in a device constructed of a piezoelectric crystal or ceramic; this wave is delayed as it propagates across the device, before being converted back to an electrical signal by further electrodes. The delayed outputs are recombined to produce a direct analog implementation of a finite impulse response filter. This hybrid filtering technique is also found in an analog sampled filter.\nSAW filters are limited to frequencies up to 3 GHz. The filters were developed by Professor Ted Paige and others.\n\nBAW (bulk acoustic wave) filters are electromechanical devices. BAW filters can implement ladder or lattice filters. BAW filters typically operate at frequencies from around 2 to around 16 GHz, and may be smaller or thinner than equivalent SAW filters. Two main variants of BAW filters are making their way into devices: thin-film bulk acoustic resonator or FBAR and solid mounted bulk acoustic resonators.\n\nAnother method of filtering, at microwave frequencies from 800 MHz to about 5 GHz, is to use a synthetic single crystal yttrium iron garnet sphere made of a chemical combination of yttrium and iron (YIGF, or yttrium iron garnet filter). The garnet sits on a strip of metal driven by a transistor, and a small loop antenna touches the top of the sphere. An electromagnet changes the frequency that the garnet will pass. The advantage of this method is that the garnet can be tuned over a very wide frequency by varying the strength of the magnetic field.\n\nFor even higher frequencies and greater precision, the vibrations of atoms must be used. Atomic clocks use caesium masers as ultra-high \"Q\" filters to stabilize their primary oscillators. Another method, used at high, fixed frequencies with very weak radio signals, is to use a ruby maser tapped delay line.\n\nThe transfer function of a filter is most often defined in the domain of the complex frequencies. The back and forth passage to/from this domain is operated by the Laplace transform and its inverse (therefore, here below, the term \"input signal\" shall be understood as \"the Laplace transform of\" the time representation of the input signal, and so on).\n\nThe transfer function formula_1 of a filter is the ratio of the output signal formula_2 to the input signal formula_3 as a function of the complex frequency formula_4:\n\nwith formula_6.\n\nFor filters that are constructed of discrete components (lumped elements):\n- Their transfer function will be the ratio of polynomials in formula_4, i.e. a rational function of formula_4. The order of the transfer function will be the highest power of formula_4 encountered in either the numerator or the denominator polynomial.\n- The polynomials of the transfer function will all have real coefficients. Therefore, the poles and zeroes of the transfer function will either be real or occur in complex-conjugate pairs.\n- Since the filters are assumed to be stable, the real part of all poles (i.e. zeroes of the denominator) will be negative, i.e. they will lie in the left half-plane in complex frequency space.\n\nDistributed-element filters do not, in general, have rational-function transfer fundtions, but can approximate them.\n\nThe construction of a transfer function involves the Laplace transform, and therefore it is needed to assume null initial conditions, because\nAnd when \"f\"(0) = 0 we can get rid of the constants and use the usual expression\n\nAn alternative to transfer functions is to give the behavior of the filter as a convolution of the time-domain input with the filter's impulse response. The convolution theorem, which holds for Laplace transforms, guarantees equivalence with transfer functions.\n\nCertain filters may be specified by family and bandform. A filter's family is specified by the approximating polynomial used, and each leads to certain characteristics of the transfer function of the filter. Some common filter families and their particular characteristics are:\n\n- Butterworth filter – no gain ripple in pass band and stop band, slow cutoff\n- Chebyshev filter (Type I) – no gain ripple in stop band, moderate cutoff\n- Chebyshev filter (Type II) – no gain ripple in pass band, moderate cutoff\n- Bessel filter – no group delay ripple, no gain ripple in both bands, slow gain cutoff\n- Elliptic filter – gain ripple in pass and stop band, fast cutoff\n- Optimum \"L\" filter\n- Gaussian filter – no ripple in response to step function\n- Raised-cosine filter\n\nEach family of filters can be specified to a particular order. The higher the order, the more the filter will approach the \"ideal\" filter; but also the longer the impulse response is and the longer the latency will be. An ideal filter has full transmission in the pass band, complete attenuation in the stop band, and an abrupt transition between the two bands, but this filter has infinite order (i.e., the response cannot be expressed as a linear differential equation with a finite sum) and infinite latency (i.e., its compact support in the Fourier transform forces its time response to be ever lasting).\n\nHere is an image comparing Butterworth, Chebyshev, and elliptic filters. The filters in this illustration are all fifth-order low-pass filters. The particular implementation – analog or digital, passive or active – makes no difference; their output would be the same. As is clear from the image, elliptic filters are sharper than the others, but they show ripples on the whole bandwidth.\n\nAny family can be used to implement a particular bandform of which frequencies are transmitted, and which, outside the passband, are more or less attenuated. The transfer function completely specifies the behavior of a linear filter, but not the particular technology used to implement it. In other words, there are a number of different ways of achieving a particular transfer function when designing a circuit. A particular bandform of filter can be obtained by transformation of a prototype filter of that family.\n\nImpedance matching structures invariably take on the form of a filter, that is, a network of non-dissipative elements. For instance, in a passive electronics implementation, it would likely take the form of a ladder topology of inductors and capacitors. The design of matching networks shares much in common with filters and the design invariably will have a filtering action as an incidental consequence. Although the prime purpose of a matching network is not to filter, it is often the case that both functions are combined in the same circuit. The need for impedance matching does not arise while signals are in the digital domain.\n\nSimilar comments can be made regarding power dividers and directional couplers. When implemented in a distributed-element format, these devices can take the form of a distributed-element filter. There are four ports to be matched and widening the bandwidth requires filter-like structures to achieve this. The inverse is also true: distributed-element filters can take the form of coupled lines.\n\n- Audio filter\n- Line filter\n- Scaled correlation, high-pass filter for correlations\n- Texture filtering\n\n- Wiener filter\n- Kalman filter\n- Savitzky–Golay smoothing filter\n\n", "related": "\n- Lifter (signal processing)\n- Electronic filter topology\n- Sallen–Key topology\n- Smoothing\n\n- Miroslav D. Lutovac, Dejan V. Tošić, Brian Lawrence Evans, \"Filter Design for Signal Processing Using MATLAB and Mathematica\", Miroslav Lutovac, 2001 .\n- B. A. Shenoi, \"Introduction to Digital Signal Processing and Filter Design\", John Wiley & Sons, 2005 .\n- L. D. Paarmann, \"Design and Analysis of Analog Filters: A Signal Processing Perspective\", Springer, 2001 .\n- J.S.Chitode, \"Digital Signal Processing\", Technical Publications, 2009 .\n- Leland B. Jackson, \"Digital Filters and Signal Processing\", Springer, 1996 .\n"}
{"id": "2965318", "url": "https://en.wikipedia.org/wiki?curid=2965318", "title": "Time reversal signal processing", "text": "Time reversal signal processing\n\nTime Reversal Signal Processing is a technique for focusing waves. A Time Reversal Mirror (TRM) is a device that can focus waves using the time reversal method. TRMs are also known as time reversal mirror arrays, as they are usually arrays of transducers, but they do not have to be arrays. TRM are known and used for decades in the optical domain, and are also used in the ultrasonic domain.\n\nIf the source is passive, i.e. some type of isolated reflector, an iterative technique can be used to focus energy on it. The TRM transmits a plane wave which travels toward the target and is reflected off it. The reflected wave returns to the TRM, where it looks as if the target has emitted a (weak) signal. The TRM reverses and retransmits the signal as usual, and a more focused wave travels toward the target. As the process is repeated, the waves become more and more focused on the target.\n\nYet another variation is to use a single transducer and an ergodic cavity. Intuitively, an ergodic cavity is one that will allow a wave originating at any point to reach any other point. An example of an ergodic cavity is an irregularly shaped swimming pool: if someone dives in, eventually the entire surface will be rippling with no clear pattern. If the propagation medium is lossless and the boundaries are perfect reflectors, a wave starting at any point will reach all other points an infinite number of times. This property can be exploited by using a single transducer and recording for a long time to get as many reflections as possible.\n\nThe time reversal technique is based upon a feature of the wave equation known as reciprocity: given a solution to the wave equation, then the time reversal (using a negative time) of that solution is also a solution. This occurs because the standard wave equation only contains even order derivatives. Some media are not reciprocal (e.g. very lossy or noisy media), but many very useful ones are approximately so, including sound waves in water or air, ultrasonic waves in human bodies, and electromagnetic waves in free space. The medium must also be approximately linear.\n\nTime reversal techniques can be modeled as a matched filter. If a delta function is the original signal, then the received signal at the TRM is the impulse response of the channel. The TRM sends the reversed version of the impulse response back through the same channel, effectively autocorrelating it. This autocorrelation function has a peak at the origin, where the original source was. It is important to realize that the signal is concentrated in both space and time (in many applications, autocorrelation functions are functions of time only).\n\nAnother way to think of a time reversal experiment is that the TRM is a \"channel sampler\". The TRM measures the channel during the recording phase, and uses that information in the transmission phase to optimally focus the wave back to the source.\n\nA notable researcher is Mathias Fink of École Supérieure de Physique et de Chimie Industrielles de la Ville de Paris. His team has done numerous experiments with ultrasonic TRMs. An interesting experiment involved a single source transducer, a 96-element TRM, and 2000 thin steel rods located between the source and the array. The source sent a 1 μs pulse both with and without the steel scatterers. The source point was measured for both time width and spatial width in the retransmission step. The spatial width was about 6 times narrower with the scatterers than without. Moreover, the spatial width was less than the diffraction limit as determined by the size of the TRM with the scatterers. This is possible because the scatterers increased the effective aperture of the array. Even when the scatterers were moved slightly (on the order of a wavelength) in between the receive and transmit steps, the focusing was still quite good, showing that time reversal techniques can be robust in the face of a changing medium.\nIn addition, José M. F. Moura of Carnegie Mellon University was leading a research team working to extend the principles of Time Reversal to electromagnetic waves, and they have achieved resolution in excess of the Rayleigh resolution limit, proving the efficacy of Time Reversal techniques. Their efforts are focused on radar systems, and trying to improve detection and imaging schemes in highly cluttered environments, where Time Reversal techniques seem to provide the greatest benefit.\n\nThe beauty of time reversal signal processing is that one need not know any details of the channel. The step of sending a wave through the channel effectively measures it, and the retransmission step uses this data to focus the wave. Thus one doesn't have to solve the wave equation to optimize the system, one only needs to know that the medium is reciprocal. Time reversal is therefore suited to applications with inhomogeneous media.\n\nAn attractive aspect of time reversal signal processing is the fact that it makes use of multipath propagation. Many wireless communication systems must compensate and correct for multipath effects. Time reversal techniques use multipath to their advantage by using the energy from all paths.\n\nFink imagines a cryptographic application based on the ergodic cavity configuration. The key would be composed of the locations of two transducers. One plays the message, the other records waves after they have bounced throughout the cavity; this recording will look like noise. When the recorded message is time reversed and played back, there is only one location to launch the waves from in order for them to focus. Given that the playback location is correct, only one other location will exhibit the focused message wave; all other locations should look noisy.\n\n", "related": "\n- Phase conjugation\n\n- Mathias Fink. Time Reversal of Ultrasonic Fields--Part 1: Basic Principles. IEEE Trans. Ultrasonics, Ferroelectrics, and Frequency Control. 39(5):pp 555--566. September 1992.\n"}
{"id": "24526532", "url": "https://en.wikipedia.org/wiki?curid=24526532", "title": "Sombrero function", "text": "Sombrero function\n\nA sombrero function (sometimes called besinc function or jinc function) is the 2-dimensional polar coordinate analog of the sinc function, and is so-called because it is shaped like a sombrero hat. This function is frequently used in image processing. It can be defined through the Bessel function of the first kind where .\n\nThe normalization factor makes . Sometimes the factor is omitted, giving the following alternative definition:\n\nThe factor of 2 is also often omitted, giving yet another definition and causing the function maximum to be 0.5:\n", "related": "NONE"}
{"id": "161005", "url": "https://en.wikipedia.org/wiki?curid=161005", "title": "Head-related transfer function", "text": "Head-related transfer function\n\nA head-related transfer function (HRTF), also sometimes known as the \"anatomical transfer function\" (ATF), is a response that characterizes how an ear receives a sound from a point in space. As sound strikes the listener, the size and shape of the head, ears, ear canal, density of the head, size and shape of nasal and oral cavities, all transform the sound and affect how it is perceived, boosting some frequencies and attenuating others. Generally speaking, the HRTF boosts frequencies from 2–5 kHz with a primary resonance of +17 dB at 2,700 Hz. But the response curve is more complex than a single bump, affects a broad frequency spectrum, and varies significantly from person to person.\n\nA pair of HRTFs for two ears can be used to synthesize a binaural sound that seems to come from a particular point in space. It is a transfer function, describing how a sound from a specific point will arrive at the ear (generally at the outer end of the auditory canal). Some consumer home entertainment products designed to reproduce surround sound from stereo (two-speaker) headphones use HRTFs. Some forms of HRTF-processing have also been included in computer software to simulate surround sound playback from loudspeakers.\n\nHumans have just two ears, but can locate sounds in three dimensions – in range (distance), in direction above and below (elevation), in front and to the rear, as well as to either side (azimuth). This is possible because the brain, inner ear and the external ears (pinna) work together to make inferences about location. This ability to localize sound sources may have developed in humans and ancestors as an evolutionary necessity, since the eyes can only see a fraction of the world around a viewer, and vision is hampered in darkness, while the ability to localize a sound source works in all directions, to varying accuracy, \nregardless of the surrounding light.\n\nHumans estimate the location of a source by taking cues derived from one ear (\"monaural cues\"), and by comparing cues received at both ears (\"difference cues\" or \"binaural cues\"). Among the difference cues are time differences of arrival and intensity differences. The monaural cues come from the interaction between the sound source and the human anatomy, in which the original source sound is modified before it enters the ear canal for processing by the auditory system. These modifications encode the source location, and may be captured via an impulse response which relates the source location and the ear location. This impulse response is termed the \"head-related impulse response\" (HRIR). Convolution of an arbitrary source sound with the HRIR converts the sound to that which would have been heard by the listener if it had been played at the source location, with the listener's ear at the receiver location. HRIRs have been used to produce virtual surround sound. \n\nThe HRTF is the Fourier transform of HRIR.\n\nHRTFs for left and right ear (expressed above as HRIRs) describe the filtering of a sound source (\"x\"(\"t\")) before it is perceived at the left and right ears as \"x\"(\"t\") and \"x\"(\"t\"), respectively.\n\nThe HRTF can also be described as the modifications to a sound from a direction in free air to the sound as it arrives at the eardrum. These modifications include the shape of the listener's outer ear, the shape of the listener's head and body, the acoustic characteristics of the space in which the sound is played, and so on. All these characteristics will influence how (or whether) a listener can accurately tell what direction a sound is coming from.\n\nIn the AES69-2015 standard, the Audio Engineering Society (AES) has defined the SOFA file format for storing spatially oriented acoustic data like head-related transfer functions (HRTFs). SOFA software libraries and files are collected at the Sofa Conventions website.\n\nThe associated mechanism varies between individuals, as their head and ear shapes differ.\n\nHRTF describes how a given sound wave input (parameterized as frequency and source location) is filtered by the diffraction and reflection properties of the head, pinna, and torso, before the sound reaches the transduction machinery of the eardrum and inner ear (see auditory system). Biologically, the source-location-specific prefiltering effects of these external structures aid in the neural determination of source location, particularly the determination of the source's elevation (see vertical sound localization).\n\nLinear systems analysis defines the transfer function as the complex ratio between the output signal spectrum and the input signal spectrum as a function of frequency. Blauert (1974; cited in Blauert, 1981) initially defined the transfer function as the free-field transfer function (FFTF). Other terms include free-field to eardrum transfer function and the pressure transformation from the free-field to the eardrum. Less specific descriptions include the pinna transfer function, the outer ear transfer function, the pinna response, or directional transfer function (DTF).\n\nThe transfer function \"H\"(\"f\") of any linear time-invariant system at frequency \"f\" is:\n\nOne method used to obtain the HRTF from a given source location is therefore to measure the head-related impulse response (HRIR), \"h\"(\"t\"), at the ear drum for the impulse \"Δ\"(\"t\") placed at the source. The HRTF \"H\"(\"f\") is the Fourier transform of the HRIR \"h\"(\"t\").\n\nEven when measured for a \"dummy head\" of idealized geometry, HRTF are complicated functions of frequency and the three spatial variables. For distances greater than 1 m from the head, however, the HRTF can be said to attenuate inversely with range. It is this far field HRTF, \"H\"(\"f\", \"θ\", \"φ\"), that has most often been measured. At closer range, the difference in level observed between the ears can grow quite large, even in the low-frequency region within which negligible level differences are observed in the far field.\n\nHRTFs are typically measured in an anechoic chamber to minimize the influence of early reflections and reverberation on the measured response. HRTFs are measured at small increments of \"θ\" such as 15° or 30° in the horizontal plane, with interpolation used to synthesize \"HRTF\"s for arbitrary positions of \"θ\". Even with small increments, however, interpolation can lead to front-back confusion, and optimizing the interpolation procedure is an active area of research.\n\nIn order to maximize the signal-to-noise ratio (SNR) in a measured HRTF, it is important that the impulse being generated be of high volume. In practice, however, it can be difficult to generate impulses at high volumes and, if generated, they can be damaging to human ears, so it is more common for HRTFs to be directly calculated in the frequency domain using a frequency-swept sine wave or by using maximum length sequences. User fatigue is still a problem, however, highlighting the need for the ability to interpolate based on fewer measurements.\n\nThe head-related transfer function is involved in resolving the Cone of Confusion, a series of points where ITD and ILD are identical for sound sources from many locations around the \"0\" part of the cone. When a sound is received by the ear it can either go straight down the ear into the ear canal or it can be reflected off the pinnae of the ear, into the ear canal a fraction of a second later. The sound will contain many frequencies, so therefore many copies of this signal will go down the ear all at different times depending on their frequency (according to reflection, diffraction, and their interaction with high and low frequencies and the size of the structures of the ear.) These copies overlap each other, and during this, certain signals are enhanced (where the phases of the signals match) while other copies are canceled out (where the phases of the signal do not match). Essentially, the brain is looking for frequency notches in the signal that correspond to particular known directions of sound. \n\nIf another person's ears were substituted, the individual would not immediately be able to localize sound, as the patterns of enhancement and cancellation would be different from those patterns the person's auditory system is used to. However, after some weeks, the auditory system would adapt to the new head-related transfer function. The inter-subject variability in the spectra of HRTFs has been studied through cluster analyses.\n\nAssessing the variation through changes between the person's ear, we can limit our perspective with the degrees of freedom of the head and its relation with the spatial domain. Through this, we eliminate the tilt and other co-ordinate parameters that add complexity. For the purpose of calibration we are only concerned with the direction level to our ears, ergo a specific degree of freedom. Some of the ways in which we can deduce an expression to calibrate the HRTF are:\n\n1. Localization of sound in Virtual Auditory space\n2. HRTF Phase synthesis\n3. HRTF Magnitude synthesis\n\nA basic assumption in the creation of a virtual auditory space is that if the acoustical waveforms present at a listener's eardrums are the same under headphones as in free field, then the listener's experience should also be the same.\n\nTypically, sounds generated from headphones appear to originate from within the head. In the virtual auditory space, the headphones should be able to \"externalize\" the sound. Using the HRTF, sounds can be spatially positioned using the technique described below.\n\nLet \"x\"(\"t\") represent an electrical signal driving a loudspeaker and \"y\"(\"t\") represent the signal received by a microphone inside the listener's eardrum. Similarly, let \"x\"(\"t\") represent the electrical signal driving a headphone and \"y\"(\"t\") represent the microphone response to the signal. The goal of the virtual auditory space is to choose \"x\"(\"t\") such that \"y\"(\"t\") = \"y\"(\"t\"). Applying the Fourier transform to these signals, we come up with the following two equations:\n\nwhere \"L\" is the transfer function of the loudspeaker in the free field, \"F\" is the HRTF, \"M\" is the microphone transfer function, and \"H\" is the headphone-to-eardrum transfer function. Setting \"Y\" = \"Y\", and solving for \"X\" yields\n\nBy observation, the desired transfer function is\n\nTherefore, theoretically, if \"x\"(\"t\") is passed through this filter and the resulting \"x\"(\"t\") is played on the headphones, it should produce the same signal at the eardrum. Since the filter applies only to a single ear, another one must be derived for the other ear. This process is repeated for many places in the virtual environment to create an array of head-related transfer functions for each position to be recreated while ensuring that the sampling conditions are set by the Nyquist criteria.\n\nThere is less reliable phase estimation in the very low part of the frequency band, and in the upper frequencies the phase response is affected by the features of the pinna. Earlier studies also show that the HRTF phase response is mostly linear and that listeners are insensitive to the details of the interaural phase spectrum as long as the interaural time delay (ITD) of the combined low-frequency part of the waveform is maintained. This is the modeled phase response of the subject HRTF as a time delay, dependent on the direction and elevation.\n\nA scaling factor is a function of the anthropometric features. For example, a training set of N subjects would consider each HRTF phase and describe a single ITD scaling factor as the average delay of the group. This computed scaling factor can estimate the time delay as function of the direction and elevation for any given individual. Converting the time delay to phase response for the left and the right ears is trivial.\n\nThe HRTF phase can be described by the ITD scaling factor. This is in turn is quantified by the anthropometric data of a given individual taken as the source of reference. For a generic case we consider \"β\" as a sparse vector\n\nthat represents the subject's anthropometric features as a linear superposition of the anthropometric features from the training data (y = β X), and then apply the same sparse vector directly on the scaling vector H. We can write this task as a minimization problem, for a non-negative shrinking parameter \"λ\":\n\nFrom this,\nITD scaling factor value H is estimated as:\n\nwhere The ITD scaling factors for all persons in the dataset are stacked in a vector \"H\" ∈ R, so the value \"H\" corresponds to the scaling factor of the n-th person.\n\nWe solve the above minimization problem using Least Absolute Shrinkage and Selection Operator (LASSO). We assume that the HRTFs are represented by the same relation as the anthropometric features. Therefore, once we learn the sparse vector β from the anthropometric features, we directly apply it to the HRTF tensor data and the subject's HRTF values H given by:\n\nwhere The HRTFs for each subject are described by a tensor of size \"D\" × \"K\", where \"D\" is the number of HRTF directions and \"K\" is the number of frequency bins. All \"H\" corresponds to all the HRTFs of the training set are stacked in a new tensor \"H\" ∈ R, so the value H corresponds to the \"k\"-th frequency bin for \"d\"-th HRTF direction of the \"n\"-th person. Also \"H\" corresponds to \"k\"-th frequency for every d-th HRTF direction of the synthesized HRTF.\n\nRecordings processed via an HRTF, such as in a computer gaming environment (see A3D, EAX and OpenAL), which approximates the HRTF of the listener, can be heard through stereo headphones or speakers and interpreted as if they comprise sounds coming from all directions, rather than just two points on either side of the head. The perceived accuracy of the result depends on how closely the HRTF data set matches the characteristics of one's own ears.\n\n", "related": "\n- 3D sound reconstruction\n- A3D\n- Binaural recording\n- Dummy head recording\n- Environmental audio extensions\n- OpenAL\n- Sound Retrieval System\n- Sound localization\n- Soundbar\n- Sensaura\n- Transfer function\n\n- Spatial Sound Tutorial\n- CIPIC HRTF Database\n- Listen HRTF Database\n- High-resolution HRTF and 3D ear model database (48 subjects)\n- AIR Database (HRTF database in reverberant environments)\n- Full Sphere HRIR/HRTF Database of the Neumann KU100\n- MIT Database (one dataset)\n- ARI (Acoustics Research Institute) Database (90+ datasets)\n"}
{"id": "7252626", "url": "https://en.wikipedia.org/wiki?curid=7252626", "title": "Constant amplitude zero autocorrelation waveform", "text": "Constant amplitude zero autocorrelation waveform\n\nIn signal processing, a Constant Amplitude Zero AutoCorrelation waveform (CAZAC) is a periodic complex-valued signal with modulus one and out-of-phase periodic (cyclic) autocorrelations equal to zero. CAZAC sequences find application in wireless communication systems, for example in 3GPP Long Term Evolution for synchronization of mobile phones with base stations. Zadoff–Chu sequences are well-known CAZAC sequences with special properties.\n\n- CAZAC Sequence Generator (Java applet)\n", "related": "NONE"}
{"id": "9002263", "url": "https://en.wikipedia.org/wiki?curid=9002263", "title": "Blind equalization", "text": "Blind equalization\n\nBlind equalization is a digital signal processing technique in which the transmitted signal is inferred (equalized) from the received signal, while making use only of the transmitted signal statistics. Hence, the use of the word \"blind\" in the name.\n\nBlind equalization is essentially blind deconvolution applied to digital communications. Nonetheless, the emphasis in blind equalization is on online estimation of the equalization filter, which is the inverse of the channel impulse response, rather than the estimation of the channel impulse response itself. This is due to blind deconvolution common mode of usage in digital communications systems, as a means to extract the continuously transmitted signal from the received signal, with the channel impulse response being of secondary intrinsic importance.\n\nThe estimated equalizer is then convolved with the received signal to yield an estimation of the transmitted signal. \n\nAssuming a linear time invariant channel with impulse response formula_1, the noiseless model relates the received signal formula_2 to the transmitted signal formula_3 via\n\nThe blind equalization problem can now be formulated as follows; Given the received signal formula_2, find a filter formula_6, called an equalization filter, such that\n\nwhere formula_8 is an estimation of formula_9.\nThe solution formula_8 to the blind equalization problem is not unique. In fact, it may be determined only up to a signed scale factor and an arbitrary time delay. That is, if formula_11 are estimations of the transmitted signal and channel impulse response, respectively, then formula_12 give rise to the same received signal formula_13 for any real scale factor formula_14 and integral time delay formula_15. In fact, by symmetry, the roles of formula_9 and formula_17 are Interchangeable.\n\nIn the noisy model, an additional term, formula_18, representing additive noise, is included. The model is therefore\n\nMany algorithms for the solution of the blind equalization problem have been suggested over the years.\nHowever, as one usually has access to only a finite number of samples from the received signal formula_20, further restrictions must be imposed over the above models to render the blind equalization problem tractable.\nOne such assumption, common to all algorithms described below is to assume that the channel has finite impulse response, formula_21, where formula_22 is an arbitrary natural number.\n\nThis assumption may be justified on physical grounds, since the energy of any real signal must be finite, and therefore its impulse response must tend to zero. Thus it may be assumed that all coefficients beyond a certain point are negligibly small.\n\nIf the channel impulse response is assumed to be minimum phase, the problem becomes trivial.\n\nBussgang methods make use of the Least mean squares filter algorithm\n\nwith\n\nwhere formula_25 is an appropriate positive adaptation step and formula_26 is a suitable nonlinear function.\n\nPolyspectra techniques utilize higher order statistics in order to compute the equalizer.\n\n", "related": "\n- Independent component analysis\n- Principal components analysis\n- Blind deconvolution\n- Linear predictive coding\n\n[1] C. RICHARD JOHNSON, JR., et. el., \"Blind Equalization Using the Constant Modulus Criterion: A Review\", PROCEEDINGS OF THE IEEE, VOL. 86, NO. 10, OCTOBER 1998.\n"}
{"id": "13793754", "url": "https://en.wikipedia.org/wiki?curid=13793754", "title": "Geophysical MASINT", "text": "Geophysical MASINT\n\nGeophysical MASINT is a branch of Measurement and Signature Intelligence (MASINT) that involves phenomena transmitted through the earth (ground, water, atmosphere) and manmade structures including emitted or reflected sounds, pressure waves, vibrations, and magnetic field or ionosphere disturbances.\n\nAccording to the United States Department of Defense, MASINT is technically derived intelligence (excluding traditional imagery IMINT and signals intelligence SIGINT) that – when collected, processed, and analyzed by dedicated MASINT systems – results in intelligence that detects, tracks, identifies, or describes the signatures (distinctive characteristics) of fixed or dynamic target sources. MASINT was recognized as a formal intelligence discipline in 1986. Another way to describe MASINT is a \"non-literal\" discipline. It feeds on a target's unintended emissive by-products, the \"trails\" - the spectral, chemical or RF that an object leaves behind. These trails form distinct signatures, which can be exploited as reliable discriminators to characterize specific events or disclose hidden targets.\"\n\nAs with many branches of MASINT, specific techniques may overlap with the six major conceptual disciplines of MASINT defined by the Center for MASINT Studies and Research, which divides MASINT into Electro-optical, Nuclear, Geophysical, Radar, Materials, and Radiofrequency disciplines.\n\nGeophysical sensors have a long history in conventional military and commercial applications, from weather prediction for sailing, to fish finding for commercial fisheries, to nuclear test ban verification. New challenges, however, keep emerging.\n\nFor first-world military forces opposing other conventional militaries, there is an assumption that if a target can be located, it can be destroyed. As a result, concealment and deception have taken on new criticality. \"Stealth\" low-observability aircraft have gotten much attention, and new surface ship designs feature observability reduction. Operating in the confusing littoral environment produces a great deal of concealing interference.\n\nOf course, submariners feel they invented low observability, and others are simply learning from them. They know that going deep or at least ultraquiet, and hiding among natural features, makes them very hard to detect.\n\nTwo families of military applications, among many, represent new challenges against which geophysical MASINT can be tried. Also, see Unattended Ground Sensors.\n\nOne of the easiest ways for nations to protect weapons of mass destruction, command posts, and other critical structures is to bury them deeply, perhaps enlarging natural caves or disused mines. Deep burial is not only a means of protection against physical attack, as even without the use of nuclear weapons, there are deeply penetrating precision guided bombs that can attack them. Deep burial, with appropriate concealment during construction, is a way to avoid the opponent's knowing the buried facility's position well enough to direct precision guided weapons against it.\n\nFinding deeply buried structures, therefore, is a critical military requirement. The usual first step in finding a deep structure is IMINT, especially using hyperspectral IMINT sensors to help eliminate concealment. \"Hyperspectral images can help reveal information not obtainable through other forms of imagery intelligence such as the moisture content of soil. This data can also help distinguish camouflage netting from natural foliage.\" Still, a facility dug under a busy city would be extremely hard to find during construction. When the opponent knows that it is suspected that a deeply buried facility exists, there can be a variety of decoys and lures, such as buried heat sources to confuse infrared sensors, or simply digging holes and covering them, with nothing inside.\n\nMASINT using acoustic, seismic, and magnetic sensors would appear to have promise, but these sensors must be fairly close to the target. Magnetic Anomaly Detection (MAD) is used in antisubmarine warfare, for final localization before attack. The existence of the submarine is usually established through passive listening and refined with directional passive sensors and active sonar.\n\nOnce these sensors (as well as HUMINT and other sources) have failed, there is promise for surveying large areas and deeply concealed facilities using gravitimetric sensors. Gravity sensors are a new field, but military requirements are making it important while the technology to do it is becoming possible.\n\nEspecially in today's \"green water\" and \"brown water\" naval applications, navies are looking at MASINT solutions to meet new challenges of operating in littoral areas of operations. This symposium found it useful to look at five technology areas, which are interesting to contrast to the generally accepted categories of MASINT: acoustics and geology and geodesy/sediments/transport, nonacoustical detection (biology/optics/chemistry), physical oceanography, coastal meteorology, and electromagnetic detection.\n\nAlthough it is unlikely there will ever be another World War II-style opposed landing on a fortified beach, another aspect of the littoral is being able to react to opportunities for amphibious warfare. Detecting shallow-water and beach mines remains a challenge, since mine warfare is a deadly \"poor man's weapon.\"\n\nWhile initial landings from an offshore force would be from helicopters or tiltrotor aircraft, with air cushion vehicles bringing ashore larger equipment, traditional landing craft, portable causeways, or other equipment will eventually be needed to bring heavy equipment across a beach. Shallow depth and natural underwater obstacles can block beach access to these craft and equipment, as can shallow-water mines. Synthetic Aperture Radar (SAR), airborne laser detection and ranging (LIDAR) and use of bioluminescence to detect wake trails around underwater obstacles all may help solve this challenge.\n\nMoving onto and across the beach has its own challenges. Remotely operated vehicles may be able to map landing routes, and they, as well as LIDAR and multispectral imaging, may be able to detect shallow water. Once on the beach, the soil has to support heavy equipment. Techniques here include estimating soil type from multispectral imaging, or from an airdropped penetrometer that actually measures the loadbearing capacity of the surface.\n\nThe science and art of weather prediction used the ideas of measurement and signatures to predict phenomena, long before there were any electronic sensors. Masters of sailing ships might have no more sophisticated instrument than a wetted finger raised to the wind, and the flapping of sails.\n\nWeather information, in the normal course of military operations, has a major effect on tactics. High winds and low pressures can change artillery trajectories. High and low temperatures cause both people and equipment to require special protection. Aspects of weather, however, also can be measured and compared with signatures, to confirm or reject the findings of other sensors.\n\nThe state of the art is to fuse meteorological, oceanographic, and acoustic data in a variety of display modes. Temperature, salinity and sound speed can be displayed horizontally, vertically, or in three-dimensional perspective.\n\nWhile early sailors had no sensors beyond their five senses, the modern meteorologist has a wide range of geophysical and electro-optical measuring devices, operating on platforms from the bottom of the sea to deep space. Prediction based on these measurements are based on signatures of past weather events, a deep understanding of theory, and computational models.\n\nWeather predictions can give significant negative intelligence, when the signature of some combat system is such that it can operate only under certain weather conditions. Weather has long been an extremely critical part of modern military operations, as when the decision to land at Normandy on June 6, rather than June 5, 1944 depended on Dwight D. Eisenhower's trust in his staff weather advisor, Group Captain James Martin Stagg. It is rarely understood that something as fast as a ballistic missile reentry vehicle, or as \"smart\" as a precision guided munition, can still be affected by winds in the target area.\n\nAs part of Unattended Ground Sensors. The Remote Miniature Weather Station (RMWS), from System Innovations, is an air-droppable version with a lightweight, expendable and modular system with two components: a meteorological (MET) sensor and a ceilometer (cloud ceiling height) with limited MET. The basic MET system is surface-based and measures wind speed and direction, horizontal visibility, surface atmospheric pressure, air temperature and relative humidity. The ceilometer sensor determines cloud height and discrete cloud layers. The system provides near-real-time data capable of 24-hour operation for 60 days. The RMWS can also go in with US Air Force Special Operations combat weathermen\n\nThe man-portable version, brought in by combat weathermen, has an additional function, as remote miniature ceilometer. Designed to measure multiple layer cloud ceiling heights and then send that data via satellite communications link to an operator display, the system uses a Neodinum YAG (NdYAG), 4 megawatt non-eye safe laser. According to one weatherman, \"We have to watch that one,” he said. “Leaving it out there basically we’re worried about civilian populace going out there and playing with it—firing the laser and there goes somebody’s eye. There are two different units [to RMWS]. One has the laser and one doesn’t. The basic difference is the one with the laser is going to give you cloud height.\"\n\nHydrographic MASINT is subtly different from weather, in that it considers factors such as water temperature and salinity, biologic activities, and other factors that have a major effect on sensors and weapons used in shallow water. ASW equipment, especially acoustic performance depends on the season the specific coastal site. Water column conditions, such as temperature, salinity, and turbidity are more variable in shallow than deep water. Water depth will influence bottom bounce conditions, as will the material of the bottom. Seasonal water column conditions (particularly summer versus winter) are inherently more variable in shallow water than in deep water.\n\nWhile much attention is given to shallow waters of the littoral, other areas have unique hydrographic characteristics.\n\n- Regional areas with fresh water eddies\n- Open ocean salinity fronts\n- Near ice floes\n- Under ice\n\nA submarine tactical development activity observed, \"Fresh water eddies exist in many areas of the world. As we have experienced recently in the Gulf of Mexico using the Tactical Oceanographic Monitoring System (TOMS), there exist very distinct surface ducts that causes the Submarine Fleet Mission Program Library (SFMPL) sonar prediction to be unreliable. Accurate bathythermic information is paramount and a precursor for accurate sonar predictions.”\n\nCritical to the prediction of sound, needed by active and passive MASINT systems operating in water is knowing the temperature and salinity at specific depths. Antisubmarine aircraft, ships, and submarines can release independent sensors that measure the water temperature at various depths. The water temperature is critically important in acoustic detections, as changes in water temperature at thermoclines can act as a \"barrier\" or \"layer\" to acoustic propagation. To hunt a submarine, which is aware of water temperature, the hunter must drop acoustic sensors below the thermocline.\n\nWater conductivity is used as a surrogate marker for salinity. The current and most recently developed software, however, does not give information on suspended material in the water or bottom characteristics, both considered critical in shallow-water operations.\n\nThe US Navy does this by dropping expendable probes, which transmit to a recorder, of 1978-1980 vintage, the AN/BQH-7 for submarines and the AN/BQH-71 for surface ships. While the redesign of the late seventies did introduce digital logic, the devices kept hard-to-maintain analog recorders, and maintainability became critical by 1995. A project was begun to extend with COTS components, to result in the AN/BQH-7/7A EC-3. In 1994-5, the maintainability of the in-service units became critical.\n\nVariables in selecting the appropriate probe include:\n\n- Maximum depth sounded\n- Speed of launching vessel\n- Resolution vertical distance between data points (ft)\n- Depth accuracy\n\nLarge schools of fish contain enough entrapped air to conceal the sea floor, or manmade underwater vehicles and structures. Fishfinders, developed for commercial and recreational fishing, are specialized sonars that can identify acoustic reflections between the surface and the bottom. Variations on commercial equipment are apt to be needed, especially in littoral areas rich in marine life.\n\nA variety of sensors can be used to characterise the sea bottom into, for example, mud, sand, and gravel. Active acoustic sensors are the most obvious, but there is potential information from gravitimetric sensors, electro-optical and radar sensors for making inferences from the water surface, etc.\n\nRelatively simple sonars such as echo sounders can be promoted to seafloor classification systems via add-on modules, converting echo parameters into sediment type. Different algorithms exist, but they are all based on changes in the energy or shape of the reflected sounder pings.\n\nSide-scan sonars can be used to derive maps of the topography of an area by moving the sonar across it just above the bottom. Multibeam hull-mounted sonars are not as precise as a sensor near the bottom, but both can give reasonable three-dimensional visualization.\n\nAnother approach comes from greater signal processing of existing military sensors. The US Naval Research Laboratory demonstrated both seafloor characterization, as well as subsurface characteristics of the seafloor. Sensors used, in different demonstrations, included normal incidence beams from the AM/UQN-4 surface ship depthfinder, and AN/BQN-17 submarine fathometer; backscatter from the Kongsberg EM-121 commercial multibeam sonar; AN/UQN-4 fathometers on mine countermeasures (MCM) ships, and the AN/AQS-20 mine-hunting system. These produced the \"Bottom and Subsurface Characterization\" graphic.\n\nOne of the improvements in the Fuchs 2 reconnaissance vehicle is adding onboard weather instrumentations, including data such as wind direction and speed;, air and ground temperature; barometric pressure and humidity.\n\nThis includes the collection of passive or active emitted or reflected sounds, pressure waves or vibrations in the atmosphere (ACOUSTINT) or in the water (ACINT) or conducted through the ground Going well back into the Middle Ages, military engineers would listen to the ground for sounds of telltale digging under fortifications.\n\nIn modern times, acoustic sensors were first used in the air, as with artillery ranging in World War I. Passive hydrophones were used by the World War I Allies against German submarines; the UC-3, was sunk with the aid of hydrophone on 23 April 1916. Since submerged submarines cannot use radar, passive and active acoustic systems are their primary sensors. Especially for the passive sensors, the submarine acoustic sensor operators must have extensive libraries of acoustic signatures, to identify sources of sound.\n\nIn shallow water, there are sufficient challenges to conventional acoustic sensors that additional MASINT sensors may be required. Two major confounding factors are:\n\n- Boundary interactions. The effects of the seafloor and the sea surface on acoustic systems in shallow water are highly complex, making range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.\n- Practical limitations. Another key issue is the range dependence of shallow water propagation and reverberation. For example, shallow water limits the depth of towed sound detection arrays, thus increasing the possibility of the system's detecting its own noise. In addition, closer ship spacing increases the potential for mutual interference effects. It is believed that nonacoustic sensors, of magnetic, optical, bioluminescent, chemical, and hydrodynamic disturbances will be necessary in shallow-water naval operations.\n\nWhile now primarily of historical interest, one of the first applications of acoustic and optical MASINT was locating enemy artillery by the sound of their firing and flashes respectively during World War I. Effective sound ranging was pioneered by the British Army under the leadership of the Nobel Lauriate William Bragg. Flash spotting developed in parallel in the British, French and German armies. The combination of sound ranging (i.e., acoustic MASINT) and flash ranging (i.e., before modern optoelectronics) gave information unprecedented for the time, in both accuracy and timeliness. Enemy gun positions were located within 25 to 100 yards, with the information coming in three minutes or less.\n\nIn the \"Sound Ranging\" graphic, the manned Listening (or Advanced) Post, is sited a few 'sound seconds (or about 2000 yards) forward of the line of the unattended microphones, it sends an electrical signal to the recording station to switch on the recording apparatus. The positions of the microphones are precisely known. The differences in sound time of arrival, taken from the recordings, were then used to plot the source of the sound by one of several techniques. See http://nigelef.tripod.com/p_artyint-cb.htm#SoundRanging\n\nWhere sound ranging is a time-of-arrival technique not dissimilar to that of modern multistatic sensors, flash spotting used optical instruments to take bearings on the flash from accurately surveyed observation posts. The location of the gun was determined by plotting the bearings reported to the same gun flashes. See http://nigelef.tripod.com/p_artyint-cb.htm#FieldSurveyCoy Flash ranging, today, would be called electro-optical MASINT.\n\nArtillery sound and flash ranging remained in use through World War II and in its latest forms until the present day, although flash spotting generally ceased in the 1950s due to the widespread adoption of flashless propellants and the increasing range of artillery. Mobile counterbattery radars able to detect guns, itself a MASINT radar sensor, became available in the late 1970s, although countermortar radars appeared in World War II. These techniques paralleled radio direction finding in SIGINT that started in World War I, using graphical bearing plotting and now, with the precision time synchronization from GPS, is often time-of-arrival.\n\nArtillery positions now are located primarily with Unmanned Air Systems and IMINT or counterartillery radar, such as the widely used Swedish ArtHuR. SIGINT also may give clues to positions, both with COMINT for firing orders, and ELINT for such things as weather radar. Still, there is renewed interest in both acoustic and electro-optical systems to complement counterartillery radar.\n\nAcoustic sensors have come a long way since World War I. Typically, the acoustic sensor is part of a combined system, in which it cues radar or electro-optical sensors of greater precision, but narrower field of view.\n\nThe UK's hostile artillery locating system (HALO) has been in service with the British Army since the 1990s. HALO is not as precise as radar, but especially complements the directional radars. It passively detects artillery cannon, mortars and tank guns, with 360 degree coverage and can monitor over 2,000 square kilometers. HALO has worked in urban areas, the mountains of the Balkans, and the deserts of Iraq.\n\nThe system consists of three or more unmanned sensor positions, each with four microphones and local processing, these deduce the bearing to a gun, mortar, etc. These bearings are automatically communicated to a central processor that combines them to triangulate the source of the sound. It can compute location data on up to 8 rounds per second, and display the data to the system operator. HALO may be used in conjunction with COBRA and ArtHur counter battery radars, which are not omnidirectional, to focus on the correct sector.\n\nAnother acoustic system is the Unattended Transient Acoustic MASINT Sensor (UTAMS), developed by the U.S. Army Research Laboratory, which detects mortar and rocket launches and impacts. UTAMS remains the primary cueing sensor for the Persistent Threat Detection System (PTDS). ARL mounted aerostats with UTAMS, developing the system in a little over two months. After receiving a direct request from Iraq, ARL merged components from several programs to enable the rapid fielding of this capability. \n\nUTAMS has three to five acoustic arrays, each with four microphones, a processor, radio link, power source, and a laptop control computer. UTAMS, which was first operational in Iraq, first tested in November 2004 at a Special Forces Operating Base (SFOB) in Iraq. UTAMS was used in conjunction with AN/TPQ-36 and AN/TPQ-37 counter-artillery radar. While UTAMS was intended principally for detecting indirect artillery fire, Special Forces and their fire support officer learned it could pinpoint improvised explosive device (IED) explosions and small arms/rocket-propelled grenade (RPG) fires. It detected Points of Origin (POO) up to 10 kilometers from the sensor.\n\nAnalyzing the UTAMS and radar logs revealed several patterns. The opposing force was firing 60 mm mortars during observed dining hours, presumably since that gave the largest groupings of personnel and the best chance of producing heavy casualties. That would have been obvious from the impact history alone, but these MASINT sensors established a pattern of the enemy firing locations.\n\nThis allowed the US forces to move mortars into range of the firing positions, give coordinates to cannon when the mortars were otherwise committed, and to use attack helicopters as a backup to both. The opponents changed to night fires, which, again, were countered with mortar, artillery, and helicopter fires. They then moved into an urban area where US artillery was not allowed to fire, but a combination of PSYOPS leaflet drops and deliberate near misses convinced the locals not to give sanctuary to the mortar crews.\n\nOriginally for a Marine requirement in Afghanistan, UTAMS was combined with electro-optical MASINT to produce the Rocket Launch Spotter (RLS) system useful against both rockets and mortars.\n\nIn the Rocket Launch Spotter (RLS) application, each array consists of four microphones and processing equipment. Analyzing the time delays between an acoustic wavefront’s interaction with each microphone in the array UTAMS provides an azimuth of origin. The azimuth from each tower is reported to the UTAMS processor at the control station, and a POO is triangulated and displayed. The UTAMS subsystem can also detect and locate the point of impact (POI), but, due to the difference between the speeds of sound and light, it may take UTAMS as long as 30 seconds to determine the POO for a rocket launch 13 km away. In this application, the electro-optical component of RLS will detect the rocket POO earlier, while UTAMS may do better with the mortar prediction.\n\nModern hydrophones convert sound to electrical energy, which then can undergo additional signal processing, or that can be transmitted immediately to a receiving station. They may be directional or omnidirectional.\n\nNavies use a variety of acoustic systems, especially passive, in antisubmarine warfare, both tactical and strategic. For tactical use, passive hydrophones, both on ships and airdropped sonobuoys, are used extensively in antisubmarine warfare. They can detect targets far further away than with active sonar, but generally will not have the precision location of active sonar, approximating it with a technique called Target Motion Analysis (TMA). Passive sonar has the advantage of not revealing the position of the sensor.\n\nThe Integrated Undersea Surveillance System (IUSS) consists of multiple subsystems in SOSUS, Fixed Distributed System (FDS), and the Advanced Deployable System (ADS or SURTASS). Reducing the emphasis on Cold War blue-water operations put SOSUS, with more flexible \"tuna boat\" sensing vessels called SURTASS being the primary blue-water long-range sensors\nSURTASS used longer, more sensitive towed passive acoustic arrays than could be deployed from maneuvering vessels, such as submarines and destroyers.\n\nSURTASS is now being complemented by Low Frequency Active (LFA) sonar; see the sonar section.\n\nPassive sonobuoys, such as the AN/SSQ-53F, can be directional or omnidirectional and can be set to sink to a specific depth. These would be dropped from helicopters and maritime patrol aircraft such as the P-3.\n\nThe US installed massive Fixed Surveillance System (FSS, also known as SOSUS) hydrophone arrays on the ocean floor, to track Soviet and other submarines.\n\nPurely from the standpoint of detection, towed hydrophone arrays offer a long baseline and exceptional measurement capability. Towed arrays, however, are not always feasible, because when deployed, their performance can suffer, or they can suffer outright damage, from fast speeds or radical turns.\n\nSteerable sonar arrays on the hull or bow usually have a passive as well as active mode, as do variable-depth sonars\n\nSurface ships may have warning receivers to detect hostile sonar.\n\nModern submarines have multiple passive hydrophone systems, such as a steerable array in a bow dome, fixed sensors along the sides of the submarines, and towed arrays. They also have specialized acoustic receivers, analogous to radar warning receivers, to alert the crew to the use of active sonar against their submarine.\n\nUS submarines made extensive clandestine patrols to measure the signatures of Soviet submarines and surface vessels. This acoustic MASINT mission included both routine patrols of attack submarines, and submarines sent to capture the signature of a specific vessel. US antisubmarine technicians on air, surface, and subsurface platforms had extensive libraries of vessel acoustic signatures.\n\nPassive acoustic sensors can detect aircraft flying low over the sea.\n\nVietnam-era acoustic MASINT sensors included \"Acoubuoy (36 inches long, 26 pounds) floated down by camouflaged parachute and caught in the trees, where it hung to listen. The Spikebuoy (66 inches long, 40 pounds) planted itself in the ground like a lawn dart. Only the antenna, which looked like the stalks of weeds, was left showing above ground.\"\nThis was part of Operation Igloo White.\n\nPart of the AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) is a passive acoustic sensor, which, with other MASINT sensors, detects vehicles and personnel on a battlefield. Passive acoustic sensors provide additional measurements that can be compared with signatures, and used to complement other sensors. I-REMBASS control will integrate, in approximately 2008, with the .\n\nFor example, a ground search radar may not be able to differentiate between a tank and a truck moving at the same speed. Adding acoustic information, however, may quickly distinguish between them.\n\nCombatant vessels, of course, made extensive use of active sonar, which is yet another acoustic MASINT sensor. Besides the obvious application in antisubmarine warfare, specialized active acoustic systems have roles in:\n\n- Mapping the seafloor for navigation and collision avoidance. These include basic depth gauges, but quickly get into devices that do 3-dimensional underwater mapping\n- Determining seafloor characteristics, for applications varying from understanding its sound-reflecting properties, to predicting the type of marine life that may be found there, to knowing when a surface is appropriate for anchoring or for using various equipment that will contact the seafloor\n\nVarious synthetic aperture sonars have been built in the laboratory and some have entered use in mine-hunting and search systems. An explanation of their operation is given in synthetic aperture sonar.\n\nThe water surface and bottom are reflecting and scattering boundaries. Large schools of fish, with air in their swim bladder balance apparatus, can also have a significant effect on acoustic propagation.\n\nFor many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. \"The effects of the seafloor and the sea surface on acoustic systems in shallow water are highly complex, making range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.\"\n\nThe acoustic impedance mismatch between water and the bottom is generally much less than at the surface and is more complex. It depends on the bottom material types and depth of the layers. Theories have been developed for predicting the sound propagation in the bottom in this case, for example by Biot and by Buckingham.\n\nFor high frequency sonars (above about 1 kHz) or when the sea is rough, some of the incident sound is scattered, and this is taken into account by assigning a reflection coefficient whose magnitude is less than one.\n\nRather than measuring surface effects directly from a ship, radar MASINT, in aircraft or satellites, may give better measurements. These measurements would then be transmitted to the vessel's acoustic signal processor.\n\nA surface covered with ice, of course, is tremendously difference than even storm-driven water Purely from a collision avoidance and acoustic propagation, a submarine needs to know how close it is to the bottom of ice. Less obvious is the need to know the three-dimensional structure of the ice, because submarines may need to break through it to launch missiles, to raise electronic masts, or to surface the boat. Three-dimensional ice information also can tell the submarine captain whether antisubmarine warfare aircraft can detect or attack the boat.\n\nThe state of the art is providing the submarine with a three-dimensional visualization of the ice above: the lowest part (ice keel) and the ice canopy. While sound will propagate differently in ice than liquid water, the ice still needs to be considered as a volume, to understand the nature of reverberations within it.\n\nA typical basic depth measuring device is the US AN/UQN-4A. Both the water surface and bottom are reflecting and scattering boundaries. For many purposes, but not all naval tactical applications, the sea-air surface can be thought of as a perfect reflector. In reality, there are complex interactions of water surface activity, seafloor characteristics, water temperature and salinity, and other factors that make \"...range predictions difficult. Multi-path degradation affects overall figure of merit and active classification. As a result, false target identifications are frequent.\"\n\nThis device, however, does not give information on the characteristics of the bottom. In many respects, commercial fishing and marine scientists have equipment that is perceived as needed for shallow water operation.\n\nA further complication is the presence of wind generated bubbles or fish close to the sea surface.\n. The bubbles can also form plumes that absorb some of the incident and scattered sound, and scatter some of the sound themselves.\n\nThis problem is distinct from biologic interference caused by acoustic energy generated by marine life, such as the squeaks of porpoises and other cetaceans, and measured by acoustic receivers. The signatures of biologic sound generators need to be differentiated from more deadly denizens of the depths. Classifying biologics is a very good example of an acoustic MASINT process.\n\nModern surface combatants with an ASW mission will have a variety of active systems, with a hull- or bow-mounted array, protected from water by a rubber dome; a \"variable-depth\" dipping sonar on a cable, and, especially on smaller vessels, a fixed acoustic generator and receiver.\n\nSome, but not all, vessels carry passive towed arrays, or combined active-passive arrays. These depend on target noise, which, in the combined littoral environment of ultraquiet submarines in the presence of much ambient noise. Vessels that have deployed towed arrays cannot make radical course maneuvers. Especially when active capabilities are included, the array can be treated as a bistatic or multistatic sensor, and act as a synthetic aperture sonar (SAS)\n\nFor ships that cooperate with aircraft, they will need a data link to sonobuoys and a sonobuoy signal processor, unless the aircraft has extensive processing capability and can send information that can be accepted directly by tactical computers and displays.\n\nSignal processors not only analyze the signals, but constantly track propagation conditions. The former is usually considered part of a particular sonar, but the US Navy has a separate propagation predictor called the AN/UYQ-25B(V) Sonar \"in situ\" Mode Assessment System (SIMAS)\nEcho Tracker Classifiers (ETC) are adjuncts, with a clear MASINT flavor, to existing surface ship sonars\nETC is an application of synthetic aperture sonar (SAS). SAS is already used for minehunting, but could help existing surface combatants, as well as future vessels and unmanned surface vehicles (USV), detect threats, such as very silent air-independent propulsion non-nuclear submarines, outside torpedo range. Torpedo range, especially in shallow water, is considered anything greater than 10 nmi.\n\nConventional active sonar may be more effective than towed arrays, but the small size of modern littoral submarines makes them difficult threats. Highly variable bottom paths, biologics, and other factors complicate sonar detection. If the target is slow-moving or waiting on the bottom, they have little or no Doppler effect, which current sonars use to recognize threats.\n\nContinual active tracking measurement of all acoustically detected objects, with recognition of signatures as deviations from ambient noise, still gives a high false alarm rate (FAR) with conventional sonar. SAS processing, however, improves the resolution, especially of azimuth measurements, by assembling the data from multiple pings into a synthetic beam that gives the effect of a far larger receiver.\n\nMASINT-oriented SAS measures shape characteristics and eliminates acoustically detected objects that do not conform to the signature of threats. Shape recognition is only one of the parts of the signature, which include course and Doppler when available.\n\nActive sonobuoys, containing a sonar transmitter and receiver, can be dropped from fixed-wing maritime patrol aircraft (e.g., P-3, Nimrod, Chinese Y-8, Russian and Indian Bear ASW variants), antisubmarine helicopters, and carrier-based antisubmarine aircraft (e.g., S-3). While there have been some efforts to use other aircraft simply as carriers of sonobuoys, the general assumption is that the sonobuoy-carrying aircraft can issue commands to the sonobuoys and receive, and to some extent process, their signals.\n\nThe Directional Hydrophone Command Activated Sonobuoy system (DICASS) both generate sound and listen for it. A typical modern active sonobuoy, such as the AN/SSQ 963D, generates multiple acoustic frequencies \n. Other active sonobuoys, such as the AN/SSQ 110B, generate small explosions as acoustic energy sources.\n\nAntisubmarine helicopters can carry a \"dipping\" sonar head at the end of a cable, which the helicopter can raise from or lower into the water. The helicopter would typically dip the sonar when trying to localize a target submarine, usually in cooperation with other ASW platforms or with sonobuoys. Typically, the helicopter would raise the head after dropping an ASW weapon, to avoid damaging the sensitive receiver. Not all variants of the same basic helicopter, even assigned to ASW, carry dipping sonar; some may trade the weight of the sonar for more sonobuoy or weapon capacity.\n\nThe EH101 helicopter, used by a number of nations, has a variety of dipping sonars. The (British) Royal Navy version has Ferranti/Thomson-CSF (now Thales) sonar, while the Italian version uses the HELRAS. Russian Ka-25 helicopters carry dipping sonar, as does the US LAMPS, US MH-60R helicopter, which carries the Thales AQS-22 dipping sonar. The older SH-60F helicopter carries the AQS-13F dipping sonar.\n\nNewer Low-Frequency Active (LFA) systems are controversial, as their very high sound pressures may be hazardous to whales and other marine life \nA decision has been made to employ LFA on SURTASS vessels, after an environmental impact statement that indicated, if LFA is used with decreased power levels in certain high-risk areas for marine life, it would be safe when employed from a moving ship. The ship motion, and the variability of the LFA signal, would limit the exposure to individual sea animals. LFA operates in the low-frequency (LF) acoustic band of 100–500 Hz. It has an active component, the LFA proper, and the passive SURTASS hydrophone array. \"The active component of the system, LFA, is a set of 18 LF acoustic transmitting source elements (called projectors) suspended by cable from underneath an oceanographic surveillance vessel, such as the Research Vessel (R/V) Cory Chouest, USNS Impeccable (T-AGOS 23), and the Victorious class (TAGOS 19 class).\n\n\"The source level of an individual projector is 215 dB. These projectors produce the active sonar signal or “ping.” A \"ping,\" or transmission, can last between 6 and 100 seconds. The time between transmissions is typically 6 to 15 minutes with an average transmission of 60 seconds. Average duty cycle (ratio of sound “on” time to total time) is less than 20 percent. The typical duty cycle, based on historical LFA operational parameters (2003 to 2007), is normally 7.5 to 10 percent.\"\n\nThis signal \"...is not a continuous tone, but rather a transmission of waveforms that vary in frequency and duration. The duration of each continuous frequency sound transmission is normally 10 seconds or less. The signals are loud at the source, but levels diminish rapidly over the first kilometer.\"\n\nThe primary tactical active sonar of a submarine is usually in the bow, covered with a protective dome. Submarines for blue-water operations used active systems such as the AN/SQS-26 and AN/SQS-53 have been developed but were generally designed for convergence zone and single bottom bounce environments.\n\nSubmarines that operate in the Arctic also have specialized sonar for under-ice operation; think of an upside-down fathometer.\n\nSubmarines also may have minehunting sonar. Using measurements to differentiate between biologic signatures and signatures of objects that will permanently sink the submarine is as critical a MASINT application as could be imagined.\n\nSonars optimized to detect objects of the size and shapes of mines can be carried by submarines, remotely operated vehicles, surface vessels (often on a boom or cable) and specialized helicopters.\n\nThe classic emphasis on minesweeping, and detonating the mine released from its tether using gunfire, has been replaced with the AN/SLQ-48(V)2 mine neutralization system (MNS)AN/SLQ-48 - (remotely operated) Mine Neutralization Vehicle. This works well for rendering save mines in deep water, by placing explosive charges on the mine and/or its tether. The AN/SLQ-48 is not well suited to the neutralization of shallow-water mines. The vehicle tends to be underpowered and may leave on the bottom a mine that looks like a mine to any subsequent sonar search and an explosive charge subject to later detonation under proper impact conditions.\n\nThere is mine-hunting sonar, as well as (electro-optical) television on the ROV, and AN/SQQ-32 minehunting sonar on the ship.\n\nAn assortment of time-synchronized sensors can characterize conventional or nuclear explosions. One pilot study, the Active Radio Interferometer for Explosion Surveillance (ARIES). This technique implements an operational system for monitoring ionospheric pressure waves resulting from surface or atmospheric nuclear or chemical explosives. Explosions produce pressure waves that can be detected by measuring phase variations between signals generated by ground stations along two different paths to a satellite. This is a very modernized version, on a larger scale, of World War I sound ranging.\n\nAs can many sensors, ARIES can be used for additional purposes. Collaborations are being pursued with the Space Forecast Center to use ARIES data for total electron content measures on a global scale, and with the meteorology/global environment community to monitor global climate change (via tropospheric water vapor content measurements), and by the general ionospheric physics community to study travelling ionospheric disturbances.\n\nSensors relatively close to a nuclear event, or a high-explosive test simulating a nuclear event, can detect, using acoustic methods, the pressure produced by the blast. These include infrasound microbarographs (acoustic pressure sensors) that detect very low-frequency sound waves in the atmosphere produced by natural and man-made events.\n\nClosely related to the microbarographs, but detecting pressure waves in water, are hydro-acoustic sensors, both underwater microphones and specialized seismic sensors that detect the motion of islands.\n\nUS Army Field Manual 2-0 defines seismic intelligence as \"The passive collection and measurement of seismic waves or vibrations in the earth surface.\" One strategic application of seismic intelligence makes use of the science of seismology to locate and characterize nuclear testing, especially underground testing. Seismic sensors also can characterize large conventional explosions that are used in testing the high-explosive components of nuclear weapons. Seismic intelligence also can help locate such things as large underground construction projects.\n\nSince many areas of the world have a great deal of natural seismic activity, seismic MASINT is one of the emphatic arguments that there must be a long-term commitment to measuring, even during peacetime, so that the signatures of natural behavior is known before it is necessary to search for variations from signatures.\n\nFor nuclear test detection, seismic intelligence is limited by the \"threshold principle\" coined in 1960 by George Kistiakowsky, which recognized that while detection technology would continue to improve, there would be a threshold below which small explosions could not be detected.\n\nThe most common sensor in the Vietnam-era \"McNamara Line\" of remote sensors was the ADSID (Air-Delivered Seismic Intrusion Detector) sensed earth motion to detect people and vehicles. It resembled the Spikebuoy, except it was smaller and lighter (31 inches long, 25 pounds).\nThe challenge for the seismic sensors (and for the analysts) was not so much in detecting the people and the trucks as it was in separating out the false alarms generated by wind, thunder, rain, earth tremors, and animals—especially frogs.\"\n\nThis subdiscipline is also called piezoelectric MASINT after the sensor most often used to sense vibration, but vibration detectors need not be piezoelectric. Note that some discussions treat seismic and vibration sensors as a subset of acoustic MASINT. Other possible detectors could be moving coil or surface acoustic wave.\n. Vibration, as a form of geophysical energy to be sensed, has similarities to acoustic and seismic MASINT, but also has distinct differences that make it useful, especially in unattended ground sensors (UGS). In the UGS application, one advantage of a piezoelectric sensor is that it generates electricity when triggered, rather than consuming electricity, an important consideration for remote sensors whose lifetime may be determined by their battery capacity.\n\nWhile acoustic signals at sea travel through water, on land, they can be assumed to come through the air. Vibration, however, is conducted through a solid medium on land. It has a higher frequency than is typical of seismic conducted signals.\n\nA typical detector, the Thales MA2772 vibration is a piezoelectric cable, shallowly buried below the ground surface, and extended for 750 meters. Two variants are available, a high-sensitivity version for personnel detection, and lower-sensitivity version to detect vehicles. Using two or more sensors will determine the direction of travel, from the sequence in which the sensors trigger.\n\nIn addition to being buried, piezoelectric vibration detectors, in a cable form factor, also are used as part of high-security fencing. They can be embedded in walls or other structures that need protection.\n\nA magnetometer is a scientific instrument used to measure the strength and/or direction of the magnetic field in the vicinity of the instrument. The measurements they make can be compared to signatures of vehicles on land, submarines underwater, and atmospheric radio propagation conditions. They come in two basic types: \n\n- Scalar magnetometers measure the total strength of the magnetic field to which they are subjected, and\n- Vector magnetometers have the capability to measure the component of the magnetic field in a particular direction.\n\nEarth's magnetism varies from place to place and differences in the Earth's magnetic field (the magnetosphere) can be caused by two things:\n\n- The differing nature of rocks\n- The interaction between charged particles from the sun and the magnetosphere\n\nMetal detectors use electromagnetic induction to detect metal. They can also determine the changes in existing magnetic fields caused by metallic objects.\n\nOne of the first means for detecting submerged submarines, first installed by the Royal Navy in 1914, was the effect of their passage over an anti-submarine indicator loop on the bottom of a body of water. A metal object passing over it, such as a submarine, will, even if degaussed, have enough magnetic properties to induce a current in the loop's cable. . In this case, the motion of the metal submarine across the indicating coil acts as an oscillator, producing electric current.\n\nA magnetic anomaly detector (MAD) is an instrument used to detect minute variations in the Earth's magnetic field. The term refers specifically to magnetometers used either by military forces to detect submarines (a mass of ferromagnetic material creates a detectable disturbance in the magnetic field)Magnetic anomaly detectors were first employed to detect submarines during World War II. MAD gear was used by both Japanese and U.S. anti-submarine forces, either towed by ship or mounted in aircraft to detect shallow submerged enemy submarines. After the war, the U.S. Navy continued to develop MAD gear as a parallel development with sonar detection technologies.\n\nTo reduce interference from electrical equipment or metal in the fuselage of the aircraft, the MAD sensor is placed at the end of a boom or a towed aerodynamic device. Even so, the submarine must be very near the aircraft's position and close to the sea surface for detection of the change or anomaly. The detection range is normally related to the distance between the sensor and the submarine. The size of the submarine and its hull composition determine the detection range. MAD devices are usually mounted on aircraft or helicopters.\n\nThere is some misunderstanding of the mechanism of detection of submarines in water using the MAD boom system. Magnetic moment displacement is ostensibly the main disturbance, yet submarines are detectable even when oriented parallel to the Earth's magnetic field, despite construction with non-ferromagnetic hulls.\n\nFor example, the Soviet-Russian Alfa class submarine, was constructed out of titanium. This light, strong material, as well as a unique nuclear power system, allowed the submarine to break speed and depth records for operational boats. It was thought that nonferrous titanium would defeat magnetic ASW sensors, but this was not the case. to give dramatic submerged performance and protection from detection by MAD sensors, is still detectable.\n\nSince titanium structures are detectable, MAD sensors do not directly detect deviations in the Earth's magnetic field. Instead, they may be described as long-range electric and electromagnetic field detector arrays of great sensitivity.\n\nAn electric field is set up in conductors experiencing a variation in physical environmental conditions, providing that they are contiguous and possess sufficient mass. Particularly in submarine hulls, there is a measurable temperature difference between the bottom and top of the hull producing a related salinity difference, as salinity is affected by temperature of water. The difference in salinity creates an electric potential across the hull. An electric current then flows through the hull, between the laminae of sea-water separated by depth and temperature. The resulting dynamic electric field produces an electromagnetic field of its own, and thus even a titanium hull will be detectable on a MAD scope, as will a surface ship for the same reason.\n\nThe Remotely Emplaced Battlefield Surveillance System (REMBASS) is a US Army program for detecting the presence, speed, and direction of a ferrous object, such as a tank. Coupled with acoustic sensors that recognize the sound signature of a tank, it could offer high accuracy. It also collects weather information.\n\nThe Army's AN/GSQ-187 Improved Remote Battlefield Sensor System (I-REMBASS) includes both magnetic-only and combined passive infrared/magnetic intrusion detectors. The DT-561/GSQ hand emplaced MAG \"sensor detects vehicles (tracked or wheeled) and personnel carrying ferrous metal. It also provides information on which to base a count of objects passing through its detection zone and reports their direction of travel relative to its location. The monitor uses two different (MAG and IR) sensors and their identification codes to determine direction of travel.\n\nMagnetic sensors, much more sophisticated than the early inductive loops, can trigger the explosion of mines or torpedoes. Early in World War II, the US tried to put magnetic torpedo exploder far beyond the limits of the technology of the time, and had to disable it, and then work on also-unreliable contact fuzing, to make torpedoes more than blunt objects than banged into hulls.\n\nSince water is incompressible, an explosion under the keel of a vessel is far more destructive than one at the air-water interface. Torpedo and mine designers want to place the explosions in that vulnerable spot, and countermeasures designers want to hide the magnetic signature of a vessel. Signature is especially relevant here, as mines may be made selective for warships, merchant vessel unlikely to be hardened against underwater explosions, or submarines.\n\nA basic countermeasure, started in World War II, was degaussing, but it is impossible to remove all magnetic properties.\n\nLandmines often contain enough ferrous metal to be detectable with appropriate magnetic sensors. Sophisticated mines, however, may also sense a metal-detection oscillator, and, under preprogrammed conditions, detonate to deter demining personnel.\n\nNot all landmines have enough metal to activate a magnetic detector. While, unfortunately, the greatest number of unmapped minefields are in parts of the world that cannot afford high technology, a variety of MASINT sensors could help demining. These would include ground-mapping radar, thermal and multispectral imaging, and perhaps synthetic aperture radar to detect disturbed soil.\n\nGravity is a function of mass. While the average value of Earth's surface gravity is approximately 9.8 meters per second squared, given sufficiently sensitive instrumentation, it is possible to detect local variations in gravity from the different densities of natural materials: the value of gravity will be greater on top of a granite monolith than over a sand beach. Again with sufficiently sensitive instrumentation, it should be possible to detect gravitational differences between solid rock, and rock excavated for a hidden facility.\n\nStreland 2003 points out that the instrumentation indeed must be sensitive: variations of the force of gravity on the earth’s surface are on the order of 10 of the average value. A practical gravitimetric detector of buried facilities would need to be able to measure \"less than one one millionth of the force that caused the apple to fall on Sir Isaac Newton’s head.\" To be practical, it would be necessary for the sensor to be able to be used while in motion, measuring the change in gravity between locations. This change over distance is called the \"gravity gradient\", which can be measured with a gravity gradiometer.\n\nDeveloping an operationally useful gravity gradiometer is a major technical challenge. One type, the SQUID Superconducting Quantum Interference Device gradiometer, may have adequate sensitivity, but it needs extreme cryogenic cooling, even if in space, a logistic nightmare. Another technique, far more operationally practical but lacking the necessary sensitivity, is the Gravity Recovery and Climate Experiment (GRACE) technique, currently using radar to measure the distance between pairs of satellites, whose orbits will change based on gravity. Substituting lasers for radar will make GRACE more sensitive, but probably not sensitive enough.\n\nA more promising technique, although still in the laboratory, is quantum gradiometry, which is an extension of atomic clock techniques, much like those in GPS. Off-the-shelf atomic clocks measure changes in atomic waves over time rather than the spatial changes measured in a quantum gravity gradiometer. One advantage of using GRACE in satellites is that measurements can be made from a number of points over time, with a resulting improvement as seen in synthetic aperture radar and sonar. Still, finding deeply buried structures of human scale is a tougher problem than the initial goals of finding mineral deposits and ocean currents.\n\nTo make this operationally feasible, there would have to be a launcher to put fairly heavy satellites into polar orbits, and as many earth stations as possible to reduce the need for large on-board storage of the large amounts of data the sensors will produce. Finally, there needs to be a way to convert the measurements into a form that can be compared against available signatures in geodetic data bases. Those data bases would need significant improvement, from measured data, to become sufficiently precise that a buried facility signature would stand out.\n", "related": "NONE"}
{"id": "1543837", "url": "https://en.wikipedia.org/wiki?curid=1543837", "title": "Phase response", "text": "Phase response\n\nIn signal processing, phase response is the relationship between the phase of a sinusoidal input and the output signal passing through any device that accepts input and produces an output signal, such as an amplifier or a filter.\n\nAmplifiers, filters, and other devices are often categorized by their amplitude and/or phase response. The amplitude response is the ratio of output amplitude to input, usually a function of the frequency. Similarly, phase response is the phase of the output with the input as reference. The input is defined as zero phase. A phase response is not limited to lying between 0° and 360°, as phase can accumulate to any amount of time.\n\n", "related": "\n- Group delay and phase delay\n"}
{"id": "3005828", "url": "https://en.wikipedia.org/wiki?curid=3005828", "title": "Adaptive beamformer", "text": "Adaptive beamformer\n\nAn adaptive beamformer is a system that performs adaptive spatial signal processing with an array of transmitters or receivers. The signals are combined in a manner which increases the signal strength to/from a chosen direction. Signals to/from other directions are combined in a benign or destructive manner, resulting in degradation of the signal to/from the undesired direction. This technique is used in both radio frequency and acoustic arrays, and provides for directional sensitivity without physically moving an array of receivers or transmitters.\n\nAdaptive beamforming was initially developed in the 1960s for the military applications of sonar and radar. There exist several modern applications for beamforming, one of the most visible applications being commercial wireless networks such as LTE. Initial applications of adaptive beamforming were largely focused in radar and electronic countermeasures to mitigate the effect of signal jamming in the military domain. \n- Radar uses can be seen here Phased array radar. Although not strictly adaptive, these radar applications make use of either static or dynamic (scanning) beamforming.\n- Commercial wireless standards such as 3GPP Long Term Evolution (LTE (telecommunication)) and IEEE 802.16 WiMax rely on adaptive beamforming to enable essential services within each standard.\n\nAn adaptive beamforming system relies on principles of wave propagation and phase relationships. See Constructive interference, and Beamforming. Using the principles of superimposing waves, a higher or lower amplitude wave is created (e.g. by delaying and weighting the signal received). The adaptive beamforming system dynamically adapts in order to maximize or minimize a desired parameter, such as Signal-to-interference-plus-noise ratio.\n\nThere are several ways to approach the beamforming design, the first approach was implemented by maximizing the signal to noise ratio (SNR) by Applebaum 1965. This technique adapts the system parameters in order to maximize the receive signal power, while minimizing noise (such as interference or jamming). Another approach is the Least Mean Squares (LMS) error method implemented by Widrow, and Maximum Likelihood Method (MLM), developed in 1969 by Capon. Both the Applebaum and the Widrow algorithms are very similar, and converge toward an optimal solution. However, these techniques have implementation drawbacks. In 1974, Reed demonstrated a technique known as Sample-Matrix Inversion (SMI). SMI determines the adaptive antenna array weights directly, unlike the algorithms of Applebaum and Widrow.\n\nA detailed explanation of the adaptive techniques introduced above can be found here:\n\n- Least Mean Squares Algorithm\n- Sample Matrix Inversion Algorithm\n- Recursive Least Square Algorithm\n- Conjugate gradient method\n- Constant Modulus Algorithm\n\n", "related": "\n- Beamforming is spatial signal processing which makes spatial beam focused on the target direction and spatial beam nulled interference signal.\n- Smart Antennas is multiple antenna systems having one of three structures which are single-input and multiple-output (SIMO), multiple-input and single-output (MISO), and multiple-input and multiple-output (MIMO) structures.\n- MIMO is an advanced smart antenna system which has multiple transmit antennas at the transmitter and multiple receive antennas at the receiver.\n"}
{"id": "3304717", "url": "https://en.wikipedia.org/wiki?curid=3304717", "title": "Causal filter", "text": "Causal filter\n\nIn signal processing, a causal filter is a linear and time-invariant causal system. The word \"causal\" indicates that the filter output depends only on past and present inputs. A filter whose output also depends on future inputs is non-causal, whereas a filter whose output depends \"only\" on future inputs is anti-causal. Systems (including filters) that are \"realizable\" (i.e. that operate in real time) must be causal because such systems cannot act on a future input. In effect that means the output sample that best represents the input at time formula_1 comes out slightly later. A common design practice for digital filters is to create a realizable filter by shortening and/or time-shifting a non-causal impulse response. If shortening is necessary, it is often accomplished as the product of the impulse-response with a window function.\n\nAn example of an anti-causal filter is a maximum phase filter, which can be defined as a stable, anti-causal filter whose inverse is also stable and anti-causal.\n\nThe following definition is a moving (or \"sliding\") average of input data formula_2. A constant factor of 1/2 is omitted for simplicity:\n\nwhere \"x\" could represent a spatial coordinate, as in image processing. But if formula_4 represents time formula_5, then a moving average defined that way is non-causal (also called \"non-realizable\"), because formula_6 depends on future inputs, such as formula_7. A realizable output is\n\nwhich is a delayed version of the non-realizable output.\n\nAny linear filter (such as a moving average) can be characterized by a function \"h\"(\"t\") called its impulse response. Its output is the convolution\n\nIn those terms, causality requires\n\nand general equality of these two expressions requires \"h\"(\"t\") = 0 for all \"t\" < 0.\n\nLet \"h\"(\"t\") be a causal filter with corresponding Fourier transform \"H\"(ω). Define the function\n\nwhich is non-causal. On the other hand, \"g\"(\"t\") is Hermitian and, consequently, its Fourier transform \"G\"(ω) is real-valued. We now have the following relation\n\nwhere Θ(\"t\") is the Heaviside unit step function.\n\nThis means that the Fourier transforms of \"h\"(\"t\") and \"g\"(\"t\") are related as follows\n\nwhere formula_14 is a Hilbert transform done in the frequency domain (rather than the time domain). The sign of formula_14 may depend on the definition of the Fourier Transform.\n\nTaking the Hilbert transform of the above equation yields this relation between \"H\" and its Hilbert transform:\n\n", "related": "NONE"}
{"id": "3841160", "url": "https://en.wikipedia.org/wiki?curid=3841160", "title": "Constant fraction discriminator", "text": "Constant fraction discriminator\n\nA constant fraction discriminator (CFD) is an electronic signal processing device, designed to mimic the mathematical operation of finding a maximum of a pulse by finding the zero of its slope.\nSome signals do not have a sharp maximum, but short rise times formula_1.\n\nTypical input signals for CFDs are pulses from plastic scintillation counters, such as those used for lifetime measurement in positron annihilation experiments. The scintillator pulses have identical rise times that are much longer than the desired temporal resolution. This forbids simple threshold triggering, which causes a dependence of the trigger time on the signal's peak height, an effect called \"time walk\" (see diagram). Identical rise times and peak shapes permit triggering not on a fixed threshold but on a \"constant fraction\" of the total peak height, yielding trigger times independent from peak heights.\n\nA time to digital converter assigns timestamps. The time to digital converter needs fast rising edges with normed height.\nThe plastic scintillation counter delivers fast rising edge with varying heights.\nTheoretically, the signal could be split into two parts. One part would be delayed and the other low pass filtered, inverted and then used in a variable gain amplifier to amplify the original signal to the desired height. Practically, it is difficult to achieve a high dynamic range for the variable gain amplifier, and analog computers have problems with the inverse value.\n\nThe incoming signal is split into three components.\nOne component is delayed by a time formula_2, with formula_3\n- it may be multiplied by a small factor to put emphasis on the leading edge of the pulse -\nand connected to the noninverting input of a comparator.\nOne component is connected to the inverting input of this comparator.\nOne component is connected to the noninverting input of another comparator.\nA threshold value is connected to the inverting input of the other comparator.\nThe output of both comparators is fed through an AND gate.\nA discriminator without that constant fraction would just be a comparator.\n\nTherefore the word discriminator is used for something different\n(namely for an FM-demodulator).\n\nOften the logic levels are shifted from -15 V < low < 0 < high < 15 V delivered by the comparator to 0 V < low < 1.5 V < high < 3.3 V needed by CMOS logic.\n\nIf the discriminator triggers a sampler with a following comparator this is called a single channel analyzer (SCA).\nIf an Analog-to-digital converter is used, this is called a multi channel analyzer (MCA).\n\nhttp://www.ortec-online.com/download/Fast-Timing-Discriminator-Introduction.pdf\n\n- Beuzekom, M. (2006). \"Identifying fast hadrons with silicon detectors\", Appendix A, University of Groningen Faculty of Mathematics and Natural Sciences Dissertation\n", "related": "NONE"}
{"id": "2017203", "url": "https://en.wikipedia.org/wiki?curid=2017203", "title": "Nichols plot", "text": "Nichols plot\n\nThe Nichols plot is a plot used in signal processing and control design, named after American engineer Nathaniel B. Nichols.\n\nGiven a transfer function,\n\nformula_1\n\nwith the closed-loop transfer function defined as,\n\nformula_2\n\nthe Nichols plots displays formula_3 versus formula_4. Loci of constant formula_5 and formula_6 are overlaid to allow the designer to obtain the closed loop transfer function directly from the open loop transfer function. Thus, the frequency formula_7 is the parameter along the curve. This plot may be compared to the Bode plot in which the two inter-related graphs - formula_3 versus formula_9 and formula_4 versus formula_9) - are plotted.\n\nIn feedback control design, the plot is useful for assessing the stability and robustness of a linear system. This application of the Nichols plot is central to the quantitative feedback theory (QFT) of Horowitz and Sidi, which is a well known method for robust control system design.\n\nIn most cases, formula_4 refers to the phase of the system's response. Although similar to a Nyquist plot, a Nichols plot is plotted in a Cartesian coordinate system while a Nyquist plot is plotted in a Polar coordinate system.\n\n", "related": "\n- Hall circles\n- Nyquist plot\n- Transfer function\n\n- Mathematica function for creating the Nichols plot\n"}
{"id": "3305211", "url": "https://en.wikipedia.org/wiki?curid=3305211", "title": "Quadrature filter", "text": "Quadrature filter\n\nIn signal processing, a quadrature filter formula_1 is the analytic representation of the impulse response formula_2 of a real-valued filter:\n\nIf the quadrature filter formula_1 is applied to a signal formula_5, the result is\n\nwhich implies that formula_7 is the analytic representation of formula_8.\n\nSince formula_9 is an analytic signal, it is either zero or complex-valued. In practice, therefore, formula_9 is often implemented as two real-valued filters, which correspond to the real and imaginary parts of the filter, respectively.\n\nAn ideal quadrature filter cannot have a finite support, but by choosing the function formula_2 carefully, it is possible to design quadrature filters which are localized such that they can be approximated by means of functions of finite support.\n\nNotice that the computation of an ideal analytic signal for general signals cannot be made in practice since it involves convolutions with the function\n\nwhich is difficult to approximate as a filter which is either causal or of finite support, or both. According to the above result, however, it is possible to obtain an analytic signal by convolving the signal formula_5 with a quadrature filter formula_1. Given that formula_1 is designed with some care, it can be approximated by means of a filter which can be implemented in practice. The resulting function formula_7 is the analytic signal of formula_17 rather than of formula_18. This implies that formula_19 should be chosen such that convolution by formula_19 affects the signal as little as possible. Typically, formula_2 is a band-pass filter, removing low and high frequencies, but allowing frequencies within a range which includes the interesting components of the signal to pass.\n\nFor single frequency signals (in practice narrow bandwidth signals) with frequency formula_22 the \"magnitude\" of the response of a quadrature filter equals the signal's amplitude \"A\" times the frequency function of the filter at frequency formula_22.\n\nThis property can be useful when the signal \"s\" is a narrow-bandwidth signal of unknown frequency. By choosing a suitable frequency function \"Q\" of the filter, we may generate known functions of the unknown frequency formula_22 which then can be estimated.\n\n", "related": "\n- Analytic signal\n- Hilbert transform\n"}
{"id": "1658083", "url": "https://en.wikipedia.org/wiki?curid=1658083", "title": "Direction of arrival", "text": "Direction of arrival\n\nIn signal processing, direction of arrival (DOA) denotes the direction from which usually a propagating wave arrives at a point, where usually a set of sensors are located. These set of sensors forms what is called a sensor array. Often there is the associated technique of beamforming which is estimating the signal from a given direction. Various engineering problems addressed in the associated literature are:\n- Find the direction relative to the array where the sound source is located\n- Direction of different sound sources around you are also located by you using a process similar to those used by the algorithms in the literature\n- Radio telescopes use these techniques to look at a certain location in the sky\n- Recently beamforming has also been used in radio frequency (RF) applications such as wireless communication. Compared with the spatial diversity techniques, beamforming is preferred in terms of complexity. On the other hand, beamforming in general has much lower data rates. In multiple access channels (code-division multiple access (CDMA), frequency-division multiple access (FDMA), time-division multiple access (TDMA)), beamforming is necessary and sufficient\n- Various techniques for calculating the direction of arrival, such as angle of arrival (AoA), time difference of arrival (TDOA), frequency difference of arrival (FDOA), or other similar associated techniques.\n\n- Periodogram\n- MUSIC\n- SAMV\n- Maximum likelihood\n", "related": "NONE"}
{"id": "29293", "url": "https://en.wikipedia.org/wiki?curid=29293", "title": "Optical spectrometer", "text": "Optical spectrometer\n\nAn optical spectrometer (spectrophotometer, spectrograph or spectroscope) is an instrument used to measure properties of light over a specific portion of the electromagnetic spectrum, typically used in spectroscopic analysis to identify materials. The variable measured is most often the light's intensity but could also, for instance, be the polarization state. The independent variable is usually the wavelength of the light or a unit directly proportional to the photon energy, such as reciprocal centimeters or electron volts, which has a reciprocal relationship to wavelength.\n\nA spectrometer is used in spectroscopy for producing spectral lines and measuring their wavelengths and intensities. Spectrometers may also operate over a wide range of non-optical wavelengths, from gamma rays and X-rays into the far infrared. If the instrument is designed to measure the spectrum in absolute units rather than relative units, then it is typically called a spectrophotometer. The majority of spectrophotometers are used in spectral regions near the visible spectrum.\n\nIn general, any particular instrument will operate over a small portion of this total range because of the different techniques used to measure different portions of the spectrum. Below optical frequencies (that is, at microwave and radio frequencies), the spectrum analyzer is a closely related electronic device.\n\nSpectrometers are used in many fields. For example, they are used in astronomy to analyze the radiation from astronomical objects and deduce chemical composition. The spectrometer uses a prism or a grating to spread the light from a distant object into a spectrum. This allows astronomers to detect many of the chemical elements by their characteristic spectral fingerprints. If the object is glowing by itself, it will show spectral lines caused by the glowing gas itself. These lines are named for the elements which cause them, such as the hydrogen alpha, beta, and gamma lines. Chemical compounds may also be identified by absorption. Typically these are dark bands in specific locations in the spectrum caused by energy being absorbed as light from other objects passes through a gas cloud. Much of our knowledge of the chemical makeup of the universe comes from spectra.\n\nSpectroscopes are often used in astronomy and some branches of chemistry. Early spectroscopes were simply prisms with graduations marking wavelengths of light. Modern spectroscopes generally use a diffraction grating, a movable slit, and some kind of photodetector, all automated and controlled by a computer.\n\nJoseph von Fraunhofer developed the first modern spectroscope by combining a prism, diffraction slit and telescope in a manner that increased the spectral resolution and was reproducible in other laboratories. Fraunhofer also went on to invent the first diffraction spectroscope. Gustav Robert Kirchhoff and Robert Bunsen discovered the application of spectroscopes to chemical analysis and used this approach to discover caesium and rubidium. Kirchhoff and Bunsen's analysis also enabled a chemical explanation of stellar spectra, including Fraunhofer lines.\n\nWhen a material is heated to incandescence it emits light that is characteristic of the atomic makeup of the material.\nParticular light frequencies give rise to sharply defined bands on the scale which can be thought of as fingerprints. For example, the element sodium has a very characteristic double yellow band known as the Sodium D-lines at 588.9950 and 589.5924 nanometers, the color of which will be familiar to anyone who has seen a low pressure sodium vapor lamp.\n\nIn the original spectroscope design in the early 19th century, light entered a slit and a collimating lens transformed the light into a thin beam of parallel rays. The light then passed through a prism (in hand-held spectroscopes, usually an Amici prism) that refracted the beam into a spectrum because different wavelengths were refracted different amounts due to dispersion. This image was then viewed through a tube with a scale that was transposed upon the spectral image, enabling its direct measurement.\n\nWith the development of photographic film, the more accurate spectrograph was created. It was based on the same principle as the spectroscope, but it had a camera in place of the viewing tube. In recent years, the electronic circuits built around the photomultiplier tube have replaced the camera, allowing real-time spectrographic analysis with far greater accuracy. Arrays of photosensors are also used in place of film in spectrographic systems. Such spectral analysis, or spectroscopy, has become an important scientific tool for analyzing the composition of unknown material and for studying astronomical phenomena and testing astronomical theories.\n\nIn modern spectrographs in the UV, visible, and near-IR spectral ranges, the spectrum is generally given in the form of photon number per unit wavelength (nm or μm), wavenumber (μm, cm), frequency (THz), or energy (eV), with the units indicated by the abscissa. In the mid- to far-IR, spectra are typically expressed in units of Watts per unit wavelength (μm) or wavenumber (cm). In many cases, the spectrum is displayed with the units left implied (such as \"digital counts\" per spectral channel).\n\nA spectrograph is an instrument that separates light by its wavelengths and records this data. A spectrograph typically has a multi-channel detector system or camera that detects and records the spectrum of light. \n\nThe term was first used in 1876 by Dr. Henry Draper when he invented the earliest version of this device, and which he used to take several photographs of the spectrum of Vega. This earliest version of the spectrograph was cumbersome to use and difficult to manage.\n\nThere are several kinds of machines referred to as \"spectrographs\", depending on the precise nature of the waves. The first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.\n\nA spectrograph is sometimes called polychromator, as an analogy to monochromator.\n\nThe first spectrographs used photographic paper as the detector. The star spectral classification and discovery of the main sequence, Hubble's law and the Hubble sequence were all made with spectrographs that used photographic paper. The plant pigment phytochrome was discovered using a spectrograph that used living plants as the detector. More recent spectrographs use electronic detectors, such as CCDs which can be used for both visible and UV light. The exact choice of detector depends on the wavelengths of light to be recorded.\n\nThe forthcoming James Webb Space Telescope will contain both a near-infrared spectrograph (NIRSpec) and a mid-infrared spectrograph (MIRI).\n\nAn Echelle spectrograph uses two diffraction gratings, rotated 90 degrees with respect to each other and placed close to one another. Therefore, an entrance point and not a slit is used and a 2d CCD-chip records the spectrum. Usually one would guess to retrieve a spectrum on the diagonal, but when both gratings have a wide spacing and one is blazed so that only the first order is visible and the other is blazed that a lot of higher orders are visible, one gets a very fine spectrum nicely folded onto a small common CCD-chip. The small chip also means that the collimating optics need not to be optimized for coma or astigmatism, but the spherical aberration can be set to zero.\n\n", "related": "\n- Circular dichroism\n- Cosmic Origins Spectrograph\n- Czerny-Turner monochromator\n- Imaging spectrometer\n- List of light sources\n- Long-slit spectroscopy\n- Prism spectrometer\n- Scanning mobility particle sizer\n- Spectrogram\n- Spectrometer\n- Spectroradiometer\n- Spectroscopy\n- Virtually imaged phased array\n- J. F. James and R. S. Sternberg (1969), \"The Design of Optical Spectrometers\" (Chapman and Hall Ltd)\n- James, John (2007), \"Spectrograph Design Fundamentals\" (Cambridge University Press)\n- Browning, John (1882), \"How to work with the spectroscope : a manual of practical manipulation with spectroscopes of all kinds\"\n\n- Spectrograph for astronomical Spectra\n- Photographs of spectrographs used in the Lick Observatory from the Lick Observatory Records Digital Archive, UC Santa Cruz Library’s Digital Collections\n"}
{"id": "26685290", "url": "https://en.wikipedia.org/wiki?curid=26685290", "title": "Signal chain", "text": "Signal chain\n\nSignal chain, or signal-processing chain is a term used in signal processing and mixed-signal system design to describe a series of signal-conditioning electronic components that receive input (data acquired from sampling either real-time phenomena or from stored data) in tandem, with the output of one portion of the chain supplying input to the next. \n\nSignal chains are often used in signal processing applications to gather and process data or to apply system controls based on analysis of real-time phenomena.\n\nThis definition comes from common usage in the electronics industry and can be derived from definitions of its parts:\n\n- Signal: \"The event, phenomenon, or electrical quantity, that conveys information from one point to another\".\n- Chain: \"1. Any series of items linked together. 2. Pertaining to a routine consisting of segments which are run through the computer in tandem, only one segment being within the computer at any one time and each segment using the output from the previous program as its input\".\n\nThe concept of a \"signal chain\" is familiar to electrical engineers, but the term has many synonyms such as \"circuit topology\". The goal of any signal chain is to process a variety of \"signals\" to monitor or control an analog-, digital-, or analog-digital system.\n\n", "related": "\n- Audio signal flow\n- Daisy chain (electrical engineering)\n- Feedback\n\n- Signal Chain Basics from www.planetanalog.com\n"}
{"id": "26792343", "url": "https://en.wikipedia.org/wiki?curid=26792343", "title": "Total variation denoising", "text": "Total variation denoising\n\nIn signal processing, total variation denoising, also known as total variation regularization, is a process, most often used in digital image processing, that has applications in noise removal. It is based on the principle that signals with excessive and possibly spurious detail have high total variation, that is, the integral of the absolute gradient of the signal is high. According to this principle, reducing the total variation of the signal subject to it being a close match to the original signal, removes unwanted detail whilst preserving important details such as edges. The concept was pioneered by Rudin, Osher, and Fatemi in 1992 and so is today known as the \"ROF model\".\n\nThis noise removal technique has advantages over simple techniques such as linear smoothing or median filtering which reduce noise but at the same time smooth away edges to a greater or lesser degree. By contrast, total variation denoising is remarkably effective at simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios.\n\nFor a digital signal formula_1, we can, for example, define the total variation as\n\nGiven an input signal formula_3, the goal of total variation denoising is to find an approximation, call it formula_1, that has smaller total variation than formula_3 but is \"close\" to formula_3. One measure of closeness is the sum of square errors:\n\nSo the total-variation denoising problem amounts to minimizing the following discrete functional over the signal formula_1:\n\nBy differentiating this functional with respect to formula_1, we can derive a corresponding Euler–Lagrange equation, that can be numerically integrated with the original signal formula_3 as initial condition. This was the original approach. Alternatively, since this is a convex functional, techniques from convex optimization can be used to minimize it and find the solution formula_1.\n\nThe regularization parameter formula_13 plays a critical role in the denoising process. When formula_14, there is no smoothing and the result is the same as minimizing the sum of squares. As formula_15, however, the total variation term plays an increasingly strong role, which forces the result to have smaller total variation, at the expense of being less like the input (noisy) signal. Thus, the choice of regularization parameter is critical to achieving just the right amount of noise removal.\n\nWe now consider 2D signals \"y\", such as images. \nThe total-variation norm proposed by the 1992 article is\n\nThe Rudin–Osher–Fatemi model was a pivotal component in producing the first image of a black hole.\n\n", "related": "\n- Anisotropic diffusion\n- Bounded variation\n- Digital image processing\n- Noise reduction\n- Non-local means\n- Signal processing\n- Total variation\n- Stanley Osher\n- Basis pursuit denoising\n- Lasso (statistics)\n\n- TVDIP: Full-featured Matlab 1D total variation denoising implementation.\n- Efficient Primal-Dual Total Variation\n- TV-L1 image denoising algorithm in Matlab\n"}
{"id": "26332696", "url": "https://en.wikipedia.org/wiki?curid=26332696", "title": "Equalization (communications)", "text": "Equalization (communications)\n\nIn telecommunication, equalization is the reversal of distortion incurred by a signal transmitted through a channel. Equalizers are used to render the frequency response—for instance of a telephone line—\"flat\" from end-to-end. When a channel has been equalized the frequency domain attributes of the signal at the input are faithfully reproduced at the output. Telephones, DSL lines and television cables use equalizers to prepare data signals for transmission.\n\nEqualizers are critical to the successful operation of electronic systems such as analog broadcast television. In this application the actual waveform of the transmitted signal must be preserved, not just its frequency content. Equalizing filters must cancel out any group delay and phase delay between different frequency components.\n\nEarly telephone systems used equalization to correct for the reduced level of high frequencies in long cables, typically using Zobel networks. These kinds of equalizers can also be used to produce a circuit with a wider bandwidth than the standard telephone band of 300 Hz to 3.4 & nbsp ;kHz. This was particularly useful for broadcasters who needed \"music\" quality, not \"telephone\" quality on landlines carrying program material. It is necessary to remove or cancel any loading coils in the line before equalization can be successful. Equalization was also applied to correct the response of the transducers, for example, a particular microphone might be more sensitive to low frequency sounds than to high frequency sounds, so an equalizer would be used to increase the volume of the higher frequencies (\"boost\"), and reduce the volume of the low frequency sounds (\"cut\").\n\nA similar approach to audio was taken with television landlines with two important additional complications. The first of these is that the television signal is a wide bandwidth covering many more octaves than an audio signal. A television equalizer consequently typically requires more filter sections than an audio equalizer. To keep this manageable, television equalizer sections were often combined into a single network using ladder topology to form a Cauer equalizer.\n\nThe second issue is that phase equalization is essential for an analog television signal. Without it dispersion causes the loss of integrity of the original wave-shape and is seen as smearing of what were originally sharp edges in the picture.\n\n- Zobel network\n- Lattice phase equalizer\n- Bridged T delay equalizer\n\nModern digital telephone systems have less trouble in the voice frequency range as only the local line to the subscriber now remains in analog format, but DSL circuits operating in the MHz range on those same wires may suffer severe attenuation distortion, which is dealt with by automatic equalization or by abandoning the worst frequencies. Picturephone circuits also had equalizers.\n\nIn digital communications, the equalizer's purpose is to reduce intersymbol interference to allow recovery of the transmit symbols. It may be a simple linear filter or a complex algorithm.\n\n- Linear equalizer: processes the incoming signal with a linear filter\n- MMSE equalizer: designs the filter to minimize E[|e|], where e is the error signal, which is the filter output minus the transmitted signal.\n- Zero forcing equalizer: approximates the inverse of the channel with a linear filter.\n- Decision feedback equalizer: augments a linear equalizer by adding a filtered version of previous symbol estimates to the original filter output.\n- Blind equalizer: estimates the transmitted signal without knowledge of the channel statistics, using only knowledge of the transmitted signal's statistics.\n- Adaptive equalizer: is typically a linear equalizer or a DFE. It updates the equalizer parameters (such as the filter coefficients) as it processes the data. Typically, it uses the MSE cost function; it assumes that it makes the correct symbol decisions, and uses its estimate of the symbols to compute e, which is defined above.\n- Viterbi equalizer: Finds the maximum likelihood (ML) optimal solution to the equalization problem. Its goal is to minimize the probability of making an error over the entire sequence.\n- BCJR equalizer: uses the BCJR algorithm (also called the Forward-backward algorithm) to find the maximum \"a posteriori\" (MAP) solution. Its goal is to minimize the probability that a given bit was incorrectly estimated.\n- Turbo equalizer: applies turbo decoding while treating the channel as a convolutional code.\n\n", "related": "\n- Electronic filter\n- Weighting filter\n- RIAA equalization\n\n- Interactive demonstration of various linear and non-linear equalizers\n- Interactive demonstration of a Viterbi equalizer\n"}
{"id": "27388322", "url": "https://en.wikipedia.org/wiki?curid=27388322", "title": "Icophone", "text": "Icophone\n\nThe icophone is an instrument of speech synthesis conceived by Émile Leipp in 1964 and used for synthesizing the French language. The two first icophones were made in the laboratory of physical mechanics of Saint-Cyr-l'École.\n\nThe principle of the icophone is the representation of the sound by a spectrograph. The spectrogram analyzes a word, a phrase, or more generally a sound, and shows the distribution of the different frequencies with their relative intensities. The first machines to synthesize words were made by displaying the form of the spectrogram on a transparent tape, which controls a series of oscillators following the presence or absence of a black mark on the tape. Leipp succeeded in decomposing the segments of a spoken sound phenomenon, and in synthesizing them from a very simplified display.\n", "related": "NONE"}
{"id": "26527624", "url": "https://en.wikipedia.org/wiki?curid=26527624", "title": "Hilbert spectroscopy", "text": "Hilbert spectroscopy\n\nHilbert Spectroscopy uses Hilbert transforms to analyze broad spectrum signals from gigahertz to terahertz frequency radio. One suggested use is to quickly analyze liquids inside airport passenger luggage.\n", "related": "NONE"}
{"id": "6768984", "url": "https://en.wikipedia.org/wiki?curid=6768984", "title": "Phase margin", "text": "Phase margin\n\nIn electronic amplifiers, the phase margin (PM) is the difference between the phase lag (< 0) and -180°, for an amplifier's output signal (relative to its input) at zero dB gain or output is same as of input.\n\nFor example, if the amplifier's open-loop gain crosses 0 dB at a frequency where the phase lag is -135°, then the phase margin of this feedback system is -135° - (-180°) = 45°. See Bode plot#Gain margin and phase margin for more details.\n\nTypically the open-loop phase lag (relative to input, < 0) varies with frequency, progressively increasing to exceed 180°, at which frequency the output signal becomes inverted, or antiphase in relation to the input. The PM will be positive but decreasing at frequencies less than the frequency at which inversion sets in (at which PM = 0), and PM is negative (PM < 0) at higher frequencies. In the presence of negative feedback, a zero or negative PM at a frequency where the loop gain exceeds unity (1) guarantees instability. Thus positive PM is a \"safety margin\" that ensures proper (non-oscillatory) operation of the circuit. This applies to amplifier circuits as well as more generally, to active filters, under various load conditions (e.g. reactive loads). In its simplest form, involving ideal negative feedback \"voltage\" amplifiers with non-reactive feedback, the phase margin is measured at the frequency where the open-loop voltage gain of the amplifier equals the desired closed-loop DC voltage gain.\n\nMore generally, PM is defined as that of the amplifier and its feedback network combined (the \"loop\", normally opened at the amplifier input), measured at a frequency where the loop gain is unity, and prior to the closing of the loop, through tying the output of the open loop to the input source, in such a way as to subtract from it.\n\nIn the above loop-gain definition, it is assumed that the amplifier input presents zero load. To make this work for non-zero-load input, the output of the feedback network needs to be loaded with an equivalent load for the purpose of determining the frequency response of the loop gain.\n\nIt is also assumed that the graph of gain vs. frequency crosses unity gain with a negative slope and does so only once. This consideration matters only with reactive and active feedback networks, as may be the case with active filters.\n\nPhase margin and its important companion concept, gain margin, are measures of stability in closed-loop, dynamic-control systems. Phase margin indicates relative stability, the tendency to oscillate during its damped response to an input change such as a step function. Gain margin indicates absolute stability and the degree to which the system will oscillate, without limit, given any disturbance.\n\nThe output signals of all amplifiers exhibit a time delay when compared to their input signals. This delay causes a phase difference between the amplifier's input and output signals. If there are enough stages in the amplifier, at some frequency, the output signal will lag behind the input signal by one cycle period at that frequency. In this situation, the amplifier's output signal will be in phase with its input signal though lagging behind it by 360°, i.e., the output will have a phase angle of −360°. This lag is of great consequence in amplifiers that use feedback. The reason: the amplifier will oscillate if the fed-back output signal is in phase with the input signal at the frequency at which its open-loop voltage gain equals its closed-loop voltage gain and the open-loop voltage gain is one or greater. The oscillation will occur because the fed-back output signal will then reinforce the input signal at that frequency. In conventional operational amplifiers, the critical output phase angle is −180° because the output is fed back to the input through an inverting input which adds an additional −180°.\n\nIn practice, feedback amplifiers must be designed with phase margins substantially in excess of 0°, even though amplifiers with phase margins of, say, 1° are theoretically stable. The reason is that many practical factors can reduce the phase margin below the theoretical minimum. A prime example is when the amplifier's output is connected to a capacitive load. Therefore, operational amplifiers are usually compensated to achieve a minimum phase margin of 45° or so. This means that at the frequency at which the open and closed loop gains meet, the phase angle is −135°. The calculation is: -135° - (-180°) = 45°. See Warwick\nor Stout\nfor a detailed analysis of the techniques and results of compensation to ensure adequate phase margins. See also the article \"Pole splitting\". Often amplifiers are designed to achieve a typical phase margin of 60 degrees. If the typical phase margin is around 60 degrees then the minimum phase margin will typically be greater than 45 degrees. A phase margin of 60 degrees is also a magic number because it allows for the fastest settling time when attempting to follow a voltage step input (a Butterworth design). An amplifier with lower phase margin will ring for longer and an amplifier with more phase margin will take a longer time to rise to the voltage step's final level.\n", "related": "NONE"}
{"id": "26331205", "url": "https://en.wikipedia.org/wiki?curid=26331205", "title": "Turbo equalizer", "text": "Turbo equalizer\n\nIn digital communications, a turbo equalizer is a type of receiver used to receive a message corrupted by a communication channel with intersymbol interference (ISI). It approaches the performance of a maximum a posteriori (MAP) receiver via iterative message passing between a soft-in soft-out (SISO) equalizer and a SISO decoder. It is related to turbo codes in that a turbo equalizer may be considered a type of iterative decoder if the channel is viewed as a non-redundant convolutional code. The turbo equalizer is different from classic a turbo-like code, however, in that the 'channel code' adds no redundancy and therefore can only be used to remove non-gaussian noise.\n\nTurbo codes were invented by Claude Berrou in 1990–1991. In 1993, turbo codes were introduced publicly via a paper listing authors Berrou, Glavieux, and Thitimajshima. In 1995 a novel extension of the turbo principle was applied to an equalizer by Douillard, Jézéquel, and Berrou. In particular, they formulated the ISI receiver problem as a turbo code decoding problem, where the channel is thought of as a rate 1 convolutional code and the error correction coding is the second code. In 1997, Glavieux, Laot, and Labat demonstrated that a linear equalizer could be used in a turbo equalizer framework. This discovery made turbo equalization computationally efficient enough to be applied to a wide range of applications.\n\nBefore discussing turbo equalizers, it is necessary to understand the basic receiver in the context of a communication system. This is the topic of this section.\n\nAt the transmitter, information bits are encoded. Encoding adds redundancy by mapping the information bits formula_1 to a longer bit vector – the code bit vector formula_2. The encoded bits formula_2 are then interleaved. Interleaving permutes the order of the code bits formula_2 resulting in bits formula_5. The main reason for doing this is to insulate the information bits from bursty noise. Next, the symbol mapper maps the bits formula_5 into complex symbols formula_7. These digital symbols are then converted into analog symbols with a D/A converter. Typically the signal is then up-converted to pass band frequencies by mixing it with a carrier signal. This is a necessary step for complex symbols. The signal is then ready to be transmitted through the channel.\n\nAt the receiver, the operations performed by the transmitter are reversed to recover formula_8, an estimate of the information bits. The down-converter mixes the signal back down to baseband. The A/D converter then samples the analog signal, making it digital. At this point, formula_9 is recovered. The signal formula_9 is what would be received if formula_7 were transmitted through the digital baseband equivalent of the channel plus noise. The signal is then equalized. The equalizer attempts to unravel the ISI in the received signal to recover the transmitted symbols. It then outputs the bits formula_12 associated with those symbols. The vector formula_12 may represent hard decisions on the bits or soft decisions. If the equalizer makes soft decisions, it outputs information relating to the probability of the bit being a 0 or a 1. If the equalizer makes hard decisions on the bits, it quantizes the soft bit decisions and outputs either a 0 or a 1. Next, the signal is deinterleaved which is a simple permutation transformation that undoes the transformation the interleaver executed. Finally, the bits are decoded by the decoder. The decoder estimates formula_8 from formula_15.\n\nA diagram of the communication system is shown below. In this diagram, the channel is the equivalent baseband channel, meaning that it encompasses the D/A, the up converter, the channel, the down converter, and the A/D.\n\nThe block diagram of a communication system employing a turbo equalizer is shown below. The turbo equalizer encompasses the equalizer, the decoder, and the blocks in between.\n\nThe difference between a turbo equalizer and a standard equalizer is the feedback loop from the decoder to the equalizer. Due to the structure of the code, the decoder not only estimates the information bits formula_1, but it also discovers new information about the coded bits formula_2. The decoder is therefore able to output extrinsic information, formula_18 about the likelihood that a certain code bit stream was transmitted. Extrinsic information is new information that is not derived from information input to the block. This extrinsic information is then mapped back into information about the transmitted symbols formula_7 for use in the equalizer. These extrinsic symbol likelihoods, formula_20, are fed into the equalizer as \"a priori\" symbol probabilities. The equalizer uses this \"a priori\" information as well as the input signal formula_9 to estimate extrinsic probability information about the transmitted symbols. The \"a priori\" information fed to the equalizer is initialized to 0, meaning that the initial estimate formula_8 made by the turbo equalizer is identical to the estimate made by the standard receiver. The information formula_23 is then mapped back into information about formula_2 for use by the decoder. The turbo equalizer repeats this iterative process until a stopping criterion is reached.\n\nIn practical turbo equalization implementations, an additional issue need to be considered. The \"channel state information (CSI)\" that the equalizer operates on comes from some channel estimation technique, and hence un-reliable. Firstly, in order to improve the reliability of the CSI, it is desirable to include the channel estimation block also into the turbo equalization loop, and parse soft or hard decision directed channel estimation within each turbo equalization iteration. Secondly, incorporating the presence of CSI uncertainty into the turbo equalizer design leads to a more robust approach with significant performance gains in practical scenarios.\n\n- Turbo Equalization a Signal Processing Magazine primer on turbo equalization. Since it was written for the signal processing community in general, it is relatively accessible.\n- Turbo Equalization: Principles and New Results an IEEE Transactions on Communications journal article that offers a detailed, clear explanation of turbo equalization.\n\n", "related": "\n- Equalizer (communications)\n"}
{"id": "8008408", "url": "https://en.wikipedia.org/wiki?curid=8008408", "title": "Pulse shaping", "text": "Pulse shaping\n\nIn electronics and telecommunications, pulse shaping is the process of changing the waveform of transmitted pulses. Its purpose is to make the transmitted signal better suited to its purpose or the communication channel, typically by limiting the effective bandwidth of the transmission. By filtering the transmitted pulses this way, the intersymbol interference caused by the channel can be kept in control. In RF communication, pulse shaping is essential for making the signal fit in its frequency band.\n\nTypically pulse shaping occurs after line coding and modulation.\n\nTransmitting a signal at high modulation rate through a band-limited channel can create intersymbol interference. As the modulation rate increases, the signal's bandwidth increases. When the signal's bandwidth becomes larger than the channel bandwidth, the channel starts to introduce distortion to the signal. This distortion usually manifests itself as intersymbol interference.\n\nThe signal's spectrum is determined by the modulation scheme and data rate used by the transmitter, but can be modified with a pulse shaping filter. Usually the transmitted symbols are represented as a time sequence of dirac delta pulses. This theoretical signal is then filtered with the pulse shaping filter, producing the transmitted signal. \n\nIn many base band communication systems the pulse shaping filter is implicitly a boxcar filter. Its Fourier transform is of the form \"sin(x)/x\", and has significant signal power at frequencies higher than symbol rate. This is not a big problem when optical fibre or even twisted pair cable is used as the communication channel. However, in RF communications this would waste bandwidth, and only tightly specified frequency bands are used for single transmissions. In other words, the channel for the signal is band-limited. Therefore better filters have been developed, which attempt to minimise the bandwidth needed for a certain symbol rate.\n\nAn example in other areas of electronics is the generation of pulses where the rise time need to be short; one way to do this is to start with a slower-rising pulse, and decrease the rise time, for example with a step recovery diode circuit.\n\nNot every filter can be used as a pulse shaping filter. The filter itself must not introduce intersymbol interference — it needs to satisfy certain criteria. The Nyquist ISI criterion is a commonly used criterion for evaluation, because it relates the frequency spectrum of the transmitter signal to intersymbol interference.\n\nExamples of pulse shaping filters that are commonly found in communication systems are:\n\n- Sinc shaped filter\n- Raised-cosine filter\n- Gaussian filter\n\nSender side pulse shaping is often combined with a receiver side matched filter to achieve optimum tolerance for noise in the system. In this case the pulse shaping is equally distributed between the sender and receiver filters. The filters' amplitude responses are thus pointwise square roots of the system filters.\n\nOther approaches that eliminate complex pulse shaping filters have been invented. In OFDM, the carriers are modulated so slowly that each carrier is virtually unaffected by the bandwidth limitation of the channel.\n\nIt is also called as Boxcar filter as its frequency domain equivalent is a rectangular shape. Theoretically the best pulse shaping filter would be the sinc filter, but it cannot be implemented precisely. It is a non-causal filter with relatively slowly decaying tails. It is also problematic from a synchronisation point of view as any phase error results in steeply increasing intersymbol interference.\n\nRaised-cosine is similar to sinc, with the tradeoff of smaller sidelobes for a slightly larger spectral width.\nRaised-cosine filters are practical to implement and they are in wide use. They have a configurable excess bandwidth, so communication systems can choose a trade off between a simpler filter and spectral efficiency.\n\nThis gives an output pulse shaped like a Gaussian function.\n\n", "related": "\n- Nyquist ISI criterion\n- Raised-cosine filter\n- Matched filter\n- Femtosecond pulse shaping\n- Pulse (signal processing)\n\n- \"John G. Proakis\", \"Digital Communications, 3rd Edition\" Chapter 9, \"McGraw-Hill Book Co., 1995\".\n- National Instruments Signal Generator Tutorial, Pulse Shaping to Improve Spectral Efficiency\n- National Instruments Measurement Fundamentals Tutorial, Pulse-Shape Filtering in Communications Systems\n- Root Raised Cosine Filters & Pulse Shaping in Communication Systems by Erkin Cubukcu (ntrs.nasa.gov).\n"}
{"id": "9672025", "url": "https://en.wikipedia.org/wiki?curid=9672025", "title": "Beat detection", "text": "Beat detection\n\nIn signal analysis, beat detection is using computer software or computer hardware to detect the beat of a musical score. There are many methods available and beat detection is always a tradeoff between accuracy and speed. Beat detectors are common in music visualization software such as some media player plugins. The algorithms used may utilize simple statistical models based on sound energy or may involve sophisticated comb filter networks or other means. They may be fast enough to run in real time or may be so slow as to only be able to analyze short sections of songs.\n\n", "related": "\n- Pitch detection\n\n- Beat This > Beat Detection Algorithm\n- Audio Analysis using the Discrete Wavelet Transform\n"}
{"id": "9706285", "url": "https://en.wikipedia.org/wiki?curid=9706285", "title": "Gating signal", "text": "Gating signal\n\nA gating signal is a digital signal or pulse (sometimes called a \"trigger\") that provides a time window so that a particular event or signal from among many will be selected and others will be eliminated or discarded.\n\nIn a multiple input AND/OR gate, a signal at one of the inputs triggers the passage of a signal at other inputs; i.e., it passes through or blocks the signal at other inputs. Such a signal is called a gating signal.\n\nSignal gating means to mask unwanted signal transitions from propagating forward.\n", "related": "NONE"}
{"id": "3199778", "url": "https://en.wikipedia.org/wiki?curid=3199778", "title": "Blind deconvolution", "text": "Blind deconvolution\n\nIn electrical engineering and applied mathematics, blind deconvolution is deconvolution without explicit knowledge of the impulse response function used in the convolution. This is usually achieved by making appropriate assumptions of the input to estimate the impulse response by analyzing the output. Blind deconvolution is not solvable without making assumptions on input and impulse response. Most of the algorithms to solve this problem are based on assumption that both input and impulse response live in respective known subspaces. However, blind deconvolution remains a very challenging non-convex optimization problem even with this assumption.\n\nIn image processing, blind deconvolution is a deconvolution technique that permits recovery of the target scene from a single or set of \"blurred\" images in the presence of a poorly determined or unknown point spread function (PSF). Regular linear and non-linear deconvolution techniques utilize a known PSF. For blind deconvolution, the PSF is estimated from the image or image set, allowing the deconvolution to be performed. Researchers have been studying blind deconvolution methods for several decades, and have approached the problem from different directions.\n\nMost of the work on blind deconvolution started in early 1970s. Blind deconvolution is used in astronomical imaging and medical imaging.\n\nBlind deconvolution can be performed iteratively, whereby each iteration improves the estimation of the PSF and the scene, or non-iteratively, where one application of the algorithm, based on exterior information, extracts the PSF. Iterative methods include maximum a posteriori estimation and expectation-maximization algorithms. A good estimate of the PSF is helpful for quicker convergence but not necessary.\n\nExamples of non-iterative techniques include SeDDaRA, the cepstrum transform and APEX. The cepstrum transform and APEX methods assume that the PSF has a specific shape, and one must estimate the width of the shape. For SeDDaRA, the information about the scene is provided in the form of a reference image. The algorithm estimates the PSF by comparing the spatial frequency information in the blurred image to that of the target image.\n\nLimitation of Blind deconvolution is that both input image and blur kernel must live in fixed subspace. That means input image, represented by w, has to be written as w=Bh, where B is random matrix of size L by K (K<L) and h is of size K by 1, whereas blur kernel, if represented by x, has to be written as x=Cm, where C is random matrix of size L by N (N<L) and m is of size N by 1.Observed image, if represented by y, given by y=w*x, can only be reconstructed if L >=K +N.\n\nExamples\n\nAny blurred image can be given as input to blind deconvolution algorithm, it can deblur the image, but essential condition for working of this algorithm must not be violated as discussed above. In the first example (picture of shapes), recovered image was very fine, exactly similar to original image because L > K + N. In the second example (picture of a girl), L < K + N, so essential condition is violated, hence recovered image is far different from original image.\n\nIn the case of deconvolution of seismic data, the original unknown signal is made of spikes hence is possible to characterize with sparsity constraints or regularizations such as l\" norm/l\" norm norm ratios, suggested by W. C. Gray in 1978.\n\nAudio deconvolution (often referred to as \"dereverberation\") is a reverberation reduction in audio mixtures. It is part of audio processing of recordings in ill-posed cases such as the cocktail party effect. One possibility is to use ICA.\n\nSuppose we have a signal transmitted through a channel. The channel can usually be modeled as a linear shift-invariant system, so the receptor receives a convolution of the original signal with the impulse response of the channel. If we want to reverse the effect of the channel, to obtain the original signal, we must process the received signal by a second linear system, inverting the response of the channel. This system is called an equalizer.\n\nIf we are given the original signal, we can use a supervising technique, such as finding a Wiener filter, but without it, we can still explore what we do know about it to attempt its recovery. For example, we can filter the received signal to obtain the desired spectral power density. This is what happens, for example, when the original signal is known to have no auto correlation, and we \"whiten\" the received signal.\n\nWhitening usually leaves some phase distortion in the results. Most blind deconvolution techniques use higher-order statistics of the signals, and permit the correction of such phase distortions. We can optimize the equalizer to obtain a signal with a PSF approximating what we know about the original PSF. \n\nBlind deconvolution algorithms often make use of high-order statistics, with moments higher than two. This can be implicit or explicit.\n\n", "related": "\n- Channel model\n- Inverse problem\n- Regularization (mathematics)\n- Blind equalization\n- Maximum a posteriori estimation\n- Maximum likelihood\n\n- ImageJ plugin for deconvolution\n"}
{"id": "28854622", "url": "https://en.wikipedia.org/wiki?curid=28854622", "title": "Beta encoder", "text": "Beta encoder\n\nA beta encoder is an analog-to-digital conversion (A/D) system in which a real number in the unit interval is represented by a finite representation of a sequence in \"base beta\", with beta being a real number between 1 and 2. Beta encoders are an alternative to traditional approaches to pulse-code modulation.\n\nAs a form of non-integer representation, beta encoding contrasts with traditional approaches to binary quantization, in which each value is mapped to the first \"N\" bits of its base-2 expansion. Rather than using base 2, beta encoders use \"base beta\" as a beta-expansion.\n\nIn practice, beta encoders have attempted to exploit the redundancy provided by the non-uniqueness of the expansion in base beta to produce more robust results. An early beta encoder, the \"Golden ratio encoder\" used the golden ratio base for its value of beta, but was susceptible to hardware errors. Although integrator leaks in hardware elements make some beta encoders imprecise, specific algorithms can be used to provide exponentially accurate approximations for the value of beta, despite the imprecise results provided by some circuit components.\n\nAn alternative design called the \"negative beta encoder\" (called so due to the negative eigenvalue of the transition probability matrix) has been proposed to further reduce the quantization error.\n\n", "related": "\n- Pulse-code modulation\n- Quantization (signal processing)\n- Sampling (signal processing)\n"}
{"id": "28857054", "url": "https://en.wikipedia.org/wiki?curid=28857054", "title": "Higher-order sinusoidal input describing function", "text": "Higher-order sinusoidal input describing function\n\nThe higher-order sinusoidal input describing functions (HOSIDF) were first introduced by dr. ir. P.W.J.M. Nuij. The HOSIDFs are an extension of the sinusoidal input describing function which describe the response (gain and phase) of a system at harmonics of the base frequency of a sinusoidal input signal. The HOSIDFs bear an intuitive resemblance to the classical frequency response function and define the periodic output of a stable, causal, time invariant nonlinear system to a sinusoidal input signal:\n\nformula_1\n\nThis output is denoted by formula_2 and consists of harmonics of the input frequency:\n\nformula_3\n\nDefining the single sided spectra of the input and output as formula_4 and formula_5, such that formula_6 yields the definition of the k-th order HOSIDF:\n\nformula_7\n\nThe application and analysis of the HOSIDFs is advantageous both when a nonlinear model is already identified and when no model is known yet. In the latter case the HOSIDFs require little model assumptions and can easily be identified while requiring no advanced mathematical tools. Moreover, even when a model is already identified, the analysis of the HOSIDFs often yields significant advantages over the use of the identified nonlinear model. First of all, the HOSIDFs are intuitive in their identification and interpretation while other nonlinear model structures often yield limited direct information about the behavior of the system in practice. Furthermore, the HOSIDFs provide a natural extension of the widely used sinusoidal describing functions in case nonlinearities cannot be neglected. In practice the HOSIDFs have two distinct applications: Due to their ease of identification, HOSIDFs provide a tool to provide on-site testing during system design. Finally, the application of HOSIDFs to (nonlinear) controller design for nonlinear systems is shown to yield significant advantages over conventional time domain based tuning.\n", "related": "NONE"}
{"id": "202094", "url": "https://en.wikipedia.org/wiki?curid=202094", "title": "Process gain", "text": "Process gain\n\nIn a spread-spectrum system, the process gain (or \"processing gain\") is the ratio of the spread (or RF) bandwidth to the unspread (or baseband) bandwidth. It is usually expressed in decibels (dB).\n\nFor example, if a 1 kHz signal is spread to 100 kHz, the process gain expressed as a numerical ratio would be / = 100. Or in decibels, 10 log(100) = 20 dB.\n\nNote that process gain does not reduce the effects of wideband thermal noise. It can be shown that a direct-sequence spread-spectrum (DSSS) system has exactly the same bit error behavior as a non-spread-spectrum system with the same modulation format. Thus, on an additive white Gaussian noise (AWGN) channel without interference, a spread system requires the same transmitter power as an unspread system, all other things being equal.\n\nUnlike a conventional communication system, however, a DSSS system does have a certain resistance against narrowband interference, as the interference is not subject to the process gain of the DSSS signal, and hence the signal-to-interference ratio is improved.\n\nIn frequency modulation (FM), the processing gain can be expressed as\n\nwhere:\n", "related": "NONE"}
{"id": "2201538", "url": "https://en.wikipedia.org/wiki?curid=2201538", "title": "Recurrence quantification analysis", "text": "Recurrence quantification analysis\n\nRecurrence quantification analysis (RQA) is a method of nonlinear data analysis (cf. chaos theory) for the investigation of dynamical systems. It quantifies the number and duration of recurrences of a dynamical system presented by its phase space trajectory.\n\nThe recurrence quantification analysis (RQA) was developed in order to quantify differently appearing recurrence plots (RPs), based on the small-scale structures therein. Recurrence plots are tools which visualise the recurrence behaviour of the phase space trajectory formula_1 of dynamical systems:\n\nwhere formula_3 and formula_4 a predefined distance.\n\nRecurrence plots mostly contain single dots and lines which are parallel to the mean diagonal (\"line of identity\", LOI) or which are vertical/horizontal. Lines parallel to the LOI are referred to as \"diagonal lines\" and the vertical structures as \"vertical lines\". Because an RP is usually symmetric, horizontal and vertical lines correspond to each other, and, hence, only vertical lines are considered. The lines correspond to a typical behaviour of the phase space trajectory: whereas the diagonal lines represent such segments of the phase space trajectory which run parallel for some time, the vertical lines represent segments which remain in the same phase space region for some time.\n\nIf only a time series is available, the phase space can be reconstructed by using a time delay embedding (see Takens' theorem):\n\nwhere formula_6 is the time series, formula_7 the embedding dimension and formula_8 the time delay.\n\nThe RQA quantifies the small-scale structures of recurrence plots, which present the number and duration of the recurrences of a dynamical system. The measures introduced for the RQA were developed heuristically between 1992 and 2002 (Zbilut & Webber 1992; Webber & Zbilut 1994; Marwan et al. 2002). They are actually measures of complexity. The main advantage of the recurrence quantification analysis is that it can provide useful information even for short and non-stationary data, where other methods fail.\n\nRQA can be applied to almost every kind of data. It is widely used in physiology, but was also successfully applied on problems from engineering, chemistry, Earth sciences etc.\n\nThe simplest measure is the recurrence rate, which is the density of recurrence points in a recurrence plot:\n\nThe recurrence rate corresponds with the probability that a specific state will recur. It is almost equal with the definition of the correlation sum, where the LOI is excluded from the computation.\n\nThe next measure is the percentage of recurrence points which form diagonal lines in the recurrence plot of minimal length formula_10:\n\nwhere formula_12 is the frequency distribution of the lengths formula_13 of the diagonal lines (i.e., it counts how many instances have length formula_13). This measure is called determinism and is related with the predictability of the dynamical system, because white noise has a recurrence plot with almost only single dots and very few diagonal lines, whereas a deterministic process has a recurrence plot with very few single dots but many long diagonal lines.\n\nThe amount of recurrence points which form vertical lines can be quantified in the same way:\n\nwhere formula_16 is the frequency distribution of the lengths formula_17 of the vertical lines, which have at least a length of formula_18. This measure is called laminarity and is related with the amount of laminar phases in the system (intermittency).\n\nThe lengths of the diagonal and vertical lines can be measured as well. The averaged diagonal line length\n\nis related with the \"predictability time\" of the dynamical system\nand the trapping time, measuring the average length\nof the vertical lines,\n\nis related with the \"laminarity time\" of the dynamical system, i.e. how long the system remains in a specific state.\n\nBecause the length of the diagonal lines is related on the time how long segments of the phase space trajectory run parallel, i.e. on the divergence behaviour of the trajectories, it was sometimes stated that the reciprocal of the maximal length of the diagonal lines (without LOI) would be an estimator for the positive maximal Lyapunov exponent of the dynamical system. Therefore, the maximal diagonal line length formula_21 or the divergence\n\nare also measures of the RQA. However, the relationship between these measures with the positive maximal Lyapunov exponent is not as easy as stated, but even more complex (to calculate the Lyapunov exponent from an RP, the whole frequency distribution of the diagonal lines has to be considered). The divergence can have the trend of the positive maximal Lyapunov exponent, but not more. Moreover, also RPs of white noise processes can have a really long diagonal line, although very seldom, just by a finite probability. Therefore the divergence cannot reflect the maximal Lyapunov exponent.\n\nThe probability formula_23 that a diagonal line has exactly length formula_13 can be estimated from the frequency distribution formula_12 with formula_26. The Shannon entropy of this probability,\n\nreflects the complexity of the deterministic structure in the system. However, this entropy depends sensitively on the bin number and, thus, may differ for different realisations of the same process, as well as for different data preparations.\n\nThe last measure of the RQA quantifies the thinning-out of the recurrence plot. The trend is the regression coefficient of a linear relationship between the density of recurrence points in a line parallel to the LOI and its distance to the LOI. More exactly, consider the recurrence rate in a diagonal line parallel to LOI of distance \"k\" (\"diagonal-wise recurrence rate\"):\n\nthen the trend is defined by\n\nwith formula_30 as the average value and formula_31. This latter relation should ensure to avoid the edge effects of too low recurrence point densities in the edges of the recurrence plot. The measure \"trend\" provides information about the stationarity of the system.\n\nSimilar to the diagonal-wise defined recurrence rate, the other measures based on the diagonal lines (DET, L, ENTR) can be defined diagonal-wise. These definitions are useful to study interrelations or synchronisation between different systems (using recurrence plots or cross recurrence plots).\n\nInstead of computing the RQA measures of the entire recurrence plot, they can be computed in small windows moving over the recurrence plot along the LOI. This provides time-dependent RQA measures which allow detecting, e.g., chaos-chaos transitions (Marwan et al. 2002). Note: the choice of the size of the window can strongly influence the measure \"trend\".\n\n", "related": "\n- Recurrence plot, a powerful visualisation tool of recurrences in dynamical (and other) systems.\n- Recurrence period density entropy, an information-theoretic method for summarising the recurrence properties of both deterministic and stochastic dynamical systems.\n- Approximate entropy\n\n- Paper no. TPWRS-01211-2014\n\n- http://www.recurrence-plot.tk/\n- http://www.scitopics.com/Recurrence_Quantification_Analysis.html\n"}
{"id": "25678949", "url": "https://en.wikipedia.org/wiki?curid=25678949", "title": "Automated ECG interpretation", "text": "Automated ECG interpretation\n\nAutomated ECG interpretation is the use of artificial intelligence and pattern recognition software and knowledge bases to carry out automatically the interpretation, test reporting, and computer-aided diagnosis of electrocardiogram tracings obtained usually from a patient.\n\nThe first automated ECG programs were developed in the 1970s, when digital ECG machines became possible by third generation digital signal processing boards. Commercial models, such as those developed by Hewlett-Packard, incorporated these programs into clinically used devices.\n\nDuring the 1980s and 1990s, extensive research was carried out by companies and by university labs in order to improve the accuracy rate, which was not very high in the first models. For this purpose, several signal databases with normal and abnormal ECGs were built by institutions such as MIT and used to test the algorithms and their accuracy.\n\n1. A digital representation of each recorded ECG channel is obtained, by means of an analog-digital conversion device and a special data acquisition software or a digital signal processing (DSP) chip.\n2. The resulting digital signal is processed by a series of specialized algorithms, which start by conditioning it, e.g., removal of noise, baselevel variation, etc.\n3. Feature extraction: mathematical analysis is now performed on the clean signal of all channels, to identify and measure a number of features which are important for interpretation and diagnosis, this will constitute the input to AI-based programs, such as the peak amplitude, area under the curve, displacement in relation to baseline, etc., of the P, Q, R, S and T waves, the time delay between these peaks and valleys, heart rate frequency (instantaneous and average), and many others. Some sort of secondary processing such as Fourier analysis and wavelet analysis may also be performed in order to provide input to pattern recognition-based programs.\n4. Logical processing and pattern recognition, using rule-based expert systems, probabilistic Bayesian analysis or fuzzy logics algorithms, cluster analysis, artificial neural networks, genetic algorithms and others techniques are used to derive conclusions, interpretation and diagnosis.\n5. A reporting program is activated and produces a proper display of original and calculated data, as well as the results of automated interpretation.\n6. In some applications, such as automatic defibrillators, an action of some sort may be triggered by results of the analysis, such as the occurrence of an atrial fibrillation or a cardiac arrest, the sounding of alarms in a medical monitor in intensive-care unit applications, and so on.\n\nThe manufacturing industries of ECG machines is now entirely digital, and many models incorporate embedded software for analysis and interpretation of ECG recordings with 3 or more leads. Consumer products, such as home ECG recorders for simple, 1-channel heart arrhythmia detection, also use basic ECG analysis, essentially to detect abnormalities. Some application areas are:\n- Incorporation into automatic defibrillators, so that autonomous decision can be reached whether there is a cause for administering the electrical shock on basis of an atrial or ventricular arrhythmia;\n- Portable ECG used in telemedicine. These machines are used to send ECG recordings via a telecommunications link, such as telephone, cellular data communication or Internet\n- Conventional ECG machines to be used in primary healthcare settings where a trained cardiologist is not available\n\nThe automated ECG interpretation is a useful tool when access to a specialist is not possible. Although considerable effort has been made to improve automated ECG algorithms, the sensitivity of the automated ECG interpretation is of limited value in the case of STEMI equivalent as for example with \"hyperacute T waves\", de Winter ST-T complex, Wellens phenomenon, Left ventricular hypertrophy, left bundle branch block or in presence of a pacemaker. Automated monitoring of ST-segment during patient transport is increasingly used and improves STEMI detection sensitivity, as ST elevation is a dynamical phenomenon.\n\n", "related": "\n- Medical monitor\n- Holter monitor\n- Open ECG project\n- SCP-ECG\n\n- Sabbatini, RME: O computador no processamento de sinais biológicos. Revista Informédica, 2 (12): 5-9, 1995. Computers in the processing of biological signals. (In Portuguese)\n<br>Translated and reproduced by permission of the author.\n\n- ecgAUTO in-depth ECG analysis software for preclinical research\n- Kligfield, P. Automated Analysis of ECG Rhythm\n- Physionet\n- Telemedical ECG Interpretation training module\n"}
{"id": "23238482", "url": "https://en.wikipedia.org/wiki?curid=23238482", "title": "Cross-recurrence quantification", "text": "Cross-recurrence quantification\n\nCross-recurrence quantification (CRQ) is a non-linear method that quantifies how similarly two observed data series unfold over time. CRQ produces measures reflecting coordination, such as how often two data series have similar values or reflect similar system states (called percentage recurrence, or %REC), among other measures.\n", "related": "NONE"}
{"id": "2055436", "url": "https://en.wikipedia.org/wiki?curid=2055436", "title": "Digital room correction", "text": "Digital room correction\n\nDigital room correction (or DRC) is a process in the field of acoustics where digital filters designed to ameliorate unfavorable effects of a room's acoustics are applied to the input of a sound reproduction system. Modern room correction systems produce substantial improvements in the time domain and frequency domain response of the sound reproduction system.\n\nThe use of analog filters, such as equalizers, to normalize the frequency response of a playback system has a long history; however, analog filters are very limited in their ability to correct the distortion found in many rooms. Although digital implementations of the equalizers have been available for some time, digital room correction is usually used to refer to the construction of filters which attempt to invert the impulse response of the room and playback system, at least in part. Digital correction systems are able to use acausal filters, and are able to operate with optimal time resolution, optimal frequency resolution, or any desired compromise along the Gabor limit. Digital room correction is a fairly new area of study which has only recently been made possible by the computational power of modern CPUs and DSPs.\n\nThe configuration of a digital room correction system begins with measuring the impulse response of the room at the listening location for each of the loudspeakers. Then, computer software is used to compute a FIR filter, which reverses the effects of the room and linear distortion in the loudspeakers. Finally, the calculated filter is loaded into a computer or other room correction device which applies the filter in real time. Because most room correction filters are acausal, there is some delay. Most DRC systems allow the operator to control the added delay through configurable parameters.\n\nDRC systems are not normally used to create a perfect inversion of the room's response because a perfect correction would only be valid at the location where it was measured: a few millimeters away the arrival times from various reflections will differ and the inversion will be imperfect. The imperfectly corrected signal may end up sounding worse than the uncorrected signal because the acausal filters used in digital room correction may cause pre-echo. Room correction filter calculation systems instead favor a robust approach, and employ sophisticated processing to attempt to produce an inverse filter which will work over a usably large volume, and which avoid producing bad-sounding artifacts outside of that volume, at the expense of peak accuracy at the measurement location.\n\n", "related": "\n- Deconvolution\n- Digital filter\n- Filter (signal processing)\n- Filter design\n- LARES\n- Stereophonic sound\n- Surround sound\n\n- Michael Gerzon's paper on Digital Room Equalization, on audiosignal.co.uk.\n\n- Python Open Room Correction (PORC)\n- DRC: Digital Room Correction\n\n- Free Room EQ plug-in for Foobar2000 audio player\n\n- On Room Correction and Equalization of Sound Systems, by Dr. Mathias Johansson, Dirac Research AB\n- Digital Room Equalization, by Michael Gerzon\n- Audio Equalization with Fixed-Pole Parallel Filters: An Efficient Alternative to Complex Smoothing, by Balazs Bank\n\n- Room Correction: A Primer, by Nyal Mellor of Acoustic Frontiers\n- Sound Correction in the Frequency and Time Domain, by Bernt Ronningsbak of Audiolense\n- The Three Acoustical Issues a Room Correction Product Can't Actually Correct, by Nyal Mellor of Acoustic Frontiers\n"}
{"id": "29262224", "url": "https://en.wikipedia.org/wiki?curid=29262224", "title": "Periodic summation", "text": "Periodic summation\n\nIn signal processing, any periodic function, formula_1 with period P, can be represented by a summation of an infinite number of instances of an aperiodic function, formula_2, that are offset by integer multiples of P. This representation is called periodic summation: \n\nWhen  formula_4  is alternatively represented as a complex Fourier series, the Fourier coefficients are proportional to the values (or \"samples\") of the continuous Fourier transform, formula_5  at intervals of 1/P.  That identity is a form of the Poisson summation formula. Similarly, a Fourier series whose coefficients are samples of  formula_6  at constant intervals (T) is equivalent to a periodic summation of  formula_7  which is known as a discrete-time Fourier transform.\n\nThe periodic summation of a Dirac delta function is the Dirac comb. Likewise, the periodic summation of an integrable function is its convolution with the Dirac comb.\n\nIf a periodic function is represented using the quotient space domain\nformula_8 then one can write\n\ninstead. The arguments of formula_11 are equivalence classes of real numbers that share the same fractional part when divided by formula_12.\n\n", "related": "\n- Dirac comb\n- Circular convolution\n- Discrete-time Fourier transform\n "}
{"id": "1075916", "url": "https://en.wikipedia.org/wiki?curid=1075916", "title": "Generalized signal averaging", "text": "Generalized signal averaging\n\nIn many cases only one image with noise is available, and averaging is then realized in a local neighbourhood. Results are acceptable if the noise is smaller in size than the smallest objects of interest in the image, but blurring of edges is a serious disadvantage. In the case of smoothing within a single image, one has to assume that there are no changes in the gray levels of the underlying image data. This assumption is clearly violated at locations of image edges, and edge blurring is a direct consequence of violating the assumption. \nAveraging is a special case of discrete convolution. For a 3 by 3 neighbourhood, the convolution mask \"M\" is:\n\nformula_1\nThe significance of the central pixel may be increased, as it approximates the properties of noise with a Gaussian probability distribution:\n\nformula_2\n\nformula_3\n\nA suitable page for beginners about matrices is at:\nhttps://web.archive.org/web/20060819141930/http://www.gamedev.net/reference/programming/features/imageproc/page2.asp\n\nThe whole article starts on page: https://web.archive.org/web/20061019072001/http://www.gamedev.net/reference/programming/features/imageproc/\n", "related": "NONE"}
{"id": "3967", "url": "https://en.wikipedia.org/wiki?curid=3967", "title": "Bandwidth (signal processing)", "text": "Bandwidth (signal processing)\n\nBandwidth is the difference between the upper and lower frequencies in a continuous band of frequencies. It is typically measured in hertz, and depending on context, may specifically refer to \"passband bandwidth\" or \"baseband bandwidth\". Passband bandwidth is the difference between the upper and lower cutoff frequencies of, for example, a band-pass filter, a communication channel, or a signal spectrum. Baseband bandwidth applies to a low-pass filter or baseband signal; the bandwidth is equal to its upper cutoff frequency.\n\nBandwidth in hertz is a central concept in many fields, including electronics, information theory, digital communications, radio communications, signal processing, and spectroscopy and is one of the determinants of the capacity of a given communication channel.\n\nA key characteristic of bandwidth is that any band of a given width can carry the same amount of information, regardless of where that band is located in the frequency spectrum. For example, a 3 kHz band can carry a telephone conversation whether that band is at baseband (as in a POTS telephone line) or modulated to some higher frequency.\n\nBandwidth is a key concept in many telecommunications applications. In radio communications, for example, bandwidth is the frequency range occupied by a modulated carrier signal. An FM radio receiver's tuner spans a limited range of frequencies. A government agency (such as the Federal Communications Commission in the United States) may apportion the regionally available bandwidth to broadcast license holders so that their signals do not mutually interfere. In this context, bandwidth is also known as channel spacing.\n\nFor other applications there are other definitions. One definition of bandwidth, for a system, could be the range of frequencies over which the system produces a specified level of performance. A less strict and more practically useful definition will refer to the frequencies beyond which Performance is degraded. In the case of frequency response, degradation could, for example, mean more than 3 dB below the maximum value or it could mean below a certain absolute value. As with any definition of the \"width\" of a function, many definitions are suitable for different purposes.\n\nIn the context of, for example, the sampling theorem and Nyquist sampling rate, bandwidth typically refers to baseband bandwidth. In the context of Nyquist symbol rate or Shannon-Hartley channel capacity for communication systems it refers to passband bandwidth.\n\nThe Rayleigh bandwidth of a simple radar pulse is defined as the inverse of its duration. For example, a one-microsecond pulse has a Rayleigh bandwidth of one megahertz.\n\nThe essential bandwidth is defined as the portion of a signal spectrum in the frequency domain which contains most of the energy of the signal.\n\nIn some contexts, the signal bandwidth in hertz refers to the frequency range in which the signal's spectral density (in W/Hz or V/Hz) is nonzero or above a small threshold value. That definition is used in calculations of the lowest sampling rate that will satisfy the sampling theorem. The threshold value is often defined relative to the maximum value, and is most commonly the , that is the point where the spectral density is half its maximum value (or the spectral amplitude, in V or V/Hz, is 70.7% of its maximum).\n\nThe word bandwidth applies to signals as described above, but is also used to denote system bandwidth, for example in filter or communication channel systems. To say that a system has a certain bandwidth means that the system can process signals of that bandwidth, or that the system reduces the bandwidth of a white noise input to that bandwidth.\n\nThe 3 dB bandwidth of an electronic filter or communication channel is the part of the system's frequency response that lies within 3 dB of the response at its peak, which in the passband filter case is typically at or near its center frequency, and in the low-pass filter is near 0 hertz. If the maximum gain is 0 dB, the 3 dB bandwidth is the frequency range where the gain is more than −3 dB, or the attenuation is less than 3 dB. This is also the range of frequencies where the amplitude gain is above 70.7% of the maximum amplitude gain, and the power gain is above half the maximum power gain. This same \"half-power gain\" convention is also used in spectral width, and more generally for extent of functions as full width at half maximum (FWHM).\n\nIn electronic filter design, a filter specification may require that within the filter passband, the gain is nominally 0 dB ± a small number of dB, for example within the ±1 dB interval. In the stopband(s), the required attenuation in dB is above a certain level, for example >100 dB. In a transition band the gain is not specified. In this case, the filter bandwidth corresponds to the passband width, which in this example is the 1 dB-bandwidth. If the filter shows amplitude ripple within the passband, the \"x\" dB point refers to the point where the gain is \"x\" dB below the nominal passband gain rather than \"x\" dB below the maximum gain.\n\nA commonly used quantity is fractional bandwidth. This is the bandwidth of a device divided by its center frequency. E.g., a passband filter that has a bandwidth of 2 MHz with center frequency 10 MHz will have a fractional bandwidth of 2/10, or 20%.\n\nIn communication systems, in calculations of the Shannon–Hartley channel capacity, bandwidth refers to the 3 dB-bandwidth. In calculations of the maximum symbol rate, the Nyquist sampling rate, and maximum bit rate according to the Hartley formula, the bandwidth refers to the frequency range within which the gain is non-zero, or the gain in dB is below a very large value.\n\nThe fact that in equivalent baseband models of communication systems, the signal spectrum consists of both negative and positive frequencies, can lead to confusion about bandwidth, since they are sometimes referred to only by the positive half, and one will occasionally see expressions such as formula_1, where formula_2 is the total bandwidth (i.e. the maximum passband bandwidth of the carrier-modulated RF signal and the minimum passband bandwidth of the physical passband channel), and formula_3 is the positive bandwidth (the baseband bandwidth of the equivalent channel model). For instance, the baseband model of the signal would require a low-pass filter with cutoff frequency of at least formula_3 to stay intact, and the physical passband channel would require a passband filter of at least formula_2 to stay intact.\n\nIn signal processing and control theory the bandwidth is the frequency at which the closed-loop system gain drops 3 dB below peak.\n\nIn basic electric circuit theory, when studying band-pass and band-reject filters, the bandwidth represents the distance between the two points in the frequency domain where the signal is formula_6 of the maximum signal amplitude (half power).\n\nIn the field of antennas, two different methods of expressing relative bandwidth are used for narrowband and wideband antennas. For either, a set of criteria is established to define the extents of the bandwidth, such as input impedance, pattern, or polarization.\n\n\"Percent bandwidth\", usually used for narrowband antennas, is used defined as formula_7. The theoretical limit to percent bandwidth is 200%, which occurs for formula_8.\n\n\"Fractional bandwidth or ratio bandwidth\", usually used for wideband antennas, is defined as formula_9 and is typically presented in the form of formula_10. Fractional bandwidth is used for wideband antennas because of the compression of the percent bandwidth that occurs mathematically with percent bandwidths above 100%, which corresponds to a fractional bandwidth of 3:1.\n\nIf formula_11\n\nthen formula_12.\n\nIn photonics, the term \"bandwidth\" occurs in a variety of meanings:\n- the bandwidth of the output of some light source, e.g., an ASE source or a laser; the bandwidth of ultrashort optical pulses can be particularly large\n- the width of the frequency range that can be transmitted by some element, e.g. an optical fiber\n- the gain bandwidth of an optical amplifier\n- the width of the range of some other phenomenon (e.g., a reflection, the phase matching of a nonlinear process, or some resonance)\n- the maximum modulation frequency (or range of modulation frequencies) of an optical modulator\n- the range of frequencies in which some measurement apparatus (e.g., a powermeter) can operate\n- the data rate (e.g., in Gbit/s) achieved in an optical communication system; see bandwidth (computing).\n\nA related concept is the spectral linewidth of the radiation emitted by excited atoms.\n\n", "related": "\n- Rise time\n- Bandwidth (Wiktionary entry)\n- Bandwidth efficiency\n- Bandwidth extension\n- Broadband\n- Essential bandwidth\n"}
{"id": "23965361", "url": "https://en.wikipedia.org/wiki?curid=23965361", "title": "Gating (telecommunication)", "text": "Gating (telecommunication)\n\nIn telecommunication, the term gating has the following meanings:\n\n1. The process of selecting only those portions of a wave between specified time intervals or between specified amplitude limits.\n2. The controlling of signals by means of combinational logic elements.\n3. A process in which a predetermined set of conditions, when established, permits a second process to occur.\n", "related": "NONE"}
{"id": "29687730", "url": "https://en.wikipedia.org/wiki?curid=29687730", "title": "Scanning mobility particle sizer", "text": "Scanning mobility particle sizer\n\nA scanning mobility particle sizer (SMPS) is an analytical instrument that measures the size and number concentration of aerosol particles with diameters from 2.5 nm to 1000 nm. They employ a continuous, fast-scanning technique to provide high-resolution measurements.\n\nThe particles that are investigated can be of biological or chemical nature. The instrument can be used for air quality measurement indoors, vehicle exhaust, research in bioaerosols, atmospheric studies, and toxicology testing.\n", "related": "NONE"}
{"id": "202672", "url": "https://en.wikipedia.org/wiki?curid=202672", "title": "Spectral density", "text": "Spectral density\n\nThe power spectrum formula_1 of a time series formula_2 describes the distribution of power into frequency components composing that signal. According to Fourier analysis, any physical signal can be decomposed into a number of discrete frequencies, or a spectrum of frequencies over a continuous range. The statistical average of a certain signal or sort of signal (including noise) as analyzed in terms of its frequency content, is called its spectrum.\n\nWhen the energy of the signal is concentrated around a finite time interval, especially if its total energy is finite, one may compute the energy spectral density. More commonly used is the power spectral density (or simply power spectrum), which applies to signals existing over \"all\" time, or over a time period large enough (especially in relation to the duration of a measurement) that it could as well have been over an infinite time interval. The power spectral density (PSD) then refers to the spectral energy distribution that would be found per unit time, since the total energy of such a signal over all time would generally be infinite. Summation or integration of the spectral components yields the total power (for a physical process) or variance (in a statistical process), identical to what would be obtained by integrating formula_3 over the time domain, as dictated by Parseval's theorem.\n\nThe spectrum of a physical process formula_2 often contains essential information about the nature of formula_5. For instance, the pitch and timbre of a musical instrument are immediately determined from a spectral analysis. The color of a light source is determined by the spectrum of the electromagnetic wave's electric field formula_6 as it fluctuates at an extremely high frequency. Obtaining a spectrum from time series such as these involves the Fourier transform, and generalizations based on Fourier analysis. In many cases the time domain is not specifically employed in practice, such as when a dispersive prism is used to obtain a spectrum of light in a spectrograph, or when a sound is perceived through its effect on the auditory receptors of the inner ear, each of which is sensitive to a particular frequency.\nHowever this article concentrates on situations in which the time series is known (at least in a statistical sense) or directly measured (such as by a microphone sampled by a computer). The power spectrum is important in statistical signal processing and in the statistical study of stochastic processes, as well as in many other branches of physics and engineering. Typically the process is a function of time, but one can similarly discuss data in the spatial domain being decomposed in terms of spatial frequency.\n\nAny signal that can be represented as a variable that varies in time has a corresponding frequency spectrum. This includes familiar entities such as visible light (perceived as color), musical notes (perceived as pitch), radio/TV (specified by their frequency, or sometimes wavelength) and even the regular rotation of the earth. When these signals are viewed in the form of a frequency spectrum, certain aspects of the received signals or the underlying processes producing them are revealed. In some cases the frequency spectrum may include a distinct peak corresponding to a sine wave component. And additionally there may be peaks corresponding to harmonics of a fundamental peak, indicating a periodic signal which is \"not\" simply sinusoidal. Or a continuous spectrum may show narrow frequency intervals which are strongly enhanced corresponding to resonances, or frequency intervals containing almost zero power as would be produced by a notch filter.\n\nIn physics, the signal might be a wave, such as an electromagnetic wave, an acoustic wave, or the vibration of a mechanism. The \"power spectral density\" (PSD) of the signal describes the power present in the signal as a function of frequency, per unit frequency. Power spectral density is commonly expressed in watts per hertz (W/Hz).\n\nWhen a signal is defined in terms only of a voltage, for instance, there is no unique power associated with the stated amplitude. In this case \"power\" is simply reckoned in terms of the square of the signal, as this would always be \"proportional\" to the actual power delivered by that signal into a given impedance. So one might use units of V Hz for the PSD and V s Hz for the ESD (\"energy spectral density\") even though no actual \"power\" or \"energy\" is specified.\n\nSometimes one encounters an \"amplitude spectral density\" (ASD), which is the square root of the PSD; the ASD of a voltage signal has units of V Hz. This is useful when the \"shape\" of the spectrum is rather constant, since variations in the ASD will then be proportional to variations in the signal's voltage level itself. But it is mathematically preferred to use the PSD, since only in that case is the area under the curve meaningful in terms of actual power over all frequency or over a specified bandwidth.\n\nIn the general case, the units of PSD will be the ratio of units of variance per unit of frequency; so, for example, a series of displacement values (in meters) over time (in seconds) will have PSD in units of m/Hz.\nFor random vibration analysis, units of \"g\" Hz are frequently used for the PSD of acceleration. Here \"g\" denotes the g-force.\n\nMathematically, it is not necessary to assign physical dimensions to the signal or to the independent variable. In the following discussion the meaning of \"x(t)\" will remain unspecified, but the independent variable will be assumed to be that of time.\n\nEnergy spectral density describes how the energy of a signal or a time series is distributed with frequency. Here, the term energy is used in the generalized sense of signal processing; that is, the energy formula_7 of a signal formula_2 is\n\nThe energy spectral density is most suitable for transients—that is, pulse-like signals—having a finite total energy. In this case, Parseval's theorem gives us an alternate expression for the energy of the signal:\n\nwhere\n\nis the Fourier transform of the signal and formula_12 is the frequency in Hz, i.e., cycles per second. Often used is the angular frequency formula_13. Since the integral on the right-hand side is the energy of the signal, the integrand formula_14 can be interpreted as a density function describing the energy per unit frequency contained in the signal at the frequency formula_12. In light of this, the energy spectral density of a signal formula_2 is defined as\n\nAs a physical example of how one might measure the energy spectral density of a signal, suppose formula_17 represents the potential (in volts) of an electrical pulse propagating along a transmission line of impedance formula_18, and suppose the line is terminated with a matched resistor (so that all of the pulse energy is delivered to the resistor and none is reflected back). By Ohm's law, the power delivered to the resistor at time formula_19 is equal to formula_20, so the total energy is found by integrating formula_20 with respect to time over the duration of the pulse. To find the value of the energy spectral density formula_1 at frequency formula_12, one could insert between the transmission line and the resistor a bandpass filter which passes only a narrow range of frequencies (formula_24, say) near the frequency of interest and then measure the total energy formula_25 dissipated across the resistor. The value of the energy spectral density at formula_12 is then estimated to be formula_27. In this example, since the power formula_20 has units of V Ω, the energy formula_25 has units of V s Ω = J, and hence the estimate formula_30 of the energy spectral density has units of J Hz, as required. In many situations, it is common to forgo the step of dividing by formula_18 so that the energy spectral density instead has units of V s Hz.\n\nThis definition generalizes in a straightforward manner to a discrete signal with an infinite number of values formula_32 such as a signal sampled at discrete times formula_33:\n\nwhere formula_35 is the discrete-time Fourier transform of formula_36 and formula_37 is the complex conjugate of formula_38 The sampling interval formula_39 is needed to keep the correct physical units and to ensure that we recover the continuous case in the limit formula_40; however, in the mathematical sciences, the interval is often set to 1.\n\nThe above definition of energy spectral density is suitable for transients (pulse-like signals) whose energy is concentrated around one time window; then the Fourier transforms of the signals generally exist. For continuous signals over all time, such as stationary processes, one must rather define the \"power spectral density\" (PSD); this describes how power of a signal or time series is distributed over frequency, as in the simple example given previously. Here, power can be the actual physical power, or more often, for convenience with abstract signals, is simply identified with the squared value of the signal. For example, statisticians study the variance of a function over time formula_2 (or over another independent variable), and using an analogy with electrical signals (among other physical processes), it is customary to refer to it as the \"power spectrum\" even when there is no physical power involved. If one were to create a physical voltage source which followed formula_2 and applied it to the terminals of a 1 ohm resistor, then indeed the instantaneous power dissipated in that resistor would be given by formula_43 watts.\n\nThe average power formula_44 of a signal formula_2 over all time is therefore given by the following time average:\n\nNote that a stationary process, for instance, may have a finite power but an infinite energy. After all, energy is the integral of power, and the stationary signal continues over an infinite time. That is the reason that we cannot use the energy spectral density as defined above in such cases.\n\nIn analyzing the frequency content of the signal formula_2, one might like to compute the ordinary Fourier transform formula_48; however, for many signals of interest the Fourier transform does not formally exist. Because of this complication one can as well work with a truncated Fourier transform where the signal is integrated only over a finite interval formula_49:\n\nThis is the amplitude spectral density. Then the power spectral density can be defined as\n\nHere formula_51 denotes the expected value; explicitly, we have\n\nIn the latter form (for a stationary random process), one can make the change of variables formula_53 and with the limits of integration (rather than formula_49) approaching infinity, the resulting power spectral density formula_55 and the autocorrelation function of this signal are seen to be Fourier transform pairs (Wiener–Khinchin theorem). The autocorrelation function is a statistic defined as\n\nor more generally as\n\nin the case that formula_58 is complex-valued. Provided that formula_59 is absolutely integrable (which is not always true),\n\nMany authors use this equality to actually \"define\" the power spectral density.\n\nThe power of the signal in a given frequency band formula_60 (or formula_61) can be calculated by integrating over frequency. Since formula_62, an equal amount of power can be attributed to positive and negative frequencies, which accounts for the factor of 2 in the following form (such trivial factors dependent on conventions used):\n\nMore generally, similar techniques may be used to estimate a time-varying spectral density. In this case the truncated Fourier transform defined above over the finite time interval formula_64 is \"not\" evaluated in the limit of formula_65 approaching infinity. This results in decreased spectral coverage and resolution since frequencies of less than formula_66 are not sampled, and results at frequencies which are not an integer multiple of formula_66 are not independent. Just using a single such time series, the estimated power spectrum will be very \"noisy\"; however this can be alleviated if it is possible to evaluate the expected value (in the above equation) using a large (or infinite) number of short-term spectra corresponding to statistical ensembles of realizations of formula_2 evaluated over the specified time window.\n\nThis definition of the power spectral density can be generalized to discrete time variables formula_32. As above we can consider a \"finite\" window of formula_70 with the signal sampled at discrete times formula_33 for a total measurement period formula_72. Then a single estimate of the PSD can be obtained through summation rather than integration:\n\nAs before, the actual PSD is achieved when formula_74 (and thus formula_65) approach infinity and the expected value is formally applied. In a real-world application, one would typically average this single-measurement PSD over many trials to obtain a more accurate estimate of the theoretical PSD of the physical process underlying the individual measurements. This computed PSD is sometimes called a periodogram. This periodogram converges to the true PSD as the number of estimates as well as the averaging time interval formula_65 approach infinity (Brown & Hwang).\n\nIf two signals both possess power spectral densities, then the cross-spectral density can similarly be calculated; as the PSD is related to the autocorrelation, so is the cross-spectral density related to the cross-correlation.\n\nSome properties of the PSD include:\n\n- The spectrum of a real valued process (or even a complex process using the above definition) is real and an even function of frequency: formula_62.\n- If the process is continuous and purely indeterministic, the autocovariance function can be reconstructed by using the Inverse Fourier transform\n- The PSD can be used to compute the variance (net power) of a process by integrating over frequency:\n- Being based on the Fourier transform, the PSD is a linear function of the autocovariance function in the sense that if formula_79 is decomposed into two functions\n\nThe \"integrated spectrum\" or \"power spectral distribution\" formula_82 is defined as\n\nGiven two signals formula_2 and formula_85, each of which possess power spectral densities formula_55 and formula_87, it is possible to define a cross power spectral density (CPSD) or cross spectral density (CSD) given by\n\nThe cross-spectral density (or 'cross power spectrum') is thus the Fourier transform of the cross-correlation function:\n\nwhere formula_90 is the cross-correlation of formula_2 and formula_85.\n\nIn light of this, the PSD is seen to be a special case of the CSD for formula_93.\n\nFor discrete signals \"x\" and \"y\", the relationship between the cross-spectral density and the cross-covariance is\n\nThe goal of spectral density estimation is to estimate the spectral density of a random signal from a sequence of time samples. Depending on what is known about the signal, estimation techniques can involve parametric or non-parametric approaches, and may be based on time-domain or frequency-domain analysis. For example, a common parametric technique involves fitting the observations to an autoregressive model. A common non-parametric technique is the periodogram.\n\nThe spectral density is usually estimated using Fourier transform methods (such as the Welch method), but other techniques such as the maximum entropy method can also be used.\n\n- The spectral density of formula_95 and the autocorrelation of formula_95 form a Fourier transform pair (for PSD versus ESD, different definitions of autocorrelation function are used). This result is known as Wiener–Khinchin theorem.\n- One of the results of Fourier analysis is Parseval's theorem which states that the area under the energy spectral density curve is equal to the area under the square of the magnitude of the signal, the total energy:\n\n- The spectral centroid of a signal is the midpoint of its spectral density function, i.e. the frequency that divides the distribution into two equal parts.\n- The spectral edge frequency of a signal is an extension of the previous concept to any proportion instead of two equal parts.\n- The spectral density is a function of frequency, not a function of time. However, the spectral density of small windows of a longer signal may be calculated, and plotted versus time associated with the window. Such a graph is called a \"spectrogram\". This is the basis of a number of spectral analysis techniques such as the short-time Fourier transform and wavelets.\n- A \"spectrum\" generally means the power spectral density, as discussed above, which depicts the distribution of signal content over frequency. This is not to be confused with the frequency response of a transfer function which also includes a phase (or equivalently, a real and imaginary part as a function of frequency). For transfer functions, (e.g., Bode plot, chirp) the complete frequency response may be graphed in two parts, amplitude versus frequency and phase versus frequency (or less commonly, as real and imaginary parts of the transfer function). The impulse response (in the time domain) formula_99, cannot generally be uniquely recovered from the amplitude spectral density part alone without the phase function. Although these are also Fourier transform pairs, there is no symmetry (as there is for the autocorrelation) forcing the Fourier transform to be real-valued. See spectral phase and phase noise.\n\nThe concept and use of the power spectrum of a signal is fundamental in electrical engineering, especially in electronic communication systems, including radio communications, radars, and related systems, plus passive remote sensing technology. Electronic instruments called spectrum analyzers are used to observe and measure the power spectra of signals.\n\nThe spectrum analyzer measures the magnitude of the short-time Fourier transform (STFT) of an input signal. If the signal being analyzed can be considered a stationary process, the STFT is a good smoothed estimate of its power spectral density.\n\nPrimordial fluctuations, density variations in the early universe, are quantified by a power spectrum which gives the power of the variations as a function of spatial scale.\n\n", "related": "\n- Noise spectral density\n- Spectral density estimation\n- Spectral efficiency\n- Spectral power distribution\n- Brightness temperature\n- Colors of noise\n- Spectral leakage\n- Window function\n- Bispectrum\n- Whittle likelihood\n\n- Power Spectral Density Matlab scripts\n"}
{"id": "1135333", "url": "https://en.wikipedia.org/wiki?curid=1135333", "title": "Ambiguity function", "text": "Ambiguity function\n\nIn pulsed radar and sonar signal processing, an ambiguity function is a two-dimensional function of time delay and Doppler frequency\nformula_1 showing the distortion of a returned pulse due to the receiver matched filter (commonly, but not exclusively, used in pulse compression radar) due to the Doppler shift of the return from a moving target. The ambiguity\nfunction is determined by the properties of the pulse and the matched filter, and not any particular target scenario. Many definitions of the ambiguity function exist; Some are restricted to narrowband signals and others are suitable to describe the propagation delay and Doppler relationship of wideband signals. Often the definition of the ambiguity function is given as the magnitude squared of other definitions (Weiss). \nFor a given complex baseband pulse formula_2, the narrowband ambiguity function is given by\n\nwhere formula_4 denotes the complex conjugate and formula_5 is the imaginary unit. Note that for zero Doppler shift (formula_6) this reduces to the autocorrelation of formula_2. A more concise way of representing the\nambiguity function consists of examining the one-dimensional\nzero-delay and zero-Doppler \"cuts\"; that is, formula_8 and\nformula_9, respectively. The matched filter output as a function of a time (the signal one would observe in a radar system) is a Doppler cut, with constant frequency given by the target's Doppler shift: formula_10.\n\nPulse-Doppler radar equipment sends out a series of radio frequency pulses. Each pulse has a certain shape (waveform)—how long the pulse is, what its frequency is, whether the frequency changes during the pulse, and so on. If the waves reflect off a single object, the detector will see a signal which, in the simplest case, is a copy of the original pulse but delayed by a certain time formula_11—related to the object's distance—and shifted by a certain frequency formula_12—related to the object's velocity (Doppler shift). If the original emitted pulse waveform is formula_2, then the detected signal (neglecting noise, attenuation, and distortion, and wideband corrections) will be:\n\nThe detected signal will never be \"exactly\" equal to any formula_15 because of noise. Nevertheless, if the detected signal has a high correlation with formula_15, for a certain delay and Doppler shift formula_17, then that suggests that there is an object with formula_17. Unfortunately, this procedure may yield false positives, i.e. wrong values formula_19 which are nevertheless highly correlated with the detected signal. In this sense, the detected signal may be \"ambiguous\".\n\nThe ambiguity occurs specifically when there is a high correlation between formula_15 and formula_21 for formula_22. This motivates the \"ambiguity function\" formula_23. The defining property of formula_23 is that the correlation between formula_15 and formula_21 is equal to formula_27.\n\nDifferent pulse shapes (waveforms) formula_2 have different ambiguity functions, and the ambiguity function is relevant when choosing what pulse to use.\n\nThe function formula_23 is complex-valued; the degree of \"ambiguity\" is related to its magnitude formula_30.\n\nThe ambiguity function plays a key role in the field of time–frequency signal processing, as it is related to the Wigner–Ville distribution by a 2-dimensional Fourier transform. This relationship is fundamental to the formulation of other time–frequency distributions: the bilinear time–frequency distributions are obtained by a 2-dimensional filtering in the ambiguity domain (that is, the ambiguity function of the signal). This class of distribution may be better adapted to the signals considered.\n\nMoreover, the ambiguity distribution can be seen as the short-time Fourier transform of a signal using the signal itself as the window function. This remark has been used to define an ambiguity distribution over the time-scale domain instead of the time-frequency domain.\n\nThe wideband ambiguity function of formula_31 is:\n\nwhere \"formula_33\" is a time scale factor of the received signal relative to the transmitted signal given by:\n\nfor a target moving with constant radial velocity \"v\". The reflection of the signal is represented with compression (or expansion) in time by the factor \"formula_35\", which is equivalent to a compression by the factor \"formula_36\" in the frequency domain (with an amplitude scaling). When the wave speed in the medium is sufficiently faster than the target speed, as is common with radar, this compression in frequency is closely approximated by a shift in frequency Δf = f*v/c (known as the doppler shift). For a narrow band signal, this approximation results in the narrowband ambiguity function given above, which can be computed efficiently by making use of the FFT algorithm.\n\nAn ambiguity function of interest is a 2-dimensional Dirac delta function or \"thumbtack\" function; that is, a function which is infinite at (0,0) and zero elsewhere.\n\nAn ambiguity function of this kind would be somewhat of a misnomer; it would have no ambiguities at all, and both the zero-delay and zero-Doppler cuts would be an impulse. This is not usually desirable (if a target has any Doppler shift from an unknown velocity it will disappear from the radar picture), but if Doppler processing is independently performed, knowledge of the precise Doppler frequency allows ranging without interference from any other targets which are not also moving at exactly the same velocity.\n\nThis type of ambiguity function is produced by ideal white noise (infinite in duration and infinite in bandwidth). However, this would require infinite power and is not physically realizable. There is no pulse formula_2 that will produce formula_39 from the definition of the ambiguity function. Approximations exist, however, and noise-like signals such as binary phase-shift keyed waveforms using maximal-length sequences are the best known performers in this regard.\n\n(1) Maximum value\n\n(2) Symmetry about the origin\n\n(3) Volume invariance\n\n(4) Modulation by a linear FM signal\n\n(5) Frequency energy spectrum\n\n(6) Upper bounds for formula_45 and lower bounds for formula_46 exist for the formula_47 power integrals\n\nThese bounds are sharp and are achieved if and only if formula_49 is a Gaussian function.\n\nConsider a simple square pulse of duration formula_11 and\namplitude formula_51:\n\nwhere formula_53 is the Heaviside step function. The\nmatched filter output is given by the autocorrelation of the pulse, which is a triangular pulse of height formula_54 and\nduration formula_55 (the zero-Doppler cut). However, if the\nmeasured pulse has a frequency offset due to Doppler shift, the\nmatched filter output is distorted into a sinc function. The\ngreater the Doppler shift, the smaller the peak of the resulting sinc,\nand the more difficult it is to detect the target. \n\nIn general, the square pulse is not a desirable waveform from a pulse compression standpoint, because the autocorrelation function is too short in amplitude, making it difficult to detect targets in noise, and too wide in time, making it difficult to discern multiple overlapping targets.\n\nA commonly used radar or sonar pulse is the linear frequency modulated (LFM) pulse (or \"chirp\"). It has the advantage of greater bandwidth while keeping the pulse duration short and envelope constant. A constant envelope LFM pulse has an ambiguity function similar to that of the square pulse, except that it is skewed in the delay-Doppler plane. Slight Doppler mismatches for the LFM pulse do not change the general shape of the pulse and reduce the amplitude very little, but they do appear to shift the pulse\nin time. Thus, an uncompensated Doppler shift changes the target's apparent range; this phenomenon is called range-Doppler coupling.\n\nThe ambiguity function can be extended to multistatic radars, which comprise multiple non-colocated transmitters and/or receivers (and can include bistatic radar as a special case).\n\nFor these types of radar, the simple linear relationship between time and range that exists in the monostatic case no longer applies, and is instead dependent on the specific geometry – i.e. the relative location of transmitter(s), receiver(s) and target. Therefore, the multistatic ambiguity function is mostly usefully defined as a function of two- or three-dimensional position and velocity vectors for a given multistatic geometry and transmitted waveform.\n\nJust as the monostatic ambiguity function is naturally derived from the matched filter, the multistatic ambiguity function is derived from the corresponding optimal \"multistatic\" detector – i.e. that which maximizes the probability of detection given a fixed probability of false alarm through joint processing of the signals at all receivers. The nature of this detection algorithm depends on whether or not the target fluctuations observed by each bistatic pair within the multistatic system are mutually correlated. If so, the optimal detector performs phase coherent summation of received signals which can result in very high target location accuracy. If not, the optimal detector performs incoherent summation of received signals which gives diversity gain. Such systems are sometimes described as \"MIMO radars\" due to the information theoretic similarities to MIMO communication systems.\n\n", "related": "\n- Matched filter\n- Pulse compression\n- Pulse-Doppler radar\n- Digital signal processing\n- Philip Woodward\n\n- Richards, Mark A. \"Fundamentals of Radar Signal Processing\". McGraw–Hill Inc., 2005. .\n- Ipatov, Valery P. \"Spread Spectrum and CDMA\". Wiley & Sons, 2005.\n- Chernyak V.S. \"Fundamentals of Multisite Radar Systems\", CRC Press, 1998.\n- Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n- Nadav Levanon, and Eli Mozeson. Radar signals. Wiley. com, 2004.\n- Augusto Aubry, Antonio De Maio, Bo Jiang, and Shuzhong Zhang. \"Ambiguity function shaping for cognitive radar via complex quartic optimization.\" IEEE Transactions on Signal Processing 61 (2013): 5603-5619.\n- Mojtaba Soltanalian, and Petre Stoica. \"Computational design of sequences with good correlation properties.\" IEEE Transactions on Signal Processing, 60.5 (2012): 2180-2193.\n- G. Krötzsch, M. A. Gómez-Méndez, Transformada Discreta de Ambigüedad, Revista Mexicana de Física, Vol. 63, pp. 505--515 (2017). \"Transformada Discreta de Ambigüedad\".\n"}
{"id": "41079", "url": "https://en.wikipedia.org/wiki?curid=41079", "title": "Dynamic range", "text": "Dynamic range\n\nDynamic range (abbreviated DR, DNR, or DYR) is the ratio between the largest and smallest values that a certain quantity can assume. It is often used in the context of signals, like sound and light. It is measured either as a ratio or as a base-10 (decibel) or base-2 (doublings, bits or stops) logarithmic value of the difference between the smallest and largest signal values.\n\nElectronically reproduced audio and video is often processed to fit the original material with a wide dynamic range into a narrower recorded dynamic range that can more easily be stored and reproduced; this processing is called dynamic range compression.\n\nThe human senses of sight and hearing have a relatively high dynamic range. However, a human cannot perform these feats of perception at both extremes of the scale at the same time. The human eye takes time to adjust to different light levels, and its dynamic range in a given scene is actually quite limited due to optical glare. The instantaneous dynamic range of human audio perception is similarly subject to masking so that, for example, a whisper cannot be heard in loud surroundings.\n\nA human is capable of hearing (and usefully discerning) anything from a quiet murmur in a soundproofed room to the loudest heavy metal concert. Such a difference can exceed 100 dB which represents a factor of 100,000 in amplitude and a factor 10,000,000,000 in power. The dynamic range of human hearing is roughly 140 dB, varying with frequency, from the threshold of hearing (around −9 dB SPL at 3 kHz) to the threshold of pain (from 120–140 dB SPL). This wide dynamic range cannot be perceived all at once, however; the tensor tympani, stapedius muscle, and outer hair cells all act as mechanical dynamic range compressors to adjust the sensitivity of the ear to different ambient levels.\n\nA human can see objects in starlight or in bright sunlight, even though on a moonless night objects receive 1/1,000,000,000 of the illumination they would on a bright sunny day; a dynamic range of 90 dB.\n\nIn practice, it is difficult for humans to achieve the full dynamic experience using electronic equipment. For example, a good quality LCD has a dynamic range limited to around 1000:1, and some of the latest CMOS image sensors now have measured dynamic ranges of about 23,000:1. Paper reflectance can produce a dynamic range of about 100:1. A professional video camera such as the Sony Digital Betacam achieves a dynamic range of greater than 90 dB in audio recording.\n\nAudio engineers use \"dynamic range\" to describe the ratio of the amplitude of the loudest possible undistorted signal to the noise floor, say of a microphone or loudspeaker. Dynamic range is therefore the signal-to-noise ratio (SNR) for the case where the signal is the loudest possible for the system. For example, if the ceiling of a device is 5 V (rms) and the noise floor is 10 µV (rms) then the dynamic range is 500000:1, or 114 dB:\n\nIn digital audio theory the dynamic range is limited by quantization error. The maximum achievable dynamic range for a digital audio system with \"Q\"-bit uniform quantization is calculated as the ratio of the largest sine-wave rms to rms noise is:\n\nHowever, the usable dynamic range may be greater, as a properly dithered recording device can record signals well below the noise floor.\n\nThe 16-bit compact disc has a theoretical undithered dynamic range of about 96 dB; however, the \"perceived\" dynamic range of 16-bit audio can be 120 dB or more with noise-shaped dither, taking advantage of the frequency response of the human ear.\n\nDigital audio with undithered 20-bit quantization is theoretically capable of 120 dB dynamic range. 24-bit digital audio affords 144 dB dynamic range. Most Digital audio workstations process audio with 32-bit floating-point representation which affords even higher dynamic range and so loss of dynamic range is no longer a concern in terms of digital audio processing. Dynamic range limitations typically result from improper gain staging, recording technique including ambient noise and intentional application of dynamic range compression.\n\nDynamic range in analog audio is the difference between low-level thermal noise in the electronic circuitry and high-level signal saturation resulting in increased distortion and, if pushed higher, clipping. Multiple noise processes determine the noise floor of a system. Noise can be picked up from microphone self-noise, preamp noise, wiring and interconnection noise, media noise, etc.\n\nEarly 78 rpm phonograph discs had a dynamic range of up to 40 dB, soon reduced to 30 dB and worse due to wear from repeated play. Vinyl microgroove phonograph records typically yield 55-65 dB, though the first play of the higher-fidelity outer rings can achieve a dynamic range of 70 dB.\n\nGerman magnetic tape in 1941 was reported to have had a dynamic range of 60 dB, though modern day restoration experts of such tapes note 45-50 dB as the observed dynamic range. Ampex tape recorders in the 1950s achieved 60 dB in practical usage, In the 1960s, improvements in tape formulation processes resulted in 7 dB greater range, and Ray Dolby developed the Dolby A-Type noise reduction system that increased low- and mid-frequency dynamic range on magnetic tape by 10 dB, and high-frequency by 15 dB, using companding (compression and expansion) of four frequency bands. The peak of professional analog magnetic recording tape technology reached 90 dB dynamic range in the midband frequencies at 3% distortion, or about 80 dB in practical broadband applications. The Dolby SR noise reduction system gave a 20 dB further increased range resulting in 110 dB in the midband frequencies at 3% distortion. \n\nCompact Cassette tape performance ranges from 50 to 56 dB depending on tape formulation, with type IV tape tapes giving the greatest dynamic range, and systems such as XDR, dbx and Dolby noise reduction system increasing it further. Specialized bias and record head improvements by Nakamichi and Tandberg combined with Dolby C noise reduction yielded 72 dB dynamic range for the cassette.\n\nA dynamic microphone is able to withstand high sound intensity and can have a dynamic range of up to 140 dB. Condenser microphones are also rugged but their dynamic range may be limited by the overloading of their associated electronic circuitry. Practical considerations of acceptable distortion levels in microphones combined with typical practices in a recording studio result in a useful dynamic range of 125 dB.\n\nIn 1981, researchers at Ampex determined that a dynamic range of 118 dB on a dithered digital audio stream was necessary for subjective noise-free playback of music in quiet listening environments.\n\nSince the early 1990s, it has been recommended by several authorities, including the Audio Engineering Society, that measurements of dynamic range be made with an audio signal present, which is then filtered out in the noise floor measurement used in determining dynamic range. This avoids questionable measurements based on the use of blank media, or muting circuits.\n\nWhen showing a movie or a game, a display is able to show both shadowy nighttime scenes and bright outdoor sunlit scenes, but in fact the level of light coming from the display is much the same for both types of scene (perhaps different by a factor of 10). Knowing that the display does not have a huge dynamic range, the producers do not attempt to make the nighttime scenes accurately dimmer than the daytime scenes, but instead use other cues to suggest night or day. A nighttime scene will usually contain duller colours and will often be lit with blue lighting, which reflects the way that the sensitive rod cells in the human eye sees colours at low light levels.\n\nIn electronics dynamic range is used in the following contexts: \n- Specifies the ratio of a maximum level of a parameter, such as power, current, voltage or frequency, to the minimum detectable value of that parameter. (See Audio system measurements.)\n- In a transmission system, the ratio of the overload level (the maximum signal power that the system can tolerate without distortion of the signal) to the noise level of the system.\n- In digital systems or devices, the ratio of maximum and minimum signal levels required to maintain a specified bit error ratio.\n- Optimization of bit width of digital data path (according to the dynamic ranges of signal) can reduce the area, cost, and power consumption of digital circuits and systems while improving their performance. Optimal bit width for a digital data path is the smallest bit width that can satisfy the required signal-to-noise ratio and also avoid overflow.\n\nIn audio and electronics applications, the ratio involved is often large enough that it is converted to a logarithm and specified in decibels.\n\nIn metrology, such as when performed in support of science, engineering or manufacturing objectives, dynamic range refers to the range of values that can be measured by a sensor or metrology instrument. Often this dynamic range of measurement is limited at one end of the range by saturation of a sensing signal sensor or by physical limits that exist on the motion or other response capability of a mechanical indicator. The other end of the dynamic range of measurement is often limited by one or more sources of random noise or uncertainty in signal levels that may be described as defining the sensitivity of the sensor or metrology device. When digital sensors or sensor signal converters are a component of the sensor or metrology device, the dynamic range of measurement will be also related to the number of binary digits (bits) used in a digital numeric representation in which the measured value is linearly related to the digital number. For example, a 12-bit digital sensor or converter can provide a dynamic range in which the ratio of the maximum measured value to the minimum measured value is up to 2 = 4096.\n\nMetrology systems and devices may use several basic methods to increase their basic dynamic range. These methods include averaging and other forms of filtering, correction of receivers characteristics, repetition of measurements, nonlinear transformations to avoid saturation, etc. In more advance forms of metrology, such as multiwavelength digital holography, interferometry measurements made at different scales (different wavelengths) can be combined to retain the same low-end resolution while extending the upper end of the dynamic range of measurement by orders of magnitude.\n\nIn music, dynamic range describes the difference between the quietest and loudest volume of an instrument, part or piece of music. In modern recording, this range is often limited through dynamic range compression, which allows for louder volume, but can make the recording sound less exciting or live.\n\nThe term \"dynamic range\" may be confusing in music because it has two conflicting definitions, particularly in the understanding of the loudness war phenomenon. \"Dynamic range\" may refer to micro-dynamics, related to crest factor, whereas the European Broadcasting Union, in EBU3342 Loudness Range, defines \"dynamic range\" as the difference between the quietest and loudest volume, a matter of macro-dynamics.\n\nThe dynamic range of music as normally perceived in a concert hall does not exceed 80 dB, and human speech is normally perceived over a range of about 40 dB.\n\nPhotographers use \"dynamic range\" to describe the luminance range of a scene being photographed, or the limits of luminance range that a given digital camera or film can capture, or the opacity range of developed film images, or the reflectance range of images on photographic papers.\n\nThe dynamic range of digital photography is comparable to the capabilities of photographic film and both are comparable to the capabilities of the human eye.\n\nThere are photographic techniques that support higher dynamic range. \n- Graduated neutral density filters are used to decrease the dynamic range of scene luminance that can be captured on photographic film (or on the image sensor of a digital camera): The filter is positioned in front of the lens at the time the exposure is made; the top half is dark and the bottom half is clear. The dark area is placed over a scene's high-intensity region, such as the sky. The result is more even exposure in the focal plane, with increased detail in the shadows and low-light areas. Though this doesn't increase the fixed dynamic range available at the film or sensor, it stretches usable dynamic range in practice.\n- Digital imaging algorithms have been developed to map the image differently in shadow and in highlight in order to better distribute the lighting range across the image. These techniques are known as local tone mapping, and usually involves overcoming the limited dynamic range of the sensor by selectively combining multiple exposures of the same scene in order to retain detail in light and dark areas. The same approach has been used in chemical photography to capture an extremely wide dynamic range: A three-layer film with each underlying layer at 1/100 the sensitivity of the next higher one has, for example, been used to record nuclear-weapons tests.\n\nConsumer-grade image file formats sometimes restrict dynamic range. The most severe dynamic-range limitation in photography may not involve encoding, but rather reproduction to, say, a paper print or computer screen. In that case, not only local tone mapping, but also \"dynamic range adjustment\" can be effective in revealing detail throughout light and dark areas: The principle is the same as that of dodging and burning (using different lengths of exposures in different areas when making a photographic print) in the chemical darkroom. The principle is also similar to gain riding or automatic level control in audio work, which serves to keep a signal audible in a noisy listening environment and to avoid peak levels which overload the reproducing equipment, or which are unnaturally or uncomfortably loud.\n\nIf a camera sensor is incapable of recording the full dynamic range of a scene, high-dynamic-range (HDR) techniques may be used in postprocessing, which generally involve combining multiple exposures using software.\n\n", "related": "\n- Loudness war\n- High dynamic range\n- High-dynamic-range imaging\n- High-dynamic-range rendering\n- High-dynamic-range video\n- Highlight headroom\n- Range fractionation\n- Spurious-free dynamic range\n\n- Audible dynamic range (online test)\n- https://web.archive.org/web/20060905003034/http://history.sandiego.edu/gen/recording/dynamic.html\n- http://www.seis.com.au/TechNotes/TN200410A_SNR.html\n"}
{"id": "1383899", "url": "https://en.wikipedia.org/wiki?curid=1383899", "title": "Linear time-invariant system", "text": "Linear time-invariant system\n\nLinear time-invariant theory, commonly known as LTI system theory, investigates the response of a linear and time-invariant system to an arbitrary input signal. Trajectories of these systems are commonly measured and tracked as they move through time (e.g., an acoustic waveform), but in applications like image processing and field theory, the LTI systems also have trajectories in spatial dimensions. Thus, these systems are also called \"linear translation-invariant\" to give the theory the most general reach. In the case of generic discrete-time (i.e., sampled) systems, \"linear shift-invariant\" is the corresponding term. A good example of LTI systems are electrical circuits that can be made up of resistors, capacitors, and inductors.. \nIt has been used in applied mathematics and has direct applications in NMR spectroscopy, seismology, circuits, signal processing, control theory, and other technical areas.\n\nThe defining properties of any LTI system are \"linearity\" and \"time invariance\".\n\n- \"Linearity\" means that the relationship between the input and the output of the system is a linear map: If input formula_1 produces response formula_2 and input formula_3 produces response formula_4 then the \"scaled\" and \"summed\" input formula_5 produces the scaled and summed response formula_6 where formula_7 and formula_8 are real scalars. It follows that this can be extended to an arbitrary number of terms, and so for real numbers formula_9,\n\n- \"Time invariance\" means that whether we apply an input to the system now or \"T\" seconds from now, the output will be identical except for a time delay of \"T\" seconds. That is, if the output due to input formula_15 is formula_16, then the output due to input formula_17 is formula_18. Hence, the system is time invariant because the output does not depend on the particular time the input is applied.\n\nThe fundamental result in LTI system theory is that any LTI system can be characterized entirely by a single function called the system's impulse response. The output of the system is simply the convolution of the input to the system with the system's impulse response. This method of analysis is often called the \"time domain\" point-of-view. The same result is true of discrete-time linear shift-invariant systems in which signals are discrete-time samples, and convolution is defined on sequences.\n\nEquivalently, any LTI system can be characterized in the \"frequency domain\" by the system's transfer function, which is the Laplace transform of the system's impulse response (or Z transform in the case of discrete-time systems). As a result of the properties of these transforms, the output of the system in the frequency domain is the product of the transfer function and the transform of the input. In other words, convolution in the time domain is equivalent to multiplication in the frequency domain.\n\nFor all LTI systems, the eigenfunctions, and the basis functions of the transforms, are complex exponentials. This is, if the input to a system is the complex waveform formula_19 for some complex amplitude formula_20 and complex frequency formula_21, the output will be some complex constant times the input, say formula_22 for some new complex amplitude formula_23. The ratio formula_24 is the transfer function at frequency formula_21.\n\nSince sinusoids are a sum of complex exponentials with complex-conjugate frequencies, if the input to the system is a sinusoid, then the output of the system will also be a sinusoid, perhaps with a different amplitude and a different phase, but always with the same frequency upon reaching steady-state. LTI systems cannot produce frequency components that are not in the input.\n\nLTI system theory is good at describing many important systems. Most LTI systems are considered \"easy\" to analyze, at least compared to the time-varying and/or nonlinear case. Any system that can be modeled as a linear homogeneous differential equation with constant coefficients is an LTI system. Examples of such systems are electrical circuits made up of resistors, inductors, and capacitors (RLC circuits). Ideal spring–mass–damper systems are also LTI systems, and are mathematically equivalent to RLC circuits.\n\nMost LTI system concepts are similar between the continuous-time and discrete-time (linear shift-invariant) cases. In image processing, the time variable is replaced with two space variables, and the notion of time invariance is replaced by two-dimensional shift invariance. When analyzing filter banks and MIMO systems, it is often useful to consider vectors of signals.\n\nA linear system that is not time-invariant can be solved using other approaches such as the Green function method. The same method must be used when the initial conditions of the problem are not null.\n\nThe behavior of a linear, continuous-time, time-invariant system with input signal \"x\"(\"t\") and output signal \"y\"(\"t\") is described by the convolution integral:\n\nwhere formula_26 is the system's response to an impulse:  formula_27   formula_28 is therefore proportional to a weighted average of the input function formula_29  The weighting function is formula_30 simply shifted by amount formula_31   As formula_32 changes, the weighting function emphasizes different parts of the input function. When formula_33 is zero for all negative formula_34  formula_35 depends only on values of formula_36 prior to time formula_37  and the system is said to be causal.\n\nTo understand why the convolution produces the output of an LTI system, let the notation formula_38 represent the function formula_39 with variable formula_40 and constant formula_41  And let the shorter notation formula_42 represent formula_43 Then a continuous-time system transforms an input function, formula_44 into an output function, formula_45. And in general, every value of the output can depend on every value of the input. This concept is represented by:\n\nwhere formula_47 is the transformation operator for time formula_32. In a typical system, formula_35 depends most heavily on the values of formula_36 that occurred near time formula_31  Unless the transform itself changes with formula_37 the output function is just constant, and the system is uninteresting.\n\nFor a linear system, formula_53 must satisfy :\n\nAnd the time-invariance requirement is:\n\nIn this notation, we can write the impulse response as  formula_54\n\nSimilarly:\n\nSubstituting this result into the convolution integral:\n\nwhich has the form of the right side of for the case formula_56 and formula_57<br>\n\nIn summary, the input function, formula_44  can be represented by a continuum of time-shifted impulse functions, combined \"linearly\", as shown at . The system's linearity property allows the system's response to be represented by the corresponding continuum of impulse responses, combined in the same way.  And the time-invariance property allows that combination to be represented by the convolution integral.\n\nThe mathematical operations above have a simple graphical simulation.\n\nAn eigenfunction is a function for which the output of the operator is a scaled version of the same function. That is,\nwhere \"f\" is the eigenfunction and formula_61 is the eigenvalue, a constant.\n\nThe exponential functions formula_62, where formula_63, are eigenfunctions of a linear, time-invariant operator. A simple proof illustrates this concept. Suppose the input is formula_64. The output of the system with impulse response formula_65 is then\n\nwhich, by the commutative property of convolution, is equivalent to\n\nwhere the scalar\nis dependent only on the parameter \"s\".\n\nSo the system's response is a scaled version of the input. In particular, for any formula_63, the system output is the product of the input formula_70 and the constant formula_71. Hence, formula_62 is an eigenfunction of an LTI system, and the corresponding eigenvalue is formula_71.\n\nIt is also possible to directly derive complex exponentials as eigenfunctions of LTI systems.\n\nLet's set formula_74 some complex exponential and formula_75 a time-shifted version of it.\n\nformula_76 by linearity with respect to the constant formula_77 .\n\nformula_78 by time invariance of formula_79 .\n\nSo formula_80. Setting formula_81 and renaming we get :\n\nformula_82\n\ni.e. that a complex exponential formula_83 as input will give a complex exponential of same frequency as output.\n\nThe eigenfunction property of exponentials is very useful for both analysis and insight into LTI systems. The Laplace transform\n\nis exactly the way to get the eigenvalues from the impulse response. Of particular interest are pure sinusoids (i.e., exponential functions of the form formula_85 where formula_86 and formula_87). These are generally called complex exponentials even though the argument is purely imaginary. The Fourier transform formula_88 gives the eigenvalues for pure complex sinusoids. Both of formula_71 and formula_90 are called the \"system function\", \"system response\", or \"transfer function\".\n\nThe Laplace transform is usually used in the context of one-sided signals, i.e. signals that are zero for all values of \"t\" less than some value. Usually, this \"start time\" is set to zero, for convenience and without loss of generality, with the transform integral being taken from zero to infinity (the transform shown above with lower limit of integration of negative infinity is formally known as the bilateral Laplace transform).\n\nThe Fourier transform is used for analyzing systems that process signals that are infinite in extent, such as modulated sinusoids, even though it cannot be directly applied to input and output signals that are not square integrable. The Laplace transform actually works directly for these signals if they are zero before a start time, even if they are not square integrable, for stable systems. The Fourier transform is often applied to spectra of infinite signals via the Wiener–Khinchin theorem even when Fourier transforms of the signals do not exist.\n\nDue to the convolution property of both of these transforms, the convolution that gives the output of the system can be transformed to a multiplication in the transform domain, given signals for which the transforms exist\n\nNot only is it often easier to do the transforms, multiplication, and inverse transform than the original convolution, but one can also gain insight into the behavior of the system from the system response. One can look at the modulus of the system function |\"H\"(\"s\")| to see whether the input formula_92 is \"passed\" (let through) the system or \"rejected\" or \"attenuated\" by the system (not let through).\n\n- A simple example of an LTI operator is the derivative.\n- formula_93   (i.e., it is linear)\n- formula_94   (i.e., it is time invariant)\n\n- Another simple LTI operator is an averaging operator\n\nSome of the most important properties of a system are causality and stability. Causality is a necessity if the independent variable is time, but not all systems have time as an independent variable. For example, a system that processes still images does not need to be causal. Non-causal systems can be built and can be useful in many circumstances. Even non-real systems can be built and are very useful in many contexts.\nA system is causal if the output depends only on present and past, but not future inputs. A necessary and sufficient condition for causality is\n\nwhere formula_65 is the impulse response. It is not possible in general to determine causality from the Laplace transform, because the inverse transform is not unique. When a region of convergence is specified, then causality can be determined.\n\nA system is bounded-input, bounded-output stable (BIBO stable) if, for every bounded input, the output is finite. Mathematically, if every input satisfying\n\nleads to an output satisfying\n\n(that is, a finite maximum absolute value of formula_15 implies a finite maximum absolute value of formula_16), then the system is stable. A necessary and sufficient condition is that formula_65, the impulse response, is in L (has a finite L norm):\n\nIn the frequency domain, the region of convergence must contain the imaginary axis formula_111.\n\nAs an example, the ideal low-pass filter with impulse response equal to a sinc function is not BIBO stable, because the sinc function does not have a finite L norm. Thus, for some bounded input, the output of the ideal low-pass filter is unbounded. In particular, if the input is zero for formula_112 and equal to a sinusoid at the cut-off frequency for formula_113, then the output will be unbounded for all times other than the zero crossings.\n\nAlmost everything in continuous-time systems has a counterpart in discrete-time systems. \n\nIn many contexts, a discrete time (DT) system is really part of a larger continuous time (CT) system. For example, a digital recording system takes an analog sound, digitizes it, possibly processes the digital signals, and plays back an analog sound for people to listen to.\n\nFormally, the DT signals studied are almost always uniformly sampled versions of CT signals. If formula_15 is a CT signal, then an analog to digital converter will transform it to the DT signal:\n\nwhere \"T\" is the sampling period. It is very important to limit the range of frequencies in the input signal for faithful representation in the DT signal, since then the sampling theorem guarantees that no information about the CT signal is lost. A DT signal can only contain a frequency range of formula_116; other frequencies are aliased to the same range.\n\nLet formula_117 represent the sequence formula_118\n\nAnd let the shorter notation formula_119 represent formula_120\n\nA discrete system transforms an input sequence, formula_121 into an output sequence, formula_122 In general, every element of the output can depend on every element of the input. Representing the transformation operator by formula_123, we can write:\n\nNote that unless the transform itself changes with n, the output sequence is just constant, and the system is uninteresting. (Thus the subscript, n.) In a typical system, y[n] depends most heavily on the elements of x whose indices are near n.\n\nFor the special case of the Kronecker delta function, formula_125 the output sequence is the impulse response:\n\nFor a linear system, formula_123 must satisfy:\nAnd the time-invariance requirement is:\n\nIn such a system, the impulse response, formula_128 characterizes the system completely. I.e., for any input sequence, the output sequence can be calculated in terms of the input and the impulse response. To see how that is done, consider the identity:\n\nwhich expresses formula_119 in terms of a sum of weighted delta functions.\n\nTherefore:\n\nwhere we have invoked for the case formula_132 and formula_133\n\nAnd because of , we may write:\n\nTherefore:\n\nwhich is the familiar discrete convolution formula. The operator formula_135 can therefore be interpreted as proportional to a weighted average of the function x[k].\nThe weighting function is h[-k], simply shifted by amount n. As n changes, the weighting function emphasizes different parts of the input function. Equivalently, the system's response to an impulse at n=0 is a \"time\" reversed copy of the unshifted weighting function. When h[k] is zero for all negative k, the system is said to be causal.\n\nAn eigenfunction is a function for which the output of the operator is the same function, scaled by some constant. In symbols,\nwhere \"f\" is the eigenfunction and formula_61 is the eigenvalue, a constant.\n\nThe exponential functions formula_138, where formula_139, are eigenfunctions of a linear, time-invariant operator. formula_140 is the sampling interval, and formula_141. A simple proof illustrates this concept.\n\nSuppose the input is formula_142. The output of the system with impulse response formula_143 is then\n\nwhich is equivalent to the following by the commutative property of convolution\n\nwhere\nis dependent only on the parameter \"z\".\n\nSo formula_147 is an eigenfunction of an LTI system because the system response is the same as the input times the constant formula_148.\n\nThe eigenfunction property of exponentials is very useful for both analysis and insight into LTI systems. The Z transform \nis exactly the way to get the eigenvalues from the impulse response. Of particular interest are pure sinusoids, i.e. exponentials of the form formula_150, where formula_86. These can also be written as formula_147 with formula_153. These are generally called complex exponentials even though the argument is purely imaginary.\nThe discrete-time Fourier transform (DTFT) formula_154\ngives the eigenvalues of pure sinusoids. Both of formula_148 and formula_156 are called the \"system function\", \"system response\", or \"transfer function\"'.\n\nThe Z transform is usually used in the context of one-sided signals, i.e. signals that are zero for all values of t less than some value. Usually, this \"start time\" is set to zero, for convenience and without loss of generality. The Fourier transform is used for analyzing signals that are infinite in extent.\n\nDue to the convolution property of both of these transforms, the convolution that gives the output of the system can be transformed to a multiplication in the transform domain. That is,\n\nJust as with the Laplace transform transfer function in continuous-time system analysis, the Z transform makes it easier to analyze systems and gain insight into their behavior. One can look at the modulus of the system function \"|H(z)|\" to see whether the input formula_147 is \"passed\" (let through) by the system, or \"rejected\" or \"attenuated\" by the system (not let through).\n\n- A simple example of an LTI operator is the delay operator formula_159.\n- formula_160   (i.e., it is linear)\n- formula_161   (i.e., it is time invariant)\n\n- Another simple LTI operator is the averaging operator\n\nThe input-output characteristics of discrete-time LTI system are completely described by its impulse response formula_143.\nSome of the most important properties of a system are causality and stability. Unlike CT systems, non-causal DT systems can be realized. It is trivial to make an acausal FIR system causal by adding delays. It is even possible to make acausal IIR systems. Non-stable systems can be built and can be useful in many circumstances. Even non-real systems can be built and are very useful in many contexts.\n\nA discrete-time LTI system is causal if the current value of the output depends on only the current value and past values of the input., A necessary and sufficient condition for causality is\n\nwhere formula_143 is the impulse response. It is not possible in general to determine causality from the Z transform, because the inverse transform is not unique. When a region of convergence is specified, then causality can be determined.\n\nA system is bounded input, bounded output stable (BIBO stable) if, for every bounded input, the output is finite. Mathematically, if\n\nimplies that\n\n(that is, if bounded input implies bounded output, in the sense that the maximum absolute values of formula_171 and formula_172 are finite), then the system is stable. A necessary and sufficient condition is that formula_143, the impulse response, satisfies\n\nIn the frequency domain, the region of convergence must contain the unit circle (i.e., the locus satisfying formula_175 for complex \"z\").\n\n", "related": "\n- Circulant matrix\n- Frequency response\n- Impulse response\n- System analysis\n- Green function\n- Signal-flow graph\n\n\n- ECE 209: Review of Circuits as LTI Systems – Short primer on the mathematical analysis of (electrical) LTI systems.\n- ECE 209: Sources of Phase Shift – Gives an intuitive explanation of the source of phase shift in two common electrical LTI systems.\n- JHU 520.214 Signals and Systems course notes. An encapsulated course on LTI system theory. Adequate for self teaching.\n- LTI system example: RC low-pass filter. Amplitude and phase response.\n"}
{"id": "11685115", "url": "https://en.wikipedia.org/wiki?curid=11685115", "title": "Overlap–add method", "text": "Overlap–add method\n\nIn signal processing, the Overlap–add method is an efficient way to evaluate the discrete convolution of a very long signal formula_1 with a finite impulse response (FIR) filter formula_2:\n\nwhere for \"m\" outside the region .\n\nThe concept is to divide the problem into multiple convolutions of \"h\"[\"n\"] with short segments of formula_1:\n\nwhere \"L\" is an arbitrary segment length. Then:\n\nand \"y\"[\"n\"] can be written as a sum of short convolutions:\n\nwhere the linear convolution formula_7 is zero outside the region . And for any parameter formula_8 it is equivalent to the \"N\"-point circular convolution of formula_9 with formula_10 in the .  The advantage is that the circular convolution can be computed more efficiently than linear convolution, according to the circular convolution theorem:\n\nwhere:\n- DFT and IDFT refer to the Discrete Fourier transform and its inverse, evaluated over \"N\" discrete points, and\n- is customarily chosen such that is an integer power-of-2, and the transforms are implemented with the FFT algorithm, for efficiency.\n\nThe following is a pseudocode of the algorithm:\n\nWhen the DFT and IDFT are implemented by the FFT algorithm, the pseudocode above requires about complex multiplications for the FFT, product of arrays, and IFFT. Each iteration produces output samples, so the number of complex multiplications per output sample is about:\n\nFor example, when M=201 and N=1024, equals 13.67, whereas direct evaluation of would require up to 201 complex multiplications per output sample, the worst case being when both x and h are complex-valued. Also note that for any given M, has a minimum with respect to N. Figure 2 is a graph of the values of N that minimize for a range of filter lengths (M).\n\nInstead of , we can also consider applying to a long sequence of length formula_11 samples. The total number of complex multiplications would be:\n\nComparatively, the number of complex multiplications required by the pseudocode algorithm is:\n\nHence the \"cost\" of the overlap–add method scales almost as formula_14 while the cost of a single, large circular convolution is almost formula_15. The two methods are also compared in Figure 3, created by Matlab simulation. The contours are lines of constant ratio of the times it takes to perform both methods. When the overlap-add method is faster, the ratio exceeds 1, and ratios as high as 3 are seen.\n\n", "related": "\n- Overlap–save method\n\n"}
{"id": "313416", "url": "https://en.wikipedia.org/wiki?curid=313416", "title": "Spectrum analyzer", "text": "Spectrum analyzer\n\nA spectrum analyzer measures the magnitude of an input signal versus frequency within the full frequency range of the instrument. The primary use is to measure the power of the spectrum of known and unknown signals. The input signal that most common spectrum analyzers measure is electrical; however, spectral compositions of other signals, such as acoustic pressure waves and optical light waves, can be considered through the use of an appropriate transducer. Spectrum analyzers for other types of signals also exist, such as optical spectrum analyzers which use direct optical techniques such as a monochromator to make measurements.\n\nBy analyzing the spectra of electrical signals, dominant frequency, power, distortion, harmonics, bandwidth, and other spectral components of a signal can be observed that are not easily detectable in time domain waveforms. These parameters are useful in the characterization of electronic devices, such as wireless transmitters.\n\nThe display of a spectrum analyzer has frequency on the horizontal axis and the amplitude displayed on the vertical axis. To the casual observer, a spectrum analyzer looks like an oscilloscope and, in fact, some lab instruments can function either as an oscilloscope or a spectrum analyzer.\n\nThe first spectrum analyzers, in the 1960s, were swept-tuned instruments.\n\nFollowing the discovery of the fast Fourier transform (FFT) in 1965, the first FFT-based analyzers were introduced in 1967.\n\nToday, there are three basic types of analyzer: the swept-tuned spectrum analyzer, the vector signal analyzer, and the real-time spectrum analyzer.\n\nSpectrum analyzer types are distinguished by the methods used to obtain the spectrum of a signal. There are swept-tuned and fast Fourier transform (FFT) based spectrum analyzers:\n\n- A \"swept-tuned\" analyzer uses a superheterodyne receiver to down-convert a portion of the input signal spectrum to the center frequency of a narrow band-pass filter, whose instantaneous output power is recorded or displayed as a function of time. By sweeping the receiver's center-frequency (using a voltage-controlled oscillator) through a range of frequencies, the output is also a function of frequency. But while the sweep centers on any particular frequency, it may be missing short-duration events at other frequencies.\n- An FFT analyzer computes a time-sequence of periodograms. \"FFT\" refers to a particular mathematical algorithm used in the process. This is commonly used in conjunction with a receiver and analog-to-digital converter. As above, the receiver reduces the center-frequency of a portion of the input signal spectrum, but the portion is not swept. The purpose of the receiver is to reduce the sampling rate that the analyzer must contend with. With a sufficiently low sample-rate, FFT analyzers can process all the samples (100% duty-cycle), and are therefore able to avoid missing short-duration events.\n\nSpectrum analyzers tend to fall into four form factors: benchtop, portable, handheld and networked.\n\nThis form factor is useful for applications where the spectrum analyzer can be plugged into AC power, which generally means in a lab environment or production/manufacturing area. Bench top spectrum analyzers have historically offered better performance and specifications than the portable or handheld form factor. Bench top spectrum analyzers normally have multiple fans (with associated vents) to dissipate heat produced by the processor. Due to their architecture, bench top spectrum analyzers typically weigh more than . Some bench top spectrum analyzers offer optional battery packs, allowing them to be used away from AC power. This type of analyzer is often referred to as a \"portable\" spectrum analyzer.\n\nThis form factor is useful for any applications where the spectrum analyzer needs to be taken outside to make measurements or simply carried while in use. Attributes that contribute to a useful portable spectrum analyzer include:\n- Optional battery-powered operation to allow the user to move freely outside.\n- Clearly viewable display to allow the screen to be read in bright sunlight, darkness or dusty conditions..\n- Light weight (usually less than ).\n\nThis form factor is useful for any application where the spectrum analyzer needs to be very light and small. Handheld analyzers usually offer a limited capability relative to larger systems. Attributes that contribute to a useful handheld spectrum analyzer include:\n- Very low power consumption.\n- Battery-powered operation while in the field to allow the user to move freely outside.\n- Very small size\n- Light weight (usually less than ).\n\nThis form factor does not include a display and these devices are designed to enable a new class of geographically-distributed spectrum monitoring and analysis applications. The key attribute is the ability to connect the analyzer to a network and monitor such devices across a network. While many spectrum analyzers have an Ethernet port for control, they typically lack efficient data transfer mechanisms and are too bulky or expensive to be deployed in such a distributed manner. Key applications for such devices include RF intrusion detection systems for secure facilities where wireless signaling is prohibited. As well cellular operators are using such analyzers to remotely monitor interference in licensed spectral bands. The distributed nature of such devices enable geo-location of transmitters, spectrum monitoring for dynamic spectrum access and many other such applications.\n\nKey attributes of such devices include:\n- Network-efficient data transfer\n- Low power consumption\n- The ability to synchronize data captures across a network of analyzers\n- Low cost to enable mass deployment.\n\nAs discussed above in types, a swept-tuned spectrum analyzer down-converts a portion of the input signal spectrum to the center frequency of a band-pass filter by sweeping the voltage-controlled oscillator through a range of frequencies, enabling the consideration of the full frequency range of the instrument.\n\nThe bandwidth of the band-pass filter dictates the resolution bandwidth, which is related to the minimum bandwidth detectable by the instrument. As demonstrated by the animation to the right, the smaller the bandwidth, the more spectral resolution. However, there is a trade-off between how quickly the display can update the full frequency span under consideration and the frequency resolution, which is relevant for distinguishing frequency components that are close together. For a swept-tuned architecture, this relation for sweep time is useful:\n\nWhere ST is sweep time in seconds, k is proportionality constant, Span is the frequency range under consideration in hertz, and RBW is the resolution bandwidth in Hertz.\nSweeping too fast, however, causes a drop in displayed amplitude and a shift in the displayed frequency.\n\nAlso, the animation contains both up- and down-converted spectra, which is due to a frequency mixer producing both sum and difference frequencies. The local oscillator feedthrough is due to the imperfect isolation from the IF signal path in the mixer.\n\nFor very weak signals, a pre-amplifier is used, although harmonic and intermodulation distortion may lead to the creation of new frequency components that were not present in the original signal.\n\nWith an FFT based spectrum analyzer, the frequency resolution is formula_2, the inverse of the time \"T\" over which the waveform is measured and Fourier transformed.\n\nWith Fourier transform analysis in a digital spectrum analyzer, it is necessary to sample the input signal with a sampling frequency formula_3 that is at least twice the bandwidth of the signal, due to the Nyquist limit. A Fourier transform will then produce a spectrum containing all frequencies from zero to formula_4. This can place considerable demands on the required analog-to-digital converter and processing power for the Fourier transform, making FFT based spectrum analyzers limited in frequency range.\nSince FFT based analyzers are only capable of considering narrow bands, one technique is to combine swept and FFT analysis for consideration of wide and narrow spans. This technique allows for faster sweep time.\n\nThis method is made possible by first down converting the signal, then digitizing the intermediate frequency and using superheterodyne or FFT techniques to acquire the spectrum.\n\nOne benefit of digitizing the intermediate frequency is the ability to use digital filters, which have a range of advantages over analog filters such as near perfect shape factors and improved filter settling time. Also, for consideration of narrow spans, the FFT can be used to increase sweep time without distorting the displayed spectrum.\n\nA realtime spectrum analyser does not have any such blind time—up to some maximum span, often called the \"realtime bandwidth\". The analyser is able to sample the incoming RF spectrum in the time domain and convert the information to the frequency domain using the FFT process. FFT's are processed in parallel, gapless and overlapped so there are no gaps in the calculated RF spectrum and no information is missed.\n\nIn a sense, any spectrum analyzer that has vector signal analyzer capability is a realtime analyzer. It samples data fast enough to satisfy Nyquist Sampling theorem and stores the data in memory for later processing. This kind of analyser is only realtime for the amount of data / capture time it can store in memory and still produces gaps in the spectrum and results during processing time.\n\nMinimizing distortion of information is important in all spectrum analyzers. The FFT process applies windowing techniques to improve the output spectrum due to producing less side lobes. The effect of windowing may also reduce the level of a signal where it is captured on the boundary between one FFT and the next. For this reason FFT's in a Realtime spectrum analyzer are overlapped. Overlapping rate is approximately 80%. An analyzer that utilises a 1024-point FFT process will re-use approximately 819 samples from the previous FFT process.\n\nThis is related to the sampling rate of the analyser and the FFT rate. It is also important for the realtime spectrum analyzer to give good level accuracy.\n\nExample: for an analyser with of realtime bandwidth (the maximum RF span that can be processed in realtime) approximately (complex) are needed. If the spectrum analyzer produces an FFT calculation is produced every For a FFT a full spectrum is produced approximately every This also gives us our overlap rate of 80% (20 μs − 4 μs) / 20 μs = 80%.\n\nRealtime spectrum analyzers are able to produce much more information for users to examine the frequency spectrum in more detail. A normal swept spectrum analyzer would produce max peak, min peak displays for example but a realtime spectrum analyzer is able to plot all calculated FFT's over a given period of time with the added colour-coding which represents how often a signal appears. For example, this image shows the difference between how a spectrum is displayed in a normal swept spectrum view and using a \"Persistence\" view on a realtime spectrum analyzer.\n\nRealtime spectrum analyzers are able to see signals hidden behind other signals. This is possible because no information is missed and the display to the user is the output of FFT calculations. An example of this can be seen on the right.\n\nIn a typical spectrum analyzer there are options to set the start, stop, and center frequency. The frequency halfway between the stop and start frequencies on a spectrum analyzer display is known as the center frequency. This is the frequency that is in the middle of the display's frequency axis. Span specifies the range between the start and stop frequencies. These two parameters allow for adjustment of the display within the frequency range of the instrument to enhance visibility of the spectrum measured.\n\nAs discussed in the operation section, the resolution bandwidth filter or RBW filter is the bandpass filter in the IF path. It's the bandwidth of the RF chain before the detector (power measurement device). It determines the RF noise floor and how close two signals can be and still be resolved by the analyzer into two separate peaks. Adjusting the bandwidth of this filter allows for the discrimination of signals with closely spaced frequency components, while also changing the measured noise floor. Decreasing the bandwidth of an RBW filter decreases the measured noise floor and vice versa. This is due to higher RBW filters passing more frequency components through to the envelope detector than lower bandwidth RBW filters, therefore a higher RBW causes a higher measured noise floor.\n\nThe video bandwidth filter or VBW filter is the low-pass filter directly after the envelope detector. It's the bandwidth of the signal chain after the detector. Averaging or peak detection then refers to how the digital storage portion of the device records samples—it takes several samples per time step and stores only one sample, either the average of the samples or the highest one. The video bandwidth determines the capability to discriminate between two different power levels. This is because a narrower VBW will remove noise in the detector output. This filter is used to \"smooth\" the display by removing noise from the envelope. Similar to the RBW, the VBW affects the sweep time of the display if the VBW is less than the RBW. If VBW is less than RBW, this relation for sweep time is useful:\n\nHere \"t\" is the sweep time, \"k\" is a dimensionless proportionality constant, \"f\" − \"f\" is the frequency range of the sweep, RBW is the resolution bandwidth, and VBW is the video bandwidth.\n\nWith the advent of digitally based displays, some modern spectrum analyzers use analog-to-digital converters to sample spectrum amplitude after the VBW filter. Since displays have a discrete number of points, the frequency span measured is also digitised. Detectors are used in an attempt to adequately map the correct signal power to the appropriate frequency point on the display. There are in general three types of detectors: sample, peak, and average\n- Sample detection – sample detection simply uses the midpoint of a given interval as the display point value. While this method does represent random noise well, it does not always capture all sinusoidal signals.\n- Peak detection – peak detection uses the maximum measured point within a given interval as the display point value. This insures that the maximum sinusoid is measured within the interval; however, smaller sinusoids within the interval may not be measured. Also, peak detection does not give a good representation of random noise.\n- Average detection – average detection uses all of the data points within the interval to consider the display point value. This is done by power (rms) averaging, voltage averaging, or log-power averaging.\n\nThe Displayed Average Noise Level (DANL) is just what it says it is—the average noise level displayed on the analyzer. This can either be with a specific resolution bandwidth (e.g. −120 dBm @1 kHz RBW), or normalized to 1 Hz (usually in dBm/Hz) e.g. −170 dBm(Hz).This is also called the sensitivity of the spectrum analyzer. If a signal level equal to the average noise level is fed there will be a 3 dB display. To increase the sensitivity of the spectrum analyzer a preamplifier with lower noise figure may be connected at the input of the spectrum analyzer. co\n\nSpectrum analyzers are widely used to measure the frequency response, noise and distortion characteristics of all kinds of radio-frequency (RF) circuitry, by comparing the input and output spectra. For example, in RF mixers, spectrum analyzer is used to find the levels of third order inter-modulation products and conversion loss. In RF oscillators, spectrum analyzer is used to find the levels of different harmonics.\n\nIn telecommunications, spectrum analyzers are used to determine occupied bandwidth and track interference sources. For example, cell planners use this equipment to determine interference sources in the GSM frequency bands and UMTS frequency bands.\n\nIn EMC testing, a spectrum analyzer is used for basic precompliance testing; however, it can not be used for full testing and certification. Instead, an EMI receiver is used.\n\nA spectrum analyzer is used to determine whether a wireless transmitter is working according to defined standards for purity of emissions. Output signals at frequencies other than the intended communications frequency appear as vertical lines (pips) on the display. A spectrum analyzer is also used to determine, by direct observation, the bandwidth of a digital or analog signal.\n\nA spectrum analyzer interface is a device that connects to a wireless receiver or a personal computer to allow visual detection and analysis of electromagnetic signals over a defined band of frequencies. This is called panoramic reception and it is used to determine the frequencies of sources of interference to wireless networking equipment, such as Wi-Fi and wireless routers.\n\nSpectrum analyzers can also be used to assess RF shielding. RF shielding is of particular importance for the siting of a magnetic resonance imaging machine since stray RF fields would result in artifacts in an MR image.\n\nSpectrum analysis can be used at audio frequencies to analyse the harmonics of an audio signal. A typical application is to measure the distortion of a nominally sinewave signal; a very-low-distortion sinewave is used as the input to equipment under test, and a spectrum analyser can examine the output, which will have added distortion products, and determine the percentage distortion at each harmonic of the fundamental. Such analysers were at one time described as \"wave analysers\". Analysis can be carried out by a general-purpose digital computer with a sound card selected for suitable performance and appropriate software. Instead of using a low-distortion sinewave, the input can be subtracted from the output, attenuated and phase-corrected, to give only the added distortion and noise, which can be analysed.\n\nAn alternative technique, total harmonic distortion measurement, cancels out the fundamental with a notch filter and measures the total remaining signal, which is total harmonic distortion plus noise; it does not give the harmonic-by-harmonic detail of an analyser.\n\nSpectrum analyzers are also used by audio engineers to assess their work. In these applications, the spectrum analyzer will show volume levels of frequency bands across the typical range of human hearing, rather than displaying a wave. In live sound applications, engineers can use them to pinpoint feedback.\n\nAn optical spectrum analyzer uses reflective or refractive techniques to separate out the wavelengths of light. An electro-optical detector is used to measure the intensity of the light, which is then normally displayed on a screen in a similar manner to a radio- or audio-frequency spectrum analyzer.\n\nThe input to an optical spectrum analyzer may be simply via an aperture in the instrument's case, an optical fiber or an optical connector to which a fiber-optic cable can be attached.\n\nDifferent techniques exist for separating out the wavelengths. One method is to use a monochromator, for example a Czerny–Turner design, with an optical detector placed at the output slit. As the grating in the monochromator moves, bands of different frequencies (colors) are 'seen' by the detector, and the resulting signal can then be plotted on a display. More precise measurements (down to MHz in the optical spectrum) can be made with a scanning Fabry–Pérot interferometer along with analog or digital control electronics, which sweep the resonant frequency of an optically resonant cavity using a voltage ramp to piezoelectric motor that varies the distance between two highly reflective mirrors. A sensitive photodiode embedded in the cavity provides an intensity signal, which is plotted against the ramp voltage to produce a visual representation of the optical power spectrum.\n\nThe frequency response of optical spectrum analyzers tends to be relatively limited, e.g. (near-infrared), depending on the intended purpose, although (somewhat) wider-bandwidth general purpose instruments are available.\n\nA vibration spectrum analyzer allows to analyze vibration amplitudes at various component frequencies, In this way, vibration occurring at specific frequencies can be identified and tracked. Since particular machinery problems generate vibration at specific frequencies, machinery faults can be detected or diagnosed. Vibration Spectrum Analyzers use the signal from different types of sensor, such as: accelerometers, velocity transducers and proximity sensors. The uses of a vibration spectrum analyzer in machine condition monitoring allows to detect and identify machine faults such as: rotor imbalance, shaft misalignment, mechanical looseness, bearing defects, among others. Vibration analysis can also be used in structures to identify structural resonances or to perform modal analysis.\n\n", "related": "\n- Electrical measurements\n- Electromagnetic spectrum\n- Measuring receiver\n- Radio-frequency sweep\n- Spectral leakage\n- Spectral music\n- Radio spectrum scope\n- Stationary-wave integrated Fourier-transform spectrometry\n\n- Sri Welaratna, \"\", \"Sound and Vibration\" (January 1997, 30th anniversary issue). A historical review of hardware spectrum-analyzer devices.\n"}
{"id": "28227121", "url": "https://en.wikipedia.org/wiki?curid=28227121", "title": "Variance Adaptive Quantization", "text": "Variance Adaptive Quantization\n\nVariance Adaptive Quantization (VAQ) is a video encoding algorithm that was first introduced in the open source video encoder x264. According to Xvid Builds FAQ: \"It's an algorithm that tries to optimally choose a quantizer for each macroblock using advanced math algorithms.\" It was later ported to programs which encode video content in other video standards, like MPEG-4 ASP or MPEG-2.\n\nIn the case of Xvid, the algorithm is intended to make up for the earlier limitations in its Adaptive Quantization mode. The first Xvid library containing this improvement was released in February 2008.\n\n- Implementation of variance-based adaptive quantization in x264\n- The intuitive justification (handwaving)\n", "related": "NONE"}
{"id": "27960185", "url": "https://en.wikipedia.org/wiki?curid=27960185", "title": "Radio-frequency sweep", "text": "Radio-frequency sweep\n\nRadio frequency sweep or \"frequency sweep\" or \"RF sweep\" refer to scanning a radio frequency band for detecting signals being transmitted there. This is implemented using a radio receiver having a tunable receiving frequency. As the frequency of the receiver is changed to scan (sweep) a desired frequency band, a display indicates the power of the signals received at each frequency.\n\nA spectrum analyzer is a standard instrument used for RF sweep. It includes an electronically tunable receiver and a display. The display presents measured power (y axis) vs frequency (x axis).\nThe power spectrum display is a two-dimensional display of measured power vs. frequency. The power may be either in linear units, or logarithmic units (dBm). Usually the logarithmic display is more useful, because it presents a larger dynamic range with better detail at each value. An RF sweep relates to a receiver which changes its frequency of operation continuously from a minimum frequency to a maximum (or from maximum to minimum). Usually the sweep is performed at a fixed, controllable rate, for example 5 MHz/sec.\n\nSome systems use frequency hopping, switching from one frequency of operation to another. One method of CDMA uses frequency hopping. Usually frequency hopping is performed in a random or pseudo-random pattern.\n\nFrequency sweeps may be used by regulatory agencies to monitor the radio spectrum, to ensure that users only transmit according to their licenses. The FCC for example controls and monitors the use of the spectrum in the U.S. In testing of new electronic devices, a frequency sweep may be done to measure the performance of electronic components or systems. For example, RF oscillators are measured for phase noise, harmonics and spurious signals; computers for consumer sale are tested to avoid radio frequency interference with radio systems. Portable sweep equipment may be used to detect some types of covert listening device (bugs).\n\nIn professional audio, the optimum use of wireless microphones and wireless intercoms may require performing a sweep of the local radio spectrum, especially if many wireless devices are being used simultaneously. The sweep is generally limited in bandwidth to only the operating bandwidth of the wireless devices. For instance, at American Super Bowl games, audio engineers monitor (sweep) the radio spectrum in real time to make certain that all local wireless microphones are operating at previously agreed-upon and coordinated frequencies.\n\n", "related": "\n- Measuring receiver\n- Spectrum management\n- Technical surveillance counter-measures\n- Donald G. Fink, Donald Christiansen – Electronic Engineer's Handbook, Second edition.\n- Ulrich L. Rohde, Jerry C. Whitaker, T.T.N. Bucher – Communications Receivers: Principles and Design, Second edition.\n"}
{"id": "574024", "url": "https://en.wikipedia.org/wiki?curid=574024", "title": "Hilbert transform", "text": "Hilbert transform\n\nIn mathematics and in signal processing, the Hilbert transform is a specific linear operator that takes a function, \"u\"(\"t\") of a real variable and produces another function of a real variable \"H\"(\"u\")(\"t\"). This linear operator is given by convolution with the function formula_1:\n\nthe improper integral being understood in the principal value sense. The Hilbert transform has a particularly simple representation in the frequency domain: it imparts a phase shift of -90° to every Fourier component of a function. For example, the Hilbert transform of formula_3, where ω > 0, is formula_4.\n\nThe Hilbert transform is important in signal processing, where it derives the analytic representation of a real-valued signal \"u\"(\"t\"). Specifically, the Hilbert transform of \"u\" is its harmonic conjugate \"v\", a function of the real variable \"t\" such that the complex-valued function admits an extension to the complex upper half-plane satisfying the Cauchy–Riemann equations. The Hilbert transform was first introduced by David Hilbert in this setting, to solve a special case of the Riemann–Hilbert problem for analytic functions.\n\nThe Hilbert transform of \"u\" can be thought of as the convolution of \"u\"(\"t\") with the function \"h\"(\"t\") = 1/(\"t\"), known as the Cauchy kernel. Because \"h\"(\"t\") is not integrable, the integral defining the convolution does not always converge. Instead, the Hilbert transform is defined using the Cauchy principal value (denoted here by p.v.). Explicitly, the Hilbert transform of a function (or signal) \"u\"(\"t\") is given by\n\nprovided this integral exists as a principal value. This is precisely the convolution of \"u\" with the tempered distribution p.v. 1/\"t\" (due to ; see ). Alternatively, by changing variables, the principal value integral can be written explicitly as\n\nWhen the Hilbert transform is applied twice in succession to a function \"u\", the result is negative \"u\":\n\nprovided the integrals defining both iterations converge in a suitable sense. In particular, the inverse transform is −\"H\". This fact can most easily be seen by considering the effect of the Hilbert transform on the Fourier transform of \"u\"(\"t\") (see Relationship with the Fourier transform below).\n\nFor an analytic function in upper half-plane, the Hilbert transform describes the relationship between the real part and the imaginary part of the boundary values. That is, if \"f\"(\"z\") is analytic in the plane Im \"z\" > 0, and \"u\"(\"t\") = Re \"f\"(\"t\" + 0·\"i\" ), then Im \"f\"(\"t\" + 0·\"i\" ) = \"H\"(\"u\")(\"t\") up to an additive constant, provided this Hilbert transform exists.\n\nIn signal processing the Hilbert transform of \"u\"(\"t\") is commonly denoted by formula_8 (e.g., ). However, in mathematics, this notation is already extensively used to denote the Fourier transform of \"u\"(\"t\") (e.g., ). Occasionally, the Hilbert transform may be denoted by formula_9. Furthermore, many sources define the Hilbert transform as the negative of the one defined here (e.g., ).\n\nThe Hilbert transform arose in Hilbert's 1905 work on a problem Riemann posed concerning analytic functions (; ), which has come to be known as the Riemann–Hilbert problem. Hilbert's work was mainly concerned with the Hilbert transform for functions defined on the circle (; ). Some of his earlier work related to the Discrete Hilbert Transform dates back to lectures he gave in Göttingen. The results were later published by Hermann Weyl in his dissertation . Schur improved Hilbert's results about the discrete Hilbert transform and extended them to the integral case . These results were restricted to the spaces \"L\" and ℓ. In 1928, Marcel Riesz proved that the Hilbert transform can be defined for \"u\" in \"L\"(R) for 1 ≤ \"p\" < ∞, that the Hilbert transform is a bounded operator on \"L\"(R) for 1 < \"p\" < ∞, and that similar results hold for the Hilbert transform on the circle as well as the discrete Hilbert transform . The Hilbert transform was a motivating example for Antoni Zygmund and Alberto Calderón during their study of singular integrals . Their investigations have played a fundamental role in modern harmonic analysis. Various generalizations of the Hilbert transform, such as the bilinear and trilinear Hilbert transforms are still active areas of research today.\n\nThe Hilbert transform is a multiplier operator . The multiplier of \"H\" is σ(ω) = −\"i\" sgn(ω), where sgn is the signum function. Therefore:\n\nwhere formula_11 denotes the Fourier transform. Since sgn(\"x\") = sgn(2\"x\"), it follows that this result applies to the three common definitions of formula_12.\n\nBy Euler's formula,\n\nTherefore, \"H\"(\"u\")(\"t\") has the effect of shifting the phase of the negative frequency components of \"u\"(\"t\") by +90° (/2 radians) and the phase of the positive frequency components by −90°. And \"i\"·\"H\"(\"u\")(\"t\") has the effect of restoring the positive frequency components while shifting the negative frequency ones an additional +90°, resulting in their negation.\n\nWhen the Hilbert transform is applied twice, the phase of the negative and positive frequency components of \"u\"(\"t\") are respectively shifted by +180° and −180°, which are equivalent amounts. The signal is negated; i.e., \"H\"(\"H\"(\"u\")) = −\"u\", because\n\nIn the following table, the frequency parameter formula_15 is real.\n\n- Notes\n\nAn extensive table of Hilbert transforms is available ().\nNote that the Hilbert transform of a constant is zero.\n\nIt is by no means obvious that the Hilbert transform is well-defined at all, as the improper integral defining it must converge in a suitable sense. However, the Hilbert transform is well-defined for a broad class of functions, namely those in \"L\"(R) for 1 < \"p\" < ∞.\n\nMore precisely, if \"u\" is in \"L\"(R) for 1 < \"p\" < ∞, then the limit defining the improper integral\n\nexists for almost every \"t\". The limit function is also in \"L\"(R) and is in fact the limit in the mean of the improper integral as well. That is,\n\nas ε→0 in the \"L\"-norm, as well as pointwise almost everywhere, by the Titchmarsh theorem .\n\nIn the case \"p\" = 1, the Hilbert transform still converges pointwise almost everywhere, but may itself fail to be integrable, even locally . In particular, convergence in the mean does not in general happen in this case. The Hilbert transform of an \"L\" function does converge, however, in \"L\"-weak, and the Hilbert transform is a bounded operator from \"L\" to \"L\" . (In particular, since the Hilbert transform is also a multiplier operator on \"L\", Marcinkiewicz interpolation and a duality argument furnishes an alternative proof that \"H\" is bounded on \"L\".)\n\nIf 1 < \"p\" < ∞, then the Hilbert transform on \"L\"(R) is a bounded linear operator, meaning that there exists a constant \"C\" such that\n\nfor all \"u\"∈\"L\"(R). This theorem is due to ; see also .\nThe best constant \"C\" is given by\n\nThis result is due to ; see also . The same best constants hold for the periodic Hilbert transform.\n\nThe boundedness of the Hilbert transform implies the \"L\"(R) convergence of the symmetric partial sum operator \n\nto \"f\" in \"L\"(R), see for example .\n\nThe Hilbert transform is an anti-self adjoint operator relative to the duality pairing between \"L\"(R) and the dual space \"L\"(R), where \"p\" and \"q\" are Hölder conjugates and 1 < \"p\",\"q\" < ∞. Symbolically,\n\nfor \"u\" ∈ \"L\"(R) and \"v\" ∈ \"L\"(R) .\n\nThe Hilbert transform is an anti-involution , meaning that\n\nprovided each transform is well-defined. Since \"H\" preserves the space \"L\"(R), this implies in particular that the Hilbert transform is invertible on \"L\"(R), and that\n\nBecause \"H\"=−\"Id\" on the real Banach space of \"real\"-valued functions in \"L\"(R), the Hilbert transform defines a linear complex structure on this Banach space. In particular, when \"p\"=2, the Hilbert transform gives the Hilbert space of real-valued functions in \"L\"(R) the structure of a \"complex\" Hilbert space.\n\nThe (complex) eigenstates of the Hilbert transform admit representations as holomorphic functions in the upper and lower half-planes in the Hardy space \"H\" by the Paley–Wiener theorem.\n\nFormally, the derivative of the Hilbert transform is the Hilbert transform of the derivative, i.e. these two linear operators commute:\n\nIterating this identity,\n\nThis is rigorously true as stated provided \"u\" and its first \"k\" derivatives belong to \"L\"(R) . One can check this easily in the frequency domain, where differentiation becomes multiplication by ω.\n\nThe Hilbert transform can formally be realized as a convolution with the tempered distribution \n\nThus formally,\n\nHowever, \"a priori\" this may only be defined for \"u\" a distribution of compact support. It is possible to work somewhat rigorously with this since compactly supported functions (which are distributions \"a fortiori\") are dense in \"L\". Alternatively, one may use the fact that \"h\"(\"t\") is the distributional derivative of the function log|\"t\"|/; to wit\n\nFor most operational purposes the Hilbert transform can be treated as a convolution. For example, in a formal sense, the Hilbert transform of a convolution is the convolution of the Hilbert transform on either factor:\n\nThis is rigorously true if \"u\" and \"v\" are compactly supported distributions since, in that case,\n\nBy passing to an appropriate limit, it is thus also true if \"u\" ∈ \"L\" and \"v\" ∈ \"L\" provided\n\na theorem due to .\n\nThe Hilbert transform has the following invariance properties on \"L\"(R).\n- It commutes with translations. That is, it commutes with the operators \"T\"ƒ(\"x\") = ƒ(\"x\" + \"a\") for all \"a\" in R.\n- It commutes with positive dilations. That is it commutes with the operators \"M\"ƒ(\"x\") = ƒ(λ\"x\") for all λ > 0.\n- It anticommutes with the reflection \"R\"ƒ(\"x\") = ƒ(−x).\n\nUp to a multiplicative constant, the Hilbert transform is the only bounded operator on \"L\" with these properties .\n\nIn fact there is a larger group of operators commuting with the Hilbert transform. The group SL(2,R) acts by unitary operators \"U\" on the space \"L\"(R) by the formula\n\nThis unitary representation is an example of a principal series representation of SL(2,R). In this case it is reducible, splitting as the orthogonal sum of two invariant subspaces, Hardy space \"H\"(R) and its conjugate. These are the spaces of \"L\" boundary values of holomorphic functions on the upper and lower halfplanes. \"H\"(R) and its conjugate consist of exactly those \"L\" functions with Fourier transforms vanishing on the negative and positive parts of the real axis respectively. Since the Hilbert transform is equal to \"H\" = −\"i\" (2\"P\" − \"I\"), with \"P\" being the orthogonal projection from \"L\"(R) onto \"H\"(R), it follows that \"H\"(R) and its orthogonal are eigenspaces of \"H\" for the eigenvalues ± \"i\". In other words, \"H\" commutes with the operators \"U\". The restrictions of the operators \"U\" to \"H\"(R) and its conjugate give irreducible representations of SL(2,R)—the so-called limit of discrete series representations.\n\nIt is further possible to extend the Hilbert transform to certain spaces of distributions . Since the Hilbert transform commutes with differentiation, and is a bounded operator on \"L\", \"H\" restricts to give a continuous transform on the inverse limit of Sobolev spaces:\n\nThe Hilbert transform can then be defined on the dual space of formula_34, denoted formula_35, consisting of \"L\" distributions. This is accomplished by the duality pairing:<br>\nFor formula_36, define:\n\nIt is possible to define the Hilbert transform on the space of tempered distributions as well by an approach due to , but considerably more care is needed because of the singularity in the integral.\n\nThe Hilbert transform can be defined for functions in \"L\"(R) as well, but it requires some modifications and caveats. Properly understood, the Hilbert transform maps \"L\"(R) to the Banach space of bounded mean oscillation (BMO) classes.\n\nInterpreted naïvely, the Hilbert transform of a bounded function is clearly ill-defined. For instance, with \"u\" = sgn(\"x\"), the integral defining \"H\"(\"u\") diverges almost everywhere to ±∞. To alleviate such difficulties, the Hilbert transform of an \"L\"-function is therefore defined by the following regularized form of the integral\n\nwhere as above \"h\"(\"x\") = 1/\"x\" and\n\nThe modified transform \"H\" agrees with the original transform on functions of compact support by a general result of ; see . The resulting integral, furthermore, converges pointwise almost everywhere, and with respect to the BMO norm, to a function of bounded mean oscillation.\n\nA deep result of and is that a function is of bounded mean oscillation if and only if it has the form \"ƒ\" + \"H\"(\"g\") for some \"ƒ\", \"g\" ∈ \"L\"(R).\n\nThe Hilbert transform can be understood in terms of a pair of functions \"f\"(\"x\") and \"g\"(\"x\") such that the function\nis the boundary value of a holomorphic function \"F\"(\"z\") in the upper half-plane . Under these circumstances, if \"f\" and \"g\" are sufficiently integrable, then one is the Hilbert transform of the other.\n\nSuppose that \"f\" ∈ \"L\"(R). Then, by the theory of the Poisson integral, \"f\" admits a unique harmonic extension into the upper half-plane, and this extension is given by\n\nwhich is the convolution of \"f\" with the Poisson kernel\n\nFurthermore, there is a unique harmonic function \"v\" defined in the upper half-plane such that \"F\"(\"z\") = \"u\"(\"z\") + i\"v\"(\"z\") is holomorphic and\n\nThis harmonic function is obtained from \"f\" by taking a convolution with the conjugate Poisson kernel\n\nThus\n\nIndeed, the real and imaginary parts of the Cauchy kernel are\n\nso that \"F\" = \"u\" + i\"v\" is holomorphic by Cauchy's integral formula.\n\nThe function \"v\" obtained from \"u\" in this way is called the harmonic conjugate of \"u\". The (non-tangential) boundary limit of \"v\"(\"x\",\"y\") as \"y\" → 0 is the Hilbert transform of \"f\". Thus, succinctly,\n\nTitchmarsh's theorem, named for Edward Charles Titchmarsh who included it in his 1937 work, makes precise the relationship between the boundary values of holomorphic functions in the upper half-plane and the Hilbert transform . It gives necessary and sufficient conditions for a complex-valued square-integrable function \"F\"(\"x\") on the real line to be the boundary value of a function in the Hardy space \"H\"(\"U\") of holomorphic functions in the upper half-plane \"U\".\n\nThe theorem states that the following conditions for a complex-valued square-integrable function \"F\" : R → C are equivalent:\n\n- \"F\"(\"x\") is the limit as \"z\" → \"x\" of a holomorphic function \"F\"(\"z\") in the upper half-plane such that\n\n- The real and imaginary parts of F(x) are Hilbert transforms of each other.\n- The Fourier transform formula_49 vanishes for \"x\" < 0.\n\nA weaker result is true for functions of class \"L\" for \"p\" > 1 . Specifically, if \"F\"(\"z\") is a holomorphic function such that\n\nfor all \"y\", then there is a complex-valued function \"F\"(\"x\") in \"L\"(R) such that \"F\"(\"x\" + i\"y\") → \"F\"(\"x\") in the \"L\" norm as \"y\" → 0 (as well as holding pointwise almost everywhere). Furthermore,\n\nwhere ƒ is a real-valued function in \"L\"(R) and \"g\" is the Hilbert transform (of class \"L\") of ƒ.\n\nThis is not true in the case \"p\" = 1. In fact, the Hilbert transform of an \"L\" function ƒ need not converge in the mean to another \"L\" function. Nevertheless, , the Hilbert transform of ƒ does converge almost everywhere to a finite function \"g\" such that\n\nThis result is directly analogous to one by Andrey Kolmogorov for Hardy functions in the disc . Although usually called Titchmarsh's theorem, the result aggregates much work of others, including \nHardy, Paley and Wiener (see Paley–Wiener theorem) as well as work by Riesz, Hille, and Tamarkin (see section 4.22 in ).\n\nOne form of the Riemann–Hilbert problem seeks to identify pairs of functions \"F\" and \"F\" such that \"F\" is holomorphic on the upper half-plane and \"F\" is holomorphic on the lower half-plane, such that for \"x\" along the real axis,\n\nwhere \"f\"(\"x\") is some given real-valued function of \"x\" ∈ R. The left-hand side of this equation may be understood either as the difference of the limits of \"F\" from the appropriate half-planes, or as a hyperfunction distribution. Two functions of this form are a solution of the Riemann–Hilbert problem.\n\nFormally, if \"F\" solve the Riemann–Hilbert problem\n\nthen the Hilbert transform of \"f\"(\"x\") is given by\n\nFor a periodic function \"f\" the circular Hilbert transform is defined:\n\nThe circular Hilbert transform is used in giving a characterization of Hardy space and in the study of the conjugate function in Fourier series. The kernel, \n\nThe Hilbert kernel (for the circular Hilbert transform) can be obtained by making the Cauchy kernel 1/\"x\" periodic. More precisely, for \"x\" ≠ 0\n\nMany results about the circular Hilbert transform may be derived from the corresponding results for the Hilbert transform from this correspondence.\n\nAnother more direct connection is provided by the Cayley transform \"C\"(\"x\") = (\"x\" – \"i\") / (\"x\" + \"i\"), which carries the real line onto the circle and the upper half plane onto the unit disk. It induces a unitary map\n\nof \"L\"(T) onto \"L\"(R). The operator \"U\" carries the Hardy space \"H\"(T) onto the Hardy space \"H\"(R).\n\nBedrosian's theorem states that the Hilbert transform of the product of a low-pass and a high-pass signal with non-overlapping spectra is given by the product of the low-pass signal and the Hilbert transform of the high-pass signal, or\n\nwhere \"f\" and \"f\" are the low- and high-pass signals respectively .\n\nAmplitude modulated signals are modeled as the product of a bandlimited \"message\" waveform, \"u\"(\"t\"), and a sinusoidal \"carrier\":\n\nWhen \"u\"(\"t\") has no frequency content above the carrier frequency, formula_62 then by Bedrosian's theorem:\n\nIn the context of signal processing, the conjugate function interpretation of the Hilbert transform, discussed above, gives the analytic representation of a signal \"u\"(\"t\"):\n\nwhich is a holomorphic function in the upper half plane.\n\nFor the narrowband model (above), the analytic representation is:\n\nThis complex heterodyne operation shifts all the frequency components of \"u\"(\"t\") above 0 Hz. In that case, the imaginary part of the result is a Hilbert transform of the real part. This is an indirect way to produce Hilbert transforms.\n\nThe form:\n\nis called angle modulation, which includes both phase modulation and frequency modulation. The instantaneous frequency is  formula_67  For sufficiently large ω, compared to  formula_68:\n\nand:\n\nWhen \"u\"(\"t\") in   is also an analytic representation (of a message waveform), that is:\n\nthe result is single-sideband modulation:\n\nwhose transmitted component is:\n\nThe function \"h\" with \"h\"(\"t\") = 1/(\"t\") is a non-causal filter and therefore cannot be implemented as is, if \"u\" is a time-dependent signal. If \"u\" is a function of a non-temporal variable (e.g., spatial) the non-causality might not be a problem. The filter is also of infinite support, which may be a problem in certain applications. Another issue relates to what happens with the zero frequency (DC), which can be avoided by assuring that \"s\" does not contain a DC-component.\n\nA practical implementation in many cases implies that a finite support filter, which in addition is made causal by means of a suitable delay, is used to approximate the computation. The approximation may also imply that only a specific frequency range is subject to the characteristic phase shift related to the Hilbert transform. See also quadrature filter.\n\nFor a discrete function,  formula_74  with discrete-time Fourier transform (DTFT),  formula_75  and discrete Hilbert transform  formula_76  the DTFT of  formula_77  in the region  − < ω <  is given by:\n\nThe inverse DTFT, using the convolution theorem, is:\n\nwhere:\n\nwhich is an infinite impulse response (IIR). When the convolution is performed numerically, an FIR approximation is substituted for \"h\"[\"n\"], as shown in Figure 1. An FIR filter with an odd number of anti-symmetric coefficients is called \"Type III\", which inherently exhibits responses of zero magnitude at frequencies 0 and Nyquist, resulting in this case in a bandpass filter shape. A \"Type IV\" design (even number of anti-symmetric coefficients) is shown in Figure 2. Since the magnitude response at Nyquist does not drop out, it approximates an ideal Hilbert transformer a little better than the odd-tap filter. However:\n- A typical (i.e. properly filtered and sampled) \"u\"[\"n\"] sequence has no useful components at the Nyquist frequency.\n- The Type IV impulse response requires a ½ sample shift in the h[n] sequence. That causes the zero-valued coefficients to become non-zero, as seen in Figure 2. So a Type III design is potentially twice as efficient as Type IV.\n- The group delay of a Type III design is an integer number of samples, which facilitates aligning formula_77 with formula_74 to create an analytic signal. The group delay of Type IV is halfway between two samples.\n\nThe MATLAB function, hilbert(u,N), convolves a u[n] sequence with the periodic summation:\n\nand returns one cycle (N samples) of the periodic result in the imaginary part of a complex-valued output sequence. The convolution is implemented in the frequency domain as the product of the array  formula_84  with samples of the −i•sgn(ω) distribution (whose real and imaginary components are all just 0 or ±1). Figure 3 compares a half-cycle of \"h\"[\"n\"] with an equivalent length portion of \"h\"[\"n\"]. Given an FIR approximation for  formula_85 denoted by  formula_86  substituting  formula_87  for the −i•sgn(ω) samples results in an FIR version of the convolution.\n\nThe real part of the output sequence is the original input sequence, so that the complex output is an analytic representation of \"u\"[\"n\"]. When the input is a segment of a pure cosine, the resulting convolution for two different values of N is depicted in Figure 4 (red and blue plots). Edge effects prevent the result from being a pure sine function (green plot). Since \"h\"[\"n\"] is not an FIR sequence, the theoretical extent of the effects is the entire output sequence. But the differences from a sine function diminish with distance from the edges. Parameter N is the output sequence length. If it exceeds the length of the input sequence, the input is modified by appending zero-valued elements. In most cases, that reduces the magnitude of the differences. But their duration is dominated by the inherent rise and fall times of the \"h\"[\"n\"] impulse response.\n\nAn appreciation for the edge effects is important when a method called overlap-save is used to perform the convolution on a long \"u\"[\"n\"] sequence. Segments of length N are convolved with the periodic function:\n\nWhen the duration of non-zero values of  formula_89  is M < N, the output sequence includes \"N\" − \"M\" + 1 samples of  formula_90  \"M\"-1 outputs are discarded from each block of N, and the input blocks are overlapped by that amount to prevent gaps.\n\nFigure 5 is an example of using both the IIR hilbert() function and the FIR approximation. In the example, a sine function is created by computing the Discrete Hilbert transform of a cosine function, which was processed in four overlapping segments, and pieced back together. As the FIR result (blue) shows, the distortions apparent in the IIR result (red) are not caused by the difference between \"h\"[\"n\"] and \"h\"[\"n\"] (green and red in Fig 3). The fact that \"h\"[\"n\"] is tapered (\"windowed\") is actually helpful in this context. The real problem is that it's not windowed enough. Effectively, \"M\" = \"N\", whereas the overlap-save method needs \"M\" < \"N\".\n\nThe number theoretic Hilbert transform is an extension () of the discrete Hilbert transform to integers modulo an appropriate prime number. In this it follows the generalization of discrete Fourier transform to number theoretic transforms. The number theoretic Hilbert transform can be used to generate sets of orthogonal discrete sequences().\n\n", "related": "\n- Analytic signal\n- Harmonic conjugate\n- Hilbert spectroscopy\n- Hilbert transform in the complex plane\n- Hilbert–Huang transform\n- Kramers–Kronig relation\n- Riesz transform\n- Single-sideband signal\n- Singular integral operators of convolution type\n\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n- .\n\n- Derivation of the boundedness of the Hilbert transform\n- Mathworld Hilbert transform — Contains a table of transforms\n- Analytic Signals and Hilbert Transform Filters\n- a student level summary of the Hilbert transformation.\n- an entry level introduction to Hilbert transformation.\n"}
{"id": "1156527", "url": "https://en.wikipedia.org/wiki?curid=1156527", "title": "Detection theory", "text": "Detection theory\n\nDetection theory or signal detection theory is a means to measure the ability to differentiate between information-bearing patterns (called stimulus in living organisms, signal in machines) and random patterns that distract from the information (called noise, consisting of background stimuli and random activity of the detection machine and of the nervous system of the operator). In the field of electronics, the separation of such patterns from a disguising background is referred to as signal recovery.\n\nAccording to the theory, there are a number of determiners of how a detecting system will detect a signal, and where its threshold levels will be. The theory can explain how changing the threshold will affect the ability to discern, often exposing how adapted the system is to the task, purpose or goal at which it is aimed. When the detecting system is a human being, characteristics such as experience, expectations, physiological state (e.g., fatigue) and other factors can affect the threshold applied. For instance, a sentry in wartime might be likely to detect fainter stimuli than the same sentry in peacetime due to a lower criterion, however they might also be more likely to treat innocuous stimuli as a threat.\n\nMuch of the early work in detection theory was done by radar researchers. By 1954, the theory was fully developed on the theoretical side as described by Peterson, Birdsall and Fox and the foundation for the psychological theory was made by Wilson P. Tanner, David M. Green, and John A. Swets, also in 1954.\nDetection theory was used in 1966 by John A. Swets and David M. Green for psychophysics. Green and Swets criticized the traditional methods of psychophysics for their inability to discriminate between the real sensitivity of subjects and their (potential) response biases.\n\nDetection theory has applications in many fields such as diagnostics of any kind, quality control, telecommunications, and psychology. The concept is similar to the signal to noise ratio used in the sciences and confusion matrices used in artificial intelligence. It is also usable in alarm management, where it is important to separate important events from background noise.\n\nSignal detection theory (SDT) is used when psychologists want to measure the way we make decisions under conditions of uncertainty, such as how we would perceive distances in foggy conditions or during eyewitness identification. SDT assumes that the decision maker is not a passive receiver of information, but an active decision-maker who makes difficult perceptual judgments under conditions of uncertainty. In foggy circumstances, we are forced to decide how far away from us an object is, based solely upon visual stimulus which is impaired by the fog. Since the brightness of the object, such as a traffic light, is used by the brain to discriminate the distance of an object, and the fog reduces the brightness of objects, we perceive the object to be much farther away than it actually is (see also decision theory). According to SDT, during eyewitness identifications, witnesses base their decision as to whether a suspect is the culprit or not based on their perceived level of familiarity with the suspect.\n\nTo apply signal detection theory to a data set where stimuli were either present or absent, and the observer categorized each trial as having the stimulus present or absent, the trials are sorted into one of four categories:\n\nBased on the proportions of these types of trials, numerical estimates of sensitivity can be obtained with statistics like the sensitivity index \"d\"' and A', and response bias can be estimated with statistics like c and β.\n\nSignal detection theory can also be applied to memory experiments, where items are presented on a study list for later testing. A test list is created by combining these 'old' items with novel, 'new' items that did not appear on the study list. On each test trial the subject will respond 'yes, this was on the study list' or 'no, this was not on the study list'. Items presented on the study list are called Targets, and new items are called Distractors. Saying 'Yes' to a target constitutes a Hit, while saying 'Yes' to a distractor constitutes a False Alarm.\n\nSignal Detection Theory has wide application, both in humans and animals. Topics include memory, stimulus characteristics of schedules of reinforcement, etc.\n\nConceptually, sensitivity refers to how hard or easy it is to detect that a target stimulus is present from background events. For example, in a recognition memory paradigm, having longer to study to-be-remembered words makes it easier to recognize previously seen or heard words. In contrast, having to remember 30 words rather than 5 makes the discrimination harder. One of the most commonly used statistics for computing sensitivity is the so-called sensitivity index or \"d\"'. There are also non-parametric measures, such as the area under the ROC-curve.\n\nBias is the extent to which one response is more probable than another. That is, a receiver may be more likely to respond that a stimulus is present or more likely to respond that a stimulus is not present. Bias is independent of sensitivity. For example, if there is a penalty for either false alarms or misses, this may influence bias. If the stimulus is a bomber, then a miss (failing to detect the plane) may increase deaths, so a liberal bias is likely. In contrast, crying wolf (a false alarm) too often may make people less likely to respond, grounds for a conservative bias.\n\nAnother field which is closely related to signal detection theory is called compressed sensing (or compressive sensing). The objective of compressed sensing is to recover high dimensional but with low complexity entities from only a few measurements. Thus, one of the most important applications of compressed sensing is in the recovery of high dimensional signals which are known to be sparse (or nearly sparse) with only a few linear measurements. The number of measurements needed in the recovery of signals is by far smaller than what Nyquist sampling theorem requires provided that the signal is sparse, meaning that it only contains a few non-zero elements. There are different methods of signal recovery in compressed sensing including basis pursuit , expander recovery algorithm, CoSaMP and also fast non-iterative algorithm. In all of the recovery methods mentioned above, choosing an appropriate measurement matrix using probabilistic constructions or deterministic constructions, is of great importance. In other words, measurement matrices must satisfy certain specific conditions such as RIP (Restricted Isometry Property) or Null-Space property in order to achieve robust sparse recovery.\n\nIn the case of making a decision between two hypotheses, \"H1\", absent, and \"H2\", present, in the event of a particular observation, \"y\", a classical approach is to choose \"H1\" when \"p(H1|y) > p(H2|y)\" and \"H2\" in the reverse case. In the event that the two \"a posteriori\" probabilities are equal, one might choose to default to a single choice (either always choose \"H1\" or always choose \"H2\"), or might randomly select either \"H1\" or \"H2\". The \"a priori\" probabilities of \"H1\" and \"H2\" can guide this choice, e.g. by always choosing the hypothesis with the higher \"a priori\" probability.\n\nWhen taking this approach, usually what one knows are the conditional probabilities, \"p(y|H1)\" and \"p(y|H2)\", and the \"a priori\" probabilities formula_1 and formula_2. In this case,\n\nformula_3,\n\nformula_4\n\nwhere \"p(y)\" is the total probability of event \"y\",\n\nformula_5.\n\n\"H2\" is chosen in case\n\nformula_6\n\nformula_7\n\nand \"H1\" otherwise.\n\nOften, the ratio formula_8 is called formula_9 and formula_10 is called formula_11, the \"likelihood ratio\".\n\nUsing this terminology, \"H2\" is chosen in case formula_12. This is called MAP testing, where MAP stands for \"maximum \"a posteriori\"\").\n\nTaking this approach minimizes the expected number of errors one will make.\n\nIn some cases, it is far more important to respond appropriately to \"H1\" than it is to respond appropriately to \"H2\". For example, if an alarm goes off, indicating H1 (an incoming bomber is carrying a nuclear weapon), it is much more important to shoot down the bomber if H1 = TRUE, than it is to avoid sending a fighter squadron to inspect a false alarm (i.e., H1 = FALSE, H2 = TRUE) (assuming a large supply of fighter squadrons). The Bayes criterion is an approach suitable for such cases.\n\nHere a utility is associated with each of four situations:\n- formula_13: One responds with behavior appropriate to H1 and H1 is true: fighters destroy bomber, incurring fuel, maintenance, and weapons costs, take risk of some being shot down;\n- formula_14: One responds with behavior appropriate to H1 and H2 is true: fighters sent out, incurring fuel and maintenance costs, bomber location remains unknown;\n- formula_15: One responds with behavior appropriate to H2 and H1 is true: city destroyed;\n- formula_16: One responds with behavior appropriate to H2 and H2 is true: fighters stay home, bomber location remains unknown;\n\nAs is shown below, what is important are the differences, formula_17 and formula_18.\n\nSimilarly, there are four probabilities, formula_19, formula_20, etc., for each of the cases (which are dependent on one's decision strategy).\n\nThe Bayes criterion approach is to maximize the expected utility:\n\nformula_21\n\nformula_22\n\nformula_23\n\nEffectively, one may maximize the sum,\n\nformula_24,\n\nand make the following substitutions:\n\nformula_25\n\nformula_26\n\nwhere formula_27 and formula_28 are the \"a priori\" probabilities, formula_29 and formula_30, and formula_31 is the region of observation events, \"y\", that are responded to as though \"H1\" is true.\n\nformula_32\n\nformula_33 and thus formula_34 are maximized by extending formula_31 over the region where\n\nformula_36\n\nThis is accomplished by deciding H2 in case\n\nformula_37\n\nformula_38\n\nand H1 otherwise, where \"L(y)\" is the so-defined \"likelihood ratio\".\n\n", "related": "\n- Binary classification\n- Constant false alarm rate\n- Decision theory\n- Demodulation\n- Detector (radio)\n- Estimation theory\n- Just-noticeable difference\n- Likelihood-ratio test\n- Modulation\n- Neyman–Pearson lemma\n- Psychometric function\n- Receiver operating characteristic\n- Statistical hypothesis testing\n- Statistical signal processing\n- Two-alternative forced choice\n- Type I and type II errors\n- Coren, S., Ward, L.M., Enns, J. T. (1994) \"Sensation and Perception\". (4th Ed.) Toronto: Harcourt Brace.\n- Kay, SM. \"Fundamentals of Statistical Signal Processing: Detection Theory\" ()\n- McNichol, D. (1972) \"A Primer of Signal Detection Theory\". London: George Allen & Unwin.\n- Van Trees HL. \"Detection, Estimation, and Modulation Theory, Part 1\" (; website)\n- Wickens, Thomas D., (2002) \"Elementary Signal Detection Theory\". New York: Oxford University Press. ()\n\n- A Description of Signal Detection Theory\n- An application of SDT to safety\n- Signal Detection Theory by Garrett Neske, The Wolfram Demonstrations Project\n- Lecture by Steven Pinker\n"}
{"id": "1786306", "url": "https://en.wikipedia.org/wiki?curid=1786306", "title": "Eb/N0", "text": "Eb/N0\n\nIn digital communication or data transmission, \"E\"/\"N\" (energy per bit to noise power spectral density ratio) is a normalized signal-to-noise ratio (SNR) measure, also known as the \"SNR per bit\". It is especially useful when comparing the bit error rate (BER) performance of different digital modulation schemes without taking bandwidth into account.\n\nAs the description implies, \"E\" is the signal energy associated with each user data bit; it is equal to the signal power divided by the user bit rate (\"not\" the channel symbol rate). If signal power is in watts and bit rate is in bits per second, \"E\" is in units of joules (watt-seconds). \"N\" is the noise spectral density, the noise power in a 1 Hz bandwidth, measured in watts per hertz or joules.\n\nThese are the same units as \"E\" so the ratio \"E\"/\"N\" is dimensionless; it is frequently expressed in decibels. \"E\"/\"N\" directly indicates the power efficiency of the system without regard to modulation type, error correction coding or signal bandwidth (including any use of spread spectrum). This also avoids any confusion as to \"which\" of several definitions of \"bandwidth\" to apply to the signal.\n\nBut when the signal bandwidth is well defined, \"E\"/\"N\" is also equal to the signal-to-noise ratio (SNR) in that bandwidth divided by the \"gross\" link spectral efficiency in bit/s⋅Hz, where the bits in this context again refer to user data bits, irrespective of error correction information and modulation type.\n\n\"E\"/\"N\" must be used with care on interference-limited channels since additive white noise (with constant noise density \"N\") is assumed, and interference is not always noise-like. In spread spectrum systems (e.g., CDMA), the interference \"is\" sufficiently noise-like that it can be represented as \"I\" and added to the thermal noise \"N\" to produce the overall ratio .\n\n\"E\"/\"N\" is closely related to the carrier-to-noise ratio (CNR or \"C\"/\"N\"), i.e. the signal-to-noise ratio (SNR) of the received signal, after the receiver filter but before detection:\n\nwhere\n\nThe equivalent expression in logarithmic form (dB):\n\nCaution: Sometimes, the noise power is denoted by \"N\"/2 when negative frequencies and complex-valued equivalent baseband signals are considered rather than passband signals, and in that case, there will be a 3 dB difference.\n\n\"E\"/\"N\" can be seen as a normalized measure of the energy per symbol to noise power spectral density (\"E\"/\"N\"):\n\nwhere \"E\" is the energy per symbol in joules and \"ρ\" is the nominal spectral efficiency in bit/s⋅Hz. \"E\"/\"N\" is also commonly used in the analysis of digital modulation schemes. The two quotients are related to each other according to the following:\n\nwhere \"M\" is the number of alternative modulation symbols, e.g. \"M\" = 4 for QPSK and \"M\" = 8 for 8PSK.\n\nThis is the energy per bit, not the energy per information bit.\n\n\"E\"/\"N\" can further be expressed as:\n\nwhere\n\nThe Shannon–Hartley theorem says that the limit of reliable information rate (data rate exclusive of error-correcting codes) of a channel depends on bandwidth and signal-to-noise ratio according to:\n\nwhere\n\nThis equation can be used to establish a bound on \"E\"/\"N\" for any system that achieves reliable communication, by considering a gross bit rate \"R\" equal to the net bit rate \"I\" and therefore an average energy per bit of \"E\" = \"S\"/\"R\", with noise spectral density of \"N\" = \"N\"/\"B\". For this calculation, it is conventional to define a normalized rate \"R\" = \"R\"/2\"B\", a bandwidth utilization parameter of bits per second per half hertz, or bits per dimension (a signal of bandwidth \"B\" can be encoded with 2\"B\" dimensions, according to the Nyquist–Shannon sampling theorem). Making appropriate substitutions, the Shannon limit is:\n\nWhich can be solved to get the Shannon-limit bound on \"E\"/\"N\":\n\nWhen the data rate is small compared to the bandwidth, so that \"R\" is near zero, the bound, sometimes called the \"ultimate Shannon limit\", is:\n\nwhich corresponds to −1.59dB.\n\nThis often-quoted limit of −1.59 dB applies \"only\" to the theoretical case of infinite bandwidth. The Shannon limit for finite-bandwidth signals is always higher.\n\nFor any given system of coding and decoding, there exists what is known as a \"cutoff rate\" \"R\", typically corresponding to an \"E\"/\"N\" about 2 dB above the Shannon capacity limit. The cutoff rate used to be thought of as the limit on practical error correction codes without an unbounded increase in processing complexity, but has been rendered largely obsolete by the more recent discovery of turbo codes and low-density parity-check (LDPC) codes.\n\n- Eb/N0 Explained. An introductory article on \"E\"/\"N\"\n", "related": "NONE"}
{"id": "4826846", "url": "https://en.wikipedia.org/wiki?curid=4826846", "title": "Wavefront coding", "text": "Wavefront coding\n\nIn optics and signal processing, wavefront coding refers to the use of a phase modulating element in conjunction with deconvolution to extend the depth of field of a digital imaging system such as a video camera.\n\nWavefront coding falls under the broad category of computational photography as a technique to enhance the depth of field.\n\nThe wavefront of a light wave passing through the camera system is modulated using optical elements that introduce a spatially varying optical path length. The modulating elements must be placed at or near the plane of the aperture stop or pupil so that the same modulation is introduced for all field angles across the field-of-view. This modulation corresponds to a change in complex argument of the pupil function of such an imaging device, and it can be engineered with different goals in mind: e.g. extending the depth of focus.\n\nWavefront coding with linear phase masks works by creating an optical transfer function that encodes distance information.\n\nWavefront Coding with cubic phase masks works to blur the image uniformly using a cubic shaped waveplate so that the intermediate image, the optical transfer function, is out of focus by a constant amount. Digital image processing then removes the blur and introduces noise depending upon the physical characteristics of the processor. Dynamic range is sacrificed to extend the depth of field depending upon the type of filter used. It can also correct optical aberration.\n\nThe mask was developed by using the ambiguity function and the stationary phase method\n\nThe technique was pioneered by radar engineer Edward Dowski and his thesis adviser Thomas Cathey at the University of Colorado in the United States in the 1990s. After the university showed little interest in the research they have since founded a company to commercialize the method called CDM-Optics. The company was acquired in 2005 by OmniVision Technologies, which has released wavefront-coding-based mobile camera chips as TrueFocus sensors.\n\nTrueFocus sensors are able to simulate older autofocus technologies that use rangefinders and narrow depth of fields. In fact, the technology theoretically allows for any number of combinations of focal points per pixel for effect. It is the only technology not limited to EDoF (Extended-Depth-of-Field).\n\n- Wavefront coding finds increasing use (Laser Focus World)\n", "related": "NONE"}
{"id": "1697331", "url": "https://en.wikipedia.org/wiki?curid=1697331", "title": "Nyquist stability criterion", "text": "Nyquist stability criterion\n\nIn control theory and stability theory, the Nyquist stability criterion or Strecker–Nyquist stability criterion, independently discovered by the German electrical engineer at Siemens in 1930 and the Swedish-American electrical engineer Harry Nyquist at Bell Telephone Laboratories in 1932, is a graphical technique for determining the stability of a dynamical system. Because it only looks at the Nyquist plot of the open loop systems, it can be applied without explicitly computing the poles and zeros of either the closed-loop or open-loop system (although the number of each type of right-half-plane singularities must be known). As a result, it can be applied to systems defined by non-rational functions, such as systems with delays. In contrast to Bode plots, it can handle transfer functions with right half-plane singularities. In addition, there is a natural generalization to more complex systems with multiple inputs and multiple outputs, such as control systems for airplanes.\n\nThe Nyquist criterion is widely used in electronics and control system engineering, as well as other fields, for designing and analyzing systems with feedback. While Nyquist is one of the most general stability tests, it is still restricted to linear, time-invariant (LTI) systems. Non-linear systems must use more complex stability criteria, such as Lyapunov or the circle criterion. While Nyquist is a graphical technique, it only provides a limited amount of intuition for why a system is stable or unstable, or how to modify an unstable system to be stable. Techniques like Bode plots, while less general, are sometimes a more useful design tool.\n\nA Nyquist plot is a parametric plot of a frequency response used in automatic control and signal processing. The most common use of Nyquist plots is for assessing the stability of a system with feedback. In Cartesian coordinates, the real part of the transfer function is plotted on the X axis. The imaginary part is plotted on the Y axis. The frequency is swept as a parameter, resulting in a plot per frequency. The same plot can be described using polar coordinates, where gain of the transfer function is the radial coordinate, and the phase of the transfer function is the corresponding angular coordinate. The Nyquist plot is named after Harry Nyquist, a former engineer at Bell Laboratories.\n\nAssessment of the stability of a closed-loop negative feedback system is done by applying the Nyquist stability criterion to the Nyquist plot of the open-loop system (i.e. the same system without its feedback loop). This method is easily applicable even for systems with delays and other non-rational transfer functions, which may appear difficult to analyze by means of other methods. Stability is determined by looking at the number of encirclements of the point at (−1,0). The range of gains over which the system will be stable can be determined by looking at crossings of the real axis. \n\nThe Nyquist plot can provide some information about the shape of the transfer function. For instance, the plot provides information on the difference between the number of zeros and poles of the transfer function by the angle at which the curve approaches the origin.\n\nWhen drawn by hand, a cartoon version of the Nyquist plot is sometimes used, which shows the linearity of the curve, but where coordinates are distorted to show more detail in regions of interest. When plotted computationally, one needs to be careful to cover all frequencies of interest. This typically means that the parameter is swept logarithmically, in order to cover a wide range of values.\n\nWe consider a system whose open loop transfer function (OLTF) is formula_1; when placed in a closed loop with negative feedback formula_2, the closed loop transfer function (CLTF) then becomes formula_3. Stability can be determined by examining the roots of the desensitivity factor polynomial formula_4, e.g. using the Routh array, but this method is somewhat tedious. Conclusions can also be reached by examining the OLTF, using its Bode plots or, as here, polar plot of the OLTF formula_5 using the Nyquist criterion, as follows.\n\nAny Laplace domain transfer function formula_6 can be expressed as the ratio of two polynomials: formula_7\n\nThe roots of formula_8 are called the \"zeros\" of formula_6, and the roots of formula_10 are the \"poles\" of formula_6. The poles of formula_6 are also said to be the roots of the \"characteristic equation\" formula_13.\n\nThe stability of formula_6 is determined by the values of its poles: for stability, the real part of every pole must be negative. If formula_6 is formed by closing a negative unity feedback loop around the open-loop transfer function formula_16, then the roots of the characteristic equation are also the zeros of formula_17, or simply the roots of formula_18.\n\nFrom complex analysis, a contour formula_19 drawn in the complex formula_20 plane, encompassing but not passing through any number of zeros and poles of a function formula_21, can be mapped to another plane (named formula_21plane) by the function formula_23. Precisely, each complex point formula_20 in the contour formula_19 is mapped to the point formula_21 in the new formula_21 plane yielding a new contour. \n\nThe Nyquist plot of formula_21, which is the contour formula_29 will encircle the point formula_30 of the formula_21 plane formula_32 times, where formula_33 by Cauchy's argument principle. Here formula_34 and formula_35 are, respectively, the number of zeros of formula_36 and poles of formula_21 inside the contour formula_19. Note that we count encirclements in the formula_21 plane in the same sense as the contour formula_19 and that encirclements in the opposite direction are \"negative\" encirclements. That is, we consider clockwise encirclements to be positive and counterclockwise encirclements to be negative.\n\nInstead of Cauchy's argument principle, the original paper by Harry Nyquist in 1932 uses a less elegant approach. The approach explained here is similar to the approach used by Leroy MacColl (Fundamental theory of servomechanisms 1945) or by Hendrik Bode (Network analysis and feedback amplifier design 1945), both of whom also worked for Bell Laboratories. This approach appears in most modern textbooks on control theory.\n\nWe first construct the Nyquist contour, a contour that encompasses the right-half of the complex plane:\n- a path traveling up the formula_41 axis, from formula_42 to formula_43.\n- a semicircular arc, with radius formula_44, that starts at formula_43 and travels clock-wise to formula_42.\nThe Nyquist contour mapped through the function formula_47 yields a plot of formula_47 in the complex plane. By the Argument Principle, the number of clock-wise encirclements of the origin must be the number of zeros of formula_47 in the right-half complex plane minus the number of poles of formula_47 in the right-half complex plane. If instead, the contour is mapped through the open-loop transfer function formula_1, the result is the Nyquist Plot of formula_1. By counting the resulting contour's encirclements of -1, we find the difference between the number of poles and zeros in the right-half complex plane of formula_47. Recalling that the zeros of formula_47 are the poles of the closed-loop system, and noting that the poles of formula_47 are same as the poles of formula_1, we now state The Nyquist Criterion:\"Given a Nyquist contour formula_19, let formula_35 be the number of poles of formula_1 encircled by formula_19, and formula_34 be the number of zeros of formula_47 encircled by formula_19. Alternatively, and more importantly, if formula_34 is the number of poles of the closed loop system in the right half plane, and formula_35 is the number of poles of the open-loop transfer function formula_1 in the right half plane, the resultant contour in the formula_1-plane, formula_68 shall encircle (clock-wise) the point formula_69 formula_32 times such that formula_71.\"If the system is originally open-loop unstable, feedback is necessary to stabilize the system. Right-half-plane (RHP) poles represent that instability. For closed-loop stability of a system, the number of closed-loop roots in the right half of the s-plane must be zero. Hence, the number of counter-clockwise encirclements about formula_72 must be equal to the number of open-loop poles in the RHP. Any clockwise encirclements of the critical point by the open-loop frequency response (when judged from low frequency to high frequency) would indicate that the feedback control system would be destabilizing if the loop were closed. (Using RHP zeros to \"cancel out\" RHP poles does not remove the instability, but rather ensures that the system will remain unstable even in the presence of feedback, since the closed-loop roots travel between open-loop poles and zeros in the presence of feedback. In fact, the RHP zero can make the unstable pole unobservable and therefore not stabilizable through feedback.)\n\nThe above consideration was conducted with an assumption that the open-loop transfer function formula_1 does not have any pole on the imaginary axis (i.e. poles of the form formula_74). This results from the requirement of the argument principle that the contour cannot pass through any pole of the mapping function. The most common case are systems with integrators (poles at zero).\n\nTo be able to analyze systems with poles on the imaginary axis, the Nyquist Contour can be modified to avoid passing through the point formula_74. One way to do it is to construct a semicircular arc with radius formula_76 around formula_74, that starts at formula_78 and travels anticlockwise to formula_79. Such a modification implies that the phasor formula_1 travels along an arc of infinite radius by formula_81, where formula_82 is the multiplicity of the pole on the imaginary axis.\n\nOur goal is to, through this process, check for the stability of the transfer function of our unity feedback system with gain \"k\", which is given by\nThat is, we would like to check whether the characteristic equation of the above transfer function, given by\nhas zeros outside the open left-half-plane (commonly initialized as the OLHP).\n\nWe suppose that we have a clockwise (i.e. negatively oriented) contour formula_19 enclosing the right half plane, with indentations as needed to avoid passing through zeros or poles of the function formula_1. Cauchy's argument principle states that \nWhere formula_34 denotes the number of zeros of formula_10 enclosed by the contour and formula_35 denotes the number of poles of formula_10 by the same contour. Rearranging, we have\nformula_92, which is to say\nWe then note that formula_94 has exactly the same poles as formula_1. Thus, we may find formula_35 by counting the poles of formula_1 that appear within the contour, that is, within the open right half plane (ORHP).\n\nWe will now rearrange the above integral via substitution. That is, setting formula_98, we have\nWe then make a further substitution, setting formula_100. This gives us\n\nWe now note that formula_102 gives us the image of our contour under formula_1, which is to say our Nyquist plot. We may further reduce the integral\n\nby applying Cauchy's integral formula. In fact, we find that the above integral corresponds precisely to the number of times the Nyquist plot encircles the point formula_105 clockwise. Thus, we may finally state that\n\nWe thus find that formula_107 as defined above corresponds to a stable unity-feedback system when formula_34, as evaluated above, is equal to 0.\n\n- If the open-loop transfer function formula_1 has a zero pole of multiplicity formula_82, then the Nyquist plot has a discontinuity at formula_111. During further analysis it should be assumed that the phasor travels formula_82 times clock-wise along a semicircle of infinite radius. After applying this rule, the zero poles should be neglected, i.e. if there are no other unstable poles, then the open-loop transfer function formula_1 should be considered stable.\n- If the open-loop transfer function formula_1 is stable, then the closed-loop system is unstable for \"any\" encirclement of the point −1.\n- If the open-loop transfer function formula_1 is \"unstable\", then there must be one \"counter\" clock-wise encirclement of −1 for each pole of formula_1 in the right-half of the complex plane.\n- The number of surplus encirclements (\"N\" + \"P\" greater than 0) is exactly the number of unstable poles of the closed-loop system.\n- However, if the graph happens to pass through the point formula_117, then deciding upon even the marginal stability of the system becomes difficult and the only conclusion that can be drawn from the graph is that there exist zeros on the formula_41 axis.\n\n", "related": "\n- BIBO stability\n- Bode plot\n- Routh–Hurwitz stability criterion\n- Gain margin\n- Nichols plot\n- Hall circles\n- Phase margin\n- Barkhausen stability criterion\n- Circle criterion\n- Control engineering\n- Hankel singular value\n\n- Faulkner, E. A. (1969): \"Introduction to the Theory of Linear Systems\"; Chapman & Hall;\n- Pippard, A. B. (1985): \"Response & Stability\"; Cambridge University Press;\n- Gessing, R. (2004): \"Control fundamentals\"; Silesian University of Technology;\n- Franklin, G. (2002): \"Feedback Control of Dynamic Systems\"; Prentice Hall,\n\n- Applets with modifiable parameters\n- EIS Spectrum Analyser - a freeware program for analysis and simulation of impedance spectra\n- MATLAB function for creating a Nyquist plot of a frequency response of a dynamic system model.\n- PID Nyquist plot shaping - free interactive virtual tool, control loop simulator\n- Mathematica function for creating the Nyquist plot\n"}
{"id": "31980229", "url": "https://en.wikipedia.org/wiki?curid=31980229", "title": "Pulse-density modulation", "text": "Pulse-density modulation\n\nPulse-density modulation, or PDM, is a form of modulation used to represent an analog signal with a binary signal. In a PDM signal, specific amplitude values are not encoded into codewords of pulses of different weight as they would be in pulse-code modulation (PCM); rather, the relative density of the pulses corresponds to the analog signal's amplitude. The output of a 1-bit DAC is the same as the PDM encoding of the signal. Pulse-width modulation (PWM) is a special case of PDM where the switching frequency is fixed and all the pulses corresponding to one sample are contiguous in the digital signal. For a 50% voltage with a resolution of 8-bits, a PWM waveform will turn on for 128 clock cycles and then off for the remaining 128 cycles. With PDM and the same clock rate the signal would alternate between on and off every other cycle. The average is 50% for both waveforms, but the PDM signal switches more often. For 100% or 0% level, they are the same.\n\nIn a pulse-density modulation bitstream a 1 corresponds to a pulse of positive polarity (+\"A\") and a 0 corresponds to a pulse of negative polarity (-\"A\"). Mathematically, this can be represented as:\n\nA run consisting of all 1s would correspond to the maximum (positive) amplitude value, all 0s would correspond to the minimum (negative) amplitude value, and alternating 1s and 0s would correspond to a zero amplitude value. The continuous amplitude waveform is recovered by low-pass filtering the bipolar PDM bitstream.\n\nA single period of the trigonometric sine function, sampled 100 times and represented as a PDM bitstream, is:\n\n0101011011110111111111111111111111011111101101101010100100100000010000000000000000000001000010010101\nTwo periods of a higher frequency sine wave would appear as:\n\n0101101111111111111101101010010000000000000100010011011101111111111111011010100100000000000000100101\nIn pulse-\"density\" modulation, a high \"density\" of 1s occurs at the peaks of the sine wave, while a low \"density\" of 1s occurs at the troughs of the sine wave.\n\nA PDM bitstream is encoded from an analog signal through the process of delta-sigma modulation. This process uses a one bit quantizer that produces either a 1 or 0 depending on the amplitude of the analog signal. A 1 or 0 corresponds to a signal that is all the way up or all the way down, respectively. Because in the real world, analog signals are rarely all the way in one direction, there is a quantization error, the difference between the 1 or 0 and the actual amplitude it represents. This error is fed back negatively in the ΔΣ process loop. In this way, every error successively influences every other quantization measurement and its error. This has the effect of averaging out the quantization error.\n\nThe process of decoding a PDM signal into an analog one is simple: one only has to pass the PDM signal through a low-pass filter. This works because the function of a low-pass filter is essentially to average the signal. The average amplitude of pulses is measured by the density of those pulses over time, thus a low pass filter is the only step required in the decoding process.\n\nNotably, one of the ways animal nervous systems represent sensory and other information is through rate coding whereby the magnitude of the signal is related to the rate of firing of the sensory neuron. In direct analogy, each neural event – called an action potential – represents one bit (pulse), with the rate of firing of the neuron representing the pulse density.\n\nA digital model of pulse-density modulation can be obtained from a digital model of the delta-sigma modulator. Consider a signal formula_2 in the discrete time domain as the input to a first-order delta-sigma modulator, with formula_3 the output. In the discrete frequency domain, where the Z-transform has been applied to the amplitude time-series formula_2 to yield formula_5, then the output formula_6 of the delta-sigma modulator's operation is represented by\n\nwhere formula_8 is the frequency-domain quantization error of the delta-sigma modulator. \nRearranging terms, we obtain\n\nThe factor formula_10 represents a high-pass filter, so it is clear that formula_8 contributes less to the output formula_6 at low frequencies, and more at high frequencies. This demonstrates the noise shaping effect of the delta-sigma modulator: the quantization noise is \"pushed\" out of the low frequencies up into the high-frequency range.\n\nUsing the inverse Z-transform, we may convert this into a difference equation relating the input of the delta-sigma modulator to its output in the discrete time domain,\n\nThere are two additional constraints to consider: first, at each step the output sample formula_3 is chosen so as to \"minimize\" the \"running\" quantization error formula_15. Second, formula_3 is represented as a single bit, meaning it can take on only two values. We choose formula_17 for convenience, allowing us to write\n\nThis, finally, gives a formula for the output sample formula_3 in terms of the input sample formula_2. The quantization error of each sample is fed back into the input for the following sample.\n\nThe following pseudo-code implements this algorithm to convert a pulse-code modulation signal into a PDM signal:\n\nPDM is the encoding used in Sony's Super Audio CD (SACD) format, under the name Direct Stream Digital.\n\nSome systems transmit PDM stereo audio over a single data wire. The rising edge of the master clock indicates a bit from the left channel, while the falling edge of the master clock indicates a bit from the right channel.\n\n", "related": "\n- Delta modulation\n- Pulse-code modulation\n- Delta-sigma modulation\n\n- 1-bit A/D and D/A Converters – Discusses delta modulation, PDM (also known as Sigma-delta modulation or SDM), and relationships to Pulse-code modulation (PCM)\n"}
{"id": "11862679", "url": "https://en.wikipedia.org/wiki?curid=11862679", "title": "Signal subspace", "text": "Signal subspace\n\nIn signal processing, signal subspace methods are empirical linear methods for dimensionality reduction and noise reduction. These approaches have attracted significant interest and investigation recently in the context of speech enhancement, speech modeling, and speech classification research. The signal subspace is also used in radio direction finding using the MUSIC (algorithm).\n\nEssentially the methods represent the application of a principal components analysis (PCA) approach to ensembles of observed time-series obtained by sampling, for example sampling an audio signal. Such samples can be viewed as vectors in a high-dimensional vector space over the real numbers. PCA is used to identify a set of orthogonal basis vectors (basis signals) which capture as much as possible of the energy in the ensemble of observed samples. The vector space spanned by the basis vectors identified by the analysis is then the \"signal subspace\". The underlying assumption is that information in speech signals is almost completely contained in a small linear subspace of the overall space of possible sample vectors, whereas additive noise is typically distributed through the larger space isotropically (for example when it is white noise).\n\nBy projecting a sample on a signal subspace, that is, keeping only the component of the sample that is in the \"signal subspace\" defined by linear combinations of the first few most energized basis vectors, and throwing away the rest of the sample, which is in the remainder of the space orthogonal to this subspace, a certain amount of noise filtering is then obtained.\n\nSignal subspace noise-reduction can be compared to Wiener filter methods. There are two main differences:\n- The basis signals used in Wiener filtering are usually harmonic sine waves, into which a signal can be decomposed by Fourier transform. In contrast, the basis signals used to construct the signal subspace are identified empirically, and may for example be chirps, or particular characteristic shapes of transients after particular triggering events, rather than pure sinusoids.\n- The Wiener filter grades smoothly between linear components that are dominated by signal, and linear components that are dominated by noise. The noise components are filtered out, but not quite completely; the signal components are retained, but not quite completely; and there is a transition zone which is partly accepted. In contrast, the signal subspace approach represents a sharp cut-off: an orthogonal component either lies within the signal subspace, in which case it is 100% accepted, or orthogonal to it, in which case it is 100% rejected. This reduction in dimensionality, abstracting the signal into a much shorter vector, can be a particularly desired feature of the method.\n\nIn the simplest case signal subspace methods assume white noise, but extensions of the approach to colored noise removal and the evaluation of the subspace-based speech enhancement for robust speech recognition have also been reported.\n", "related": "NONE"}
{"id": "32050924", "url": "https://en.wikipedia.org/wiki?curid=32050924", "title": "WSSUS model", "text": "WSSUS model\n\nThe WSSUS (Wide-Sense Stationary Uncorrelated Scattering) model provides a statistical description of the transmission behavior of wireless channels. \"Wide-sense stationarity\" means the second-order moments of the channel are stationary, which means that they depends only on the time difference, while \"uncorrelated scattering\" refers to the delay τ due to scatterers.\nModelling of mobile channels as WSSUS (wide sense stationary uncorrelated scattering) is rather popular among specialists in recent years.\n\nA commonly used description of time variant channel applies the set of Bello functions and the theory of stochastic processes.\n\n- Kurth, R. R.; Snyder, D. L.; Hoversten, E. V. (1969) \"Detection and Estimation Theory\", \"Massachusetts Institute of Technology, Research Laboratory of Electronics, Quarterly Progress Report\", No. 93 (IX), 177–205\n\n- Wide Sense Stationary Uncorrelated Scattering at www.WirelessCommunication.NL\n", "related": "NONE"}
{"id": "4601187", "url": "https://en.wikipedia.org/wiki?curid=4601187", "title": "Autocorrelation technique", "text": "Autocorrelation technique\n\nThe autocorrelation technique is a method for estimating the dominating frequency in a complex signal, as well as its variance. Specifically, it calculates the first two moments of the power spectrum, namely the mean and variance. It is also known as the pulse-pair algorithm in radar theory.\n\nThe algorithm is both computationally faster and significantly more accurate compared to the Fourier transform, since the resolution is not limited by the number of samples used.\n\nThe autocorrelation of lag 1 can be expressed using the inverse Fourier transform of the power spectrum formula_1:\nIf we model the power spectrum as a single frequency formula_3, this becomes:\nwhere it is apparent that the phase of formula_6 equals the signal frequency.\n\nThe mean frequency is calculated based on the autocorrelation with lag one, evaluated over a signal consisting of N samples:\nThe spectral variance is calculated as follows:\n\n- Estimation of blood velocity and turbulence in \"color flow imaging\" used in medical ultrasonography.\n- Estimation of target velocity in pulse-doppler radar\n\n- A covariance approach to spectral moment estimation, Miller et al., IEEE Transactions on Information Theory.\n- Doppler Radar Meteorological Observations Doppler Radar Theory. Autocorrelation technique described on p.2-11\n- Real-Time Two-Dimensional Blood Flow Imaging Using an Autocorrelation Technique, by Chihiro Kasai, Koroku Namekawa, Akira Koyano, and Ryozo Omoto, IEEE Transactions on Sonics and Ultrasonics, Vol. SU-32, No.3, May 1985.\n", "related": "NONE"}
{"id": "29819979", "url": "https://en.wikipedia.org/wiki?curid=29819979", "title": "Cognitive hearing science", "text": "Cognitive hearing science\n\nCognitive hearing science is an interdisciplinary science field concerned with the physiological and cognitive basis of hearing and its interplay with signal processing in hearing aids. The field includes genetics, physiology, medical and technical audiology, cognitive neuroscience, cognitive psychology, linguistics and social psychology.\n\nTheoretically the research in cognitive hearing science combines a physiological model for the information transfer from the outer auditory organ to the auditory cerebral cortex, and a cognitive model for how language comprehension is influenced by the interplay between the incoming language signal and the individual's cognitive skills, especially the long-term memory and the working memory.\n\nResearchers examine the interplay between type of hearing impairment or deafness, type of signal processing in different hearing aids, type of listening environment and the individual's cognitive skills.\n\nResearch in cognitive hearing science has importance for the knowledge about different types of hearing impairment and its effects, as for the possibilities to determine which individuals can make use of certain type of signal processing in hearing aid or cochlear implant and thereby adapt hearing aid to the individual.\n\nCognitive hearing science has been introduced by researchers at the Linköping University research centre Linnaeus Centre HEAD (HEaring And Deafness) in Sweden, created in 2008 with a major 10-year grant from the Swedish Research Council.\n\n- Linnaeus Centre HEAD\n- Interview, prof. Jerker Rönnberg\n", "related": "NONE"}
{"id": "32572090", "url": "https://en.wikipedia.org/wiki?curid=32572090", "title": "Polynomial signal processing", "text": "Polynomial signal processing\n\nPolynomial signal processing is a type of non-linear signal processing. Polynomial systems maybe interpreted as conceptually straight forward extensions of linear systems to the non-linear case.\n", "related": "NONE"}
{"id": "33170733", "url": "https://en.wikipedia.org/wiki?curid=33170733", "title": "Babel function", "text": "Babel function\n\nThe Babel function (also known as cumulative coherence) measures the maximum total coherence between a fixed atom and a collection of other atoms in a dictionary.\n\nThe Babel function of a dictionary formula_1 with normalized columns is a real-valued function that is defined as\nwhere formula_3 are the columns (atoms) of the dictionary formula_4.\n\nWhen p=1, the babel function is the mutual coherence.\n\n", "related": "\n- Compressed sensing\n"}
{"id": "33136216", "url": "https://en.wikipedia.org/wiki?curid=33136216", "title": "Washout filter", "text": "Washout filter\n\nIn signal processing, a washout filter is a stable high pass filter with zero static gain. This leads to the filtering of lower frequency inputs signals, leaving the steady state output unaffected by unwanted low frequency inputs.\n\nThe common transfer function for a washout filter is:\n\nformula_1\n\nWhere formula_2 is the input variable, formula_3 is the output of the function for the filter, and the frequency of the filter is set in the denominator. This filter will only produce a non-zero output only during transient periods when the input signal is of higher frequency and not in a constant steady state value. Conversely, the filter will “wash out” sensed input signals that is of lower frequency (constant steady-state signal). [C.K. Wang]\n\nIn modern swept wing aircraft, yaw damping control systems are used to dampen and stabilize the Dutch-roll motion of an aircraft in flight. However, when a pilot inputs a command to yaw the aircraft for maneuvering (such as steady turns), the rudder becomes a single control surface that functions to dampen the Dutch-roll motion and yaw the aircraft. The result is a suppressed yaw rate and more required input from the pilot to counter the suppression. [C.K. Wang]\n\nTo counter the yaw command suppression, the installation of washout filters before the yaw dampers and rudder actuators will allow the yaw damper feedback loop in the control system to filter out the low frequency signals or state inputs. In the case of a steady turn during flight, the low frequency signal is the pilot command and the washout filter will allow the turn command signal to not be dampened by the yaw damper in the feedback circuit. [C.K. Wang] An example of this use of can be located at Yaw Damper Design for a 747® Jet.\n", "related": "NONE"}
{"id": "33301481", "url": "https://en.wikipedia.org/wiki?curid=33301481", "title": "Transmission curve", "text": "Transmission curve\n\nThe transmission curve or transmission characteristic is the mathematical function or graph that describes the transmission fraction of an optical or electronic filter as a function of frequency or wavelength. It is an instance of a transfer function but, unlike the case of, for example, an amplifier, output never exceeds input (maximum transmission is 100%). The term is often used in commerce, science, and technology to characterise filters.\n\nThe term has also long been used in fields such as geophysics and astronomy to characterise the properties of regions through which radiation passes, such as the ionosphere.\n\n", "related": "\n- Electronic filter — examples of transmission characteristics of electronic filters\n"}
{"id": "33376157", "url": "https://en.wikipedia.org/wiki?curid=33376157", "title": "Signal analyzer", "text": "Signal analyzer\n\nA signal analyzer is an instrument that measures the magnitude and phase of the input signal at a single frequency within the IF bandwidth of the instrument. It employs digital techniques to extract useful information that is carried by an electrical signal. In common usage the term is related to both spectrum analyzers and vector signal analyzers. While spectrum analyzers measure the amplitude or magnitude of signals, a signal analyzer with appropriate software or programming can measure any aspect of the signal such as modulation. Today’s high-frequency signal analyzers achieve good performance by optimizing both the analog front end and the digital back end.\n\nModern signal analyzers use a superheterodyne receiver to downconvert a portion of the signal spectrum for analysis. As shown in the figure to the right, the signal is first converted to an intermediate frequency and then filtered in order to band-limit the signal and prevent aliasing. The downconversion can operate in a swept-tuned mode similar to a traditional spectrum analyzer, or in a fixed-tuned mode. In the fixed-tuned mode the range of frequencies downconverted does not change and the downconverter output is then digitized for further analysis. The digitizing process typically involves in-phase/quadrature (I/Q) or complex sampling so that all characteristics of the signal are preserved, as opposed to the magnitude-only processing of a spectrum analyzer. The sampling rate of the digitizing process may be varied in relation to the frequency span under consideration or (more typically) the signal may be digitally resampled.\n\nSignal analyzers can perform the operations of both spectrum analyzers and vector signal analyzers. A signal analyzer can be viewed as a measurement platform, with operations such as spectrum analysis (including phase noise, power, and distortion) and vector signal analysis (including demodulation or modulation quality analysis) performed as measurement applications. These measurement applications can be built into the analyzer platform as measurement firmware or installed as changeable application software.\n", "related": "NONE"}
{"id": "22130202", "url": "https://en.wikipedia.org/wiki?curid=22130202", "title": "Gradient pattern analysis", "text": "Gradient pattern analysis\n\nGradient pattern analysis (GPA) is a geometric computing method for characterizing geometrical bilateral symmetry breaking of an ensemble of symmetric vectors regularly distributed in a square lattice. Usually, the lattice of vectors represent the first-order gradient of a scalar field, here an \"M x M\" square amplitude matrix. An important property of the gradient representation is the following: A given \"M x M\" matrix where all amplitudes are different results in an \"M x M\" gradient lattice containing formula_1 asymmetric vectors. As each vector can be characterized by its norm and phase, variations in the formula_2 amplitudes can modify the respective formula_2 gradient pattern. \nThe original concept of GPA was introduced by Rosa, Sharma and Valdivia in 1999. Usually GPA is applied for spatio-temporal pattern analysis in physics and environmental sciences operating on time-series and digital images.\n\nBy connecting all vectors using a Delaunay triangulation criterion it is possible to characterize gradient asymmetries computing the so-called \"gradient asymmetry coefficient\", that has been defined as:\nformula_4,\nwhere formula_5 is the total number of asymmetric vectors, formula_6 is the number of Delaunay connections among them and the property formula_7 \nis valid for any gradient square lattice.\nAs the asymmetry coefficient is very sensitive to small changes in the phase and modulus of each gradient vector, it can distinguish complex variability patterns (bilateral asymmetry) even when they are very similar but consist of a very fine structural difference. Note that, unlike most of the statistical tools, the GPA does not rely on the statistical properties of the data but\ndepends solely on the local symmetry properties of the correspondent gradient pattern.\nFor a complex extended pattern (matrix of amplitudes of a spatio-temporal pattern) composed by locally asymmetric fluctuations, formula_8 is nonzero, defining different classes of irregular fluctuation patterns (1/f noise, chaotic, reactive-diffusive, etc.). \n\nBesides formula_8 other measurements (called \"gradient moments\") can be calculated from the gradient lattice. Considering the sets of local norms and phases as discrete compact groups, spatially distributed in a square lattice, the gradient moments have the basic property of being globally invariant (for rotation and modulation).\n\nThe primary research on gradient lattices applied to characterize weak wave turbulence from X-ray images of solar active regions was developed in the Department of Astronomy at University of Maryland, College Park, USA. A key line of research on GPA's algorithms and applications has been developed at Lab for Computing and Applied Mathematics (LAC) at National Institute for Space Research (INPE) in Brazil.\n\nWhen GPA is conjugated with wavelet analysis, then the method is called \"Gradient spectral analysis\" (GSA), usually applied to short time series analysis.\n", "related": "NONE"}
{"id": "1044685", "url": "https://en.wikipedia.org/wiki?curid=1044685", "title": "Comb filter", "text": "Comb filter\n\nIn signal processing, a comb filter is a filter implemented by adding a delayed version of a signal to itself, causing constructive and destructive interference. The frequency response of a comb filter consists of a series of regularly spaced notches, giving the appearance of a comb.\n\nComb filters are used in a variety of signal processing applications. These include:\n\n- Cascaded integrator–comb (CIC) filters, commonly used for anti-aliasing during interpolation and decimation operations that change the sample rate of a discrete-time system.\n- 2D and 3D comb filters implemented in hardware (and occasionally software) for PAL and NTSC television decoders. The filters work to reduce artifacts such as dot crawl.\n- Audio signal processing, including delay, flanging, and digital waveguide synthesis. For instance, if the delay is set to a few milliseconds, a comb filter can be used to model the effect of acoustic standing waves in a cylindrical cavity or in a vibrating string.\n- In astronomy the astro-comb promises to increase the precision of existing spectrographs by nearly a hundredfold.\n\nIn acoustics, comb filtering can arise in some unwanted ways. For instance, when two loudspeakers are playing the same signal at different distances from the listener, there is a comb filtering effect on the signal. In any enclosed space, listeners hear a mixture of direct sound and reflected sound. Because the reflected sound takes a longer path, it constitutes a delayed version of the direct sound and a comb filter is created where the two combine at the listener.\n\nComb filters exist in two different forms, \"feedforward\" and \"feedback\"; the names refer to the direction in which signals are delayed before they are added to the input.\n\nComb filters may be implemented in discrete time or continuous time; this article will focus on discrete-time implementations; the properties of the continuous-time comb filter are very similar.\n\nThe general structure of a feedforward comb filter is shown on the right. It may be described by the following difference equation: \n\nwhere formula_2 is the delay length (measured in samples), and is a scaling factor applied to the delayed signal. If we take the transform of both sides of the equation, we obtain:\n\nWe define the transfer function as:\n\nTo obtain the frequency response of a discrete-time system expressed in the -domain, we make the substitution . Therefore, for our feedforward comb filter, we get:\n\nUsing Euler's formula, we find that the frequency response is also given by\n\nOften of interest is the \"magnitude\" response, which ignores phase. This is defined as:\n\nIn the case of the feedforward comb filter, this is:\n\nNotice that the term is constant, whereas the term varies periodically. Hence the magnitude response of the comb filter is periodic.\n\nThe graphs to the right show the magnitude response for various values of , demonstrating this periodicity. Some important properties:\n- The response periodically drops to a local minimum (sometimes known as a \"notch\"), and periodically rises to a local maximum (sometimes known as a \"peak\").\n- For positive values of , the first minimum occurs at half the delay period and repeat at even multiples of the delay frequency thereafter:\n- The levels of the maxima and minima are always equidistant from 1.\n- When , the minima have zero amplitude. In this case, the minima are sometimes known as \"nulls\".\n- The maxima for positive values of coincide with the minima for negative values of formula_10, and vice versa.\n\nThe feedforward comb filter is one of the simplest finite impulse response filters. Its response is simply the initial impulse with a second impulse after the delay.\n\nLooking again at the -domain transfer function of the feedforward comb filter:\n\nwe see that the numerator is equal to zero whenever . This has solutions, equally spaced around a circle in the complex plane; these are the zeros of the transfer function. The denominator is zero at , giving poles at . This leads to a pole–zero plot like the ones shown below.\n\nSimilarly, the general structure of a feedback comb filter is shown on the right. It may be described by the following difference equation:\n\nIf we rearrange this equation so that all terms in formula_13 are on the left-hand side, and then take the transform, we obtain:\n\nThe transfer function is therefore:\n\nIf we make the substitution into the -domain expression for the feedback comb filter, we get:\n\nThe magnitude response is as follows:\n\nAgain, the response is periodic, as the graphs to the right demonstrate. The feedback comb filter has some properties in common with the feedforward form:\n- The response periodically drops to a local minimum and rises to a local maximum.\n- The maxima for positive values of coincide with the minima for negative values of formula_10, and vice versa.\n- For positive values of , the first maximum occurs at 0 and repeats at even multiples of the delay frequency thereafter:\n\nHowever, there are also some important differences because the magnitude response has a term in the denominator:\n- The levels of the maxima and minima are no longer equidistant from 1. The maxima have an amplitude of .\n- The filter is only stable if is strictly less than 1. As can be seen from the graphs, as increases, the amplitude of the maxima rises increasingly rapidly.\n\nThe feedback comb filter is a simple type of infinite impulse response filter. If stable, the response simply consists of a repeating series of impulses decreasing in amplitude over time.\n\nLooking again at the -domain transfer function of the feedback comb filter:\n\nThis time, the numerator is zero at , giving zeros at . The denominator is equal to zero whenever . This has solutions, equally spaced around a circle in the complex plane; these are the poles of the transfer function. This leads to a pole–zero plot like the ones shown below.\n\nComb filters may also be implemented in continuous time. The feedforward form may be described by the following equation:\n\nwhere is the delay (measured in seconds). This has the following transfer function:\n\nThe feedforward form consists of an infinite number of zeros spaced along the jω axis.\n\nThe feedback form has the equation:\n\nand the following transfer function:\n\nThe feedback form consists of an infinite number of poles spaced along the jω axis.\n\nContinuous-time implementations share all the properties of the respective discrete-time implementations.\n\n", "related": "\n- Fabry–Pérot interferometer\n"}
{"id": "275871", "url": "https://en.wikipedia.org/wiki?curid=275871", "title": "Signal", "text": "Signal\n\nIn signal processing, a signal is a function that conveys information about a phenomenon. In electronics and telecommunications, it refers to any time varying voltage, current or electromagnetic wave that carries information. A signal may also be defined as an observable change in a quality such as quantity.\n\nAny quality, such as physical quantity that exhibits variation in space or time can be used as a signal to share messages between observers. According to the \"IEEE Transactions on Signal Processing\", a signal can be audio, video, speech, image, sonar and radar-related and so on. In a later effort of redefining a signal, anything that is only a function of space, such as an image, is excluded from the category of signals. Also, it is stated that a signal may or may not contain any information.\n\nIn nature, signals can be actions done by an organism to alert other organisms, ranging from the release of plant chemicals to warn nearby plants of a predator, to sounds or motions made by animals to alert other animals of food. Signalling occurs in all organisms even at cellular levels, with cell signaling. Signaling theory, in evolutionary biology, proposes that a substantial driver for evolution is the ability for animals to communicate with each other by developing ways of signaling. In human engineering, signals are typically provided by a sensor, and often the original form of a signal is converted to another form of energy using a transducer. For example, a microphone converts an acoustic signal to a voltage waveform, and a speaker does the reverse.\n\nInformation theory serves as the formal study of signals and their content, and the information of a signal is often accompanied by noise. The term \"noise\" refers to unwanted signal modifications, but is often extended to include unwanted signals conflicting with desired signals (crosstalk). The reduction of noise is covered in part under the heading of signal integrity. The separation of desired signals from background noise is the field of signal recovery, one branch of which is estimation theory, a probabilistic approach to suppressing random disturbances.\n\nEngineering disciplines such as electrical engineering have led the way in the design, study, and implementation of systems involving transmission, storage, and manipulation of information. In the latter half of the 20th century, electrical engineering itself separated into several disciplines, specialising in the design and analysis of systems that manipulate physical signals; electronic engineering and computer engineering as examples; while design engineering developed to deal with functional design of user–machine interfaces.\n\nDefinitions specific to sub-fields are common. For example, in information theory, a \"signal\" is a codified message, that is, the sequence of states in a communication channel that encodes a message. In the context of signal processing, signals are analog and digital representations of analog physical quantities.\n\nIn terms of their spatial distributions, signals may be categorized as point source signals (PSSs) and distributed source signals (DSSs).\n\nIn a communication system, a \"transmitter\" encodes a \"message\" to create a signal, which is carried to a \"receiver\" by the communications channel. For example, the words \"Mary had a little lamb\" might be the message spoken into a telephone. The telephone transmitter converts the sounds into an electrical signal. The signal is transmitted to the receiving telephone by wires; at the receiver it is reconverted into sounds.\n\nIn telephone networks, signaling, for example common-channel signaling, refers to phone number and other digital control information rather than the actual voice signal.\n\nSignals can be categorized in various ways. The most common distinction is between discrete and continuous spaces that the functions are defined over, for example discrete and continuous time domains. Discrete-time signals are often referred to as \"time series\" in other fields. Continuous-time signals are often referred to as \"continuous signals\".\n\nA second important distinction is between discrete-valued and continuous-valued. Particularly in digital signal processing, a digital signal may be defined as a sequence of discrete values, typically associated with an underlying continuous-valued physical process. In digital electronics, digital signals are the continuous-time waveform signals in a digital system, representing a bit-stream.\n\nAnother important property of a signal is its entropy or information content.\n\nIn Signals and Systems, signals can be classified according to many criteria, mainly: according to the different feature of values, classified into analog signals and digital signals; according to the determinacy of signals, classified into deterministic signals and random signals; according to the strength of signals, classified into energy signals and power signals.\n\nTwo main types of signals encountered in practice are \"analog\" and \"digital\". The figure shows a digital signal that results from approximating an analog signal by its values at particular time instants. Digital signals are \"quantized\", while analog signals are continuous.\n\nAn analog signal is any continuous signal for which the time varying feature of the signal is a representation of some other time varying quantity, i.e., \"analogous\" to another time varying signal. For example, in an analog audio signal, the instantaneous voltage of the signal varies continuously with the sound pressure. It differs from a digital signal, in which the continuous quantity is a representation of a sequence of discrete values which can only take on one of a finite number of values.\n\nThe term \"analog signal\" usually refers to electrical signals; however, analog signals may use other mediums such as mechanical, pneumatic or hydraulic. An analog signal uses some property of the medium to convey the signal's information. For example, an aneroid barometer uses rotary position as the signal to convey pressure information. In an electrical signal, the voltage, current, or frequency of the signal may be varied to represent the information.\nAny information may be conveyed by an analog signal; often such a signal is a measured response to changes in physical phenomena, such as sound, light, temperature, position, or pressure. The physical variable is converted to an analog signal by a transducer. For example, in sound recording, fluctuations in air pressure (that is to say, sound) strike the diaphragm of a microphone which induces corresponding electrical fluctuations. The voltage or the current is said to be an \"analog\" of the sound.\n\nA digital signal is a signal that is constructed from a discrete set of waveforms of a physical quantity so as to represent a sequence of discrete values. A \"logic signal\" is a digital signal with only two possible values, and describes an arbitrary bit stream. Other types of digital signals can represent three-valued logic or higher valued logics.\n\nAlternatively, a digital signal may be considered to be the sequence of codes represented by such a physical quantity. The physical quantity may be a variable electric current or voltage, the intensity, phase or polarization of an optical or other electromagnetic field, acoustic pressure, the magnetization of a magnetic storage media, etc. Digital signals are present in all digital electronics, notably computing equipment and data transmission.\nWith digital signals, system noise, provided it is not too great, will not affect system operation whereas noise always degrades the operation of analog signals to some degree.\n\nDigital signals often arise via sampling of analog signals, for example, a continually fluctuating voltage on a line that can be digitized by an analog-to-digital converter circuit, wherein the circuit will read the voltage level on the line, say, every 50 microseconds and represent each reading with a fixed number of bits. The resulting stream of numbers is stored as digital data on a discrete-time and quantized-amplitude signal. Computers and other digital devices are restricted to discrete time.\n\nAccording to the strengths of signals, practical signals can be classified into two categories: energy signals and power signals.\n\nEnergy signals: Those signals' energy are equal to a finite positive value, but their average powers are 0;\n\nformula_1\n\nPower signals: Those signals' average power are equal to a finite positive value, but their energy are infinite.\n\nformula_2\n\nDeterministic signals are those whose values at any time are predictable and can be calculated by a mathematical equation.\n\nRandom signals are signals that take on random values at any given time instant and must be modeled stochastically.\n\nEven signal satisfies the condition formula_3\n\nor equivalently if the following equation holds for all formula_4 and formula_5 in the domain of formula_6:\n\nOdd signal satisfies the condition formula_8\n\nor equivalently if the following equation holds for all formula_4 and formula_5 in the domain of formula_6:\n\nA signal is said to be periodic if it satisfies the condition:\n\nformula_13 or formula_14\n\nWhere:\n\nformula_15 = fundamental time period,\n\nformula_16= fundamental frequency.\n\nA periodic signal will repeat for every period.\n\nOne of the fundamental distinctions between different types of signals is between continuous and discrete time. In the mathematical abstraction, the domain of a continuous-time (CT) signal is the set of real numbers (or some interval thereof), whereas the domain of a discrete-time (DT) signal is the set of integers (or some interval). What these integers represent depends on the nature of the signal; most often it is time.\n\nIf for a signal, the quantities are defined only on a discrete set of times, we call it a discrete-time signal. A simple source for a discrete time signal is the sampling of a continuous, approximating the signal by a sequence of its values at particular time instants.\n\nA discrete-time real (or complex) signal can be seen as a function from (a subset of) the set of integers (the index labeling time instants) to the set of real (or complex) numbers (the function values at those instants).\n\nA continuous-time real (or complex) signal is any real-valued (or complex-valued) function which is defined at every time \"t\" in an interval, most commonly an infinite interval.\n\nIf a signal is to be represented as a sequence of numbers, it is impossible to maintain exact precision - each number in the sequence must have a finite number of digits. As a result, the values of such a signal belong to a finite set; in other words, it is quantized. Quantization is the process of converting a continuous analog audio signal to a digital signal with discrete numerical values.\n\nSignals in nature can be converted to electronic signals by various sensors. Some examples are:\n\n- \"Motion\". The motion of an object can be considered to be a signal, and can be monitored by various sensors to provide electrical signals. For example, radar can provide an electromagnetic signal for following aircraft motion. A motion signal is one-dimensional (time), and the range is generally three-dimensional. Position is thus a 3-vector signal; position and orientation of a rigid body is a 6-vector signal. Orientation signals can be generated using a gyroscope.\n- \"Sound\". Since a sound is a vibration of a medium (such as air), a sound signal associates a pressure value to every value of time and three space coordinates. A sound signal is converted to an electrical signal by a microphone, generating a voltage signal as an analog of the sound signal, making the sound signal available for further signal processing. Sound signals can be sampled at a discrete set of time points; for example, compact discs (CDs) contain discrete signals representing sound, recorded at 44,100 samples per second; each sample contains data for a left and right channel, which may be considered to be a 2-vector signal (since CDs are recorded in stereo). The CD encoding is converted to an electrical signal by reading the information with a laser, converting the sound signal to an optical signal.\n- \"Images\". A picture or image consists of a brightness or color signal, a function of a two-dimensional location. The object's appearance is presented as an emitted or reflected electromagnetic wave, one form of electronic signal. It can be converted to voltage or current waveforms using devices such as the charge-coupled device. A 2D image can have a continuous spatial domain, as in a traditional photograph or painting; or the image can be discretized in space, as in a raster scanned digital image. Color images are typically represented as a combination of images in three primary colors, so that the signal is vector-valued with dimension three.\n- \"Videos\". A video signal is a sequence of images. A point in a video is identified by its two-dimensional position and by the time at which it occurs, so a video signal has a three-dimensional domain. Analog video has one continuous domain dimension (across a scan line) and two discrete dimensions (frame and line).\n- Biological \"membrane potentials\". The value of the signal is an electric potential (\"voltage\"). The domain is more difficult to establish. Some cells or organelles have the same membrane potential throughout; neurons generally have different potentials at different points. These signals have very low energies, but are enough to make nervous systems work; they can be measured in aggregate by the techniques of electrophysiology.\n\nOther examples of signals are the output of a thermocouple, which conveys temperature information, and the output of a pH meter which conveys acidity information.\n\nA typical role for signals is in signal processing. A common example is signal transmission between different locations. The embodiment of a signal in electrical form is made by a transducer that converts the signal from its original form to a waveform expressed as a current (\"I\") or a voltage (\"V\"), or an electromagnetic waveform, for example, an optical signal or radio transmission. Once expressed as an electronic signal, the signal is available for further processing by electrical devices such as electronic amplifiers and electronic filters, and can be transmitted to a remote location by electronic transmitters and received using electronic receivers.\n\nIn Electrical engineering programs, a class and field of study known as \"signals and systems\" (S and S) is often seen as the \"cut class\" for EE careers, and is dreaded by some students as such. Depending on the school, undergraduate EE students generally take the class as juniors or seniors, normally depending on the number and level of previous linear algebra and differential equation classes they have taken.\n\nThe field studies input and output signals, and the mathematical representations between them known as systems, in four domains: Time, Frequency, \"s\" and \"z\". Since signals and systems are both studied in these four domains, there are 8 major divisions of study. As an example, when working with continuous time signals (\"t\"), one might transform from the time domain to a frequency or \"s\" domain; or from discrete time (\"n\") to frequency or \"z\" domains. Systems also can be transformed between these domains like signals, with continuous to \"s\" and discrete to \"z\".\n\nAlthough S and S falls under and includes all the topics covered in this article, as well as Analog signal processing and Digital signal processing, it actually is a subset of the field of Mathematical modeling. The field goes back to RF over a century ago, when it was all analog, and generally continuous. Today, software has taken the place of much of the analog circuitry design and analysis, and even continuous signals are now generally processed digitally. Ironically, digital signals also are processed continuously in a sense, with the software doing calculations between discrete signal \"rests\" to prepare for the next input/transform/output event.\n\nIn past EE curricula S and S, as it is often called, involved circuit analysis and design via mathematical modeling and some numerical methods, and was updated several decades ago with Dynamical systems tools including differential equations, and recently, Lagrangians. The difficulty of the field at that time included the fact that not only mathematical modeling, circuits, signals and complex systems were being modeled, but physics as well, and a deep knowledge of electrical (and now electronic) topics also was involved and required.\n\nToday, the field has become even more daunting and complex with the addition of circuit, systems and signal analysis and design languages and software, from MATLAB and Simulink to NumPy, VHDL, PSpice, Verilog and even Assembly language. Students are expected to understand the tools as well as the mathematics, physics, circuit analysis, and transformations between the 8 domains.\n\nBecause mechanical engineering topics like friction, dampening etc. have very close analogies in signal science (inductance, resistance, voltage, etc.), many of the tools originally used in ME transformations (Laplace and Fourier transforms, Lagrangians, sampling theory, probability, difference equations, etc.) have now been applied to signals, circuits, systems and their components, analysis and design in EE. Dynamical systems that involve noise, filtering and other random or chaotic attractors and repellors have now placed stochastic sciences and statistics between the more deterministic discrete and continuous functions in the field. (Deterministic as used here means signals that are completely determined as functions of time).\n\nEE taxonomists are still not decided where S&S falls within the whole field of signal processing vs. circuit analysis and mathematical modeling, but the common link of the topics that are covered in the course of study has brightened boundaries with dozens of books, journals, etc. called Signals and Systems, and used as text and test prep for the EE, as well as, recently, computer engineering exams.\n\n", "related": "\n- Current loop – a signaling system in widespread use for process control\n- Impulse function\n- Signal noise\n- Signal to noise ratio\n- Signal processing\n- Digital signal processing\n- Signal strength\n- Image processing\n\n- Hsu, P. H. \"Schaum's Theory and Problems: Signals and Systems\", McGraw-Hill 1995,\n- Lathi, B.P., \"Signal Processing & Linear Systems\", Berkeley-Cambridge Press, 1998,\n- Shannon, C. E., 2005 [1948], \"A Mathematical Theory of Communication,\" (corrected reprint), accessed Dec. 15, 2005. Orig. 1948, \"Bell System Technical Journal\", vol. 27, pp. 379–423, 623-656.\n"}
{"id": "965419", "url": "https://en.wikipedia.org/wiki?curid=965419", "title": "Stochastic resonance", "text": "Stochastic resonance\n\nStochastic resonance (SR) is a phenomenon where a signal that is normally too weak to be detected by a sensor, can be boosted by adding white noise to the signal, which contains a wide spectrum of frequencies. The frequencies in the white noise corresponding to the original signal's frequencies will resonate with each other, amplifying the original signal while not amplifying the rest of the white noise (thereby increasing the signal-to-noise ratio which makes the original signal more prominent). Further, the added white noise can be enough to be detectable by the sensor, which can then filter it out to effectively detect the original, previously undetectable signal.\n\nThis phenomenon of boosting undetectable signals by resonating with added white noise extends to many other systems, whether electromagnetic, physical or biological, and is an area of research.\n\nStochastic resonance (SR) is observed when noise added to a system changes the system's behaviour in some fashion. More technically, SR occurs if the signal-to-noise ratio of a nonlinear system or device increases for moderate values of noise intensity. It often occurs in bistable systems or in systems with a sensory threshold and when the input signal to the system is \"sub-threshold\". For lower noise intensities, the signal does not cause the device to cross threshold, so little signal is passed through it. For large noise intensities, the output is dominated by the noise, also leading to a low signal-to-noise ratio. For moderate intensities, the noise allows the signal to reach threshold, but the noise intensity is not so large as to swamp it. Thus, a plot of signal-to-noise ratio as a function of noise intensity contains a peak.\n\nStrictly speaking, stochastic resonance occurs in bistable systems, when a small periodic (sinusoidal) force is applied together with a large wide band stochastic force (noise). The system response is driven by the combination of the two forces that compete/cooperate to make the system switch between the two stable states. The degree of order is related to the amount of periodic function that it shows in the system response. When the periodic force is chosen small enough in order to not make the system response switch, the presence of a non-negligible noise is required for it to happen. When the noise is small very few switches occur, mainly at random with no significant periodicity in the system response. When the noise is very strong a large number of switches occur for each period of the sinusoid and the system response does not show remarkable periodicity. Between these two conditions, there exists an optimal value of the noise that cooperatively concurs with the periodic forcing in order to make almost exactly one switch per period (a maximum in the signal-to-noise ratio).\n\nSuch a favorable condition is quantitatively determined by the matching of two time scales: the period of the sinusoid (the deterministic time scale) and the Kramers rate (i.e., the average switch rate induced by the sole noise: the inverse of the stochastic time scale). Thus the term \"stochastic resonance\".\n\nStochastic resonance was discovered and proposed for the first time in 1981 to explain the periodic recurrence of ice ages. Since then the same principle has been applied in a wide variety of systems. Nowadays stochastic resonance is commonly invoked when noise and nonlinearity concur to determine an increase of order in the system response.\n\nSuprathreshold stochastic resonance is a particular form of stochastic resonance. It is the phenomenon where random fluctuations, or noise, provide a signal processing benefit in a nonlinear system. Unlike most of the nonlinear systems where stochastic resonance occurs, suprathreshold stochastic resonance occurs not only when the strength of the fluctuations is small relative to that of an input signal, but occurs even for the smallest amount of random noise. Furthermore, it is not restricted to a subthreshold signal, hence the qualifier.\n\nStochastic resonance has been observed in the neural tissue of the sensory systems of several organisms. Computationally, neurons exhibit SR because of non-linearities in their processing. SR has yet to be fully explained in biological systems, but neural synchrony in the brain (specifically in the gamma wave frequency) has been suggested as a possible neural mechanism for SR by researchers who have investigated the perception of \"subconscious\" visual sensation. Single neurons \"in vitro\" including cerebellar Purkinje cells and squid giant axon could also demonstrate the inverse stochastic resonance, when spiking is inhibited by synaptic noise of a particular variance.\n\nSR-based techniques have been used to create a novel class of medical devices for enhancing sensory and motor functions such as vibrating insoles especially for the elderly, or patients with diabetic neuropathy or stroke.\n\nSee the \"Review of Modern Physics\" article for a comprehensive overview of stochastic resonance.\n\nStochastic Resonance has found noteworthy application in the field of image processing.\n\nA related phenomenon is dithering applied to analog signals before analog-to-digital conversion. Stochastic resonance can be used to measure transmittance amplitudes below an instrument's detection limit. If Gaussian noise is added to a subthreshold (i.e., immeasurable) signal, then it can be brought into a detectable region. After detection, the noise is removed. A fourfold improvement in the detection limit can be obtained.\n\n", "related": "\n- Mutual coherence (linear algebra)\n- Signal detection theory\n- Stochastic resonance (sensory neurobiology)\n\n- Hannes Risken \"The Fokker-Planck Equation\", 2nd edition, Springer, 1989\n\n- N. G. Stocks, \"Suprathreshold stochastic resonance in multilevel threshold systems,\" \"Physical Review Letters,\" 84, pp. 2310–2313, 2000.\n- M. D. McDonnell, D. Abbott, and C. E. M. Pearce, \"An analysis of noise enhanced information transmission in an array of comparators,\" \"Microelectronics Journal\" 33, pp. 1079–1089, 2002.\n- M. D. McDonnell and N. G. Stocks, \"Suprathreshold stochastic resonance,\" \"Scholarpedia\" 4, Article No. 6508, 2009.\n- M. D. McDonnell, N. G. Stocks, C. E. M. Pearce, D. Abbott, \"Stochastic Resonance: From Suprathreshold Stochastic Resonance to Stochastic Signal Quantization\", Cambridge University Press, 2008.\n\n- Scholar Google profile on stochastic resonance\n- Newsweek \"Being messy, both at home and in foreign policy, may have its own advantages\" Retrieved 3 Jan 2011\n- Stochastic Resonance Conference 1998–2008 ten years of continuous growth. 17-21 Aug. 2008, Perugia (Italy)\n- Stochastic Resonance - From Suprathreshold Stochastic Resonance to Stochastic Signal Quantization (book)\n- Review of Suprathreshold Stochastic Resonance\n- A.S. Samardak, A. Nogaret, N. B. Janson, A. G. Balanov, I. Farrer and D. A. Ritchie. \"Noise-Controlled Signal Transmission in a Multithread Semiconductor Neuron\" // Phys. Rev. Lett. 102 (2009) 226802,\n"}
{"id": "18104627", "url": "https://en.wikipedia.org/wiki?curid=18104627", "title": "Ergodic process", "text": "Ergodic process\n\nIn econometrics and signal processing, a stochastic process is said to be ergodic if its statistical properties can be deduced from a single, sufficiently long, random sample of the process. The reasoning is that any collection of random samples from a process must represent the average statistical properties of the entire process. In other words, regardless of what the individual samples are, a birds-eye view of the collection of samples must represent the whole process. Conversely, a process that is not ergodic is a process that changes erratically at an inconsistent rate.\nOne can discuss the ergodicity of various statistics of a stochastic process. For example, a wide-sense stationary process formula_1 has constant mean\n\nand autocovariance \n\nthat depends only on the lag formula_4 and not on time formula_5. The properties formula_6 and formula_7\nare ensemble averages not time averages.\n\nThe process formula_1 is said to be mean-ergodic or mean-square ergodic in the first moment\nif the time average estimate\n\nconverges in squared mean to the ensemble average formula_6 as formula_11.\n\nLikewise,\nthe process is said to be autocovariance-ergodic or d moment \nif the time average estimate\n\nconverges in squared mean to the ensemble average formula_7, as formula_11.\nA process which is ergodic in the mean and autocovariance is sometimes called ergodic in the wide sense.\n\nThe notion of ergodicity also applies to discrete-time random processes\nformula_15 for integer formula_16.\n\nA discrete-time random process formula_15 is ergodic in mean if\n\nconverges in squared mean\nto the ensemble average formula_19,\nas formula_20.\n\n- An unbiased random walk is non-ergodic. Its expectation value is zero at all times, whereas its time average is a random variable with divergent variance.\n\n- Suppose that we have two coins: one coin is fair and the other has two heads. We choose (at random) one of the coins \"first\", and \"then\" perform a sequence of independent tosses of our selected coin. Let \"X\"[\"n\"] denote the outcome of the \"n\"th toss, with 1 for heads and 0 for tails. Then the ensemble average is   ( +  1) = ; yet the long-term average is for the fair coin and 1 for the two-headed coin. So the long term time-average is \"either\" 1/2 or 1. Hence, this random process is not ergodic in mean.\n\n", "related": "\n- Ergodic hypothesis\n- Ergodicity\n- Ergodic theory, a branch of mathematics concerned with a more general formulation of ergodicity\n- Loschmidt's paradox\n- Poincaré recurrence theorem\n\n"}
{"id": "34991471", "url": "https://en.wikipedia.org/wiki?curid=34991471", "title": "Time-varied gain", "text": "Time-varied gain\n\nTime varied gain (TVG) is signal compensation that is applied by the receiver electronics through analog or digital signal processing. The desired result is that targets of the same size produce echoes of the same size, regardless of target range.\n\n", "related": "\n- Automatic gain control\n"}
{"id": "33270868", "url": "https://en.wikipedia.org/wiki?curid=33270868", "title": "EigenMoments", "text": "EigenMoments\n\nEigenMoments is a set of orthogonal, noise robust, invariant to rotation, scaling and translation and distribution sensitive moments. Their application can be found in signal processing and computer vision as descriptors of the signal or image. The descriptors can later be used for classification purposes.\n\nIt is obtained by performing orthogonalization, via eigen analysis on geometric moments.\n\nEigenMoments are computed by performing eigen analysis on the moment space of an image by maximizing Signal to Noise Ratio in the feature space in form of Rayleigh quotient.\n\nThis approach has several benefits in Image processing applications:\n1. Dependency of moments in the moment space on the distribution of the images being transformed, ensures decorrelation of the final feature space after eigen analysis on the moment space.\n2. The ability of EigenMoments to take into account distribution of the image makes it more versatile and adaptable for different genres.\n3. Generated moment kernels are orthogonal and therefore analysis on the moment space becomes easier. Transformation with orthogonal moment kernels into moment space is analogous to projection of the image onto a number of orthogonal axes.\n4. Nosiy components can be removed. This makes EigenMoments robust for classification applications.\n5. Optimal information compaction can be obtained and therefore a few number of moments are needed to characterize the images.\n\nAssume that a signal vector formula_1 is taken from a certain distribution having coorelation formula_2, i.e. formula_3 where E[.] denotes expected value.\n\nDimension of signal space, n, is often too large to be useful for practical application such as pattern classification, we need to transform the signal space into a space with lower dimensionality.\n\nThis is performed by a two-step linear transformation:\n\nformula_4\n\nwhere formula_5 is the transformed signal, formula_6 a fixed transformation matrix which transforms the signal into the moment space, and formula_7 the transformation matrix which we are going to determine by maximizing the SNR of the feature space resided by formula_8. For the case of Geometric Moments, X would be the monomials. If formula_9, a full rank transformation would result, however usually we have formula_10 and formula_11. This is specially the case when formula_12 is of high dimensions.\n\nFinding formula_13 that maximizes the SNR of the feature space:\n\nformula_14\n\nwhere N is the correlation matrix of the noise signal. The problem can thus be formulated as\n\nformula_15\n\nsubject to constraints:\n\nformula_16 where formula_17 is the Kronecker delta.\n\nIt can be observed that this maximization is Rayleigh quotient by letting formula_18 and formula_19 and therefore can be written as:\n\nformula_20, formula_21\n\nOptimization of Rayleigh quotient has the form:\n\nformula_22\n\nand formula_23 and formula_24, both are symmetric and formula_24 is positive definite and therefore invertible.\nScaling formula_26 does not change the value of the object function and hence and additional scalar constraint formula_27 can be imposed on formula_26 and no solution would be lost when the objective function is optimized.\n\nThis constraint optimization problem can be solved using Lagrangian multiplier:\n\nformula_29 subject to formula_30\n\nformula_31\n\nequating first derivative to zero and we will have:\n\nformula_32\n\nwhich is an instance of Generalized Eigenvalue Problem (GEP).\nThe GEP has the form:\n\nformula_33\n\nfor any pair formula_34 that is a solution to above equation, formula_26 is called a generalized eigenvector and formula_36 is called a generalized eigenvalue.\n\nFinding formula_26 and formula_38 that satisfies this equations would produce the result which optimizes Rayleigh quotient.\n\nOne way of maximizing Rayleigh quotient is through solving the Generalized Eigen Problem. Dimension reduction can be performed by simply choosing the first components formula_39, formula_40, with the highest values for formula_41 out of the formula_42 components, and discard the rest. Interpretation of this transformation is rotating and scaling the moment space, transforming it into a feature space with maximized SNR and therefore, the first formula_43 components are the components with highest formula_43 SNR values.\n\nThe other method to look at this solution is to use the concept of simultaneous diagonalization instead of Generalized Eigen Problem.\n\n1. Let formula_18 and formula_19 as mentioned earlier. We can write formula_13 as two separate transformation matrices:\n\nformula_48\n\n1. formula_49 can be found by first diagonalize B:\n\nformula_50.\n\nWhere formula_51 is a diagonal matrix sorted in increasing order. Since formula_24 is positive definite, thus formula_53. We can discard those eigenvalues that large and retain those close to 0, since this means the energy of the noise is close to 0 in this space, at this stage it is also possible to discard those eigenvectors that have large eigenvalues.\n\nLet formula_54 be the first formula_43 columns of formula_56, now formula_57 where formula_58 is the formula_59 principal submatrix of formula_51.\n\n1. Let\n\nformula_61\n\nand hence:\n\nformula_62.\n\nformula_49 whiten formula_24 and reduces the dimensionality from formula_42 to formula_43. The transformed space resided by formula_67 is called the noise space.\n\n1. Then, we diagonalize formula_68:\n\nformula_69,\n\nwhere formula_70. formula_71 is the matrix with eigenvalues of formula_68 on its diagonal. We may retain all the eigenvalues and their corresponding eigenvectors since the most of the noise are already discarded in previous step.\n\n1. Finally the transformation is given by:\n\nformula_73\n\nwhere formula_13 diagonalizes both the numerator and denominator of the SNR,\n\nformula_75, formula_76 and the transformation of signal formula_77 is defined as formula_78.\n\nTo find the information loss when we discard some of the eigenvalues and eigenvectors we can perform following analysis:\n\nformula_79\n\nEigenmoments are derived by applying the above framework on Geometric Moments. They can be derived for both 1D and 2D signals.\n\nIf we let formula_80, i.e. the monomials, after the transformation formula_81 we obtain Geometric Moments, denoted by vector formula_82, of signal formula_83,i.e. formula_84.\n\nIn practice it is difficult to estimate the correlation signal due to insufficient number of samples, therefore parametric approaches are utilized.\n\nOne such model can be defined as:\n\nformula_85,\n\nwhere formula_86. This model of correlation can be replaced by other models however this model covers general natural images.\n\nSince formula_87 does not affect the maximization it can be dropped.\n\nformula_88\n\nThe correlation of noise can be modelled as formula_89, where formula_90 is the energy of noise. Again formula_90 can be dropped because the constant does not have any effect on the maximization problem.\n\nformula_92\nformula_93\n\nUsing the computed A and B and applying the algorithm discussed in previous section we find formula_13 and set of transformed monomials formula_95 which produces the moment kernels of EM. The moment kernels of EM decorrelate the correlation in the image.\n\nformula_96,\n\nand are orthogonal:\n\nformula_97\n\nTaking formula_98, the dimension of moment space as formula_99 and the dimension of feature space as formula_100, we will have:\n\nformula_101\n\nand\n\nformula_102\n\nThe derivation for 2D signal is the same as 1D signal except that conventional Geometric Moments are directly employed to obtain the set of 2D EigenMoments.\n\nThe definition of Geometric Moments of order formula_103 for 2D image signal is:\n\nformula_104.\n\nwhich can be denoted as formula_105. Then the set of 2D EigenMoments are:\n\nformula_106,\n\nwhere formula_107 is a matrix that contains the set of EigenMoments.\n\nformula_108.\n\nIn order to obtain a set of moment invariants we can use normalized Geometric Moments formula_109 instead of formula_82.\n\nNormalized Geometric Moments are invariant to Rotation, Scaling and Transformation and defined by:\n\nformula_111\n\nwhere:formula_112 is the centroid of the image formula_113 and\n\nformula_114.\n\nformula_115 in this equation is a scaling factor depending on the image. formula_115 is usually set to 1 for binary images.\n\n", "related": "\n- Computer vision\n- Signal processing\n- Image moment\n\n- implementation of EigenMoments in Matlab\n"}
{"id": "4326149", "url": "https://en.wikipedia.org/wiki?curid=4326149", "title": "Hann function", "text": "Hann function\n\nThe Hann function of length formula_1 used to perform Hann smoothing, is named after the Austrian meteorologist Julius von Hann, is a window function given by:\n\nFor digital signal processing, the function can be sampled symmetrically as:\n\nwhere the length of the window is formula_4 and N can be even or odd. (see Window function#Hann and Hamming windows) It is also known as the raised cosine window, Hann filter, von Hann window, etc.\n\nThe Hann window is a linear combination of modulated rectangular windows:\n\nUsing Euler's formula to expand the cosine term, we can write:\n\nwhose Fourier transform is just:\n\nThe Discrete-time Fourier transform (DTFT) of the N+1 length, time-shifted sequence is defined by a Fourier series, which also has a 3-term equivalent that is derived similarly to the Fourier transform derivation:\n\nFor even values of N, the truncated sequence formula_9 is a DFT-even (aka \"periodic\") Hann window. Since the truncated sample has value zero, it is clear from the Fourier series definition that the DTFTs are equivalent. However, the approach followed above results in a significantly different-looking, but equivalent, 3-term expression:\n\nAn N-length DFT of the window function samples the DTFT at frequencies formula_11 for integer values of formula_12 From the expression immediately above, it is easy to see that only 3 of the N DFT coefficients are non-zero. And from the other expression, it is apparent that all are real-valued. These properties are appealing for real-time applications that require both windowed and non-windowed (rectangularly windowed) transforms, because the windowed transforms can be efficiently derived from the non-windowed transforms by convolution.\n\nThe function is named in honour of von Hann, who used the three-term weighted average smoothing technique on meteorological data. However, the erroneous \"Hanning\" function is also heard of on occasion, derived from the paper in which it was named, where the term \"hanning a signal\" was used to mean applying the Hann window to it. The confusion arose from the similar Hamming function, named after Richard Hamming.\n\n", "related": "\n- Window function\n- Apodization\n- Raised cosine distribution\n- Raised-cosine filter\n\n- Hann function at MathWorld\n"}
{"id": "1398487", "url": "https://en.wikipedia.org/wiki?curid=1398487", "title": "Super-resolution imaging", "text": "Super-resolution imaging\n\nSuper-resolution imaging (SR) is a class of techniques that enhance (increase) the resolution of an imaging system. In optical SR the diffraction limit of systems is transcended, while in geometrical SR the resolution of digital imaging sensors is enhanced.\n\nIn some radar and sonar imaging applications (e.g. magnetic resonance imaging (MRI), high-resolution computed tomography), subspace decomposition-based methods (e.g. MUSIC) and compressed sensing-based algorithms (e.g., SAMV) are employed to achieve SR over standard periodogram algorithm.\n\nSuper-resolution imaging techniques are used in general image processing and in super-resolution microscopy.\n\nBecause some of the ideas surrounding super-resolution raise fundamental issues, there is need at the outset to examine the relevant physical and information-theoretical principles:\n\n- Diffraction limit: The detail of a physical object that an optical instrument can reproduce in an image has limits that are mandated by laws of physics, whether formulated by the diffraction equations in the wave theory of light or the uncertainty principle for photons in quantum mechanics. Information transfer can never be increased beyond this boundary, but packets outside the limits can be cleverly swapped for (or multiplexed with) some inside it. One does not so much “break” as “run around” the diffraction limit. New procedures probing electro-magnetic disturbances at the molecular level (in the so-called near field) remain fully consistent with Maxwell's equations.\n- Spatial-frequency domain: A succinct expression of the diffraction limit is given in the spatial-frequency domain. In Fourier optics light distributions are expressed as superpositions of a series of grating light patterns in a range of fringe widths, technically spatial frequencies. It is generally taught that diffraction theory stipulates an upper limit, the cut-off spatial-frequency, beyond which pattern elements fail to be transferred into the optical image, i.e., are not resolved. But in fact what is set by diffraction theory is the width of the passband, not a fixed upper limit. No laws of physics are broken when a spatial frequency band beyond the cut-off spatial frequency is swapped for one inside it: this has long been implemented in dark-field microscopy. Nor are information-theoretical rules broken when superimposing several bands, disentangling them in the received image needs assumptions of object invariance during multiple exposures, i.e., the substitution of one kind of uncertainty for another.\n- Information: When the term super-resolution is used in techniques of inferring object details from statistical treatment of the image within standard resolution limits, for example, averaging multiple exposures, it involves an exchange of one kind of information (extracting signal from noise) for another (the assumption that the target has remained invariant).\n- Resolution and localization: True resolution involves the distinction of whether a target, e.g. a star or a spectral line, is single or double, ordinarily requiring separable peaks in the image. When a target is known to be single, its location can be determined with higher precision than the image width by finding the centroid (center of gravity) of its image light distribution. The word \"ultra-resolution\" had been proposed for this process but it did not catch on, and the high-precision localization procedure is typically referred to as super-resolution.\n\nThe technical achievements of enhancing the performance of imaging-forming and –sensing devices now classified as super-resolution utilize to the fullest but always stay within the bounds imposed by the laws of physics and information theory.\n\nSubstituting spatial-frequency bands: Though the bandwidth allowable by diffraction is fixed, it can be positioned anywhere in the spatial-frequency spectrum. Dark-field illumination in microscopy is an example. See also aperture synthesis.\n\nAn image is formed using the normal passband of the optical device. Then some known light structure, for example a set of light fringes that is also within the passband, is superimposed on the target. The image now contains components resulting from the combination of the target and the superimposed light structure, e.g. moiré fringes, and carries information about target detail which simple, unstructured illumination does not. The “superresolved” components, however, need disentangling to be revealed. For an example, see structured illumination (figure to left).\n\nIf a target has no special polarization or wavelength properties, two polarization states or non-overlapping wavelength regions can be used to encode target details, one in a spatial-frequency band inside the cut-off limit the other beyond it. Both would utilize normal passband transmission but are then separately decoded to reconstitute target structure with extended resolution.\n\nThe usual discussion of super-resolution involved conventional imagery of an object by an optical system. But modern technology allows probing the electromagnetic disturbance within molecular distances of the source which has superior resolution properties, see also evanescent waves and the development of the new Super lens.\n\nWhen an image is degraded by noise, there can be more detail in the average of many exposures, even within the diffraction limit. See example on the right.\n\nKnown defects in a given imaging situation, such as defocus or aberrations, can sometimes be mitigated in whole or in part by suitable spatial-frequency filtering of even a single image. Such procedures all stay within the diffraction-mandated passband, and do not extend it.\n\nThe location of a single source can be determined by computing the \"center of gravity\" (centroid) of the light distribution extending over several adjacent pixels (see figure on the left). Provided that there is enough light, this can be achieved with arbitrary precision, very much better than pixel width of the detecting apparatus and the resolution limit for the decision of whether the source is single or double. This technique, which requires the presupposition that all the light comes from a single source, is at the basis of what has become known as super-resolution microscopy, e.g. stochastic optical reconstruction microscopy (STORM), where fluorescent probes attached to molecules give nanoscale distance information. It is also the mechanism underlying visual hyperacuity.\n\nSome object features, though beyond the diffraction limit, may be known to be associated with other object features that are within the limits and hence contained in the image. Then conclusions can be drawn, using statistical methods, from the available image data about the presence of the full object. The classical example is Toraldo di Francia's proposition of judging whether an image is that of a single or double star by determining whether its width exceeds the spread from a single star. This can be achieved at separations well below the classical resolution bounds, and requires the prior limitation to the choice \"single or double?\"\n\nThe approach can take the form of extrapolating the image in the frequency domain, by assuming that the object is an analytic function, and that we can exactly know the function values in some interval. This method is severely limited by the ever-present noise in digital imaging systems, but it can work for radar, astronomy, microscopy or magnetic resonance imaging. More recently, a fast single image super-resolution algorithm based on a closed-form solution to \"formula_1\" problems has been proposed and demonstrated to accelerate most of the existing Bayesian super-resolution methods significantly.\n\nGeometrical SR reconstruction algorithms are possible if and only if the input low resolution images have been under-sampled and therefore contain aliasing. Because of this aliasing, the high-frequency content of the desired reconstruction image is embedded in the low-frequency content of each of the observed images. Given a sufficient number of observation images, and if the set of observations vary in their phase (i.e. if the images of the scene are shifted by a sub-pixel amount), then the phase information can be used to separate the aliased high-frequency content from the true low-frequency content, and the full-resolution image can be accurately reconstructed.\n\nIn practice, this frequency-based approach is not used for reconstruction, but even in the case of spatial approaches (e.g. shift-add fusion), the presence of aliasing is still a necessary condition for SR reconstruction.\n\nThere are both single-frame and multiple-frame variants of SR. Multiple-frame SR uses the sub-pixel shifts between multiple low resolution images of the same scene. It creates an improved resolution image fusing information from all low resolution images, and the created higher resolution images are better descriptions of the scene. Single-frame SR methods attempt to magnify the image without producing blur. These methods use other parts of the low resolution images, or other unrelated images, to guess what the high-resolution image should look like. Algorithms can also be divided by their domain: frequency or space domain. Originally, super-resolution methods worked well only on grayscale images, but researchers have found methods to adapt them to color camera images. Recently, the use of super-resolution for 3D data has also been shown.\n\nThere is promising research on using deep convolutional networks to perform super-resolution. In particular work has been demonstrated showing the transformation of a 20x microscope image of pollen grains into a 1500x scanning electron microscope image using it.\n\n", "related": "\n- Optical resolution\n- Oversampling\n\n- ;\n"}
{"id": "1434444", "url": "https://en.wikipedia.org/wiki?curid=1434444", "title": "Autoregressive model", "text": "Autoregressive model\n\nIn statistics, econometrics and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.\n\nContrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root.\n\nThe notation formula_1 indicates an autoregressive model of order \"p\". The AR(\"p\") model is defined as\n\nwhere formula_3 are the \"parameters\" of the model, formula_4 is a constant, and formula_5 is white noise. This can be equivalently written using the backshift operator \"B\" as\n\nso that, moving the summation term to the left side and using polynomial notation, we have\n\nAn autoregressive model can thus be viewed as the output of an all-pole infinite impulse response filter whose input is white noise.\n\nSome parameter constraints are necessary for the model to remain wide-sense stationary. For example, processes in the AR(1) model with formula_8 are not stationary. More generally, for an AR(\"p\") model to be wide-sense stationary, the roots of the polynomial formula_9 must lie outside the unit circle, i.e., each (complex) root formula_10 must satisfy formula_11 (see pages 88,90 ).\n\nIn an AR process, a one-time shock affects values of the evolving variable infinitely far into the future. For example, consider the AR(1) model formula_12. A non-zero value for formula_5 at say time \"t\"=1 affects formula_14 by the amount formula_15. Then by the AR equation for formula_16 in terms of formula_14, this affects formula_16 by the amount formula_19. Then by the AR equation for formula_20 in terms of formula_16, this affects formula_20 by the amount formula_23. Continuing this process shows that the effect of formula_15 never ends, although if the process is stationary then the effect diminishes toward zero in the limit.\n\nBecause each shock affects \"X\" values infinitely far into the future from when they occur, any given value \"X\" is affected by shocks occurring infinitely far into the past. This can also be seen by rewriting the autoregression\n\n(where the constant term has been suppressed by assuming that the variable has been measured as deviations from its mean) as\n\nWhen the polynomial division on the right side is carried out, the polynomial in the backshift operator applied to formula_5 has an infinite order—that is, an infinite number of lagged values of formula_5 appear on the right side of the equation.\n\nThe autocorrelation function of an AR(\"p\") process can be expressed as \n\nwhere formula_30 are the roots of the polynomial\n\nwhere \"B\" is the backshift operator, where formula_32 is the function defining the autoregression, and where formula_33 are the coefficients in the autoregression.\n\nThe autocorrelation function of an AR(\"p\") process is a sum of decaying exponentials.\n- Each real root contributes a component to the autocorrelation function that decays exponentially.\n- Similarly, each pair of complex conjugate roots contributes an exponentially damped oscillation.\n\nThe simplest AR process is AR(0), which has no dependence between the terms. Only the error/innovation/noise term contributes to the output of the process, so in the figure, AR(0) corresponds to white noise.\n\nFor an AR(1) process with a positive formula_34, only the previous term in the process and the noise term contribute to the output. If formula_34 is close to 0, then the process still looks like white noise, but as formula_34 approaches 1, the output gets a larger contribution from the previous term relative to the noise. This results in a \"smoothing\" or integration of the output, similar to a low pass filter.\n\nFor an AR(2) process, the previous two terms and the noise term contribute to the output. If both formula_37 and formula_38 are positive, the output will resemble a low pass filter, with the high frequency part of the noise decreased. If formula_37 is positive while formula_38 is negative, then the process favors changes in sign between terms of the process. The output oscillates. This can be likened to edge detection or detection of change in direction.\n\nAn AR(1) process is given by:\n\nwhere formula_5 is a white noise process with zero mean and constant variance formula_43.\n(Note: The subscript on formula_37 has been dropped.) The process is wide-sense stationary if formula_45 since it is obtained as the output of a stable filter whose input is white noise. (If formula_46 then the variance of formula_47 depends on time lag t, so that the variance of the series diverges to infinity as t goes to infinity, and is therefore not wide sense stationary.) Assuming formula_45, the mean formula_49 is identical for all values of \"t\" by the very definition of wide sense stationarity. If the mean is denoted by formula_50, it follows from\n\nthat\n\nand hence\n\nIn particular, if formula_54, then the mean is 0.\n\nThe variance is\n\nwhere formula_56 is the standard deviation of formula_5. This can be shown by noting that\nand then by noticing that the quantity above is a stable fixed point of this relation.\n\nThe autocovariance is given by\n\nwhich yields a Lorentzian profile for the spectral density:\n\nwhere formula_61 is the angular frequency associated with the decay time formula_62.\n\nAn alternative expression for formula_47 can be derived by first substituting formula_64 for formula_65 in the defining equation. Continuing this process \"N\" times yields\n\nFor \"N\" approaching infinity, formula_67 will approach zero and:\n\nIt is seen that formula_47 is white noise convolved with the formula_70 kernel plus the constant mean. If the white noise formula_5 is a Gaussian process then formula_47 is also a Gaussian process. In other cases, the central limit theorem indicates that formula_47 will be approximately normally distributed when formula_34 is close to one.\n\nThe AR(1) model is the discrete time analogy of the continuous Ornstein-Uhlenbeck process. It is therefore sometimes useful to understand the properties of the AR(1) model cast in an equivalent form. In this form, the AR(1) model, with process parameter formula_75 is given by:\n\nBy putting this in the form formula_79, and then expanding the series for formula_80, one can show that:\n\nThe partial autocorrelation of an AR(p) process equates zero at lag which is not bigger than order of p and provide a good model for the correlation between formula_83and formula_84, so the appropriate maximum lag is the one beyond which the partial autocorrelations are all zero.\n\nThere are many ways to estimate the coefficients, such as the ordinary least squares procedure or method of moments (through Yule–Walker equations).\n\nThe AR(\"p\") model is given by the equation\n\nIt is based on parameters formula_86 where \"i\" = 1, ..., \"p\". There is a direct correspondence between these parameters and the covariance function of the process, and this correspondence can be inverted to determine the parameters from the autocorrelation function (which is itself obtained from the covariances). This is done using the Yule–Walker equations.\n\nThe Yule–Walker equations, named for Udny Yule and Gilbert Walker, are the following set of equations.\n\nwhere , yielding equations. Here formula_88 is the autocovariance function of X, formula_56 is the standard deviation of the input noise process, and formula_90 is the Kronecker delta function.\n\nBecause the last part of an individual equation is non-zero only if , the set of equations can be solved by representing the equations for in matrix form, thus getting the equation\n\nwhich can be solved for all formula_92 The remaining equation for \"m\" = 0 is\n\nwhich, once formula_94 are known, can be solved for formula_95\n\nAn alternative formulation is in terms of the autocorrelation function. The AR parameters are determined by the first p+1 elements formula_96 of the autocorrelation function. The full autocorrelation function can then be derived by recursively calculating\n\nExamples for some Low-order AR(\"p\") processes\n- p=1\n- formula_98\n- Hence formula_99\n- p=2\n- The Yule–Walker equations for an AR(2) process are\n-  formula_100\n-  formula_101\n-  Remember that formula_102\n-  Using the first equation yields formula_103\n-  Using the recursion formula yields formula_104\n\nThe above equations (the Yule–Walker equations) provide several routes to estimating the parameters of an AR(\"p\") model, by replacing the theoretical covariances with estimated values. Some of these variants can be described as follows:\n\n- Estimation of autocovariances or autocorrelations. Here each of these terms is estimated separately, using conventional estimates. There are different ways of doing this and the choice between these affects the properties of the estimation scheme. For example, negative estimates of the variance can be produced by some choices.\n- Formulation as a least squares regression problem in which an ordinary least squares prediction problem is constructed, basing prediction of values of \"X\" on the \"p\" previous values of the same series. This can be thought of as a forward-prediction scheme. The normal equations for this problem can be seen to correspond to an approximation of the matrix form of the Yule–Walker equations in which each appearance of an autocovariance of the same lag is replaced by a slightly different estimate.\n- Formulation as an extended form of ordinary least squares prediction problem. Here two sets of prediction equations are combined into a single estimation scheme and a single set of normal equations. One set is the set of forward-prediction equations and the other is a corresponding set of backward prediction equations, relating to the backward representation of the AR model:\n\nOther possible approaches to estimation include maximum likelihood estimation. Two distinct variants of maximum likelihood are available: in one (broadly equivalent to the forward prediction least squares scheme) the likelihood function considered is that corresponding to the conditional distribution of later values in the series given the initial \"p\" values in the series; in the second, the likelihood function considered is that corresponding to the unconditional joint distribution of all the values in the observed series. Substantial differences in the results of these approaches can occur if the observed series is short, or if the process is close to non-stationarity.\n\nThe power spectral density of an AR(\"p\") process with noise variance formula_106 is\n\nFor white noise (AR(0))\n\nFor AR(1)\n- If formula_110 there is a single spectral peak at f=0, often referred to as red noise. As formula_111 becomes nearer 1, there is stronger power at low frequencies, i.e. larger time lags. This is then a low-pass filter, when applied to full spectrum light, everything except for the red light will be filtered.\n- If formula_112 there is a minimum at f=0, often referred to as blue noise. This similarly acts as a high-pass filter, everything except for blue light will be filtered.\n\nAR(2) processes can be split into three groups depending on the characteristics of their roots:\n- When formula_114, the process has a pair of complex-conjugate roots, creating a mid-frequency peak at:\nOtherwise the process has real roots, and:\n- When formula_110 it acts as a low-pass filter on the white noise with a spectral peak at formula_117\n- When formula_112 it acts as a high-pass filter on the white noise with a spectral peak at formula_119.\nThe process is non-stationary when the roots are outside the unit circle.\nThe process is stable when the roots are within the unit circle, or equivalently when the coefficients are in the triangle formula_120.\n\nThe full PSD function can be expressed in real form as:\n\n- R, the \"stats\" package includes an \"ar\" function.\n- MATLAB's Econometrics Toolbox and System Identification Toolbox includes autoregressive models\n- Matlab and Octave: the \"TSA toolbox\" contains several estimation functions for uni-variate, multivariate and adaptive autoregressive models.\n- PyMC3: the Bayesian statistics and probabilistic programming framework supports autoregressive modes with \"p\" lags.\n- \"bayesloop\" supports parameter inference and model selection for the AR-1 process with time-varying parameters.\n- Python: implementation in statsmodels.\n\nThe impulse response of a system is the change in an evolving variable in response to a change in the value of a shock term \"k\" periods earlier, as a function of \"k\". Since the AR model is a special case of the vector autoregressive model, the computation of the impulse response in vector autoregression#impulse response applies here.\n\nOnce the parameters of the autoregression\n\nhave been estimated, the autoregression can be used to forecast an arbitrary number of periods into the future. First use \"t\" to refer to the first period for which data is not yet available; substitute the known preceding values \"X\" for \"i=\"1, ..., \"p\" into the autoregressive equation while setting the error term formula_5 equal to zero (because we forecast \"X\" to equal its expected value, and the expected value of the unobserved error term is zero). The output of the autoregressive equation is the forecast for the first unobserved period. Next, use \"t\" to refer to the \"next\" period for which data is not yet available; again the autoregressive equation is used to make the forecast, with one difference: the value of \"X\" one period prior to the one now being forecast is not known, so its expected value—the predicted value arising from the previous forecasting step—is used instead. Then for future periods the same procedure is used, each time using one more forecast value on the right side of the predictive equation until, after \"p\" predictions, all \"p\" right-side values are predicted values from preceding steps.\n\nThere are four sources of uncertainty regarding predictions obtained in this manner: (1) uncertainty as to whether the autoregressive model is the correct model; (2) uncertainty about the accuracy of the forecasted values that are used as lagged values in the right side of the autoregressive equation; (3) uncertainty about the true values of the autoregressive coefficients; and (4) uncertainty about the value of the error term formula_124 for the period being predicted. Each of the last three can be quantified and combined to give a confidence interval for the \"n\"-step-ahead predictions; the confidence interval will become wider as \"n\" increases because of the use of an increasing number of estimated values for the right-side variables.\n\nThe predictive performance of the autoregressive model can be assessed as soon as estimation has been done if cross-validation is used. In this approach, some of the initially available data was used for parameter estimation purposes, and some (from available observations later in the data set) was held back for out-of-sample testing. Alternatively, after some time has passed after the parameter estimation was conducted, more data will have become available and predictive performance can be evaluated then using the new data.\n\nIn either case, there are two aspects of predictive performance that can be evaluated: one-step-ahead and \"n\"-step-ahead performance. For one-step-ahead performance, the estimated parameters are used in the autoregressive equation along with observed values of \"X\" for all periods prior to the one being predicted, and the output of the equation is the one-step-ahead forecast; this procedure is used to obtain forecasts for each of the out-of-sample observations. To evaluate the quality of \"n\"-step-ahead forecasts, the forecasting procedure in the previous section is employed to obtain the predictions.\n\nGiven a set of predicted values and a corresponding set of actual values for \"X\" for various time periods, a common evaluation technique is to use the mean squared prediction error; other measures are also available (see forecasting#forecasting accuracy).\n\nThe question of how to interpret the measured forecasting accuracy arises—for example, what is a \"high\" (bad) or a \"low\" (good) value for the mean squared prediction error? There are two possible points of comparison. First, the forecasting accuracy of an alternative model, estimated under different modeling assumptions or different estimation techniques, can be used for comparison purposes. Second, the out-of-sample accuracy measure can be compared to the same measure computed for the in-sample data points (that were used for parameter estimation) for which enough prior data values are available (that is, dropping the first \"p\" data points, for which \"p\" prior data points are not available). Since the model was estimated specifically to fit the in-sample points as well as possible, it will usually be the case that the out-of-sample predictive performance will be poorer than the in-sample predictive performance. But if the predictive quality deteriorates out-of-sample by \"not very much\" (which is not precisely definable), then the forecaster may be satisfied with the performance.\n\n", "related": "\n- Moving average model\n- Linear difference equation\n- Predictive analytics\n- Linear predictive coding\n- Resonance\n- Levinson recursion\n- Ornstein–Uhlenbeck process\n\n\n- AutoRegression Analysis (AR) by Paul Bourke\n- by Mark Thoma\n"}
{"id": "797527", "url": "https://en.wikipedia.org/wiki?curid=797527", "title": "Homomorphic filtering", "text": "Homomorphic filtering\n\nHomomorphic filtering is a generalized technique for signal and image processing, involving a nonlinear mapping to a different domain in which linear filter techniques are applied, followed by mapping back to the original domain. This concept was developed in the 1960s by Thomas Stockham, Alan V. Oppenheim, and Ronald W. Schafer at MIT and independently by Bogert, Healy, and Tukey in their study of time series.\n\nHomomorphic filtering is sometimes used for image enhancement. It simultaneously normalizes the brightness across an image and increases contrast. Here homomorphic filtering is used to remove multiplicative noise. Illumination and reflectance are not separable, but their approximate locations in the frequency domain may be located. Since illumination and reflectance combine multiplicatively, the components are made additive by taking the logarithm of the image intensity, so that these multiplicative components of the image can be separated linearly in the frequency domain. Illumination variations can be thought of as a multiplicative noise, and can be reduced by filtering in the log domain.\n\nTo make the illumination of an image more even, the high-frequency components are increased and low-frequency components are decreased, because the high-frequency components are assumed to represent mostly the reflectance in the scene (the amount of light reflected off the object in the scene), whereas the low-frequency components are assumed to represent mostly the illumination in the scene. That is, high-pass filtering is used to suppress low frequencies and amplify high frequencies, in the log-intensity domain.\n\nHomomorphic filtering can be used for improving the appearance of a grayscale image by simultaneous intensity range compression (illumination) and contrast enhancement (reflection).\n\nformula_1\n\nWhere,\n\nm = image,\n\ni = illumination,\n\nr = reflectance\n\nWe have to transform the equation into frequency domain in order to apply high pass filter. However, it's very difficult to do calculation after applying Fourier transformation to this equation because it's not a product equation anymore. Therefore, we use 'log' to help solving this problem.\n\nformula_2\n\nThen, applying Fourier transformation\n\nformula_3\n\nOr formula_4\n\nNext, applying high-pass filter to the image. To make the illumination of an image more even, the high-frequency components are increased and low-frequency components are decrease.\n\nformula_5\n\nWhere\n\nH = any high-pass filter\n\nN = filtered image in frequency domain\n\nAfterward, returning frequency domain back to the spatial domain by using inverse Fourier transform.\n\nformula_6\n\nFinally, using exponential function to eliminate the log we used at the beginning to get the enhanced image\n\nformula_7\n\nThe following figures show the results by applying homomorphic filter, high-pass filter, and both homomorphic and high-pass filter . All figures are produced by using Matlab.\n\nAccording to the figures one to four, we can see that how homomorphic filtering is used for correcting non-uniform illumination in image, and the image become clearer than the original image. On the other hand, if we apply high pass filter to homomorphic filtered image, the edges of the images become sharper and the other areas become dimmer. This result is as similar as just doing high pass filter only to the original image.\n\nHomomorphic filtering is used in the log-spectral domain to separate filter effects from excitation effects, for example in the computation of the cepstrum as a sound representation; enhancements in the log spectral domain can improve sound intelligibility, for example in hearing aids.\n\nHomomorphic filtering was used to removes the effect of the stochastic impulse trains, which originates the sEMG signal, from the power spectrum of sEMG signal itself. In this way, only information on motor unit action potentialn (MUAP) shape and amplitude were maintained, and then, used to estimate the parameters of a time-domain model of the MUAP itself.\n\nHow individual neurons or networks encode information is the subject of numerous studies and research. In central nervous system it mainly happens by altering the spike firing rate (frequency encoding) or relative spike timing (time encoding).\nTime encoding consists of altering the random inter-spikes intervals (ISI) of the stochastic impulse train in output from a neuron. Homomorphic filtering was used in this latter case to obtain ISI variations from the power spectrum of the spike train in output from a neuron with or without the use of neuronal spontaneous activity. The ISI variations were caused by an input sinusoidal signal of unknown frequency and small amplitude, i.e. not sufficient, in absence of noise to excite the firing state. The frequency of the sinosoidal signal was recovered by using homomorphic filtering based procedures.\n\n", "related": "\n- Digital image processing\n- Image processing\n- Digital imaging\n- Image editing\n\nA.V. Oppenheim, R.W. Schafer, T.G. Stockham \"Nonlinear Filtering of Multiplied and Convolved Signals\" Proceedings of the IEEE Volume 56 No. 8 August 1968 pages 1264-1291\n\n- Overview of homomorphic filtering\n"}
{"id": "436912", "url": "https://en.wikipedia.org/wiki?curid=436912", "title": "Short-time Fourier transform", "text": "Short-time Fourier transform\n\nThe Short-time Fourier transform (STFT), is a Fourier-related transform used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time. In practice, the procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment. This reveals the Fourier spectrum on each shorter segment. One then usually plots the changing spectra as a function of time, known as a spectrogram or waterfall plot.\n\nSimply, in the continuous-time case, the function to be transformed is multiplied by a window function which is nonzero for only a short period of time. The Fourier transform (a one-dimensional function) of the resulting signal is taken as the window is slid along the time axis, resulting in a two-dimensional representation of the signal. Mathematically, this is written as:\n\nwhere formula_2 is the window function, commonly a Hann window or Gaussian window centered around zero, and formula_3 is the signal to be transformed (note the difference between the window function formula_4 and the frequency formula_5). formula_6 is essentially the Fourier Transform of formula_7, a complex function representing the phase and magnitude of the signal over time and frequency. Often phase unwrapping is employed along either or both the time axis, formula_8, and frequency axis, formula_5, to suppress any jump discontinuity of the phase result of the STFT. The time index formula_8 is normally considered to be \"slow\" time and usually not expressed in as high resolution as time formula_11.\n\nIn the discrete time case, the data to be transformed could be broken up into chunks or frames (which usually overlap each other, to reduce artifacts at the boundary). Each chunk is Fourier transformed, and the complex result is added to a matrix, which records magnitude and phase for each point in time and frequency. This can be expressed as:\n\nlikewise, with signal \"x\"[\"n\"] and window \"w\"[\"n\"]. In this case, \"m\" is discrete and ω is continuous, but in most typical applications the STFT is performed on a computer using the Fast Fourier Transform, so both variables are discrete and quantized.\n\nThe magnitude squared of the STFT yields the spectrogram representation of the Power Spectral Density of the function:\n\nSee also the modified discrete cosine transform (MDCT), which is also a Fourier-related transform that uses overlapping windows.\n\nIf only a small number of ω are desired, or if the STFT is desired to be evaluated for every shift \"m\" of the window, then the STFT may be more efficiently evaluated using a sliding DFT algorithm.\n\nThe STFT is invertible, that is, the original signal can be recovered from the transform by the Inverse STFT. The most widely accepted way of inverting the STFT is by using the overlap-add (OLA) method, which also allows for modifications to the STFT complex spectrum. This makes for a versatile signal processing method, referred to as the \"overlap and add with modifications\" method.\n\nGiven the width and definition of the window function \"w\"(\"t\"), we initially require the area of the window function to be scaled so that\n\nIt easily follows that\n\nand\n\nThe continuous Fourier Transform is\n\nSubstituting \"x\"(\"t\") from above:\n\nSwapping order of integration:\n\nSo the Fourier Transform can be seen as a sort of phase coherent sum of all of the STFTs of \"x\"(\"t\"). Since the inverse Fourier transform is\n\nthen \"x\"(\"t\") can be recovered from \"X\"(τ,ω) as\n\nor\n\nIt can be seen, comparing to above that windowed \"grain\" or \"wavelet\" of \"x\"(\"t\") is\n\nthe inverse Fourier transform of \"X\"(τ,ω) for τ fixed.\n\nOne of the pitfalls of the STFT is that it has a fixed resolution. The width of the windowing function relates to how the signal is represented—it determines whether there is good frequency resolution (frequency components close together can be separated) or good time resolution (the time at which frequencies change). A wide window gives better frequency resolution but poor time resolution. A narrower window gives good time resolution but poor frequency resolution. These are called narrowband and wideband transforms, respectively.\n\nThis is one of the reasons for the creation of the wavelet transform and multiresolution analysis, which can give good time resolution for high-frequency events and good frequency resolution for low-frequency events, the combination best suited for many real signals.\n\nThis property is related to the Heisenberg uncertainty principle, but not directly – see Gabor limit for discussion. The product of the standard deviation in time and frequency is limited. The boundary of the uncertainty principle (best simultaneous resolution of both) is reached with a Gaussian window function, as the Gaussian minimizes the Fourier uncertainty principle. This is called the Gabor transform (and with modifications for multiresolution becomes the Morlet wavelet transform).\n\nOne can consider the STFT for varying window size as a two-dimensional domain (time and frequency), as illustrated in the example below, which can be calculated by varying the window size. However, this is no longer a strictly time–frequency representation – the kernel is not constant over the entire signal.\n\nUsing the following sample signal formula_3 that is composed of a set of four sinusoidal waveforms joined together in sequence. Each waveform is only composed of one of four frequencies (10, 25, 50, 100 Hz). The definition of formula_3 is:\n\nThen it is sampled at 400 Hz. The following spectrograms were produced:\n\nThe 25 ms window allows us to identify a precise time at which the signals change but the precise frequencies are difficult to identify. At the other end of the scale, the 1000 ms window allows the frequencies to be precisely seen but the time between frequency changes is blurred.\n\nIt can also be explained with reference to the sampling and Nyquist frequency.\n\nTake a window of \"N\" samples from an arbitrary real-valued signal at sampling rate \"f\" . Taking the Fourier transform produces \"N\" complex coefficients. Of these coefficients only half are useful (the last \"N/2\" being the complex conjugate of the first \"N/2\" in reverse order, as this is a real valued signal).\n\nThese \"N/2\" coefficients represent the frequencies 0 to \"f\"/2 (Nyquist) and two consecutive coefficients are spaced apart by\n\"f\"/\"N\" Hz.\n\nTo increase the frequency resolution of the window the frequency spacing of the coefficients needs to be reduced. There are only two variables, but decreasing \"f\" (and keeping \"N\" constant) will cause the window size to increase — since there are now fewer samples per unit time. The other alternative is to increase \"N\", but this again causes the window size to increase. So any attempt to increase the frequency resolution causes a larger window size and therefore a reduction in time resolution—and vice versa.\n\nAs the Nyquist frequency is a limitation in the maximum frequency that can be meaningfully analysed, so is the Rayleigh frequency a limitation on the minimum frequency.\n\nThe Rayleigh frequency is the minimum frequency that can be resolved by a finite duration time window.\n\nGiven a time window that is Τ seconds long, the minimum frequency that can be resolved is 1/Τ Hz.\n\nThe Rayleigh frequency is an important consideration in applications of the short-time Fourier transform (STFT), as well as any other method of harmonic analysis on a signal of finite record-length.\n\nSTFTs as well as standard Fourier transforms and other tools are frequently used to analyze music. The spectrogram can, for example, show frequency on the horizontal axis, with the lowest frequencies at left, and the highest at the right. The height of each bar (augmented by color) represents the amplitude of the frequencies within that band. The depth dimension represents time, where each new bar was a separate distinct transform. Audio engineers use this kind of visual to gain information about an audio sample, for example, to locate the frequencies of specific noises (especially when used with greater frequency resolution) or to find frequencies which may be more or less resonant in the space where the signal was recorded. This information can be used for equalization or tuning other audio effects.\n\nOriginal function\n\nConverting into the discrete form:\n\nSuppose that \n\nThen we can write the original function into\n\na. Nyquist criterion (Avoiding the aliasing effect):\n\na. formula_38, where formula_39 is an integer\n\nb. formula_40\n\nc. Nyquist criterion (Avoiding the aliasing effect):\n\na. formula_38, where formula_39 is an integer\n\nb. formula_40\n\nc. Nyquist criterion (Avoiding the aliasing effect):\n\nd. Only for implementing the rectangular-STFT\n\nRectangular window imposes the constraint\nSubstitution gives:\n\nChange of variable for :\n\nCalculate formula_57 by the \"N\"-point FFT:\n\nwhere\n\nApplying the recursive formula to calculate formula_60\n\nso \n\n", "related": "\n- Spectral density estimation\n- Time-frequency representation\n- Reassignment method\n\nOther time-frequency transforms:\n- Cone-shape distribution function\n- Constant-Q transform\n- Fractional Fourier transform\n- Gabor transform\n- Newland transform\n- S transform\n- Wavelet transform\n- Chirplet transform\n\n- DiscreteTFDs – software for computing the short-time Fourier transform and other time-frequency distributions\n- Singular Spectral Analysis - MultiTaper Method Toolkit - a free software program to analyze short, noisy time series\n- kSpectra Toolkit for Mac OS X from SpectraWorks\n- Time stretched short time Fourier transform for time frequency analysis of ultra wideband signals\n- A BSD-licensed Matlab class to perform STFT and inverse STFT\n- LTFAT - A free (GPL) Matlab / Octave toolbox to work with short-time Fourier transforms and time-frequency analysis\n"}
{"id": "35240371", "url": "https://en.wikipedia.org/wiki?curid=35240371", "title": "Waveform shaping", "text": "Waveform shaping\n\nWaveform shaping in electronics is the modification of the shape of an electronic waveform. It is in close connection with waveform diversity and waveform design, which are extensively studied in signal processing. Shaping the waveforms\nare of particular interest in active sensing (radar, sonar) for better detection performance, as well as communication schemes (CDMA, frequency hopping), and biology (for animal stimuli design).\nSee also Modulation, Pulse compression, Spread spectrum, Transmit diversity, Ambiguity function, Autocorrelation, and Cross-correlation.\n\n- Hao He, Jian Li, and Petre Stoica. Waveform design for active sensing systems: a computational approach. Cambridge University Press, 2012.\n- Solomon W. Golomb, and Guang Gong. Signal design for good correlation: for wireless communication, cryptography, and radar. Cambridge University Press, 2005.\n- M. Soltanalian. Signal Design for Active Sensing and Communications. Uppsala Dissertations from the Faculty of Science and Technology (printed by Elanders Sverige AB), 2014.\n- Nadav Levanon, and Eli Mozeson. Radar signals. Wiley. com, 2004.\n- Jian Li, and Petre Stoica, eds. Robust adaptive beamforming. New Jersey: John Wiley, 2006.\n- Fulvio Gini, Antonio De Maio, and Lee Patton, eds. Waveform design and diversity for advanced radar systems. Institution of engineering and technology, 2012.\n- Mark R. Bell, \"Information theory and radar waveform design.\" IEEE Transactions on Information Theory, 39.5 (1993): 1578-1597.\n- Robert Calderbank, S. Howard, and Bill Moran. \"Waveform diversity in radar signal processing.\" IEEE Signal Processing Magazine, 26.1 (2009): 32-41.\n- Augusto Aubry, Antonio De Maio, Bo Jiang, and Shuzhong Zhang. \"Ambiguity function shaping for cognitive radar via complex quartic optimization.\" IEEE Transactions on Signal Processing 61 (2013): 5603-5619.\n- John J. Benedetto, Ioannis Konstantinidis, and Muralidhar Rangaswamy. \"Phase-coded waveforms and their design.\" IEEE Signal Processing Magazine, 26.1 (2009): 22-31.\n- Mojtaba Soltanalian, and Petre Stoica. \"Computational design of sequences with good correlation properties.\" IEEE Transactions on Signal Processing, 60.5 (2012): 2180-2193.\n- Mohammad Mahdi Naghsh, M. Soltanalian, P. Stoica, M. Modarres-Hashemi, A. De Maio, and A. Aubry, \"A Doppler Robust Design of Transmit Sequence and Receive Filter in the Presence of Signal-Dependent Interference\", IEEE Transactions on Signal Processing, 62.4 (2014): 772-785.\n", "related": "NONE"}
{"id": "35994457", "url": "https://en.wikipedia.org/wiki?curid=35994457", "title": "Common spatial pattern", "text": "Common spatial pattern\n\nCommon spatial pattern (CSP) is a mathematical procedure used in signal processing for separating a multivariate signal into additive subcomponents which have maximum differences in variance between two windows.\n\nLet formula_1 of size formula_2 and formula_3 of size formula_4 be two windows of a multivariate signal, where formula_5 is the number of signals and formula_6 and formula_7 are the respective number of samples.\n\nThe CSP algorithm determines the component formula_8 such that the ratio of variance (or second-order moment) is maximized between the two windows:\n\nThe solution is given by computing the two covariance matrices:\n\nThen, the simultaneous diagonalization of those two matrices (also called generalized eigenvalue decomposition) is realized. We find the matrix of eigenvectors formula_12 and the diagonal matrix formula_13 of eigenvalues formula_14 sorted by decreasing order such that:\n\nand\n\nwith formula_17 the identity matrix.\n\nThis is equivalent to the eigendecomposition of formula_18:\n\nThe eigenvectors composing formula_21 are components with variance ratio between the two windows equal to their corresponding eigenvalue:\n\nThe vectorial subspace formula_25 generated by the formula_26 first eigenvectors formula_27 will be the subspace maximizing the variance ratio of all components belonging to it:\n\nOn the same way, the vectorial subspace formula_29 generated by the formula_30 last eigenvectors formula_31 will be the subspace minimizing the variance ratio of all components belonging to it:\n\nCSP can be applied after a mean subtraction (a.k.a. \"mean centering\") on signals in order to realize a variance ratio optimization. Otherwise CSP optimizes the ratio of second-order moment.\n\n- The standard use consists on choosing the windows to correspond to two periods of time with different activation of sources (e.g. during rest and during a specific task).\n- It is also possible to choose the two windows to correspond to two different frequency bands in order to find components with specific frequency pattern. Those frequency bands can be on temporal or on frequential basis. Since the matrix formula_21 depends only of the covariance matrices, the same results can be obtained if the processing is applied on the Fourier transform of the signals.\n- Y. Wang has proposed a particular choice for the first window formula_1 in order to extract components which have a specific period. formula_1 was the mean of the different periods for the examined signals.\n- If there is only one window, formula_36 can be considered as the identity matrix and then CSP corresponds to Principal component analysis.\n\nThis method can be applied to several multivariate signals but it seems that most works on it concern electroencephalographic signals.\n\nParticularly, the method is mostly used on brain–computer interface in order to retrieve the component signals which best transduce the cerebral activity for a specific task (e.g. hand movement).\n\nIt can also be used to separate artifacts from electroencephalographics signals.\n\nThe common spatial pattern needs to be adapted for the analysis of the event-related potentials.\n\n", "related": "\n- Blind signal separation\n"}
{"id": "267637", "url": "https://en.wikipedia.org/wiki?curid=267637", "title": "Sinc filter", "text": "Sinc filter\n\nIn signal processing, a sinc filter is an idealized filter that removes all frequency components above a given cutoff frequency, without affecting lower frequencies, and has linear phase response. The filter's impulse response is a sinc function in the time domain, and its frequency response is a rectangular function.\n\nIt is an \"ideal\" low-pass filter in the frequency sense, perfectly passing low frequencies, perfectly cutting high frequencies; and thus may be considered to be a \"brick-wall filter\".\n\nReal-time filters can only approximate this ideal, since an ideal sinc filter (a.k.a. \"rectangular filter\") is non-causal and has an infinite delay, but it is commonly found in conceptual demonstrations or proofs, such as the sampling theorem and the Whittaker–Shannon interpolation formula.\n\nIn mathematical terms, the desired frequency response is the rectangular function:\n\nwhere formula_2 is an arbitrary cutoff frequency (a.k.a. \"bandwidth\"). The impulse response of such a filter is given by the inverse Fourier transform of the frequency response:\n\nwhere \"sinc\" is the normalized sinc function.\n\nAs the sinc filter has infinite impulse response in both positive and negative time directions, it must be approximated for real-world (non-abstract) applications; a windowed sinc filter is often used instead. Windowing and truncating a sinc filter kernel in order to use it on any practical real world data set reduces its ideal properties.\n\nAn idealized electronic filter, one that has full transmission in the pass band, and complete attenuation in the stop band, with abrupt transitions, is known colloquially as a \"brick-wall filter\", in reference to the shape of the transfer function. The sinc filter is a brick-wall low-pass filter, from which brick-wall band-pass filters and high-pass filters are easily constructed.\n\nThe lowpass filter with brick-wall cutoff at frequency \"B\" has impulse response and transfer function given by:\n\nThe band-pass filter with lower band edge \"B\" and upper band edge \"B\" is just the difference of two such sinc filters (since the filters are zero phase, their magnitude responses subtract directly):\n\nThe high-pass filter with lower band edge \"B\" is just a transparent filter minus a sinc filter, which makes it clear that the Dirac delta function is the limit of a narrow-in-time sinc filter:\n\nBrick-wall filters that run in realtime are not physically realizable as they have infinite latency (i.e., its compact support in the frequency domain forces its time response not to have compact support meaning that it is ever-lasting) and infinite order (i.e., the response cannot be expressed as a linear differential equation with a finite sum), but approximate implementations are sometimes used and they are frequently called brick-wall filters.\n\nThe name \"sinc filter\" is applied also to the filter shape that is rectangular in time and a sinc function in frequency, as opposed to the ideal low-pass sinc filter, which is sinc in time and rectangular in frequency. In case of confusion, one may refer to these as sinc-in-frequency and sinc-in-time, according to which domain the filter is sinc in.\n\nSinc-in-frequency CIC filters, among many other applications, are almost universally used for decimating delta-sigma ADCs, as they are easy to implement and nearly optimal for this use.\n\nThe simplest implementation of a Sinc-in-frequency filter is a group-averaging filter, also known as accumulate-and-dump filter.\nThis filter also performs a data rate reduction.\n\nIt collects N data samples, accumulates them and provides\nthe accumulator value as output. Thus, the decimation factor of this filter is N.\nIt can be modelled as a FIR filter with all N coefficients equal, followed by a N-time downsampling\nblock.\nThe simplicity of the filter, requiring just an accumulator as central data processing block, \nis foiled with strong aliasing effects: an N sample filter aliases all attenuated and unattenuated signal components lying above \nformula_10 to the baseband ranging from 0 to formula_10 ( f is the input sample rate).\n\nA group averaging filter processing N samples has N/2 transmission zeroes.\nThe picture \"transmission function of a 16sample group averaging filter\" shows how the transmission function looks like above the\nNyquist frequency.\n\nThe sinc filter is not bounded-input–bounded-output (BIBO) stable. That is, a bounded input can produce an unbounded output, because the integral of the absolute value of the sinc function is infinite. A bounded input that produces an unbounded output is sgn(sinc(\"t\")). Another is sin(2\"Bt\")u(\"t\"), a sine wave starting at time 0, at the cutoff frequency.\n\n", "related": "\n- Lanczos resampling\n- Aliasing\n- Anti-aliasing filter\n\n- Brick Wall Digital Filters and Phase Deviations\n- Brick-wall filters\n"}
{"id": "41234", "url": "https://en.wikipedia.org/wiki?curid=41234", "title": "Heterodyne", "text": "Heterodyne\n\nHeterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by combining or mixing two frequencies. Heterodyning is used to shift one frequency range into another, new one, and is also involved in the processes of modulation and demodulation. The two frequencies are combined in a nonlinear signal-processing device such as a vacuum tube, transistor, or diode, usually called a \"mixer\". In the most common application, two signals at frequencies and are mixed, creating two new signals, one at the sum of the two frequencies, and the other at the difference . These frequencies are called \"heterodynes\". Typically only one of the new frequencies is desired, and the other signal is filtered out of the output of the mixer. Heterodyne frequencies are related to the phenomenon of \"beats\" in acoustics.\n\nA major application of the heterodyne process is in the superheterodyne radio receiver circuit, which is used in virtually all modern radio receivers.\n\nIn 1901, Reginald Fessenden demonstrated a direct-conversion heterodyne receiver or beat receiver as a method of making continuous wave radiotelegraphy signals audible. Fessenden's receiver did not see much application because of its local oscillator's stability problem. A stable yet inexpensive local oscillator was not available until Lee de Forest invented the triode vacuum tube oscillator. In a 1905 patent, Fessenden stated that the frequency stability of his local oscillator was one part per thousand.\n\nIn radio telegraphy, the characters of text messages are translated into the short duration dots and long duration dashes of Morse code that are broadcast as radio signals. Radio telegraphy was much like ordinary telegraphy. One of the problems was building high power transmitters with the technology of the day. Early transmitters were spark gap transmitters. A mechanical device would make sparks at a fixed but audible rate; the sparks would put energy into a resonant circuit that would then ring at the desired transmission frequency (which might be 100 kHz). This ringing would quickly decay, so the output of the transmitter would be a succession of damped waves. When these damped waves were received by a simple detector, the operator would hear an audible buzzing sound that could be transcribed back into alpha-numeric characters.\n\nWith the development of the arc converter radio transmitter in 1904, continuous wave (CW) modulation began to be used for radiotelegraphy. CW Morse code signals are not amplitude modulated, but rather consist of bursts of sinusoidal carrier frequency. When CW signals are received by an AM receiver, the operator does not hear a sound. The direct-conversion (heterodyne) detector was invented to make continuous wave radio-frequency signals audible.\n\nThe \"heterodyne\" or \"beat\" receiver has a local oscillator that produces a radio signal adjusted to be close in frequency to the incoming signal being received. When the two signals are mixed, a \"beat\" frequency equal to the difference between the two frequencies is created. By adjusting the local oscillator frequency correctly, the beat frequency is in the audio range, and can be heard as a tone in the receiver's earphones whenever the transmitter signal is present. Thus the Morse code \"dots\" and \"dashes\" are audible as beeping sounds. This technique is still used in radio telegraphy, the local oscillator now being called the beat frequency oscillator or BFO. Fessenden coined the word \"heterodyne\" from the Greek roots \"hetero-\" \"different\", and \"dyn-\" \"power\" (cf. ).\n\nAn important and widely used application of the heterodyne technique is in the superheterodyne receiver (superhet), which was invented by U.S. engineer Edwin Howard Armstrong in 1918. In the typical superhet, the incoming radio frequency signal from the antenna is mixed (heterodyned) with a signal from a local oscillator (LO) to produce a lower fixed frequency signal called the intermediate frequency (IF) signal. The IF signal is amplified and filtered and then applied to a detector that extracts the audio signal; the audio is ultimately sent to the receiver's loudspeaker.\n\nThe superheterodyne receiver has several advantages over previous receiver designs. One advantage is easier tuning; only the RF filter and the LO are tuned by the operator; the fixed-frequency IF is tuned (\"aligned\") at the factory and is not adjusted. In older designs such as the tuned radio frequency receiver (TRF), all of the receiver stages had to be simultaneously tuned. In addition, since the IF filters are fixed-tuned, the receiver's selectivity is the same across the receiver's entire frequency band. Another advantage is that the IF signal can be at a much lower frequency than the incoming radio signal, and that allows each stage of the IF amplifier to provide more gain. To first order, an amplifying device has a fixed gain-bandwidth product. If the device has a gain-bandwidth product of 60 MHz, then it can provide a voltage gain of 3 at an RF of 20 MHz or a voltage gain of 30 at an IF of 2 MHz. At a lower IF, it would take fewer gain devices to achieve the same gain. The regenerative radio receiver obtained more gain out of one gain device by using positive feedback, but it required careful adjustment by the operator; that adjustment also changed the selectivity of the regenerative receiver. The superheterodyne provides a large, stable gain and constant selectivity without troublesome adjustment.\n\nThe superior superheterodyne system replaced the earlier TRF and regenerative receiver designs, and since the 1930s most commercial radio receivers have been superheterodynes.\n\nHeterodyning, also called \"frequency conversion\", is used very widely in communications engineering to generate new frequencies and move information from one frequency channel to another. Besides its use in the superheterodyne circuit found in almost all radio and television receivers, it is used in radio transmitters, modems, satellite communications and set-top boxes, radar, radio telescopes, telemetry systems, cell phones, cable television converter boxes and headends, microwave relays, metal detectors, atomic clocks, and military electronic countermeasures (jamming) systems.\n\nIn large scale telecommunication networks such as telephone network trunks, microwave relay networks, cable television systems, and communication satellite links, large bandwidth capacity links are shared by many individual communication channels by using heterodyning to move the frequency of the individual signals up to different frequencies, which share the channel. This is called frequency division multiplexing (FDM).\n\nFor example, a coaxial cable used by a cable television system can carry 500 television channels at the same time because each one is given a different frequency, so they don't interfere with one another. At the cable source or headend, electronic upconverters convert each incoming television channel to a new, higher frequency. They do this by mixing the television signal frequency, \"f\" with a local oscillator at a much higher frequency , creating a heterodyne at the sum , which is added to the cable. At the consumer's home, the cable set top box has a downconverter that mixes the incoming signal at frequency with the same local oscillator frequency creating the difference heterodyne frequency, converting the television channel back to its original frequency: . Each channel is moved to a different higher frequency. The original lower basic frequency of the signal is called the baseband, while the higher channel it is moved to is called the passband.\n\nMany analog videotape systems rely on a downconverted color subcarrier to record color information in their limited bandwidth. These systems are referred to as \"heterodyne systems\" or \"color-under systems\". For instance, for NTSC video systems, the VHS (and S-VHS) recording system converts the color subcarrier from the NTSC standard 3.58 MHz to ~629 kHz. PAL VHS color subcarrier is similarly downconverted (but from 4.43 MHz). The now-obsolete 3/4\" U-matic systems use a heterodyned ~688 kHz subcarrier for NTSC recordings (as does Sony's Betamax, which is at its basis a 1/2″ consumer version of U-matic), while PAL U-matic decks came in two mutually incompatible varieties, with different subcarrier frequencies, known as Hi-Band and Low-Band. Other videotape formats with heterodyne color systems include Video-8 and Hi8.\n\nThe heterodyne system in these cases is used to convert quadrature phase-encoded and amplitude modulated sine waves from the broadcast frequencies to frequencies recordable in less than 1 MHz bandwidth. On playback, the recorded color information is heterodyned back to the standard subcarrier frequencies for display on televisions and for interchange with other standard video equipment.\n\nSome U-matic (3/4″) decks feature 7-pin mini-DIN connectors to allow dubbing of tapes without conversion, as do some industrial VHS, S-VHS, and Hi8 recorders.\n\nThe theremin, an electronic musical instrument, traditionally uses the heterodyne principle to produce a variable audio frequency in response to the movement of the musician's hands in the vicinity of one or more antennas, which act as capacitor plates. The output of a fixed radio frequency oscillator is mixed with that of an oscillator whose frequency is affected by the variable capacitance between the antenna and the thereminist as that person moves her or his hand near the pitch control antenna. The difference between the two oscillator frequencies produces a tone in the audio range.\n\nThe ring modulator is a type of frequency mixer incorporated into some synthesizers or used as a stand-alone audio effect.\n\nOptical heterodyne detection (an area of active research) is an extension of the heterodyning technique to higher (visible) frequencies. This technique could greatly improve optical modulators, increasing the density of information carried by optical fibers. It is also being applied in the creation of more accurate atomic clocks based on directly measuring the frequency of a laser beam. See NIST subtopic 9.07.9-4.R for a description of research on one system to do this.\n\nSince optical frequencies are far beyond the manipulation capacity of any feasible electronic circuit, all visible frequency photon detectors are inherently energy detectors not oscillating electric field detectors. However, since energy detection is inherently \"square-law\" detection, it intrinsically mixes any optical frequencies present on the detector. Thus, sensitive detection of specific optical frequencies necessitates optical heterodyne detection, in which two different (close-by) wavelengths of light illuminate the detector so that the oscillating electrical output corresponds to the difference between their frequencies. This allows extremely narrow band detection (much narrower than any possible color filter can achieve) as well as precision measurements of phase and frequency of a light signal relative to a reference light source, as in a laser Doppler vibrometer.\n\nThis phase sensitive detection has been applied for Doppler measurements of wind speed, and imaging through dense media. The high sensitivity against background light is especially useful for lidar.\n\nIn optical Kerr effect (OKE) spectroscopy, optical heterodyning of the OKE signal and a small part of the probe signal produces a mixed signal consisting of probe, heterodyne OKE-probe and homodyne OKE signal. The probe and homodyne OKE signals can be filtered out, leaving the heterodyne frequency signal for detection.\n\nHeterodyne detection is often used in interferometry but usually confined to single point detection rather than widefield interferometry, however, widefield heterodyne interferometry is possible using a special camera. Using this technique which a reference signal extracted from a single pixel it is possible to build a highly stable widefield heterodyne interferometer by removing the piston phase component\ncaused by microphonics or vibrations of the optical components or object.\n\nHeterodyning is based on the trigonometric identity:\n\nThe product on the left hand side represents the multiplication (\"mixing\") of a sine wave with another sine wave. The right hand side shows that the resulting signal is the difference of two sinusoidal terms, one at the sum of the two original frequencies, and one at the difference, which can be considered to be separate signals.\n\nUsing this trigonometric identity, the result of multiplying two sine wave signals formula_2 and formula_3 at different frequencies formula_4 and formula_5 can be calculated:\n\nThe result is the sum of two sinusoidal signals, one at the sum and one at the difference of the original frequencies\n\nThe two signals combine in a device called a \"mixer\". As seen in the previous section, an ideal mixer would be a device that multiplies the two signals. Some widely used mixer circuits, such as the Gilbert cell, operate in this way, but they are limited to lower frequencies. However, any \"nonlinear\" electronic component also multiplies signals applied to it, producing heterodyne frequencies in its output—so a variety of nonlinear components serve as mixers. A nonlinear component is one in which the output current or voltage is a nonlinear function of its input. Most circuit elements in communications circuits are designed to be linear. This means they obey the superposition principle; if \"F\"(\"v\") is the output of a linear element with an input of \"v\":\n\nSo if two sine wave signals at frequencies and are applied to a linear device, the output is simply the sum of the outputs when the two signals are applied separately with no product terms. Thus, the function must be nonlinear to create mixer products. A perfect multiplier only produces mixer products at the sum and difference frequencies , but more general nonlinear functions produce higher order mixer products: for integers and . Some mixer designs, such as double-balanced mixers, suppress some high order undesired products, while other designs, such as harmonic mixers exploit high order differences.\n\nExamples of nonlinear components that are used as mixers are vacuum tubes and transistors biased near cutoff (class C), and diodes. Ferromagnetic core inductors driven into saturation can also be used at lower frequencies. In nonlinear optics, crystals that have nonlinear characteristics are used to mix laser light beams to create optical heterodyne frequencies.\n\nTo demonstrate mathematically how a nonlinear component can multiply signals and generate heterodyne frequencies, the nonlinear function \"F\" can be expanded in a power series (MacLaurin series):\n\nTo simplify the math, the higher order terms above are indicated by an ellipsis (\". . .\") and only the first terms are shown. Applying the two sine waves at frequencies and to this device:\n\nIt can be seen that the second term above contains a product of the two sine waves. Simplifying with trigonometric identities:\n\nSo the output contains sinusoidal terms with frequencies at the sum and difference of the two original frequencies. It also contains terms at the original frequencies and at multiples of the original frequencies , , , , etc.; the latter are called harmonics, as well as more complicated terms at frequencies of , called intermodulation products. These unwanted frequencies, along with the unwanted heterodyne frequency, must be filtered out of the mixer output by an electronic filter to leave the desired frequency.\n\n", "related": "\n- Electroencephalography\n- Homodyne\n- Transverter\n- Intermodulation – a problem with strong higher-order terms produced in some non-linear mixers\n\n\n"}
{"id": "795837", "url": "https://en.wikipedia.org/wiki?curid=795837", "title": "Half time (electronics)", "text": "Half time (electronics)\n\nIn signal processing, the half time is the time it takes for the amplitude of a pulse to drop from 100% to 50% of its peak value.\n", "related": "NONE"}
{"id": "41615", "url": "https://en.wikipedia.org/wiki?curid=41615", "title": "Quasi-analog signal", "text": "Quasi-analog signal\n\nIn telecommunication, a quasi-analog signal is a digital signal that has been converted to a form suitable for transmission over a specified analog channel. \n\nThe specification of the analog channel should include frequency range, bandwidth, signal-to-noise ratio, and envelope delay distortion. When quasi-analog form of signaling is used to convey message traffic over dial-up telephone systems, it is often referred to as voice-data. A modem may be used for the conversion process.\n", "related": "NONE"}
{"id": "41702", "url": "https://en.wikipedia.org/wiki?curid=41702", "title": "Signal compression", "text": "Signal compression\n\nSignal compression is the use of various techniques to increase the quality or quantity of signal parameters transmitted through a given telecommunications channel.\n\nTypes of signal compression include:\n\n- Bandwidth compression\n- Data compression\n- Dynamic range compression\n- Gain compression\n- Image compression\n- Lossy compression\n- One-way compression function\n", "related": "NONE"}
{"id": "549530", "url": "https://en.wikipedia.org/wiki?curid=549530", "title": "Signal reconstruction", "text": "Signal reconstruction\n\nIn signal processing, reconstruction usually means the determination of an original continuous signal from a sequence of equally spaced samples.\n\nThis article takes a generalized abstract mathematical approach to signal sampling and reconstruction. For a more practical approach based on band-limited signals, see Whittaker–Shannon interpolation formula.\n\nLet \"F\" be any sampling method, i.e. a linear map from the Hilbert space of square-integrable functions formula_1 to complex space formula_2.\n\nIn our example, the vector space of sampled signals formula_2 is \"n\"-dimensional complex space. Any proposed inverse \"R\" of \"F\" (\"reconstruction formula\", in the lingo) would have to map formula_2 to some subset of formula_1. We could choose this subset arbitrarily, but if we're going to want a reconstruction formula \"R\" that is also a linear map, then we have to choose an \"n\"-dimensional linear subspace of formula_1.\n\nThis fact that the dimensions have to agree is related to the Nyquist–Shannon sampling theorem.\n\nThe elementary linear algebra approach works here. Let formula_7 (all entries zero, except for the \"k\"th entry, which is a one) or some other basis of formula_2. To define an inverse for \"F\", simply choose, for each \"k\", an formula_9 so that formula_10. This uniquely defines the (pseudo-)inverse of \"F\".\n\nOf course, one can choose some reconstruction formula first, then either compute some sampling algorithm from the reconstruction formula, or analyze the behavior of a given sampling algorithm with respect to the given formula.\n\nIdeally, the reconstruction formula is derived by minimizing the expected error variance. This requires that either the signal statistics is known or a prior probability for the signal can be specified. Information field theory is then an appropriate mathematical formalism to derive an optimal reconstruction formula.\n\nPerhaps the most widely used reconstruction formula is as follows. Let formula_11 be a basis of formula_1 in the Hilbert space sense; for instance, one could use the eikonal\n\nalthough other choices are certainly possible. Note that here the index \"k\" can be any integer, even negative.\n\nThen we can define a linear map \"R\" by\n\nfor each formula_15, where formula_16 is the basis of formula_2 given by\n\nThe choice of range formula_15 is somewhat arbitrary, although it satisfies the dimensionality requirement and reflects the usual notion that the most important information is contained in the low frequencies. In some cases, this is incorrect, so a different reconstruction formula needs to be chosen.\n\nA similar approach can be obtained by using wavelets instead of Hilbert bases. For many applications, the best approach is still not clear today.\n\n", "related": "\n- Nyquist–Shannon sampling theorem\n- Whittaker–Shannon interpolation formula\n- Aliasing\n- Reconstruction of multidimensional signals\n"}
{"id": "41388", "url": "https://en.wikipedia.org/wiki?curid=41388", "title": "Multiplex baseband", "text": "Multiplex baseband\n\nIn telecommunication, the term multiplex baseband has the following meanings: \n1. In frequency-division multiplexing, the frequency band occupied by the aggregate of the signals in the line interconnecting the multiplexing and radio or line equipment.\n2. In frequency division multiplexed carrier systems, at the input to any stage of frequency translation, the frequency band occupied.\n\nFor example, the output of a group multiplexer consists of a band of frequencies from 60 kHz to 108 kHz. This is the group-level baseband that results from combining 12 voice-frequency input channels, having a bandwidth of 4 kHz each, including guard bands. In turn, 5 groups are multiplexed into a super group having a baseband of 312 kHz to 552 kHz. This baseband, however, does not represent a group-level baseband. Ten super groups are in turn multiplexed into one master group, the output of which is a baseband that may be used to modulate a microwave-frequency carrier.\n", "related": "NONE"}
{"id": "19570111", "url": "https://en.wikipedia.org/wiki?curid=19570111", "title": "Signal regeneration", "text": "Signal regeneration\n\nIn telecommunications, signal regeneration is signal processing that restores a signal, recovering its original characteristics.\n\nThe signal may be electrical, as in a repeater on a T-carrier line, or optical, as in an OEO optical cross-connect.\n\nThe process is used when it is necessary to change the signal type in order to transmit it via different media. Once it comes back to the original medium the signal is usually required to be regenerated so as to bring it back to its original state.\n\n", "related": "\n- Fiber-optic communication#Regeneration\n"}
{"id": "6243353", "url": "https://en.wikipedia.org/wiki?curid=6243353", "title": "Signaling Compression", "text": "Signaling Compression\n\nSignaling compression, or SigComp, is a compression method designed especially for compression of text-based communication data as SIP or RTSP. SigComp had originally been defined in RFC 3320 and was later updated with RFC 4896. A Negative Acknowledgement Mechanism for Signaling Compression is defined in RFC 4077. The SigComp work is performed in the ROHC working group in the transport area of the IETF.\n\nSigComp specifications describe a compression schema that is located in between the application layer and the transport layer (e.g. between SIP and UDP). It is implemented upon a virtual machine configuration which executes a specific set of commands that are optimized for decompression purposes (namely UDVM, Universal Decompressor Virtual Machine).\n\nOne strong point for SigComp is that the bytecode to decode messages can be sent over SigComp itself, so this allows to use any kind of compression schema given that it is expressed as bytecode for the UDVM. Thus any SigComp compatible device may use compression mechanisms that did not exist when it was released without any firmware change. Additionally, some decoders may be already been standardised, so SigComp may recall that code so it is not needed to be sent over the connection. To assure that a message is decodable the only requirement is that the UDVM code is available, so the compression of messages is executed off the virtual machine, and native code can be used.\n\nAs an independent system a mechanism to signal the application conversation (e.g. a given SIP session), a compartment mechanism is used, so a given application may have any given number of different, independent conversations, while persisting all the session status (as needed/specified per compression schema and UDVM code).\n\n- RFC 3320 – \"Signaling Compression (SigComp)\"\n- RFC 3321 – \"Signaling Compression (SigComp) – Extended Operations\"\n- RFC 3485 – \"The Session Initiation Protocol (SIP) and Session Description Protocol (SDP) Static Dictionary for Signaling Compression (SigComp)\"\n- RFC 3486 – \"Compressing the Session Initiation Protocol (SIP)\"\n- RFC 4077 – \"A Negative Acknowledgement Mechanism for Signaling Compression\"\n- RFC 4464 – \"Signaling Compression (SigComp) Users' Guide\"\n- RFC 4465 – \"Signaling Compression (SigComp) Torture Tests\"\n- RFC 4896 – \"Signaling Compression (SigComp) Corrections and Clarifications\"\n- RFC 5049 – \"Applying Signaling Compression (SigComp) to the Session Initiation Protocol (SIP)\"\n- RFC 5112 – \"The Presence-Specific Static Dictionary for Signaling Compression (Sigcomp)\"\n- 3GPP TR23.979 Annex C – \"Required SigComp performance\"\n", "related": "NONE"}
{"id": "36501659", "url": "https://en.wikipedia.org/wiki?curid=36501659", "title": "Financial signal processing", "text": "Financial signal processing\n\nFinancial signal processing is a branch of signal processing technologies which applies to financial signals. They are often used by quantitative investors to make best estimation of the movement of equity prices, such as stock prices, options prices, or other types of derivatives.\n\nThe early history of financial signal processing can be traced back to Isaac Newton. Newton lost money in the famous South Sea Company investment bubble.\n\nThe modern start of financial signal processing is often credited to Claude Shannon. Shannon was the inventor of modern communication theory. He discovered the capacity of a communication channel by analyzing entropy of information.\n\nFor a long time, financial signal processing technologies have been used by different hedge funds, such as Jim Simon's Renaissance Technologies. However, hedge funds usually do not reveal their trade secrets. Some early research results in this area are summarized by R.H. Tütüncü and M. Koenig and by T.M. Cover, J.A. Thomas. A.N. Akansu and M.U. Torun published the book in financial signal processing entitled \"A Primer for Financial Engineering: Financial Signal Processing and Electronic Trading\". An edited volume on the subject with the title \"Financial Signal Processing and Machine Learning\" was also published. There were two special issues of IEEE Journal of Selected Topics in Signal Processing published on Signal Processing Methods in Finance and Electronic Trading in 2012, and on Financial Signal Processing and Machine Learning for Electronic Trading in 2016 in addition to the special section on Signal Processing for Financial Applications in IEEE Signal Processing Magazine appeared in 2011.\n\nRecently, a new research group in Imperial College London has been formed which focuses on Financial Signal Processing as part of the Communication and Signal Processing Group of the Electrical and Electronic Engineering department, led by Anthony G. Constantinides. In June 2014 the group started a collaboration with the Schroders Multi-Asset Investments and Portfolio Solutions (MAPS) team on multi-asset study.\n\nOther research groups working on the financial signal processing include the Convex Research Group of Prof. Daniel Palomar and the Signal Processing and Computational Biology Group led by Prof. Matthew R. McKay at the Hong Kong University of Science and Technology and Stanford University Convex Optimization Group led by Prof. Stephen Boyd at the Stanford University. There are also open source libraries available for index tracking and portfolio optimization.\n\n- Vivienne Investissement: multifractality for asset price, covariance estimation for asset allocation;\n- NM FinTech;\n", "related": "NONE"}
{"id": "610583", "url": "https://en.wikipedia.org/wiki?curid=610583", "title": "Sinc function", "text": "Sinc function\n\nIn mathematics, physics and engineering, the sinc function, denoted by , has two slightly different definitions.\n\nIn mathematics, the historical unnormalized sinc function is defined for by\n\nAlternatively, the unnormalized sinc function is often called the sampling function, indicated as Sa(x).\n\nIn digital signal processing and information theory, the normalized sinc function is commonly defined for by\n\nIn either case, the value at is defined to be the limiting value\n\nThe normalization causes the definite integral of the function over the real numbers to equal 1 (whereas the same integral of the unnormalized sinc function has a value of ). As a further useful property, the zeros of the normalized sinc function are the nonzero integer values of .\n\nThe normalized sinc function is the Fourier transform of the rectangular function with no scaling. It is used in the concept of reconstructing a continuous bandlimited signal from uniformly spaced samples of that signal.\n\nThe only difference between the two definitions is in the scaling of the independent variable (the axis) by a factor of . In both cases, the value of the function at the removable singularity at zero is understood to be the limit value 1. The sinc function is then analytic everywhere and hence an entire function.\n\nThe term \"sinc\" was introduced by Philip M. Woodward in his 1952 article \"Information theory and inverse probability in telecommunication\", in which he said that the function \"occurs so often in Fourier analysis and its applications that it does seem to merit some notation of its own\", and his 1953 book \"Probability and Information Theory, with Applications to Radar\".\n\nThe zero crossings of the unnormalized sinc are at non-zero integer multiples of , while zero crossings of the normalized sinc occur at non-zero integers.\n\nThe local maxima and minima of the unnormalized sinc correspond to its intersections with the cosine function. That is, for all points where the derivative of is zero and thus a local extremum is reached. This follows from the derivative of the sinc function:\n\nThe first few terms of the infinite series for the coordinate of the -th extremum with positive coordinate are\nwhere\nand where odd lead to a local minimum, and even to a local maximum. Because of symmetry around the axis, there exist extrema with coordinates . In addition, there is an absolute maximum at .\n\nThe normalized sinc function has a simple representation as the infinite product:\nand is related to the gamma function through Euler's reflection formula:\n\nEuler discovered that\nand because of the product-to-sum identity\nthe Euler's product can be recast as a sum\n\nThe continuous Fourier transform of the normalized sinc (to ordinary frequency) is :\nwhere the rectangular function is 1 for argument between − and , and zero otherwise. This corresponds to the fact that the sinc filter is the ideal (brick-wall, meaning rectangular frequency response) low-pass filter.\n\nThis Fourier integral, including the special case\nis an improper integral (see Dirichlet integral) and not a convergent Lebesgue integral, as\n\nThe normalized sinc function has properties that make it ideal in relationship to interpolation of sampled bandlimited functions:\n- It is an interpolating function, i.e., , and for nonzero integer .\n- The functions ( integer) form an orthonormal basis for bandlimited functions in the function space , with highest angular frequency (that is, highest cycle frequency ).\n\nOther properties of the two sinc functions include:\n- The unnormalized sinc is the zeroth-order spherical Bessel function of the first kind, . The normalized sinc is .\n- formula_15\n- (not normalized) is one of two linearly independent solutions to the linear ordinary differential equation\n- formula_17\n- formula_18\n- formula_19\n- formula_20\n- The following improper integral involves the (not normalized) sinc function:\n- formula_21\n\nThe normalized sinc function can be used as a \"nascent delta function\", meaning that the following weak limit holds:\n\nThis is not an ordinary limit, since the left side does not converge. Rather, it means that\n\nfor every Schwartz function, as can be seen from the Fourier inversion theorem.\nIn the above expression, as , the number of oscillations per unit length of the sinc function approaches infinity. Nevertheless, the expression always oscillates inside an envelope of , regardless of the value of .\n\nThis complicates the informal picture of as being zero for all except at the point , and illustrates the problem of thinking of the delta function as a function rather than as a distribution. A similar situation is found in the Gibbs phenomenon.\n\nAll sums in this section refer to the unnormalized sinc function.\n\nThe sum of over integer from 1 to equals :\n\nThe sum of the squares also equals :\n\nWhen the signs of the addends alternate and begin with +, the sum equals :\n\nThe alternating sums of the squares and cubes also equal :\n\nThe Taylor series of the (unnormalized) function can be obtained immediately from that of the sine:\n\nwhich converges for all .\n\nThe product of 1-D sinc functions readily provides a multivariate sinc function for the square Cartesian grid (lattice): , whose Fourier transform is the indicator function of a square in the frequency space (i.e., the brick wall defined in 2-D space). The sinc function for a non-Cartesian lattice (e.g., hexagonal lattice) is a function whose Fourier transform is the indicator function of the Brillouin zone of that lattice. For example, the sinc function for the hexagonal lattice is a function whose Fourier transform is the indicator function of the unit hexagon in the frequency space. For a non-Cartesian lattice this function can not be obtained by a simple tensor product. However, the explicit formula for the sinc function for the hexagonal, body-centered cubic, face-centered cubic and other higher-dimensional lattices can be explicitly derived using the geometric properties of Brillouin zones and their connection to zonotopes.\n\nFor example, a hexagonal lattice can be generated by the (integer) linear span of the vectors\n\nDenoting\n\none can derive the sinc function for this hexagonal lattice as\n\nThis construction can be used to design Lanczos window for general multidimensional lattices.\n\n", "related": "\n- Anti-aliasing filter\n- Sinc filter\n- Lanczos resampling\n- Whittaker–Shannon interpolation formula\n- Shannon wavelet\n- Winkel tripel projection (cartography)\n- Trigonometric integral\n- Trigonometric functions of matrices\n- Borwein integral\n- Dirichlet integral\n"}
{"id": "4789083", "url": "https://en.wikipedia.org/wiki?curid=4789083", "title": "Sonic artifact", "text": "Sonic artifact\n\nIn sound and music production, sonic artifact, or simply artifact, refers to sonic material that is accidental or unwanted, resulting from the editing or manipulation of a sound.\n\nBecause there are always technical restrictions in the way a sound can be recorded (in the case of acoustic sounds) or designed (in the case of synthesised or processed sounds), sonic errors often occur. These errors are termed artifacts (or sound/sonic artifacts), and may be pleasing or displeasing. A sonic artifact is sometimes a type of digital artifact, and in some cases is the result of data compression (not to be confused with dynamic range compression, which also may create sonic artifacts).\n\nOften an artifact is deliberately produced for creative reasons. For example to introduce a change in timbre of the original sound or to create a sense of cultural or stylistic context. A well-known example is the overdriving of an electric guitar or electric bass signal to produce a clipped, distorted guitar tone or fuzz bass.\n\nEditing processes that deliberately produce artifacts often involve technical experimentation. A good example of the deliberate creation of sonic artifacts is the addition of grainy pops and clicks to a recent recording in order to make it sound like a vintage vinyl record.\n\nFlanging and distortion were originally regarded as sonic artifacts; as time passed they became a valued part of pop music production methods. Flanging is added to electric guitar and keyboard parts. Other magnetic tape artifacts include wow, flutter, saturation, hiss, noise, and print-through.\n\nIt is valid to consider the genuine surface noise such as pops and clicks that are audible when a vintage vinyl recording is played back or recorded onto another medium as sonic artifacts, although not all sonic artifacts must contain in their meaning or production a sense of \"past\", more so a sense of \"by-product\". Other vinyl record artifacts include turntable rumble, ticks, crackles and groove echos.\n\nIn the Nyquist–Shannon sampling theorem, inadequate sampling bandwidth creates a sonic artifact known as an \"alias\", and the resulting distortion of the sound is termed \"aliasing\". Examples of aliasing can be heard in early music samplers since they could record audio at bit rates and sampling frequencies below the Nyquist rate, considered desirable by some musicians. Aliasing is a major concern in the analog-to-digital conversion of video and audio signals.\n\nIn the creation of computer music and electronic music in the past decade, particularly in glitch music, software is used to create sonic artifacts of all stripes. They are also the primary focus of the practice of circuit bending: making sounds from products that were unintended by the makers of the circuitry.\n\n", "related": "\n- Data compression\n- Digital artifact\n- Dynamic range compression\n- Glitch (music)\n- Compression artifact\n- Sampling (information theory)\n- Signal (information theory)\n- Window function\n- Circuit bending\n- Sound reproduction\n- Noise music\n- Noise pumping\n- Breathing (noise reduction)\n"}
{"id": "5501977", "url": "https://en.wikipedia.org/wiki?curid=5501977", "title": "Zero-order hold", "text": "Zero-order hold\n\nThe zero-order hold (ZOH) is a mathematical model of the practical signal reconstruction done by a conventional digital-to-analog converter (DAC). That is, it describes the effect of converting a discrete-time signal to a continuous-time signal by holding each sample value for one sample interval. It has several applications in electrical communication.\n\nA zero-order hold reconstructs the following continuous-time waveform from a sample sequence \"x\"[\"n\"], assuming one sample per time interval \"T\":\n\nThe function formula_3 is depicted in Figure 1, and formula_4 is the piecewise-constant signal depicted in Figure 2.\n\nThe equation above for the output of the ZOH can also be modeled as the output of a linear time-invariant filter with impulse response equal to a rect function, and with input being a sequence of dirac impulses scaled to the sample values. The filter can then be analyzed in the frequency domain, for comparison with other reconstruction methods such as the Whittaker–Shannon interpolation formula suggested by the Nyquist–Shannon sampling theorem, or such as the first-order hold or linear interpolation between sample values.\n\nIn this method, a sequence of Dirac impulses, \"x\"(\"t\"), representing the discrete samples, \"x\"[\"n\"], is low-pass filtered to recover a continuous-time signal, \"x\"(\"t\").\n\nEven though this is \"not\" what a DAC does in reality, the DAC output can be modeled by applying the hypothetical sequence of dirac impulses, \"x\"(\"t\"), to a linear, time-invariant filter with such characteristics (which, for an LTI system, are fully described by the impulse response) so that each input impulse results in the correct constant pulse in the output.\n\nBegin by defining a continuous-time signal from the sample values, as above but using delta functions instead of rect functions:\n\nThe scaling by formula_6, which arises naturally by time-scaling the delta function, has the result that the mean value of \"x\"(\"t\") is equal to the mean value of the samples, so that the lowpass filter needed will have a DC gain of 1. Some authors use this scaling, while many others omit the time-scaling and the \"T\", resulting in a low-pass filter model with a DC gain of \"T\", and hence dependent on the units of measurement of time.\n\nThe zero-order hold is the hypothetical filter or LTI system that converts the sequence of modulated Dirac impulses \"x\"(\"t\")to the piecewise-constant signal (shown in Figure 2):\n\nresulting in an effective impulse response (shown in Figure 4) of:\n\nThe effective frequency response is the continuous Fourier transform of the impulse response.\n\nThe Laplace transform transfer function of the ZOH is found by substituting \"s\" = \"i\" 2 π \"f\":\n\nThe fact that practical digital-to-analog converters (DAC) do not output a sequence of dirac impulses, \"x\"(\"t\") (that, if ideally low-pass filtered, would result in the unique underlying bandlimited signal before sampling), but instead output a sequence of rectangular pulses, \"x\"(\"t\") (a piecewise constant function), means that there is an inherent effect of the ZOH on the effective frequency response of the DAC, resulting in a mild roll-off of gain at the higher frequencies (a 3.9224 dB loss at the Nyquist frequency, corresponding to a gain of sinc(1/2) = 2/π). This drop is a consequence of the \"hold\" property of a conventional DAC, and is \"not\" due to the sample and hold that might precede a conventional analog-to-digital converter (ADC).\n\n", "related": "\n- Nyquist–Shannon sampling theorem\n- First-order hold\n- Discretization of linear state space models (assuming zero-order hold)\n"}
{"id": "715886", "url": "https://en.wikipedia.org/wiki?curid=715886", "title": "Time-to-digital converter", "text": "Time-to-digital converter\n\nIn electronic instrumentation and signal processing, a time to digital converter (abbreviated TDC) is a device for recognizing events and providing a digital representation of the time they occurred. For example, a TDC might output the time of arrival for each incoming pulse. Some applications wish to measure the time interval between two events rather than some notion of an absolute time.\n\nIn electronics time-to-digital converters (TDCs) or time digitizers are devices commonly used to measure a time interval and convert it into digital (binary) output. In some cases interpolating TDCs are also called time counters (TCs).\n\nTDCs are used in many different applications, where the time interval between two signal pulses (start and stop pulse) should be determined. Measurement is started and stopped, when either the rising or the falling edge of a signal pulse crosses a set threshold. These requirements are fulfilled in many physical experiments, like time-of-flight and lifetime measurements in atomic and high energy physics, experiments that involve laser ranging and electronic research involving the testing of integrated circuits and high-speed data transfer.\n\nTDCs are used in applications where measurement events happen infrequently, such as high energy physics experiments, where the sheer number of data channels in most detectors ensures that each channel will be excited only infrequently by particles such as electrons, photons, and ions.\n\nIf the required time resolution is not high, then counters can be used to make the conversion.\n\nIn its simplest implementation, a TDC is simply a high-frequency counter that increments every clock cycle. The current contents of the counter represents the current time. When an event occurs, the counter's value is captured in an output register.\n\nIn that approach, the measurement is an integer number of clock cycles, so the measurement is quantized to a clock period. To get finer resolution, a faster clock is needed. The accuracy of the measurement depends upon the stability of the clock frequency.\n\nTypically a TDC uses a crystal oscillator reference frequency for good long term stability. High stability crystal oscillators are usually relative low frequency such as 10 MHz (or 100 ns resolution). To get better resolution, a phase-locked loop frequency multiplier can be used to generate a faster clock. One might, for example, multiply the crystal reference oscillator by 100 to get a clock rate of 1 GHz (1 ns resolution).\n\nHigh clock rates impose additional design constraints on the counter: if the clock period is short, it is difficult to update the count. Binary counters, for example, need a fast carry architecture because they essentially add one to the previous counter value. A solution is using a hybrid counter architecture. A Johnson counter, for example, is a fast non-binary counter. It can be used to count very quickly the low order count; a more conventional binary counter can be used to accumulate the high order count. The fast counter is sometime called a prescaler.\n\nThe speed of counters fabricated in CMOS-technology is limited by the capacitance between the gate and the channel and by the resistance of the channel and the signal traces. The product of both is the cut-off-frequency. Modern chip technology allows multiple metal layers and therefore coils with a large number of windings to be inserted into the chip.\nThis allows designers to peak the device for a specific frequency, which may lie above the cut-off-frequency of the original transistor.\n\nA peaked variant of the Johnson counter is the traveling-wave counter which also achieves sub-cycle resolution. Other methods to achieve sub-cycle resolution include analog-to-digital converters and vernier Johnson counters.\n\nIn most situations, the user does not want to just capture an arbitrary time that an event occurs, but wants to measure a time interval, the time between a start event and a stop event.\n\nThat can be done by measuring an arbitrary time both the start and stop events and subtracting. The measurement can be off by two counts.\n\nThe subtraction can be avoided if the counter is held at zero until the start event, counts during the interval, and then stops counting after the stop event.\n\nCoarse counters base on a reference clock with signals generated at a stable frequency formula_1. When the start signal is detected the counter starts counting clock signals and terminates counting after the stop signal is detected. The time interval formula_2 between start and stop is then\n\nwith formula_4, the number of counts and formula_5, the period of the reference clock.\n\nSince start, stop and clock signal are asynchronous, there is a uniform probability distribution of the start and stop signal-times between two subsequent clock pulses. This detuning of the start and stop signal from the clock pulses is called quantization error.\n\nFor a series of measurements on the same constant and asynchronous time interval one measures two different numbers of counted clock pulses formula_6 and formula_7 (see picture). These occur with probabilities\n\nwith formula_10 the fractional part of formula_11. The value for the time interval is then obtained by\n\nMeasuring a time interval using a coarse counter with the averaging method described above is relatively time consuming because of the many repetitions that are needed to determine the probabilities formula_13 and formula_14. In comparison to the other methods described later on, a coarse counter has a very limited resolution (1ns in case of a 1 GHz reference clock), but satisfies with its theoretically unlimited measuring range.\n\nIn contrast to the coarse counter in the previous section, fine measurement methods with much better accuracy but far smaller measuring range are presented here. Analogue methods like time interval stretching or double conversion as well as digital methods like tapped delay lines and the Vernier method are under examination. Though the analogue methods still obtain better accuracies, digital time interval measurement is often preferred due to its flexibility in integrated circuit technology and its robustness against external perturbations like temperature changes.\n\nThe counter implementation's accuracy is limited by the clock frequency. If time is measured by whole counts, then the resolution is limited to the clock period. For example, a 10 MHz clock has a resolution of 100 ns. To get resolution finer than a clock period, there are time interpolation circuits. These circuits measure the fraction of a clock period: that is, the time between a clock event and the event being measured. The interpolation circuits often require a significant amount of time to perform their function; consequently, the TDC needs a quiet interval before the next measurement.\n\nWhen counting is not feasible because the clock rate would be too high, analog methods can be used. Analog methods are often used to measure intervals that are between 10 and 200 ns. These methods often use a capacitor that is charged during the interval being measured. Initially, the capacitor is discharged to zero volts. When the start event occurs, the capacitor is charged with a constant current \"I\"; the constant current causes the voltage \"v\" on the capacitor to increase linearly with time. The rising voltage is called the fast ramp. When the stop event occurs, the charging current is stopped. The voltage on the capacitor \"v\" is directly proportional to the time interval \"T\" and can be measured with an analog-to-digital converter (ADC). The resolution of such a system is in the range of 1 to 10 ps.\n\nAlthough a separate ADC can be used, the ADC step is often integrated into the interpolator. A second constant current \"I\" is used to discharge the capacitor at a constant but much slower rate (the slow ramp). The slow ramp might be 1/1000 of the fast ramp. This discharge effectively \"stretches\" the time interval; it will take 1000 times as long for the capacitor to discharge to zero volts. The stretched interval can be measured with a counter. The measurement is similar to a dual-slope analog converter.\n\nThe dual-slope conversion can take a long time: a thousand or so clock ticks in the scheme described above. That limits how often a measurement can be made (dead time). Resolution of 1 ps with a 100 MHz (10 ns) clock requires a stretch ratio of 10,000 and implies a conversion time of 150 μs. To decrease the conversion time, the interpolator circuit can be used twice in a residual interpolator technique. The fast ramp is used initially as above to determine the time. The slow ramp is only at 1/100. The slow ramp will cross zero at some time during the clock period. When the ramp crosses zero, the fast ramp is turned on again to measure the crossing time (\"t\"). Consequently, the time can be determined to 1 part in 10,000.\n\nInterpolators are often used with a stable system clock. The start event is asynchronous, but the stop event is a following clock. For convenience, imagine that the fast ramp rises exactly 1 volt during a 100 ns clock period. Assume the start event occurs at 67.3 ns after a clock pulse; the fast ramp integrator is triggered and starts rising. The asynchronous start event is also routed through a synchronizer that takes at least two clock pulses. By the next clock pulse, the ramp has risen to .327 V. By the second clock pulse, the ramp has risen to 1.327 V and the synchronizer reports the start event has been seen. The fast ramp is stopped and the slow ramp starts. The synchronizer output can be used to capture system time from a counter. After 1327 clocks, the slow ramp returns to its starting point, and interpolator knows that the event occurred 132.7 ns before the synchronizer reported.\n\nThe interpolator is actually more involved because there are synchronizer issues and current switching is not instantaneous. Also, the interpolator must calibrate the height of the ramp to a clock period.\n\nThe vernier method is more involved. The method involves a triggerable oscillator and a coincidence circuit. At the event, the integer clock count is stored and the oscillator is started. The triggered oscillator has a slightly different frequency than the clock oscillator. For sake of argument, say the triggered oscillator has a period that is 1 ns faster than the clock. If the event happened 67 ns after the last clock, then the triggered oscillator transition will slide by −1 ns after each subsequent clock pulse. The triggered oscillator will be at 66 ns after the next clock, at 65 ns after the second clock, and so forth. A coincidence detector looks for when the triggered oscillator and the clock transition at the same time, and that indicates the fraction time that needs to be added.\n\nThe interpolator design is more involved. The triggerable clock must be calibrated to clock. It must also start quickly and cleanly.\n\nThe Vernier method is a digital version of the time stretching method. Two only slightly detuned oscillators (with frequencies formula_15 and formula_16) start their signals with the arrival of the start and the stop signal. As soon as the leading edges of the oscillator signals coincide the measurement ends and the number of periods of the oscillators (formula_6 and formula_7 respectively) lead to the original time interval formula_2:\n\nSince highly reliable oscillators with stable and accurate frequency are still quite a challenge one also realizes the vernier method via two tapped delay lines using two slightly different cell delay times formula_21. This setting is called differential delay line or vernier delay line.\n\nIn the example presented here the first delay line affiliated with the start signal contains cells of D-flip-flops with delay formula_22 which are initially set to transparent. During the transition of the start signal through one of those cells, the signal is delayed by formula_22 and the state of the flip-flop is sampled as transparent. The second delay line belonging to the stop signal is composed of a series of non-inverting buffers with delay formula_24. Propagating through its channel the stop signal latches the flip-flops of the start signal's delay line. As soon as the stop signal passes the start signal, the latter is stopped and all leftover flip-flops are sampled opaque. Analogous to the above case of the oscillators the wanted time interval formula_2 is then\n\nwith n the number of cells marked as transparent.\n\nIn general a tapped delay line contains a number of cells with well defined delay times formula_21. Propagating through this line the start signal is delayed. The state of the line is sampled at the time of the arrival of the stop signal.\nThis can be realized for example with a line of D-flip-flop cells with a delay time formula_21. The start signal propagates through this line of transparent flip-flops and is delayed by a certain number of them. The output of each flip-flop is sampled on the fly. The stop signal latches all flip-flops while propagating through its channel undelayed and the start signal cannot propagate further. Now the time interval between start and stop signal is proportional to the number of flip-flops that were sampled as transparent.\n\nCounters can measure long intervals but have limited resolution. Interpolators have high resolution but they cannot measure long intervals. A hybrid approach can achieve both long intervals and high resolution. The long interval can be measured with a counter. The counter information is supplemented with two time interpolators: one interpolator measures the (short) interval between the start event and a following clock event, and the second interpolator measure the interval between the stop event and a following clock event. The basic idea has some complications: the start and stop events are asynchronous, and one or both might happen close to a clock pulse. The counter and interpolators must agree on matching the start and end clock events. To accomplish that goal, synchronizers are used.\n\nThe common hybrid approach is the Nutt method. In this example the fine measurement circuit measures the time between start and stop pulse and the respective second nearest clock pulse of the coarse counter (\"T\", \"T\"), detected by the synchronizer (see figure). Thus the wanted time interval is\n\nwith \"n\" the number of counter clock pulses and \"T\" the period of the coarse counter.\n\nTime measurement has played a crucial role in the understanding of nature from the earliest times. Starting with sun, sand or water driven clocks we are able to use clocks today, based on the most precise caesium resonators.\n\nThe first direct predecessor of a TDC was invented in the year 1942 by Bruno Rossi for the measurement of muon lifetimes. It was designed as a time-to-amplitude-converter, constantly charging a capacitor during the measured time interval. The corresponding voltage is directly proportional to the time interval under examination.\n\nWhile the basic concepts (like Vernier methods (Pierre Vernier 1584-1638) and time stretching) of dividing time into measurable intervals are still up-to-date, the implementation changed a lot during the past 50 years. Starting with vacuum tubes and ferrite pot-core transformers those ideas are implemented in complementary metal-oxide-semiconductor (CMOS) design today.\n\nRegarding even the fine measuring methods presented, there are still errors one may wish remove or at least to consider. Non-linearities of the time-to-digital conversion for example can be identified by taking a large number of measurements of a poissonian distributed source (statistical code density test). Small deviations from the uniform distribution reveal the non-linearities.\nInconveniently the statistical code density method is quite sensitive to external temperature changes. Thus stabilizing delay or phase-locked loop (DLL or PLL) circuits are recommended.\n\nIn a similar way, offset errors (non-zero readouts at \"T\" = 0) can be removed.\n\nFor long time intervals, the error due to instabilities in the reference clock (jitter) plays a major role. Thus clocks of superior quality are needed for such TDCs.\n\nFurthermore, external noise sources can be eliminated in postprocessing by robust estimation methods.\n\nTDCs are currently built as stand-alone measuring devices in physical experiments or as system components like PCI cards. They can be made up of either discrete or integrated circuits.\n\nCircuit design changes with the purpose of the TDC, which can either be a very good solution for single-shot TDCs with long dead times or some trade-off between dead-time and resolution for multi-shot TDCs.\n\nThe time-to-digital converter measures the time between a start event and a stop event. There is also a digital-to-time converter or delay generator. The delay generator converts a number to a time delay. When the delay generator gets a start pulse at its input, then it outputs a stop pulse after the specified delay. The architectures for TDC and delay generators are similar. Both use counters for long, stable, delays. Both must consider the problem of clock quantization errors.\n\nFor example, the Tektronix 7D11 Digital Delay uses a counter architecture. A digital delay may be set from 100 ns to 1 s in 100 ns increments. An analog circuit provides an additional fine delay of 0 to 100 ns. A 5 MHz reference clock drives a phase-locked loop to produce a stable 500 MHz clock. It is this fast clock that is gated by the (fine-delayed) start event and determines the main quantization error. The fast clock is divided down to 10 MHz and fed to main counter. The instrument quantization error depends primarily on the 500 MHz clock (2 ns steps), but other errors also enter; the instrument is specified to have 2.2 ns of jitter. The recycle time is 575 ns.\n\nJust as a TDC may use interpolation to get finer than one clock period resolution, a delay generator may use similar techniques. The Hewlett-Packard 5359A High Resolution Time Synthesizer provides delays of 0 to 160 ms, has an accuracy of 1 ns, and achieves a typical jitter of 100 ps. The design uses a triggered phase-locked oscillator that runs at 200 MHz. Interpolation is done with a ramp, an 8-bit digital-to-analog converter, and a comparator. The resolution is about 45 ps.\nWhen the start pulse is received, then counts down and outputs a stop pulse. For low jitter the synchronous counter has to feed a zero flag from the most significant bit down to the least significant bit and then combine it with the output from the Johnson counter.\n\nA digital-to-analog converter (DAC) could be used to achieve sub-cycle resolution, but it is easier to either use vernier Johnson counters or traveling-wave Johnson counters.\n\nThe delay generator can be used for pulse width modulation, e.g. to drive a MOSFET to load a Pockels cell within 8 ns with a specific charge.\n\nThe output of a delay generator can gate a digital-to-analog converter and so pulses of a variable height can be generated. This allows matching to low levels needed by analog electronics, higher levels for ECL and even higher levels for TTL. If a series of DACs is gated in sequence, variable pulse shapes can be generated to account for any transfer function.\n\n", "related": "\n- Sampling frequency\n- Multivibrator\n- LIDAR\n- Time-of-flight\n\n- http://www.freepatentsonline.com/8324952.html\n- traveling wave CMOS\n- traveling wave nFET cascode\n- http://www.febo.com/pages/hp5370b/\n- http://www.g8wrb.org/useful-stuff/time/HP-5370B/\n- http://ilrs.gsfc.nasa.gov/docs/timing/artyukh_time_interval_counter.pdf\n- http://ilrs.gsfc.nasa.gov/docs/time_interval_measurements.pdf\n- http://tycho.usno.navy.mil/ptti/1994/Vol%2026_22.pdf\n- http://www.acam.de/fileadmin/Download/pdf/English/AN002_e.pdf\n- Università degli studi Roma Tre, Scuola Dottorale in Scienze Matematiche e Fisiche\n- http://www.ti.com/lit/ds/symlink/tdc7200.pdf\n"}
{"id": "22564667", "url": "https://en.wikipedia.org/wiki?curid=22564667", "title": "Signal averaging", "text": "Signal averaging\n\nSignal averaging is a signal processing technique applied in the time domain, intended to increase the strength of a signal relative to noise that is obscuring it. By averaging a set of replicate measurements, the signal-to-noise ratio (SNR) will be increased, ideally in proportion to the number of measurements.\n\nAssumed that\n- Signal formula_1 is uncorrelated to noise, and noise formula_2 is uncorrelated : formula_3.\n- Signal power formula_4 is constant in the replicate measurements.\n- Noise is random, with a mean of zero and constant variance in the replicate measurements: formula_5 and formula_6.\n- We (canonically) define Signal-to-Noise ratio as formula_7.\n\nAssuming we sample the noise, we get a per-sample variance of\n\nformula_8.\n\nAveraging a random variable leads to the following variance:\n\nformula_9.\n\nSince noise variance is constant formula_10:\n\nformula_11,\n\ndemonstrating that averaging formula_12 realizations of the same, uncorrelated noise reduces noise power by a factor of formula_12.\n\nConsidering formula_12 vectors formula_15 of signal samples of length formula_16:\n\nformula_17,\n\nthe power formula_18 of such a vector simply is\n\nformula_19.\n\nAgain, averaging the formula_12 vectors formula_21, yields the following averaged vector\n\nformula_22.\n\nIn the case where formula_23, we see that formula_24 reaches a maximum of\n\nformula_25.\n\nIn this case, the ratio of signal to noise also reaches a maximum,\n\nformula_26.\n\nThis is the oversampling case, where the observed signal is correlated (because oversampling implies that the signal observations are strongly correlated).\n\nAveraging is applied to enhance a time-locked signal component in noisy measurements; time-locking implies that the signal is observation-periodic, so we end up in the maximum case above.\n\nA specific way of obtaining replicates is to average all the odd and even trials in separate buffers. This has the advantage of allowing for comparison of even and odd results from interleaved trials. An average of odd and even averages generates the completed averaged result, while the difference between the odd and even averages constitutes an estimate of the noise.\n\nThe following is a MATLAB simulation of the averaging process:\n\nThe averaging process above, and in general, results in an estimate of the signal. When compared with the raw trace, the averaged noise component is reduced with every averaged trial. When averaging real signals, the underlying component may not always be as clear, resulting in repeated averages in a search for consistent components in two or three replicates. It is unlikely that two or more consistent results will be produced by chance alone.\n\nSignal averaging typically relies heavily on the assumption that the noise component of a signal is random, having zero mean, and being unrelated to the signal. However, there are instances in which the noise is not uncorrelated. A common example of correlated noise is a hum (e.g. 50 or 60 Hz noise originating from power lines). To apply the signal averaging technique, a few critical adaptations must be made, and the problem can be avoided by:\n- Randomizing the stimulus interval, or\n- Using a noninteger stimulus rate (e.g. 3.4 Hz instead of 3.0 Hz)\n", "related": "NONE"}
{"id": "38483165", "url": "https://en.wikipedia.org/wiki?curid=38483165", "title": "Noise (signal processing)", "text": "Noise (signal processing)\n\nIn signal processing, noise is a general term for unwanted (and, in general, unknown) modifications that a signal may suffer during capture, storage, transmission, processing, or conversion.\n\nSometimes the word is also used to mean signals that are random (unpredictable) and carry no useful information; even if they are not interfering with other signals or may have been introduced intentionally, as in comfort noise.\n\nNoise reduction, the recovery of the original signal from the noise-corrupted one, is a very common goal in the design of signal processing systems, especially filters. The mathematical limits for noise removal are set by information theory, namely the Nyquist–Shannon sampling theorem.\n\nSignal processing noise can be classified by its statistical properties (sometimes called the \"color\" of the noise) and by how it modifies the intended signal:\n- Additive noise, gets added to the unintended signal\n- White noise\n-  Additive white Gaussian noise\n- Black noise\n- Gaussian noise\n- Pink noise or flicker noise, with 1/\"f\" power spectrum\n- Brownian noise, with 1/\"f\" power spectrum\n- Contaminated Gaussian noise, whose PDF is a linear mixture of Gaussian PDFs\n- Power-law noise\n- Cauchy noise\n- Multiplicative noise, multiplies or modulates the intended signal\n- Quantization error, due to conversion from continuous to discrete values\n- Poisson noise, typical of signals that are rates of discrete events\n- Shot noise, e.g. caused by static electricity discharge\n- Transient noise, a short pulse followed by decaying oscillations\n- Burst noise, powerful but only during short intervals\n- Phase noise, random time shifts in a signal\n\nNoise may arise in signals of interest to various scientific and technical fields, often with specific features:\n- Noise (audio), such as \"hiss\" or \"hum\", in audio signals\n- Background noise, due to spurious sounds during signal capture\n- Comfort noise, added to voice communications to fill silent gaps\n- Electromagnetically induced noise, audible noise due to electromagnetic vibrations in systems involving electromagnetic fields\n- Noise (video), such as \"snow\"\n- Noise (radio), such as \"static\", in radio transmissions\n- Image noise, affects images, usually digital ones\n- Salt and pepper noise or spike noise, scattered very dark or very light pixels\n- Fixed pattern noise, that is tied to pixel sensors\n- Shadow noise, made visible by increasing brightness or contrast\n- Speckle noise, typical of radar imaging and interferograms\n- Film grain in analog photography\n- Compression artifacts or \"mosquito noise\" around edges in JPEG and other formats\n- Noise (electronics) in electrical signals\n- Johnson–Nyquist noise, in semiconductors\n- Quantum noise\n- Quantum 1/f noise, a disputed theory about quantum systems\n- Generation-recombination noise, in semiconductor devices\n- Oscillator phase noise, random fluctuations of the phase of an oscillator\n- Barkhausen effect or Barkhausen noise, in the strength of a ferromagnet\n- Spectral splatter or switch noise, caused by on/off transmitter switching\n- Ground noise, appearing at the ground terminal of audio equipment\n- Synaptic noise, observed in neuroscience\n- Neuronal noise, observed in neuroscience\n- Transcriptional noise in the transcription of genes to proteins\n- Cosmic noise, in radioastronomy\n- Phonon noise in materials science\n- Internet background noise, packets sent to unassigned or inactive IP addresses\n- Fano noise, in particle detectors\n- Mode partition noise in optical cables\n- Seismic noise, spurious ground vibrations in seismology\n- Cosmic microwave background, microwave noise left over from the Big Bang\n\nA long list of noise measures have been defined to measure noise in signal processing: in absolute terms, relative to some standard noise level, or relative to the desired signal level. They include: \n- Dynamic range, often defined by inherent noise level\n- Signal-to-noise ratio (SNR), ratio of noise power to signal power\n- Peak signal-to-noise ratio, maximum SNR in a system\n- Signal to noise ratio (imaging), for images\n- Carrier-to-noise ratio, the signal-to-noise ratio of a modulated signal\n- Noise power\n- Noise figure\n- Noise-equivalent flux density, a measure of noise in astronomy\n- Noise floor\n- Noise margin, by how much a signal exceeds the noise level\n- Reference noise, a reference level for electronic noise\n- Noise spectral density, noise power per unit of bandwidth\n- Noise temperature\n- Effective input noise temperature\n- Noise-equivalent power, a measure of sensitivity for photodetectors\n- Relative intensity noise, in a laser beam\n- Antenna noise temperature, measure of noise in telecommunications antenna\n- Received noise power, noise at a telecommunications receiver\n- Circuit noise level, ratio of circuit noise to some reference level\n- Channel noise level, some measure of noise in a communication channel\n- Noise-equivalent target, intensity of a target when the signal-to-noise level is 1\n- Equivalent noise resistance, a measure of noise based on equivalent resistor\n- Carrier-to-receiver noise density, ratio of received carrier power to receiver noise\n- Carrier-to-noise-density ratio,\n- Spectral signal-to-noise ratio\n- Antenna gain-to-noise temperature, a measure of antenna performance\n- Contrast-to-noise ratio, a measure of image quality\n- Noise print, statistical signature of ambient noise for its suppression\n- Equivalent pulse code modulation noise, measure of noise by comparing to PCM quantization noise\n\nAlmost every technique and device for signal processing has some connection to noise. Some random examples are:\n- Noise shaping\n- Antenna analyzer or noise bridge, used to measure the efficiency of antennas\n- Noise gate\n- Noise generator, a circuit that produces a random electrical signal\n- Radio noise source used to calibrate radiotelescopes\n- Friis formulas for the noise in telecommunications\n- Noise-domain reflectometry, uses existing signals to find cable faults\n- Noise-immune cavity-enhanced optical heterodyne molecular spectroscopy\n\n", "related": "\n- Anti-information\n- Signal-to-noise statistic, a mathematical formula to measure the difference of two values relative to their standard deviations\n"}
{"id": "38484030", "url": "https://en.wikipedia.org/wiki?curid=38484030", "title": "Multiplicative noise", "text": "Multiplicative noise\n\nIn signal processing, the term multiplicative noise refers to an unwanted random signal that gets multiplied into some relevant signal during capture, transmission, or other processing.\n\nAn important example is the speckle noise commonly observed in radar imagery. Examples of multiplicative noise affecting digital photographs are proper shadows due to undulations on the surface of the imaged objects, shadows cast by complex objects like foliage and Venetian blinds, dark spots caused by dust in the lens or image sensor, and variations in the gain of individual elements of the image sensor array.\n", "related": "NONE"}
{"id": "46956", "url": "https://en.wikipedia.org/wiki?curid=46956", "title": "Cepstrum", "text": "Cepstrum\n\nA cepstrum (; plural \"cepstra\") is the result of taking the inverse Fourier transform (IFT) of the logarithm of the estimated signal spectrum. There is a \"complex cepstrum\", a \"real cepstrum\", a \"power cepstrum\", and a \"phase cepstrum\".\nThe power cepstrum in particular has applications in the analysis of human speech.\n\nThe name \"cepstrum\" was derived by reversing the first four letters of \"spectrum\". Operations on cepstra are labelled \"quefrency analysis\" (aka \"quefrency alanysis\"), \"liftering\", or \"cepstral analysis\".\nIt may be pronounced in the two ways given, the second having the advantage of avoiding confusion with \"kepstrum\", which also exists (see below).\n\nThe \"power cepstrum\" was defined in a 1963 paper by Bogert et al. The power cepstrum of a signal is defined as the squared magnitude of the inverse Fourier transform of the logarithm of the squared magnitude of the Fourier transform of a signal:\n\nReferences to the Bogert paper, in a bibliography, are often edited incorrectly. The terms \"quefrency\", \"alanysis\", \"cepstrum\" and \"saphe\" were invented by the authors by rearranging some letters in frequency, analysis, spectrum and phase. The new invented terms are defined by analogies to the older terms.\n\nA short-time cepstrum analysis was proposed by Schroeder and Noll for application to pitch determination of human speech.\n\nThe \"complex cepstrum\" was defined by Oppenheim in his development of homomorphic system theory and is defined as the inverse Fourier transform of the logarithm of the complex-valued Fourier transform of the signal (with unwrapped phase):\n\nwhere is the integer required to properly unwrap the angle or imaginary part of the complex log function. It is also sometimes called the \"spectrum of a spectrum\".\n\nThe \"real cepstrum\" uses the logarithm function defined for real values. The real cepstrum is related to the power by the relationship\nand to the complex cepstrum as\n\nThe \"phase cepstrum\" is related to the complex cepstrum as\n\nThe complex cepstrum holds information about magnitude and phase of the initial spectrum, allowing the reconstruction of the signal. The real cepstrum uses only the information of the magnitude of the spectrum.\n\nMany texts define the process as FT → abs() → log → IFT, i.e., that the power cepstrum is the \"inverse Fourier transform of the log-magnitude Fourier spectrum\". (the difference between squaring or taking the absolute value amounts to an overall factor of 2).\n\nThe \"kepstrum\", which stands for \"Kolmogorov-equation power-series time response\", is similar to the cepstrum and has the same relation to it as expected value has to statistical average, i.e. cepstrum is the empirically measured quantity, while kepstrum is the theoretical quantity. It was in use before the cepstrum.\n\nThe cepstrum can be seen as information about the rate of change in the different spectrum bands. It was originally invented for characterizing the seismic echoes resulting from earthquakes and bomb explosions. It has also been used to determine the fundamental frequency of human speech and to analyze radar signal returns. Cepstrum pitch determination is particularly effective because the effects of the vocal excitation (pitch) and vocal tract (formants) are additive in the logarithm of the power spectrum and thus clearly separate.\n\nThe autocepstrum is defined as the cepstrum of the autocorrelation. The autocepstrum is more accurate than the cepstrum in the analysis of data with echoes.\n\nThe cepstrum is a representation used in homomorphic signal processing, to convert signals combined by convolution (such as a source and filter) into sums of their cepstra, for linear separation. In particular, the power cepstrum is often used as a feature vector for representing the human voice and musical signals. For these applications, the spectrum is usually first transformed using the mel scale. The result is called the mel-frequency cepstrum or MFC (its coefficients are called mel-frequency cepstral coefficients, or MFCCs). It is used for voice identification, pitch detection and much more. The cepstrum is useful in these applications because the low-frequency periodic excitation from the vocal cords and the formant filtering of the vocal tract, which convolve in the time domain and multiply in the frequency domain, are additive and in different regions in the quefrency domain.\n\nRecently cepstrum based deconvolution was used to remove the effect of the stochastic impulse trains, which originates an sEMG signal, from the power spectrum of sEMG signal itself. In this way, only information on motor unit action potential (MUAP) shape and amplitude were maintained, and then, used to estimate the parameters of a time-domain model of the MUAP itself.\n\nThe independent variable of a cepstral graph is called the quefrency. The quefrency is a measure of time, though not in the sense of a signal in the time domain. For example, if the sampling rate of an audio signal is 44100 Hz and there is a large peak in the cepstrum whose quefrency is 100 samples, the peak indicates the presence of a fundamental frequency that is 44100/100 = 441 Hz. This peak occurs in the cepstrum because the harmonics in the spectrum are periodic and the period corresponds to the fundamental frequency, since harmonics are integer multiples of said fundamental frequency. \n\nNote that a pure sine wave can not be used to test the cepstrum for its pitch determination from quefrency as a pure sine wave does not contain any harmonics and does not lead to quefrency peaks. Rather, a test signal containing harmonics should be used (such as the sum of at least two sines where the second sine is some harmonic (multiple) of the first sine, or better, a signal with a square or triangle waveform, as such signals provide many overtones in the spectrum.).\n\nPlaying further on the anagram theme, a filter that operates on a cepstrum might be called a \"lifter\". A low-pass lifter is similar to a low-pass filter in the frequency domain. It can be implemented by multiplying by a window in the quefrency domain and then converting back to the frequency domain, resulting in a smoother signal.\n\nA very important property of the cepstral domain is that the convolution of two signals can be expressed as the addition of their complex cepstra:\n\n- D. G. Childers, D. P. Skinner, R. C. Kemerait, \"The Cepstrum: A Guide to Processing\", \"Proceedings of the IEEE\", Vol. 65, No. 10, October 1977, pp. 1428–1443.\n- \"Speech Signal Analysis\"\n- \"Speech analysis: Cepstral analysis vs. LPC\", www.advsolned.com\n- \"A tutorial on Cepstrum and LPCCs\"\n- Alan V. Oppenheim and Ronald W. Schafer, \"From Frequency to Quefrency: A History of the Cepstrum\", IEEE SIGNAL PROCESSING MAGAZINE, Sep. 2004, pp. 95–99\n", "related": "NONE"}
