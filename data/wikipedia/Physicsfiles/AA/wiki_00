{"id": "22939", "url": "https://en.wikipedia.org/wiki?curid=22939", "title": "Physics", "text": "Physics\n\nPhysics (from , from \"phýsis\" 'nature') is the natural science that studies matter, its motion and behavior through space and time, and the related entities of energy and force. Physics is one of the most fundamental scientific disciplines, and its main goal is to understand how the universe behaves.\n\nPhysics is one of the oldest academic disciplines and, through its inclusion of astronomy, perhaps \"the\" oldest. Over much of the past two millennia, physics, chemistry, biology, and certain branches of mathematics were a part of natural philosophy, but during the Scientific Revolution in the 17th century these natural sciences emerged as unique research endeavors in their own right. Physics intersects with many interdisciplinary areas of research, such as biophysics and quantum chemistry, and the boundaries of physics are not rigidly defined. New ideas in physics often explain the fundamental mechanisms studied by other sciences and suggest new avenues of research in academic disciplines such as mathematics and philosophy.\n\nAdvances in physics often enable advances in new technologies. For example, advances in the understanding of electromagnetism, solid-state physics, and nuclear physics led directly to the development of new products that have dramatically transformed modern-day society, such as television, computers, domestic appliances, and nuclear weapons; advances in thermodynamics led to the development of industrialization; and advances in mechanics inspired the development of calculus.\n\nAstronomy is one of the oldest natural sciences. Early civilizations dating back before 3000 BCE, such as the Sumerians, ancient Egyptians, and the Indus Valley Civilisation, had a predictive knowledge and a basic understanding of the motions of the Sun, Moon, and stars. The stars and planets, believed to represent gods, were often worshipped. While the explanations for the observed positions of the stars were often unscientific and lacking in evidence, these early observations laid the foundation for later astronomy, as the stars were found to traverse great circles across the sky, which however did not explain the positions of the planets.\n\nAccording to Asger Aaboe, the origins of Western astronomy can be found in Mesopotamia, and all Western efforts in the exact sciences are descended from late Babylonian astronomy. Egyptian astronomers left monuments showing knowledge of the constellations and the motions of the celestial bodies, while Greek poet Homer wrote of various celestial objects in his \"Iliad\" and \"Odyssey\"; later Greek astronomers provided names, which are still used today, for most constellations visible from the Northern Hemisphere.\n\nNatural philosophy has its origins in Greece during the Archaic period (650 BCE – 480 BCE), when pre-Socratic philosophers like Thales rejected non-naturalistic explanations for natural phenomena and proclaimed that every event had a natural cause. They proposed ideas verified by reason and observation, and many of their hypotheses proved successful in experiment; for example, atomism was found to be correct approximately 2000 years after it was proposed by Leucippus and his pupil Democritus.\n\nThe Western Roman Empire fell in the fifth century, and this resulted in a decline in intellectual pursuits in the western part of Europe. By contrast, the Eastern Roman Empire (also known as the Byzantine Empire) resisted the attacks from the barbarians, and continued to advance various fields of learning, including physics.\n\nIn the sixth century Isidore of Miletus created an important compilation of Archimedes' works that are copied in the Archimedes Palimpsest.\n\nIn sixth century Europe John Philoponus, a Byzantine scholar, questioned Aristotle's teaching of physics and noted its flaws. He introduced the theory of impetus. Aristotle's physics was not scrutinized until Philoponus appeared; unlike Aristotle, who based his physics on verbal argument, Philoponus relied on observation. On Aristotle's physics Philoponus wrote:But this is completely erroneous, and our view may be corroborated by actual observation more effectively than by any sort of verbal argument. For if you let fall from the same height two weights of which one is many times as heavy as the other, you will see that the ratio of the times required for the motion does not depend on the ratio of the weights, but that the difference in time is a very small one. And so, if the difference in the weights is not considerable, that is, of one is, let us say, double the other, there will be no difference, or else an imperceptible difference, in time, though the difference in weight is by no means negligible, with one body weighing twice as much as the otherPhiloponus' criticism of Aristotelian principles of physics served as an inspiration for Galileo Galilei ten centuries later, during the Scientific Revolution. Galileo cited Philoponus substantially in his works when arguing that Aristotelian physics was flawed. In the 1300s Jean Buridan, a teacher in the faculty of arts at the University of Paris, developed the concept of impetus. It was a step toward the modern ideas of inertia and momentum.\n\nIslamic scholarship inherited Aristotelian physics from the Greeks and during the Islamic Golden Age developed it further, especially placing emphasis on observation and \"a priori\" reasoning, developing early forms of the scientific method.\n\nThe most notable innovations were in the field of optics and vision, which came from the works of many scientists like Ibn Sahl, Al-Kindi, Ibn al-Haytham, Al-Farisi and Avicenna. The most notable work was \"The Book of Optics\" (also known as Kitāb al-Manāẓir), written by Ibn al-Haytham, in which he conclusively disproved the ancient Greek idea about vision, but also came up with a new theory. In the book, he presented a study of the phenomenon of the camera obscura (his thousand-year-old version of the pinhole camera) and delved further into the way the eye itself works. Using dissections and the knowledge of previous scholars, he was able to begin to explain how light enters the eye. He asserted that the light ray is focused, but the actual explanation of how light projected to the back of the eye had to wait until 1604. His \"Treatise on Light\" explained the camera obscura, hundreds of years before the modern development of photography.\nThe seven-volume \"Book of Optics\" (\"Kitab al-Manathir\") hugely influenced thinking across disciplines from the theory of visual perception to the nature of perspective in medieval art, in both the East and the West, for more than 600 years. Many later European scholars and fellow polymaths, from Robert Grosseteste and Leonardo da Vinci to René Descartes, Johannes Kepler and Isaac Newton, were in his debt. Indeed, the influence of Ibn al-Haytham's Optics ranks alongside that of Newton's work of the same title, published 700 years later.\n\nThe translation of \"The Book of Optics\" had a huge impact on Europe. From it, later European scholars were able to build devices that replicated those Ibn al-Haytham had built, and understand the way light works. From this, such important things as eyeglasses, magnifying glasses, telescopes, and cameras were developed.\n\nPhysics became a separate science when early modern Europeans used experimental and quantitative methods to discover what are now considered to be the laws of physics.\n\nMajor developments in this period include the replacement of the geocentric model of the Solar System with the heliocentric Copernican model, the laws governing the motion of planetary bodies determined by Johannes Kepler between 1609 and 1619, pioneering work on telescopes and observational astronomy by Galileo Galilei in the 16th and 17th Centuries, and Isaac Newton's discovery and unification of the laws of motion and universal gravitation that would come to bear his name. Newton also developed calculus, the mathematical study of change, which provided new mathematical methods for solving physical problems.\n\nThe discovery of new laws in thermodynamics, chemistry, and electromagnetics resulted from greater research efforts during the Industrial Revolution as energy needs increased. The laws comprising classical physics remain very widely used for objects on everyday scales travelling at non-relativistic speeds, since they provide a very close approximation in such situations, and theories such as quantum mechanics and the theory of relativity simplify to their classical equivalents at such scales. However, inaccuracies in classical mechanics for very small objects and very high velocities led to the development of modern physics in the 20th century.\n\nModern physics began in the early 20th century with the work of Max Planck in quantum theory and Albert Einstein's theory of relativity. Both of these theories came about due to inaccuracies in classical mechanics in certain situations. Classical mechanics predicted a varying speed of light, which could not be resolved with the constant speed predicted by Maxwell's equations of electromagnetism; this discrepancy was corrected by Einstein's theory of special relativity, which replaced classical mechanics for fast-moving bodies and allowed for a constant speed of light. Black-body radiation provided another problem for classical physics, which was corrected when Planck proposed that the excitation of material oscillators is possible only in discrete steps proportional to their frequency; this, along with the photoelectric effect and a complete theory predicting discrete energy levels of electron orbitals, led to the theory of quantum mechanics taking over from classical physics at very small scales.\n\nQuantum mechanics would come to be pioneered by Werner Heisenberg, Erwin Schrödinger and Paul Dirac. From this early work, and work in related fields, the Standard Model of particle physics was derived. Following the discovery of a particle with properties consistent with the Higgs boson at CERN in 2012, all fundamental particles predicted by the standard model, and no others, appear to exist; however, physics beyond the Standard Model, with theories such as supersymmetry, is an active area of research. Areas of mathematics in general are important to this field, such as the study of probabilities and groups.\n\nIn many ways, physics stems from ancient Greek philosophy. From Thales' first attempt to characterise matter, to Democritus' deduction that matter ought to reduce to an invariant state, the Ptolemaic astronomy of a crystalline firmament, and Aristotle's book \"Physics\" (an early book on physics, which attempted to analyze and define motion from a philosophical point of view), various Greek philosophers advanced their own theories of nature. Physics was known as natural philosophy until the late 18th century.\n\nBy the 19th century, physics was realised as a discipline distinct from philosophy and the other sciences. Physics, as with the rest of science, relies on philosophy of science and its \"scientific method\" to advance our knowledge of the physical world. The scientific method employs \"a priori reasoning\" as well as \"a posteriori\" reasoning and the use of Bayesian inference to measure the validity of a given theory.\n\nThe development of physics has answered many questions of early philosophers, but has also raised new questions. Study of the philosophical issues surrounding physics, the philosophy of physics, involves issues such as the nature of space and time, determinism, and metaphysical outlooks such as empiricism, naturalism and realism.\n\nMany physicists have written about the philosophical implications of their work, for instance Laplace, who championed causal determinism, and Erwin Schrödinger, who wrote on quantum mechanics. The mathematical physicist Roger Penrose had been called a Platonist by Stephen Hawking, a view Penrose discusses in his book, \"The Road to Reality\". Hawking referred to himself as an \"unashamed reductionist\" and took issue with Penrose's views.\n\nThough physics deals with a wide variety of systems, certain theories are used by all physicists. Each of these theories were experimentally tested numerous times and found to be an adequate approximation of nature. For instance, the theory of classical mechanics accurately describes the motion of objects, provided they are much larger than atoms and moving at much less than the speed of light. These theories continue to be areas of active research today. Chaos theory, a remarkable aspect of classical mechanics was discovered in the 20th century, three centuries after the original formulation of classical mechanics by Isaac Newton (1642–1727).\n\nThese central theories are important tools for research into more specialised topics, and any physicist, regardless of their specialisation, is expected to be literate in them. These include classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, and special relativity.\n\nClassical physics includes the traditional branches and topics that were recognised and well-developed before the beginning of the 20th century—classical mechanics, acoustics, optics, thermodynamics, and electromagnetism. Classical mechanics is concerned with bodies acted on by forces and bodies in motion and may be divided into statics (study of the forces on a body or bodies not subject to an acceleration), kinematics (study of motion without regard to its causes), and dynamics (study of motion and the forces that affect it); mechanics may also be divided into solid mechanics and fluid mechanics (known together as continuum mechanics), the latter include such branches as hydrostatics, hydrodynamics, aerodynamics, and pneumatics. Acoustics is the study of how sound is produced, controlled, transmitted and received. Important modern branches of acoustics include ultrasonics, the study of sound waves of very high frequency beyond the range of human hearing; bioacoustics, the physics of animal calls and hearing, and electroacoustics, the manipulation of audible sound waves using electronics.\n\nOptics, the study of light, is concerned not only with visible light but also with infrared and ultraviolet radiation, which exhibit all of the phenomena of visible light except visibility, e.g., reflection, refraction, interference, diffraction, dispersion, and polarization of light. Heat is a form of energy, the internal energy possessed by the particles of which a substance is composed; thermodynamics deals with the relationships between heat and other forms of energy. Electricity and magnetism have been studied as a single branch of physics since the intimate connection between them was discovered in the early 19th century; an electric current gives rise to a magnetic field, and a changing magnetic field induces an electric current. Electrostatics deals with electric charges at rest, electrodynamics with moving charges, and magnetostatics with magnetic poles at rest.\n\nClassical physics is generally concerned with matter and energy on the normal scale of observation, while much of modern physics is concerned with the behavior of matter and energy under extreme conditions or on a very large or very small scale. For example, atomic and nuclear physics studies matter on the smallest scale at which chemical elements can be identified. The physics of elementary particles is on an even smaller scale since it is concerned with the most basic units of matter; this branch of physics is also known as high-energy physics because of the extremely high energies necessary to produce many types of particles in particle accelerators. On this scale, ordinary, commonsensical notions of space, time, matter, and energy are no longer valid.\n\nThe two chief theories of modern physics present a different picture of the concepts of space, time, and matter from that presented by classical physics. Classical mechanics approximates nature as continuous, while quantum theory is concerned with the discrete nature of many phenomena at the atomic and subatomic level and with the complementary aspects of particles and waves in the description of such phenomena. The theory of relativity is concerned with the description of phenomena that take place in a frame of reference that is in motion with respect to an observer; the special theory of relativity is concerned with motion in the absence of gravitational fields and the general theory of relativity with motion and its connection with gravitation. Both quantum theory and the theory of relativity find applications in all areas of modern physics.\n\nWhile physics aims to discover universal laws, its theories lie in explicit domains of applicability. Loosely speaking, the laws of classical physics accurately describe systems whose important length scales are greater than the atomic scale and whose motions are much slower than the speed of light. Outside of this domain, observations do not match predictions provided by classical mechanics. Albert Einstein contributed the framework of special relativity, which replaced notions of absolute time and space with spacetime and allowed an accurate description of systems whose components have speeds approaching the speed of light. Max Planck, Erwin Schrödinger, and others introduced quantum mechanics, a probabilistic notion of particles and interactions that allowed an accurate description of atomic and subatomic scales. Later, quantum field theory unified quantum mechanics and special relativity. General relativity allowed for a dynamical, curved spacetime, with which highly massive systems and the large-scale structure of the universe can be well-described. General relativity has not yet been unified with the other fundamental descriptions; several candidate theories of quantum gravity are being developed.\n\nMathematics provides a compact and exact language used to describe the order in nature. This was noted and advocated by Pythagoras, Plato, Galileo, and Newton.\n\nPhysics uses mathematics to organise and formulate experimental results. From those results, precise or estimated solutions are obtained, quantitative results from which new predictions can be made and experimentally confirmed or negated. The results from physics experiments are numerical data, with their units of measure and estimates of the errors in the measurements. Technologies based on mathematics, like computation have made computational physics an active area of research.\n\nOntology is a prerequisite for physics, but not for mathematics. It means physics is ultimately concerned with descriptions of the real world, while mathematics is concerned with abstract patterns, even beyond the real world. Thus physics statements are synthetic, while mathematical statements are analytic. Mathematics contains hypotheses, while physics contains theories. Mathematics statements have to be only logically true, while predictions of physics statements must match observed and experimental data.\n\nThe distinction is clear-cut, but not always obvious. For example, mathematical physics is the application of mathematics in physics. Its methods are mathematical, but its subject is physical. The problems in this field start with a \"mathematical model of a physical situation\" (system) and a \"mathematical description of a physical law\" that will be applied to that system. Every mathematical statement used for solving has a hard-to-find physical meaning. The final mathematical solution has an easier-to-find meaning, because it is what the solver is looking for.\n\nPure physics is a branch of fundamental science (also called \"basic\" science) . Physics is also called \"the fundamental science\" because all branches of natural science like chemistry, astronomy, geology, and biology are constrained by laws of physics. Similarly, chemistry is often called the central science because of its role in linking the physical sciences. For example, chemistry studies properties, structures, and reactions of matter (chemistry's focus on the molecular and atomic scale distinguishes it from physics). Structures are formed because particles exert electrical forces on each other, properties include physical characteristics of given substances, and reactions are bound by laws of physics, like conservation of energy, mass, and charge. Physics is applied in industries like engineering and medicine.\n\nApplied physics is a general term for physics research which is intended for a particular use. An applied physics curriculum usually contains a few classes in an applied discipline, like geology or electrical engineering. It usually differs from engineering in that an applied physicist may not be designing something in particular, but rather is using physics or conducting physics research with the aim of developing new technologies or solving a problem.\n\nThe approach is similar to that of applied mathematics. Applied physicists use physics in scientific research. For instance, people working on accelerator physics might seek to build better particle detectors for research in theoretical physics.\n\nPhysics is used heavily in engineering. For example, statics, a subfield of mechanics, is used in the building of bridges and other static structures. The understanding and use of acoustics results in sound control and better concert halls; similarly, the use of optics creates better optical devices. An understanding of physics makes for more realistic flight simulators, video games, and movies, and is often critical in forensic investigations.\n\nWith the standard consensus that the laws of physics are universal and do not change with time, physics can be used to study things that would ordinarily be mired in uncertainty. For example, in the study of the origin of the earth, one can reasonably model earth's mass, temperature, and rate of rotation, as a function of time allowing one to extrapolate forward or backward in time and so predict future or prior events. It also allows for simulations in engineering that drastically speed up the development of a new technology.\n\nBut there is also considerable interdisciplinarity, so many other important fields are influenced by physics (e.g., the fields of econophysics and sociophysics).\n\nPhysicists use the scientific method to test the validity of a physical theory. By using a methodical approach to compare the implications of a theory with the conclusions drawn from its related experiments and observations, physicists are better able to test the validity of a theory in a logical, unbiased, and repeatable way. To that end, experiments are performed and observations are made in order to determine the validity or invalidity of the theory.\n\nA scientific law is a concise verbal or mathematical statement of a relation that expresses a fundamental principle of some theory, such as Newton's law of universal gravitation.\n\nTheorists seek to develop mathematical models that both agree with existing experiments and successfully predict future experimental results, while experimentalists devise and perform experiments to test theoretical predictions and explore new phenomena. Although theory and experiment are developed separately, they strongly affect and depend upon each other. Progress in physics frequently comes about when experimental results defy explanation by existing theories, prompting intense focus on applicable modelling, and when new theories generate experimentally testable predictions, which inspire developing new experiments (and often related equipment, possibly roping in some applied physicists to help build it).\n\nPhysicists who work at the interplay of theory and experiment are called phenomenologists, who study complex phenomena observed in experiment and work to relate them to a fundamental theory.\n\nTheoretical physics has historically taken inspiration from philosophy; electromagnetism was unified this way. Beyond the known universe, the field of theoretical physics also deals with hypothetical issues, such as parallel universes, a multiverse, and higher dimensions. Theorists invoke these ideas in hopes of solving particular problems with existing theories. They then explore the consequences of these ideas and work toward making testable predictions.\n\nExperimental physics expands, and is expanded by, engineering and technology. Experimental physicists involved in basic research design and perform experiments with equipment such as particle accelerators and lasers, whereas those involved in applied research often work in industry developing technologies such as magnetic resonance imaging (MRI) and transistors. Feynman has noted that experimentalists may seek areas that have not been explored well by theorists.\n\nPhysics covers a wide range of phenomena, from elementary particles (such as quarks, neutrinos, and electrons) to the largest superclusters of galaxies. Included in these phenomena are the most basic objects composing all other things. Therefore, physics is sometimes called the \"fundamental science\". Physics aims to describe the various phenomena that occur in nature in terms of simpler phenomena. Thus, physics aims to both connect the things observable to humans to root causes, and then connect these causes together.\n\nFor example, the ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of \"unifying\" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section \"Current research\" below for more information).\n\nContemporary research in physics can be broadly divided into nuclear and particle physics; condensed matter physics; atomic, molecular, and optical physics; astrophysics; and applied physics. Some physics departments also support physics education research and physics outreach.\n\nSince the 20th century, the individual fields of physics have become increasingly specialised, and today most physicists work in a single field for their entire careers. \"Universalists\" such as Albert Einstein (1879–1955) and Lev Landau (1908–1968), who worked in multiple fields of physics, are now very rare.\n\nThe major fields of physics, along with their subfields and the theories and concepts they employ, are shown in the following table.\n\nParticle physics is the study of the elementary constituents of matter and energy and the interactions between them. In addition, particle physicists design and develop the high-energy accelerators, detectors, and computer programs necessary for this research. The field is also called \"high-energy physics\" because many elementary particles do not occur naturally but are created only during high-energy collisions of other particles.\n\nCurrently, the interactions of elementary particles and fields are described by the Standard Model. The model accounts for the 12 known particles of matter (quarks and leptons) that interact via the strong, weak, and electromagnetic fundamental forces. Dynamics are described in terms of matter particles exchanging gauge bosons (gluons, W and Z bosons, and photons, respectively). The Standard Model also predicts a particle known as the Higgs boson. In July 2012 CERN, the European laboratory for particle physics, announced the detection of a particle consistent with the Higgs boson, an integral part of a Higgs mechanism.\n\nNuclear physics is the field of physics that studies the constituents and interactions of atomic nuclei. The most commonly known applications of nuclear physics are nuclear power generation and nuclear weapons technology, but the research has provided application in many fields, including those in nuclear medicine and magnetic resonance imaging, ion implantation in materials engineering, and radiocarbon dating in geology and archaeology.\n\nAtomic, molecular, and optical physics (AMO) is the study of matter–matter and light–matter interactions on the scale of single atoms and molecules. The three areas are grouped together because of their interrelationships, the similarity of methods used, and the commonality of their relevant energy scales. All three areas include both classical, semi-classical and quantum treatments; they can treat their subject from a microscopic view (in contrast to a macroscopic view).\n\nAtomic physics studies the electron shells of atoms. Current research focuses on activities in quantum control, cooling and trapping of atoms and ions, low-temperature collision dynamics and the effects of electron correlation on structure and dynamics. Atomic physics is influenced by the nucleus (see hyperfine splitting), but intra-nuclear phenomena such as fission and fusion are considered part of nuclear physics.\n\nMolecular physics focuses on multi-atomic structures and their internal and external interactions with matter and light. Optical physics is distinct from optics in that it tends to focus not on the control of classical light fields by macroscopic objects but on the fundamental properties of optical fields and their interactions with matter in the microscopic realm.\n\nCondensed matter physics is the field of physics that deals with the macroscopic physical properties of matter. In particular, it is concerned with the \"condensed\" phases that appear whenever the number of particles in a system is extremely large and the interactions between them are strong.\n\nThe most familiar examples of condensed phases are solids and liquids, which arise from the bonding by way of the electromagnetic force between atoms. More exotic condensed phases include the superfluid and the Bose–Einstein condensate found in certain atomic systems at very low temperature, the superconducting phase exhibited by conduction electrons in certain materials, and the ferromagnetic and antiferromagnetic phases of spins on atomic lattices.\n\nCondensed matter physics is the largest field of contemporary physics. Historically, condensed matter physics grew out of solid-state physics, which is now considered one of its main subfields. The term \"condensed matter physics\" was apparently coined by Philip Anderson when he renamed his research group—previously \"solid-state theory\"—in 1967. In 1978, the Division of Solid State Physics of the American Physical Society was renamed as the Division of Condensed Matter Physics. Condensed matter physics has a large overlap with chemistry, materials science, nanotechnology and engineering.\n\nAstrophysics and astronomy are the application of the theories and methods of physics to the study of stellar structure, stellar evolution, the origin of the Solar System, and related problems of cosmology. Because astrophysics is a broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics.\n\nThe discovery by Karl Jansky in 1931 that radio signals were emitted by celestial bodies initiated the science of radio astronomy. Most recently, the frontiers of astronomy have been expanded by space exploration. Perturbations and interference from the earth's atmosphere make space-based observations necessary for infrared, ultraviolet, gamma-ray, and X-ray astronomy.\n\nPhysical cosmology is the study of the formation and evolution of the universe on its largest scales. Albert Einstein's theory of relativity plays a central role in all modern cosmological theories. In the early 20th century, Hubble's discovery that the universe is expanding, as shown by the Hubble diagram, prompted rival explanations known as the steady state universe and the Big Bang.\n\nThe Big Bang was confirmed by the success of Big Bang nucleosynthesis and the discovery of the cosmic microwave background in 1964. The Big Bang model rests on two theoretical pillars: Albert Einstein's general relativity and the cosmological principle. Cosmologists have recently established the ΛCDM model of the evolution of the universe, which includes cosmic inflation, dark energy, and dark matter.\n\nNumerous possibilities and discoveries are anticipated to emerge from new data from the Fermi Gamma-ray Space Telescope over the upcoming decade and vastly revise or clarify existing models of the universe. In particular, the potential for a tremendous discovery surrounding dark matter is possible over the next several years. Fermi will search for evidence that dark matter is composed of weakly interacting massive particles, complementing similar experiments with the Large Hadron Collider and other underground detectors.\n\nIBEX is already yielding new astrophysical discoveries: \"No one knows what is creating the ENA (energetic neutral atoms) ribbon\" along the termination shock of the solar wind, \"but everyone agrees that it means the textbook picture of the heliosphere—in which the Solar System's enveloping pocket filled with the solar wind's charged particles is plowing through the onrushing 'galactic wind' of the interstellar medium in the shape of a comet—is wrong.\"\n\nResearch in physics is continually progressing on a large number of fronts.\n\nIn condensed matter physics, an important unsolved theoretical problem is that of high-temperature superconductivity. Many condensed matter experiments are aiming to fabricate workable spintronics and quantum computers.\n\nIn particle physics, the first pieces of experimental evidence for physics beyond the Standard Model have begun to appear. Foremost among these are indications that neutrinos have non-zero mass. These experimental results appear to have solved the long-standing solar neutrino problem, and the physics of massive neutrinos remains an area of active theoretical and experimental research. The Large Hadron Collider has already found the Higgs boson, but future research aims to prove or disprove the supersymmetry, which extends the Standard Model of particle physics. Research on the nature of the major mysteries of dark matter and dark energy is also currently ongoing.\n\nTheoretical attempts to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved. The current leading candidates are M-theory, superstring theory and loop quantum gravity.\n\nMany astronomical and cosmological phenomena have yet to be satisfactorily explained, including the origin of ultra-high-energy cosmic rays, the baryon asymmetry, the accelerating expansion of the universe and the anomalous rotation rates of galaxies.\n\nAlthough much progress has been made in high-energy, quantum, and astronomical physics, many everyday phenomena involving complexity, chaos, or turbulence are still poorly understood. Complex problems that seem like they could be solved by a clever application of dynamics and mechanics remain unsolved; examples include the formation of sandpiles, nodes in trickling water, the shape of water droplets, mechanisms of surface tension catastrophes, and self-sorting in shaken heterogeneous collections.\n\nThese complex phenomena have received growing attention since the 1970s for several reasons, including the availability of modern mathematical methods and computers, which enabled complex systems to be modeled in new ways. Complex physics has become part of increasingly interdisciplinary research, as exemplified by the study of turbulence in aerodynamics and the observation of pattern formation in biological systems. In the 1932 \"Annual Review of Fluid Mechanics\", Horace Lamb said:\n\n", "related": "\n- Glossary of physics\n- Index of physics articles\n- Lists of physics equations\n- List of important publications in physics\n- List of physicists\n- Relationship between mathematics and physics\n- Timeline of developments in theoretical physics\n- Timeline of fundamental physics discoveries\n- Earth science\n- Neurophysics\n- Psychophysics\n- Science tourism\n\n- PhysicsCentral – Web portal run by the American Physical Society\n- Physics.org – Web portal run by the Institute of Physics\n- Usenet Physics FAQ – FAQ compiled by sci.physics and other physics newsgroups\n- Website of the Nobel Prize in physics – Award for outstanding contributions to the subject\n- World of Physics – Online encyclopedic dictionary of physics\n- \"Nature Physics\" – Academic journal\n- Physics – Online magazine by the American Physical Society\n- – Directory of physics related media\n- The Vega Science Trust – Science videos, including physics\n- HyperPhysics website – Physics and astronomy mind-map from Georgia State University\n- PHYSICS at MIT OCW – Online course material from Massachusetts Institute of Technology\n"}
{"id": "28481", "url": "https://en.wikipedia.org/wiki?curid=28481", "title": "Statistical mechanics", "text": "Statistical mechanics\n\nStatistical mechanics is one of the pillars of modern physics. It is necessary for the fundamental study of any physical system that has many degrees of freedom. The approach is based on statistical methods, probability theory and the microscopic physical laws.\n\nIt can be used to explain the thermodynamic behaviour of large systems. This branch of statistical mechanics, which treats and extends classical thermodynamics, is known as statistical thermodynamics or equilibrium statistical mechanics.\n\nStatistical mechanics describes how macroscopic observations (such as temperature and pressure) are related to microscopic parameters that fluctuate around an average. It connects thermodynamic quantities (such as heat capacity) to microscopic behavior, whereas, in classical thermodynamics, the only available option would be to measure and tabulate such quantities for various materials.\n\nStatistical mechanics can also be used to study systems that are out of equilibrium. An important subbranch known as non-equilibrium statistical mechanics (sometimes called statistical dynamics) deals with the issue of microscopically modelling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions or flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.\n\nIn physics, two types of mechanics are usually examined: classical mechanics and quantum mechanics. For both types of mechanics, the standard mathematical approach is to consider two concepts:\n1. The complete state of the mechanical system at a given time, mathematically encoded as a phase point (classical mechanics) or a pure quantum state vector (quantum mechanics).\n2. An equation of motion which carries the state forward in time: Hamilton's equations (classical mechanics) or the time-dependent Schrödinger equation (quantum mechanics)\nUsing these two concepts, the state at any other time, past or future, can in principle be calculated.\nThere is however a disconnection between these laws and everyday life experiences, as we do not find it necessary (nor even theoretically possible) to know exactly at a microscopic level the simultaneous positions and velocities of each molecule while carrying out processes at the human scale (for example, when performing a chemical reaction). Statistical mechanics fills this disconnection between the laws of mechanics and the practical experience of incomplete knowledge, by adding some uncertainty about which state the system is in.\n\nWhereas ordinary mechanics only considers the behaviour of a single state, statistical mechanics introduces the statistical ensemble, which is a large collection of virtual, independent copies of the system in various states. The statistical ensemble is a probability distribution over all possible states of the system. In classical statistical mechanics, the ensemble is a probability distribution over phase points (as opposed to a single phase point in ordinary mechanics), usually represented as a distribution in a phase space with canonical coordinates. In quantum statistical mechanics, the ensemble is a probability distribution over pure states, and can be compactly summarized as a density matrix.\n\nAs is usual for probabilities, the ensemble can be interpreted in different ways:\n- an ensemble can be taken to represent the various possible states that a \"single system\" could be in (epistemic probability, a form of knowledge), or\n- the members of the ensemble can be understood as the states of the systems in experiments repeated on independent systems which have been prepared in a similar but imperfectly controlled manner (empirical probability), in the limit of an infinite number of trials.\nThese two meanings are equivalent for many purposes, and will be used interchangeably in this article.\n\nHowever the probability is interpreted, each state in the ensemble evolves over time according to the equation of motion. Thus, the ensemble itself (the probability distribution over states) also evolves, as the virtual systems in the ensemble continually leave one state and enter another. The ensemble evolution is given by the Liouville equation (classical mechanics) or the von Neumann equation (quantum mechanics). These equations are simply derived by the application of the mechanical equation of motion separately to each virtual system contained in the ensemble, with the probability of the virtual system being conserved over time as it evolves from state to state.\n\nOne special class of ensemble is those ensembles that do not evolve over time. These ensembles are known as \"equilibrium ensembles\" and their condition is known as \"statistical equilibrium\". Statistical equilibrium occurs if, for each state in the ensemble, the ensemble also contains all of its future and past states with probabilities equal to the probability of being in that state. The study of equilibrium ensembles of isolated systems is the focus of statistical thermodynamics. Non-equilibrium statistical mechanics addresses the more general case of ensembles that change over time, and/or ensembles of non-isolated systems.\n\nThe primary goal of statistical thermodynamics (also known as equilibrium statistical mechanics) is to derive the classical thermodynamics of materials in terms of the properties of their constituent particles and the interactions between them. In other words, statistical thermodynamics provides a connection between the macroscopic properties of materials in thermodynamic equilibrium, and the microscopic behaviours and motions occurring inside the material.\n\nWhereas statistical mechanics proper involves dynamics, here the attention is focussed on \"statistical equilibrium\" (steady state). Statistical equilibrium does not mean that the particles have stopped moving (mechanical equilibrium), rather, only that the ensemble is not evolving.\n\nA sufficient (but not necessary) condition for statistical equilibrium with an isolated system is that the probability distribution is a function only of conserved properties (total energy, total particle numbers, etc.).\nThere are many different equilibrium ensembles that can be considered, and only some of them correspond to thermodynamics. Additional postulates are necessary to motivate why the ensemble for a given system should have one form or another.\n\nA common approach found in many textbooks is to take the \"equal a priori probability postulate\". This postulate states that\nThe equal a priori probability postulate therefore provides a motivation for the microcanonical ensemble described below. There are various arguments in favour of the equal a priori probability postulate:\n- Ergodic hypothesis: An ergodic system is one that evolves over time to explore \"all accessible\" states: all those with the same energy and composition. In an ergodic system, the microcanonical ensemble is the only possible equilibrium ensemble with fixed energy. This approach has limited applicability, since most systems are not ergodic.\n- Principle of indifference: In the absence of any further information, we can only assign equal probabilities to each compatible situation.\n- Maximum information entropy: A more elaborate version of the principle of indifference states that the correct ensemble is the ensemble that is compatible with the known information and that has the largest Gibbs entropy (information entropy).\nOther fundamental postulates for statistical mechanics have also been proposed.\n\nThere are three equilibrium ensembles with a simple form that can be defined for any isolated system bounded inside a finite volume. These are the most often discussed ensembles in statistical thermodynamics. In the macroscopic limit (defined below) they all correspond to classical thermodynamics.\n- Microcanonical ensemble\n- Canonical ensemble\n- Grand canonical ensemble\n\nFor systems containing many particles (the thermodynamic limit), all three of the ensembles listed above tend to give identical behaviour. It is then simply a matter of mathematical convenience which ensemble is used. The Gibbs theorem about equivalence of ensembles was developed into the theory of concentration of measure phenomenon, which has applications in many areas of science, from functional analysis to methods of artificial intelligence and big data technology.\n\nImportant cases where the thermodynamic ensembles \"do not\" give identical results include:\n- Microscopic systems.\n- Large systems at a phase transition.\n- Large systems with long-range interactions.\nIn these cases the correct thermodynamic ensemble must be chosen as there are observable differences between these ensembles not just in the size of fluctuations, but also in average quantities such as the distribution of particles. The correct ensemble is that which corresponds to the way the system has been prepared and characterized—in other words, the ensemble that reflects the knowledge about that system.\n\nOnce the characteristic state function for an ensemble has been calculated for a given system, that system is 'solved' (macroscopic observables can be extracted from the characteristic state function). Calculating the characteristic state function of a thermodynamic ensemble is not necessarily a simple task, however, since it involves considering every possible state of the system. While some hypothetical systems have been exactly solved, the most general (and realistic) case is too complex for an exact solution. Various approaches exist to approximate the true ensemble and allow calculation of average quantities.\n\nThere are some cases which allow exact solutions.\n\n- For very small microscopic systems, the ensembles can be directly computed by simply enumerating over all possible states of the system (using exact diagonalization in quantum mechanics, or integral over all phase space in classical mechanics).\n- Some large systems consist of many separable microscopic systems, and each of the subsystems can be analysed independently. Notably, idealized gases of non-interacting particles have this property, allowing exact derivations of Maxwell–Boltzmann statistics, Fermi–Dirac statistics, and Bose–Einstein statistics.\n- A few large systems with interaction have been solved. By the use of subtle mathematical techniques, exact solutions have been found for a few toy models. Some examples include the Bethe ansatz, square-lattice Ising model in zero field, hard hexagon model.\n\nOne approximate approach that is particularly well suited to computers is the Monte Carlo method, which examines just a few of the possible states of the system, with the states chosen randomly (with a fair weight). As long as these states form a representative sample of the whole set of states of the system, the approximate characteristic function is obtained. As more and more random samples are included, the errors are reduced to an arbitrarily low level.\n\n- The Metropolis–Hastings algorithm is a classic Monte Carlo method which was initially used to sample the canonical ensemble.\n- Path integral Monte Carlo, also used to sample the canonical ensemble.\n\n- For rarefied non-ideal gases, approaches such as the cluster expansion use perturbation theory to include the effect of weak interactions, leading to a virial expansion.\n- For dense fluids, another approximate approach is based on reduced distribution functions, in particular the radial distribution function.\n- Molecular dynamics computer simulations can be used to calculate microcanonical ensemble averages, in ergodic systems. With the inclusion of a connection to a stochastic heat bath, they can also model canonical and grand canonical conditions.\n- Mixed methods involving non-equilibrium statistical mechanical results (see below) may be useful.\n\nThere are many physical phenomena of interest that involve quasi-thermodynamic processes out of equilibrium, for example:\n- heat transport by the internal motions in a material, driven by a temperature imbalance,\n- electric currents carried by the motion of charges in a conductor, driven by a voltage imbalance,\n- spontaneous chemical reactions driven by a decrease in free energy,\n- friction, dissipation, quantum decoherence,\n- systems being pumped by external forces (optical pumping, etc.),\n- and irreversible processes in general.\nAll of these processes occur over time with characteristic rates, and these rates are of importance for engineering. The field of non-equilibrium statistical mechanics is concerned with understanding these non-equilibrium processes at the microscopic level. (Statistical thermodynamics can only be used to calculate the final result, after the external imbalances have been removed and the ensemble has settled back down to equilibrium.)\n\nIn principle, non-equilibrium statistical mechanics could be mathematically exact: ensembles for an isolated system evolve over time according to deterministic equations such as Liouville's equation or its quantum equivalent, the von Neumann equation. These equations are the result of applying the mechanical equations of motion independently to each state in the ensemble. Unfortunately, these ensemble evolution equations inherit much of the complexity of the underlying mechanical motion, and so exact solutions are very difficult to obtain. Moreover, the ensemble evolution equations are fully reversible and do not destroy information (the ensemble's Gibbs entropy is preserved). In order to make headway in modelling irreversible processes, it is necessary to consider additional factors besides probability and reversible mechanics.\n\nNon-equilibrium mechanics is therefore an active area of theoretical research as the range of validity of these additional assumptions continues to be explored. A few approaches are described in the following subsections.\n\nOne approach to non-equilibrium statistical mechanics is to incorporate stochastic (random) behaviour into the system. Stochastic behaviour destroys information contained in the ensemble. While this is technically inaccurate (aside from hypothetical situations involving black holes, a system cannot in itself cause loss of information), the randomness is added to reflect that information of interest becomes converted over time into subtle correlations within the system, or to correlations between the system and environment. These correlations appear as chaotic or pseudorandom influences on the variables of interest. By replacing these correlations with randomness proper, the calculations can be made much easier.\n\nAnother important class of non-equilibrium statistical mechanical models deals with systems that are only very slightly perturbed from equilibrium. With very small perturbations, the response can be analysed in linear response theory. A remarkable result, as formalized by the fluctuation-dissipation theorem, is that the response of a system when near equilibrium is precisely related to the fluctuations that occur when the system is in total equilibrium. Essentially, a system that is slightly away from equilibrium—whether put there by external forces or by fluctuations—relaxes towards equilibrium in the same way, since the system cannot tell the difference or \"know\" how it came to be away from equilibrium.\n\nThis provides an indirect avenue for obtaining numbers such as ohmic conductivity and thermal conductivity by extracting results from equilibrium statistical mechanics. Since equilibrium statistical mechanics is mathematically well defined and (in some cases) more amenable for calculations, the fluctuation-dissipation connection can be a convenient shortcut for calculations in near-equilibrium statistical mechanics.\n\nA few of the theoretical tools used to make this connection include:\n- Fluctuation–dissipation theorem\n- Onsager reciprocal relations\n- Green–Kubo relations\n- Landauer–Büttiker formalism\n- Mori–Zwanzig formalism\n\nAn advanced approach uses a combination of stochastic methods and linear response theory. As an example, one approach to compute quantum coherence effects (weak localization, conductance fluctuations) in the conductance of an electronic system is the use of the Green-Kubo relations, with the inclusion of stochastic dephasing by interactions between various electrons by use of the Keldysh method.\n\nThe ensemble formalism also can be used to analyze general mechanical systems with uncertainty in knowledge about the state of a system. Ensembles are also used in:\n- propagation of uncertainty over time,\n- regression analysis of gravitational orbits,\n- ensemble forecasting of weather,\n- dynamics of neural networks,\n- bounded-rational potential games in game theory and economics.\n\nIn 1738, Swiss physicist and mathematician Daniel Bernoulli published \"Hydrodynamica\" which laid the basis for the kinetic theory of gases. In this work, Bernoulli posited the argument, still used to this day, that gases consist of great numbers of molecules moving in all directions, that their impact on a surface causes the gas pressure that we feel, and that what we experience as heat is simply the kinetic energy of their motion.\n\nIn 1859, after reading a paper on the diffusion of molecules by Rudolf Clausius, Scottish physicist James Clerk Maxwell formulated the Maxwell distribution of molecular velocities, which gave the proportion of molecules having a certain velocity in a specific range. This was the first-ever statistical law in physics. Maxwell also gave the first mechanical argument that molecular collisions entail an equalization of temperatures and hence a tendency towards equilibrium. Five years later, in 1864, Ludwig Boltzmann, a young student in Vienna, came across Maxwell's paper and spent much of his life developing the subject further.\n\nStatistical mechanics proper was initiated in the 1870s with the work of Boltzmann, much of which was collectively published in his 1896 \"Lectures on Gas Theory\". Boltzmann's original papers on the statistical interpretation of thermodynamics, the H-theorem, transport theory, thermal equilibrium, the equation of state of gases, and similar subjects, occupy about 2,000 pages in the proceedings of the Vienna Academy and other societies. Boltzmann introduced the concept of an equilibrium statistical ensemble and also investigated for the first time non-equilibrium statistical mechanics, with his \"H\"-theorem.\n\nThe term \"statistical mechanics\" was coined by the American mathematical physicist J. Willard Gibbs in 1884. \"Probabilistic mechanics\" might today seem a more appropriate term, but \"statistical mechanics\" is firmly entrenched. Shortly before his death, Gibbs published in 1902 \"Elementary Principles in Statistical Mechanics\", a book which formalized statistical mechanics as a fully general approach to address all mechanical systems—macroscopic or microscopic, gaseous or non-gaseous. Gibbs' methods were initially derived in the framework classical mechanics, however they were of such generality that they were found to adapt easily to the later quantum mechanics, and still form the foundation of statistical mechanics to this day.\n\n", "related": "\n- Thermodynamics: non-equilibrium, chemical\n- Mechanics: classical, quantum\n- Probability, statistical ensemble\n- Numerical methods: Monte Carlo method, molecular dynamics\n- Statistical physics\n- Quantum statistical mechanics\n- List of notable textbooks in statistical mechanics\n- List of important publications in statistical mechanics\n\n- Philosophy of Statistical Mechanics article by Lawrence Sklar for the Stanford Encyclopedia of Philosophy.\n- Sklogwiki - Thermodynamics, statistical mechanics, and the computer simulation of materials. SklogWiki is particularly orientated towards liquids and soft condensed matter.\n- Statistical Thermodynamics - Historical Timeline\n- Thermodynamics and Statistical Mechanics by Richard Fitzpatrick\n- Lecture Notes in Statistical Mechanics and Mesoscopics by Doron Cohen\n- taught by Leonard Susskind.\n- Vu-Quoc, L., Configuration integral (statistical mechanics), 2008. this wiki site is down; see this article in the web archive on 2012 April 28.\n"}
{"id": "48249441", "url": "https://en.wikipedia.org/wiki?curid=48249441", "title": "Phase stretch transform", "text": "Phase stretch transform\n\nPhase stretch transform (PST) is a computational approach to signal and image processing. One of its utilities is for feature detection and classification. PST is related to time stretch dispersive Fourier transform. It transforms the image by emulating propagation through a diffractive medium with engineered 3D dispersive property (refractive index). The operation relies on symmetry of the dispersion profile and can be understood in terms of dispersive eigenfunctions or stretch modes. PST performs similar functionality as phase-contrast microscopy, but on digital images. PST can be applied to digital images and temporal (time series) data.\n\nHere the principle is described in the context of feature enhancement in digital images. The image is first filtered with a spatial kernel followed by application of a nonlinear frequency-dependent phase. The output of the transform is the phase in the spatial domain. The main step is the 2-D phase function which is typically applied in the frequency domain. The amount of phase applied to the image is frequency dependent, with higher amount of phase applied to higher frequency features of the image. Since sharp transitions, such as edges and corners, contain higher frequencies, PST emphasizes the edge information. Features can be further enhanced by applying thresholding and morphological operations. PST is a pure phase operation whereas conventional edge detection algorithms operate on amplitude. \n\nPhotonic time stretch technique can be understood by considering the propagation of an optical pulse through a dispersive fiber. By disregarding the loss and non-linearity in fiber, the non-linear Schrödinger equation governing the optical pulse propagation in fiber upon integration reduces to:\n\nformula_1 (1)\n\nwhere formula_2 =GVD parameter, z is propagation distance, formula_3 is the reshaped output pulse at distance z and time t. The response of this dispersive element in the time-stretch system can be approximated as a phase propagator as presented in \nformula_4 (2)\n<br>\nTherefore, Eq. 1 can be written as following for a pulse that propagates through the time-stretch system and is reshaped into a temporal signal with a complex envelope given by \n<br>\nformula_5 (3)\n<br>\nThe time stretch operation is formulated as generalized phase and amplitude operations,\n<br>\nformula_6 (4)\n\nwhere formula_7 is the phase filter and formula_8is the amplitude filter. Next the operator is converted to discrete domain, \n<br>\nformula_9 (5)\n<br>\nwhere formula_10 is the discrete frequency, formula_11 is the phase filter, formula_12 is the amplitude filter and FFT is Fast Fourier Transform.\n\nThe Stretch operator formula_13 for a digital image is then \n<br>\nformula_14 (6)\n\nIn the above equations, formula_15 is the input image, formula_16 and formula_17 are the spatial variables, formula_18 is the two dimensional Fast Fourier Transform, and formula_19 and formula_20 are spatial frequency variables. The function formula_21 is the warped phase kernel and the function formula_22 is a localization kernel implemented in frequency domain. PST operator is defined as the phase of the Warped Stretch Transform output as follows\n\nformula_23 (7)\n\nwhere formula_24is the angle operator.\n\nPST has been used for edge detection in biological and biomedical images as well as synthetic-aperture radar (SAR) image processing. PST has also been applied to improve the point spread function for single molecule imaging in order to achieve super-resolution. The transform exhibits intrinsic superior properties compared to conventional edge detectors for feature detection in low contrast visually impaired images.\n\nThe PST function can also be performed on 1-D temporal waveforms in the analog domain to reveal transitions and anomalies in real time.\n\nOn February 9, 2016, a UCLA Engineering research group has made public the computer code for PST algorithm that helps computers process images at high speeds and \"see\" them in ways that human eyes cannot. The researchers say the code could eventually be used in face, fingerprint, and iris recognition systems for high-tech security, as well as in self-driving cars' navigation systems or for inspecting industrial products.\nMatlab and Python implementation for PST is available for free download from our Github Repository. The Matlab implementation for PST can also be downloaded from Matlab Files Exchange. However, it is provided for research purposes only, and a license must be obtained for any commercial applications. The software is protected under a US patent.\n\n", "related": "\n- Edge detection\n- Feature detection (computer vision)\n- Time stretch analog-to-digital converter\n- Time stretch dispersive Fourier transform\n"}
{"id": "42906061", "url": "https://en.wikipedia.org/wiki?curid=42906061", "title": "Total position spread", "text": "Total position spread\n\nIn physics, the total position-spread (TPS) tensor is a quantity originally introduced in the modern theory of electrical conductivity. In the case of molecular systems, this tensor measures the fluctuation of the electrons around their mean positions, which corresponds to the delocalization of the electronic charge within a molecular system. The total position-spread can discriminate between metals and insulators taking information from the ground state wave function. This quantity can be very useful as an indicator to characterize Intervalence charge transfer processes, the bond nature of molecules (covalent, ionic or weakly bonded), and Metal–insulator transition.\n\nThe Localization Tensor (LT) is a \"per electron\" quantity proposed in the context of the theory of Kohn to characterize electrical conductivity properties. In 1964, Kohn realized that electrical conductivity is more related to the proper delocalization of the wave function than a simple band gap. In fact, he proposed that a qualitative difference between insulators and conductors also manifests as a different organization of the electrons in their ground state where one has that: the wave function is strongly localized in insulators and very delocalized in conductors.\n\nThe interesting outcome of this theory is: \"i)\" it relates the classical idea of localized electrons as a cause of insulating state; \"ii)\" the needed information can be recovered from the ground state wave function because in the insulated regime the wave function breaks down as a sum of disconnected terms.\n\nIt is until 1999 that Resta and coworkers found a way to define the Kohn delocalization by proposing the already mentioned Localization Tensor. The LT is defined as a second order moment cumulant of the position operator divided by the number of electrons in the system. The key property of the LT is that: it diverges for metals while it takes finite values for insulators in the Thermodynamic limit.\n\nRecently, the global quantity (the LT not divided by the number of electrons) has been introduced to study molecules and named Total Position-Spread tensor.\n\nThe total position spread Λ is defined as the second moment cumulant of the total electron position operator, and its units are in length square (e.g. bohr²). In order to compute this quantity, one has to take into account the position operator and its tensorial square. For a system of \"n\" electrons, the position operator and its Cartesian components are defined as:\n\nWhere the \"i\" index runs over the number of electrons. Each component of the position operator is a one-electron operator, they can be represented in second quantization as follows:\n\nwhere \"i\",\"j\" run over orbitals. The expectation values of the position components are the first moments of the electrons' position.\n\nNow we consider the tensorial square (second moment). In this sense, there are two types of them:\n\n- in quantum chemistry programs like MOLPRO or DALTON the second moment operator is a tensor defined as the sum of the tensor squares of the positions of a single electron. Then, this is a one-electron operator \"s\" defined by its Cartesian components:\n\n- there is also the square of the total position operator formula_6. This is a two-electron operator \"S\", and also defined by its Cartesian components:\n\nThe second moment of the position becomes then the sum of the one- and two-electron operators already defined:\n\nGiven a \"n\"-electron wave function formula_10, one wants to compute the \"second moment cumulant\" of it. A cumulant is a linear combination of moments so we have:\n\nThe position operator can be partitioned according to spin components.\n\nFrom the one-particle operator it is possible to define the total spin-partitioned position operator as:\n\nTherefore, the total position operator formula_14 can be expressed by the sum of the two spin parts formula_15 and formula_16:\n\nand the square of the total position operator decomposes as: \nThus, there are four joint second moment cumulant of the spin-partitioned position operator: \n\nThe Hubbard model is a very simple and approximate model employed in Condensed matter physics to describe the transition of materials from metals to insulators. It takes into account only two parameters: \"i)\" the kinetic energy or hopping integral denoted by \"-t\"; and \"ii)\" the on-site repulsion between electrons represented by \"U\" (see the ).\n\nIn Figure 1, there are two limit cases to consider: larger values of \"-t/U\" representing a strong charge fluctuation (electrons free to move) whereas for small values of \"-t/U\" the electrons are completely localized. The spin-summed total position-spread is very sensitive to these changes, because it increases faster than linearly when electrons start to present mobility (0.0 to 0.5 range of \"-t/U\").\n\nThe total position-spread is a powerful tool to monitor the wave function. In Figure 3 is shown the longitudinal spin-summed total position-spread (Λ) computed at full configuration interaction level for the H diatomic molecule. The Λ in the high repulsive region shows a value that is lower than in the asymptotic limit. This is a consequence of nuclei being near to each other's causing and enhancement of the effective nuclear charge that makes electrons to be more localized. When stretching the bond, the total position-spread starts growing until it reaches a maximum (strong delocalization of the wave function) before the bond is broken. Once the bond is broken, the wave function becomes a sum of disconnected localized regions and the tensor decreases until it reaches twice the value of the atomic limit (1 bohr² for each hydrogen atom).\n\nWhen the total position-spread tensor is partitioned according to spin (spin-partitioned total position-spread), it becomes a powerful tool to describe spin delocalization in the insulating regime. In Figure 4 is shown the longitudinal spin-partitioned total position-spread (Λ) computed at full configuration interaction level for the H diatomic molecule. The horizontal line at 0 bohr divides the same spin (positive values) and different spin (negative values) contributions of the spin partitioned total position-spread. Unlike the spin-summed total position-spread that saturates to the atomic value for R>5, the spin-partitioned total position-spread diverges as R indicating that there is a strong spin delocalization. The spin-partitioned total position-spread can also be seen as a measure of how strong the electron correlation is.\n\nThe total position-spread is a cumulant and thus it possesses the following properties:\n\n1. Cumulants can be explicitly represented only by moments of lower or equal order.\n2. Cumulants are a linear combination of the products of these moments of lower or equal order.\n3. Cumulants are additive. This is a very important property when studying molecular systems because it means that the total position-spread tensor shows size consistency.\n4. A diagonal element of the cumulant tensor is the variance (see also this article), and it is always a positive value.\n5. Cumulants also are invariant under translation of the origin of when they are of order ≥ 2. The total position-spread tensor being a second-order cumulant, is invariant under translation of the origin.\n6. The total position-spread is more sensitive to the variation of the wave function than the energy, which makes it a good indicator for instance in a Metal–insulator transition situation.\n", "related": "NONE"}
{"id": "49885288", "url": "https://en.wikipedia.org/wiki?curid=49885288", "title": "Dirac membrane", "text": "Dirac membrane\n\nA model of a charged membrane introduced by Paul Dirac in 1962. Dirac's original motivation was to explain the mass of the muon as an excitation of the ground state corresponding to an electron. Anticipating the birth of string theory by almost a decade, he was the first to introduce what is now called a type of Nambu–Goto action for membranes.\n\nIn the Dirac membrane model the repulsive electromagnetic forces on the membrane are balanced by the contracting ones coming from the positive tension. In the case of the spherical membrane, classical equations of motion imply that the balance is met for the radius formula_1, where formula_2 is the classical electron radius. Using Bohr–Sommerfeld quantisation condition for the Hamiltonian of the spherically symmetric membrane, Dirac finds the approximation of the mass corresponding to the first excitation as formula_3, where formula_4 is the mass of the electron, which is about a quarter of the observed muon mass.\n\nDirac chose a non-standard way to formulate the action principle for the membrane. Because closed membranes in formula_5 provide a natural split of space into the interior and the exterior there exists a special curvilinear system of coordinates formula_6 in spacetime and a function formula_7 such that \n- formula_8 defines a membrane\n\n- formula_9, formula_10 describe a region outside or inside the membrane\nChoosing formula_11 and the following gauge formula_12, formula_13, formula_14 \nwhere formula_15, ( formula_16) is the internal parametrization of the membrane world-volume, the membrane action proposed by Dirac is\n\nwhere the induced metric and the factors J and M are given by\n\nIn the above formula_21 are rectilinear and orthogonal. The space-time signature used is (+,-,-,-). Note that formula_22 is just a usual action for the electromagnetic field in a curvilinear system while formula_23is the integral over the membrane world-volume i.e. precisely the type of the action used later in string theory.\n\nThere are 3 equations of motion following from the variation with respect to formula_24 and formula_25. They are:\n- variation w.r.t. formula_24 for formula_27 - this results in sourceless Maxwell equations \n- variation w.r.t. formula_21 for formula_27 - this gives a consequence of Maxwell equations\n- variation w.r.t. formula_21 for formula_31\n\nThe last equation has a geometric interpretation: the r.h.s. is proportional to the curvature of the membrane. For the spherically symmetric case we get\n\nTherefore, the balance condition formula_34 implies formula_35 where formula_36 is the radius of the balanced membrane. The total energy for the spherical membrane with radius formula_37 is\nand it is minimal in the equilibrium for formula_39, hence formula_40. On the other hand, the total energy in the equilibrium should be formula_4 (in formula_42 units)\nand so we obtain formula_43.\n\nSmall oscillations about the equilibrium in the spherically symmetric case imply frequencies - formula_44. Therefore, going to quantum theory, the energy of one quantum would be formula_45.\nThis is much more than the muon mass but the frequencies are by no means small so this approximation may not work properly. To get a better quantum theory one needs to work out the Hamiltonian of the system and solve the corresponding Schroedinger equation.\n\nFor the Hamiltonian formulation Dirac introduces generalised momenta\n\n- for formula_46: formula_47 and formula_48 - momenta conjugate to formula_24 and formula_50 respectively (formula_51, coordinate choice formula_52)\n\n- for formula_53: formula_54 - momenta conjugate to formula_50\n\nThen one notices the following constraints\n\n- for the Maxwell field \n\n- for membrane momenta\n\nwhere formula_58 - reciprocal of formula_59, formula_60.\n\nThese constraints need to be included when calculating the Hamiltonian, using the Dirac bracket method. \nThe result of this calculation is the Hamiltonian of the form\nwhere formula_63 is the Hamiltonian for the electromagnetic field written in the curvilinear system.\n\nFor spherically symmetric motion the Hamiltonian is \n\nhowever the direct quantisation is not clear due to the square-root of the differential operator. To get any further Dirac considers the Bohr - Sommerfeld method:\nand finds formula_66 for formula_67.\n\n", "related": "\n- Brane\n\nP. A. M. Dirac, An Extensible Model of the Electron, Proc. Roy. Soc. A268, (1962) 57–67.\n"}
{"id": "50013014", "url": "https://en.wikipedia.org/wiki?curid=50013014", "title": "Hopkinson effect", "text": "Hopkinson effect\n\nThe Hopkinson effect is a feature of ferromagnetic or ferrimagnetic materials, in which an increase in magnetic susceptibility is observed at temperatures between the blocking temperature and the Curie temperature of the material. The Hopkinson effect can be observed as a peak in thermomagnetic curves that immediately precedes the susceptibility drop associated with the Curie temperature. It was first observed by John Hopkinson in 1889 in a study on iron.\n\nIn single domain particles, a large Hopkinson peak results from a transient superparamagnetic particle domain state.\n\n", "related": "\n- Ferroelectric effect\n- Curie's law\n"}
{"id": "50791779", "url": "https://en.wikipedia.org/wiki?curid=50791779", "title": "Energy well", "text": "Energy well\n\nIn physics, an energy well describes a 'stable' equilibrium that is not at lowest possible energy.\n\nIn general, modern physics holds the view that the universe - and systems therein - spontaneously drives toward a state of lower energy, if possible. For example, a bowling ball pitched atop a smooth hump (which has potential energy in the presence of gravity), will tend to roll down to the lowest point it possibly can. Once there, this reduces the total potential energy of the system.\n\nOn the other hand, if the bowling ball is resting in a valley between two humps - no matter how big the drops outside the humps - it will stay there indefinitely. Even though the system could achieve a lower energy state, it cannot do so without external energy being applied: (locally) it is at its lowest energy state, and only a force from outside the system can 'push' it over one of the humps so a lower state can be achieved.\n\nThe concept of an energy well is a key part of teaching basic physics, especially quantum mechanics. Here, students often solve the one-dimensional Schrödinger Equation for an electron trapped in a potential well from which it has insufficient energy to escape. The solution to this problem is a series of sinusoidal waves of fractional integral wavelengths determined by the width of the well.\n", "related": "NONE"}
{"id": "51084847", "url": "https://en.wikipedia.org/wiki?curid=51084847", "title": "Dispersive medium", "text": "Dispersive medium\n\nA dispersive medium is a medium in which waves of different frequencies travel at different velocities. With electromagnetic radiation (e.g. light, radio waves), dispersion corresponds to a frequency-dependent variation in the index of refraction of the medium.\n", "related": "NONE"}
{"id": "21348562", "url": "https://en.wikipedia.org/wiki?curid=21348562", "title": "Homeokinetics", "text": "Homeokinetics\n\nHomeokinetics is the study of self-organizing, complex systems. Standard physics studies systems at separate levels, such as atomic physics, nuclear physics, biophysics, social physics, and galactic physics. Homeokinetic physics studies the up-down processes that bind these levels. Tools such as mechanics, quantum field theory, and the laws of thermodynamics provide the key relationships. The subject, described as the physics and thermodynamics associated with the up down movement between levels of systems, originated in the late 1970s work of American physicists Harry Soodak and Arthur Iberall. Complex systems are universes, galaxies, social systems, people, or even those that seem as simple as gases. The basic premise is that the entire universe consists of atomistic-like units bound in interactive ensembles to form systems, level by level, in a nested hierarchy. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.\n\nArthur Iberall, Warren McCulloch and Harry Soodak developed the concept of homeokinetics as a new branch of physics. It began through Iberall's biophysical research for the NASA exobiology program into the dynamics of mammalian physiological processes They were observing an area that physics has neglected, that of complex systems with their very long internal factory day delays. They were observing systems associated with nested hierarchy and with an extensive range of time scale processes. It was such connections, referred to as both up-down or in-out connections (as nested hierarchy) and side-side or flatland physics among atomistic-like components (as heterarchy) that became the hallmark of homeokinetic problems. By 1975, they began to put a formal catch-phrase name on those complex problems, associating them with nature, life, human, mind, and society. The major method of exposition that they began using was a combination of engineering physics and a more academic pure physics. In 1981, Iberall was invited to the Crump Institute for Medical Engineering of UCLA, where he further refined the key concepts of homeokinetics, developing a physical scientific foundation for complex systems.\n\nA system is a collective of interacting ‘atomistic’-like entities. The word ‘atomism’ is used to stand both for the entity and the doctrine. As is known from ‘kinetic’ theory, in mobile or simple systems, the atomisms share their ‘energy’ in interactive collisions. That so-called ‘equipartitioning’ process takes place within a few collisions. Physically, if there is little or no interaction, the process is considered to be very weak. Physics deals basically with the forces of interaction—few in number—that influence the interactions. They all tend to emerge with considerable force at high ‘density’ of atomistic interaction. In complex systems, there is also a result of internal processes in the atomisms. They exhibit, in addition to the pair-by-pair interactions, internal actions such as vibrations, rotations, and association. If the energy and time involved internally creates a very large—in time—cycle of performance of their actions compared to their pair interactions, the collective system is complex. If you eat a cookie and you do not see the resulting action for hours, that is complex; if boy meets girl and they become ‘engaged’ for a protracted period, that is complex. What emerges from that physics is a broad host of changes in state and stability transitions in state. Viewing Aristotle as having defined a general basis for systems in their static-logical states and trying to identify a logic-metalogic for physics, e.g., metaphysics, then homeokinetics is viewed to be an attempt to define the dynamics of all those systems in the universe.\n\nOrdinary physics is a flatland physics, a physics at some particular level. Examples include nuclear and atomic physics, biophysics, social physics, and stellar physics. Homeokinetic physics combines flatland physics with the study of the up down processes that binds the levels. Tools, such as mechanics, quantum field theory, and the laws of thermodynamics, provide key relationships for the binding of the levels, how they connect, and how the energy flows up and down. And whether the atomisms are atoms, molecules, cells, people, stars, galaxies, or universes, the same tools can be used to understand them. Homeokinetics treats all complex systems on an equal footing, animate and inanimate, providing them with a common viewpoint. The complexity in studying how they work is reduced by the emergence of common languages in all complex systems.\n\nA homeokinetic approach to complex systems has been applied to ecological psychology, anthropology, geology,\nbioenergetics, and political science.\n\nIt has also been applied to social physics where a homeokinetics analysis shows that one must account for flow variables such as the flow of energy, of materials, of action, reproduction rate, and value-in-exchange.\n\n- www.homeokinetics.org\n- commons.trincoll.edu/homeokinetics\n- www.trincoll.edu/depts/ecopsyc/homeokinetics\n", "related": "NONE"}
{"id": "2664158", "url": "https://en.wikipedia.org/wiki?curid=2664158", "title": "Center of percussion", "text": "Center of percussion\n\nThe center of percussion is the point on an extended massive object attached to a pivot where a perpendicular impact will produce no reactive shock at the pivot. Translational and rotational motions cancel at the pivot when an impulsive blow is struck at the center of percussion. The center of percussion is often discussed in the context of a bat, racquet, door, sword or other extended object held at one end. \n\nThe same point is called the center of oscillation for the object suspended from the pivot as a pendulum, meaning that a simple pendulum with all its mass concentrated at that point will have the same period of oscillation as the compound pendulum. \n\nIn sports, the center of percussion of a bat or racquet is related to the so-called \"sweet spot\", but the latter is also related to vibrational bending of the object.\n\nImagine a rigid beam suspended from a wire by a fixture that can slide freely along the wire at point P, as shown in the Figure. An impulsive blow is applied from the left. If it is below the center of mass (CM) it will cause the beam to rotate counterclockwise around the CM and also cause the CM to move to the right. The center of percussion (CP) is below the CM. If the blow falls above the CP, the rightward translational motion will be bigger than the leftward rotational motion at P, causing the net initial motion of the fixture to be rightward. If the blow falls below the CP the opposite will occur, rotational motion at P will be larger than translational motion and the fixture will move initially leftward. Only if the blow falls exactly on the CP will the two components of motion cancel out to produce zero net initial movement at point P. \n\nWhen the sliding fixture is replaced with a pivot that cannot move left or right, an impulsive blow anywhere but at the CP results in an initial reactive force at the pivot. \n\nFor a free, rigid beam, an impulse formula_1 applied at right angle at a distance formula_2 from the center of mass (CM) will result in the CM changing velocity formula_3 according to the relation:\n\nwhere formula_5 is the mass of the beam. Similarly, the torque about the CM will change the angular velocity formula_6 according to:\n\nwhere formula_8 is the moment of inertia around the CM.\n\nFor any point P a distance formula_9 on the opposite side of the CM from the point of impact, the change in velocity of point P is\n\nwhere formula_9 is the distance of P from the CM. Hence the acceleration at P due to the impulsive blow is:\n\nWhen this acceleration is zero, formula_2 defines the center of percussion. Therefore, the CP distance, formula_2, from the CM, is given by\n\nNote that P, the rotation axis, need not be at the end of the beam, but can be chosen at any distance formula_9. \n\nLength formula_17 also defines the center of oscillation of a physical pendulum, that is, the position of the mass of a simple pendulum that has the same period as the physical pendulum.\n\nFor the special case of a beam of uniform density of length formula_18, the moment of inertia around the CM is:\n\nand for rotation about a pivot at the end, \n\nThis leads to:\n\nIt follows that the CP is 2/3 of the length of the uniform beam formula_18 from the pivoted end. \n\nFor example, a swinging door that is stopped by a doorstop placed 2/3 of the width of the door will do the job with minimal shaking of the door because the hinged end is subjected to no net reactive force. (This point is also the node in the second vibrational harmonic, which also minimizes vibration.)\n\nThe sweet spot on a baseball bat is generally defined as the point at which the impact \"feels\" best to the batter. The center of percussion defines a place where, if the bat strikes the ball and the batter's hands are at the pivot point, the batter feels no sudden reactive force. However, since a bat is not a rigid object the vibrations produced by the impact also play a role. Also, the pivot point of the swing may not be at the place where the batter's hands are placed. Research has shown that the dominant physical mechanism in determining where the sweet spot is arises from the location of nodes in the vibrational modes of the bat, not the location of the center of percussion.\n\nThe center of percussion concept can be applied to swords. Being flexible objects, the \"sweet spot\" for such cutting weapons depends not only on the center of percussion but also on the flexing and vibrational characteristics.\n", "related": "NONE"}
{"id": "48520204", "url": "https://en.wikipedia.org/wiki?curid=48520204", "title": "Computational anatomy", "text": "Computational anatomy\n\nComputational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability. It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.\n\nThe field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, biological imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. Additionally, it complements newer, interdisciplinary fields like bioinformatics and neuroinformatics in the sense that its interpretation uses metadata derived from the original sensor imaging modalities (of which Magnetic Resonance Imaging is one example). It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of Computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication medium(s).\n\nIn computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow in formula_1. The flows between coordinates in computational anatomy are constrained to be geodesic flows satisfying the principle of least action for the Kinetic energy of the flow. The kinetic energy is defined through a Sobolev smoothness norm with strictly more than two generalized, square-integrable derivatives for each component of the flow velocity, which guarantees that the flows in formula_2 are diffeomorphisms. \nIt also implies that the diffeomorphic shape momentum taken pointwise satisfying the Euler-Lagrange equation for geodesics is determined by its neighbors through spatial derivatives on the velocity field. This separates the discipline from the case of incompressible fluids for which momentum is a pointwise function of velocity. Computational anatomy intersects the study of Riemannian manifolds and nonlinear global analysis, where groups of diffeomorphisms are the central focus. Emerging high-dimensional theories of shape are central to many studies in computational anatomy, as are questions emerging from the fledgling field of shape statistics.\nThe metric structures in computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology , the metric space study of coordinate systems via diffeomorphisms.\n\nAt computational anatomy's heart is the comparison of shape by recognizing in one shape the other. This connects it to D'Arcy Wentworth Thompson's developments On Growth and Form which has led to scientific explanations of morphogenesis, the process by which patterns are formed in Biology. Albrecht Durer's Four Books on Human Proportion were arguably the earliest works on computational anatomy. The efforts of Noam Chomsky in his pioneering of Computational Linguistics inspired the original formulation of computational anatomy as a generative model of shape and form from exemplars acted upon via transformations.\n\nDue to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D. The spirit of this discipline shares strong overlap with areas such as computer vision and kinematics of rigid bodies, where objects are studied by analysing the groups responsible for the movement in question. Computational anatomy departs from computer vision with its focus on rigid motions, as the infinite-dimensional diffeomorphism group is central to the analysis of Biological shapes. It is a branch of the image analysis and pattern theory school at Brown University pioneered by Ulf Grenander. In Grenander's general Metric Pattern Theory, making spaces of patterns into a metric space is one of the fundamental operations since being able to cluster and recognize anatomical configurations often requires a metric of close and far between shapes. The diffeomorphometry metric of Computational anatomy measures how far two diffeomorphic changes of coordinates are from each other, which in turn induces a metric on the shapes and images indexed to them. The models of metric pattern theory, in particular group action on the orbit of shapes and forms is a central tool to the formal definitions in Computational anatomy.\n\nComputational anatomy is the study of shape and form at the morphome or gross anatomy millimeter, or morphology scale, focusing on the study of sub-manifolds of formula_3 points, curves surfaces and subvolumes of human anatomy.\nAn early modern computational neuro-anatomist was David Van Essen performing some of the early physical unfoldings of the human brain based on printing of a human cortex and cutting. Jean Talairach's publication of Tailarach coordinates is an important milestone at the morphome scale demonstrating the fundamental basis of local coordinate systems in studying neuroanatomy and therefore the clear link to charts of differential geometry. Concurrently, virtual mapping in computational anatomy across high resolution dense image coordinates was already happening in Ruzena Bajcy's and Fred Bookstein's earliest developments based on Computed axial tomography and Magnetic resonance imagery.\nThe earliest introduction of the use of flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Joshi, Miller, and Rabbitt.\n\nThe first formalization of computational anatomy as an orbit of exemplar templates under diffeomorphism group action was in the original lecture given by Grenander and Miller with that title in May 1997 at the 50th Anniversary of the Division of Applied Mathematics at Brown University, and subsequent publication. This was the basis for the strong departure from much of the previous work on advanced methods for spatial normalization and image registration which were historically built on notions of addition and basis expansion. The structure preserving transformations central to the modern field of Computational Anatomy, homeomorphisms and diffeomorphisms carry smooth submanifolds smoothly. They are generated via Lagrangian and Eulerian flows which satisfy a law of composition of functions forming the group property, but are not additive.\n\nThe original model of computational anatomy was as the triple, formula_4 the group formula_5, the orbit of shapes and forms formula_6, and the probability laws formula_7 which encode the variations of the objects in the orbit. The template or collection of templates are elements in the orbit formula_8 of shapes.\n\nThe Lagrangian and Hamiltonian formulations of the equations of motion of computational anatomy took off post 1997 with several pivotal meetings including the 1997 Luminy meeting organized by the Azencott school at Ecole-Normale Cachan on the \"Mathematics of Shape Recognition\" and the 1998 Trimestre at Institute Henri Poincaré organized by David Mumford \"Questions Mathématiques en Traitement du Signal et de l'Image\" which catalyzed the Hopkins-Brown-ENS Cachan groups and subsequent developments and connections of Computational anatomy to developments in global analysis.\n\nThe developments in computational anatomy included the establishment of the Sobelev smoothness conditions on the diffeomorphometry metric to insure existence of solutions of variational problems in the space of diffeomorphisms, the derivation of the Euler-Lagrange equations characterizing geodesics through the group and associated conservation laws, the demonstration of the metric properties of the right invariant metric, the demonstration that the Euler-Lagrange equations have a well-posed initial value problem with unique solutions for all time, and with the first results on sectional curvatures for the diffeomorphometry metric in landmarked spaces. Following the Los Alamos meeting in 2002, Joshi's original large deformation singular \"Landmark\" solutions in Computational anatomy were connected to peaked \"Solitons\" or \"Peakons\" as solutions for the Camassa-Holm equation. Subsequently, connections were made between Computational anatomy's Euler-Lagrange equations for momentum densities for the right-invariant metric satisfying Sobolev smoothness to Vladimir Arnold's characterization of the Euler equation for incompressible flows as describing geodesics in the group of volume preserving diffeomorphisms. The first algorithms, generally termed LDDMM for large deformation diffeomorphic mapping for computing connections between landmarks in volumes and spherical manifolds, curves, currents and surfaces, volumes, tensors, varifolds, and time-series have followed.\n\nThese contributions of computational anatomy to the global analysis associated to the infinite dimensional manifolds of subgroups of the diffeomorphism group is far from trivial. The original idea of doing differential geometry, curvature and geodesics on infinite dimensional manifolds goes back to Bernhard Riemann's Habilitation (Ueber die Hypothesen, welche der Geometrie zu Grunde liegen); the key modern book laying the foundations of such ideas in global analysis are from Michor.\n\nThe applications within medical imaging of computational anatomy continued to flourish after two organized meetings at the Institute for Pure and Applied Mathematics conferences at University of California, Los Angeles. Computational anatomy has been useful in creating accurate models of the atrophy of the human brain at the morphome scale, as well as Cardiac templates, as well as in modeling biological systems. Since the late 1990s, computational anatomy has become an important part of developing emerging technologies for the field of medical imaging. Digital atlases are a fundamental part of modern Medical-school education and in neuroimaging research at the morphome scale. Atlas based methods and virtual textbooks which accommodate variations as in deformable templates are at the center of many neuro-image analysis platforms including Freesurfer, FSL, MRIStudio, SPM. Diffeomorphic registration, introduced in the 1990s, is now an important player with existing codes bases organized around ANTS,<ref name=\"stnava/ANTs\"></ref> DARTEL, DEMONS, LDDMM,<ref name=\"NITRC: LDDMM: Tool/Resource Info\"></ref> StationaryLDDMM, FastLDDMM, are examples of actively used computational codes for constructing correspondences between coordinate systems based on sparse features and dense images. Voxel-based morphometry (VBM) is an important technology built on many of these principles.\n\nThe model of human anatomy is a deformable template, an orbit of exemplars under group action. Deformable template models have been central to Grenander's Metric Pattern theory, accounting for typicality via templates, and accounting for variability via transformation of the template. An orbit under group action as the representation of the deformable template is a classic formulation from differential geometry. The space of shapes are denoted formula_9, with the group formula_10 with law of composition formula_11; the action of the group on shapes is denoted formula_12, where the action of the group formula_13 is defined to satisfy \n\nThe orbit formula_15 of the template becomes the space of all shapes, formula_16, being homogenous under the action of the elements of formula_17.\nThe orbit model of computational anatomy is an abstract algebra - to be compared to linear algebra- since the groups act nonlinearly on the shapes. This is a generalization of the classical models of linear algebra, in which the set of finite dimensional formula_18 vectors are replaced by the finite-dimensional anatomical submanifolds (points, curves, surfaces and volumes) and images of them, and the formula_19 matrices of linear algebra are replaced by coordinate transformations based on linear and affine groups and the more general high-dimensional diffeomorphism groups.\n\nThe central objects are shapes or forms in computational anatomy, one set of examples being the 0,1,2,3-dimensional submanifolds of formula_20, a second set of examples being images generated via medical imaging such as via magnetic resonance imaging (MRI) and functional magnetic resonance imaging. The 0-dimensional manifolds are landmarks or fiducial points; 1-dimensional manifolds are curves such as sulcul and gyral curves in the brain; 2-dimensional manifolds correspond to boundaries of substructures in anatomy such as the subcortical structures of the midbrain or the gyral surface of the neocortex; subvolumes correspond to subregions of the human body, the heart, the thalamus, the kidney.\n\nThe landmarks formula_21 are a collections of points with no other structure, delineating important fiducials within human shape and form (see associated landmarked image).\nThe sub-manifold shapes such as surfaces formula_22 are collections of points modeled as parametrized by a local chart or immersion formula_23, formula_24 (see Figure showing shapes as mesh surfaces).\nThe images such as MR images or DTI images formula_25, and are dense functions\nformula_26 are scalars, vectors, and matrices (see Figure showing scalar image).\n\nGroups and group actions are familiar to the Engineering community with the universal popularization and standardization of linear algebra as a basic model for analyzing signals and systems in mechanical engineering, electrical engineering and applied mathematics. In linear algebra the matrix groups (matrices with inverses) are the central structure, with group action defined by the usual definition of formula_27 as an formula_28 matrix, acting on formula_29 as formula_30 vectors; the orbit in linear-algebra is the set of formula_31-vectors given by formula_32, which is a group action of the matrices through the orbit of formula_33.\n\nThe central group in computational anatomy defined on volumes in formula_1 are the diffeomorphisms formula_35 which are mappings with 3-components formula_36, law of composition of functions formula_37, with inverse formula_38.\n\nMost popular are scalar images, formula_39, with action on the right via the inverse.\n\nFor sub-manifolds formula_22, parametrized by a chart or immersion formula_42, the diffeomorphic action the flow of the position\n\nSeveral group actions in computational anatomy have been defined.\n\nFor the study of rigid body kinematics, the low-dimensional matrix Lie groups have been the central focus. The matrix groups are low-dimensional mappings, which are diffeomorphisms that provide one-to-one correspondences between coordinate systems, with a smooth inverse. The matrix group of rotations and scales can be generated via a closed form finite-dimensional matrices which are solution of simple ordinary differential equations with solutions given by the matrix exponential.\n\nFor the study of deformable shape in computational anatomy, a more general diffeomorphism group has been the group of choice, which is the infinite dimensional analogue. The high-dimensional differeomorphism groups used in Computational Anatomy are generated via smooth flows formula_44 which satisfy the Lagrangian and Eulerian specification of the flow fields as first introduced in., satisfying the ordinary differential equation: \n\nwith formula_45 the vector fields on formula_46 termed the Eulerian velocity of the particles at position formula_47 of the flow. The vector fields are functions in a function space, modelled as a smooth Hilbert space of high-dimension, with the Jacobian of the flow formula_48 a high-dimensional field in a function space as well, rather than a low-dimensional matrix as in the matrix groups. Flows were first introduced for large deformations in image matching; formula_49 is the instantaneous velocity of particle formula_50 at time formula_51 .\n\nThe inverse formula_52 required for the group is defined on the Eulerian vector-field with advective inverse flow\n\nThe group of diffeomorphisms is very big. To ensure smooth flows of diffeomorphisms avoiding shock-like solutions for the inverse, the vector fields must be at least 1-time continuously differentiable in space. For diffeomorphisms on formula_46, vector fields are modelled as elements of the Hilbert space formula_54 using the Sobolev embedding theorems so that each element has strictly greater than 2 generalized square-integrable spatial derivatives (thus formula_55 is sufficient), yielding 1-time continuously differentiable functions.\n\nThe diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\nwhere\nformula_56 \nwith the linear operator formula_57 mapping to the dual space formula_58, with the integral calculated by integration by parts when formula_59 is a generalized function in the dual space.\nthe images are denoted with the orbit as formula_60 and metric formula_61.\n\nIn classical mechanics the evolution of physical systems is described by solutions to the Euler–Lagrange equations associated to the Least-action principle of Hamilton. This is a standard way, for example of obtaining Newton's laws of motion of free particles. More generally, the Euler-Lagrange equations can be derived for systems of generalized coordinates. The Euler-Lagrange equation in computational anatomy describes the geodesic shortest path flows between coordinate systems of the diffeomorphism metric. In computational anatomy the generalized coordinates are the flow of the diffeomorphism and its Lagrangian velocity formula_62, the two related via the Eulerian velocity formula_63. \nHamilton's principle for generating the Euler-Lagrange equation requires the action integral on the Lagrangian given by \n\nthe Lagrangian is given by the kinetic energy:\n\nIn computational anatomy, formula_64 was first called the Eulerian or diffeomorphic shape momentum since when integrated against Eulerian velocity formula_65 gives energy density, and since there is a conservation of diffeomorphic shape momentum which holds. The operator formula_27 is the generalized moment of inertia or inertial operator.\n\nClassical calculation of the Euler-Lagrange equation from Hamilton's principle requires the perturbation of the Lagrangian on the vector field in the kinetic energy with respect to first order perturbation of the flow. This requires adjustment by the Lie bracket of vector field, given by operator formula_67 which involves the Jacobian given by \nDefining the adjoint formula_69 then the first order variation gives the Eulerian shape momentum formula_59 satisfying the generalized equation:\nmeaning for all smooth formula_71\n\nComputational anatomy is the study of the motions of submanifolds, points, curves, surfaces and volumes.\nMomentum associated to points, curves and surfaces are all singular, implying the momentum is concentrated on subsets of formula_20 which are dimension formula_74 in Lebesgue measure. In such cases, the energy is still well defined formula_75 since although formula_76 is a generalized function, the vector fields are smooth and the Eulerian momentum is understood via its action on smooth functions. The perfect illustration of this is even when it is a superposition of delta-diracs, the velocity of the coordinates in the entire volume move smoothly. The Euler-Lagrange equation () on diffeomorphisms for generalized functions formula_77 was derived in. In Riemannian Metric and Lie-Bracket Interpretation of the Euler-Lagrange Equation on Geodesics derivations are provided in terms of the adjoint operator and the Lie bracket for the group of diffeomorphisms. It has come to be called EPDiff equation for diffeomorphisms connecting to the Euler-Poincare method having been studied in the context of the inertial operator formula_78 for incompressible, divergence free, fluids.\n\nFor the momentum density case formula_79, then Euler–Lagrange equation has a classical solution:The Euler-Lagrange equation on diffeomorphisms, classically defined for momentum densities first appeared in for medical image analysis.\n\nIn medical imaging and computational anatomy, positioning and coordinatizing shapes are fundamental operations; the system for positioning anatomical coordinates and shapes built on the metric and the Euler-Lagrange equation a geodesic positioning system as first explicated in Miller Trouve and Younes.\nSolving the geodesic from the initial condition formula_80 is termed the Riemannian-exponential, a mapping formula_81 at identity to the group.\n\nThe Riemannian exponential satisfies formula_82 for initial condition formula_83, vector field dynamics formula_84,\n- for classical equation diffeomorphic shape momentum formula_85, formula_86, then\n\n- for generalized equation, then formula_88,formula_89,\n\nComputing the flow formula_80 onto coordinates Riemannian logarithm, mapping formula_92 at identity from formula_93 to vector field formula_94;\n\nformula_95\n\nExtended to the entire group they become\n\nformula_96 ; formula_97 .\n\nThese are inverses of each other for unique solutions of Logarithm; the first is called geodesic positioning, the latter geodesic coordinates (see Exponential map, Riemannian geometry for the finite dimensional version).The geodesic metric is a local flattening of the Riemannian coordinate system (see figure). \n\nIn computational anatomy the diffeomorphisms are used to push the coordinate systems, and the vector fields are used\nas the control within the\nanatomical orbit or morphological space. The model is that of a dynamical system, the flow of coordinates formula_98 and the control the vector field formula_99 related via formula_100 The Hamiltonian view\n\nThis function is the extended Hamiltonian. The Pontryagin maximum principle gives the optimizing vector field which determines the geodesic flow satisfying formula_105 as well as the reduced Hamiltonian \nThe Lagrange multiplier in its action as a linear form has its own inner product of the canonical momentum acting on the velocity of the flow which is dependent on the shape, e.g. for landmarks a sum, for surfaces a surface integral, and. for volumes it is a volume integral with respect to formula_107 on formula_1. In all cases the Greens kernels carry weights which are the canonical momentum evolving according to an ordinary differential equation which corresponds to EL but is the geodesic reparameterization in canonical momentum. The optimizing vector field is given by\nwith dynamics of canonical momentum reparameterizing the vector field along the geodesic \nWhereas the vector fields are extended across the entire background space of formula_1, the geodesic flows associated to the submanifolds has Eulerian shape momentum which evolves as a generalized function formula_111 concentrated to the submanifolds. For landmarks the geodesics have Eulerian shape momentum which are a superposition of delta distributions travelling with the finite numbers of particles; the diffeomorphic flow of coordinates have velocities in the range of weighted Green's Kernels. For surfaces, the momentum is a surface integral of delta distributions travelling with the surface.\n\nThe geodesics connecting coordinate systems satisfying have stationarity of the Lagrangian. The Hamiltonian is given by the extremum along the path formula_112, formula_113, equalling the and is stationary along . Defining the geodesic velocity at the identity formula_114, then along the geodesic\nThe stationarity of the Hamiltonian demonstrates the interpretation of the Lagrange multiplier as momentum; integrated against velocity formula_115 gives energy density. The canonical momentum has many names. In optimal control, the flows formula_47 is interpreted as the state, and formula_117 is interpreted as conjugate state, or conjugate momentum. The geodesi of EL implies specification of the vector fields formula_118 or Eulerian momentum formula_119 at formula_120, or specification of canonical momentum formula_121 determines the flow.\n\nIn computational anatomy the submanifolds are pointsets, curves, surfaces and subvolumes which are the basic primitives. The geodesic flows between the submanifolds determine the distance, and form the basic measuring and transporting tools of diffeomorphometry. At formula_120 the geodesic has vector field formula_123 determined by the conjugate momentum and the Green's kernel of the inertial operator defining the Eulerian momentum formula_124. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:\n\nGiven the least-action there is a natural definition of momentum associated to generalized coordinates; the quantity acting against velocity gives energy. The field has studied two forms, the momentum associated to the Eulerian vector field termed Eulerian diffeomorphic shape momentum, and the momentum associated to the initial coordinates or canonical coordinates termed canonical diffeomorphic shape momentum. Each has a conservation law.The conservation of momentum goes hand in hand with the . In computational anatomy, formula_64 is the Eulerian Momentum since when integrated against Eulerian velocity formula_65 gives energy density; operator formula_27 the generalized moment of inertia or inertial operator which acting on the Eulerian velocity gives momentum which is conserved along the geodesic: \n\nConservation of Eulerian shape momentum was shown in and follows from ; conservation of canonical momentum was shown in\n^T p_t</math>: \n\nLDDMM matching based on the entire tensor matrix\nhas group action becomes formula_130 transformed eigenvectors\n\nThe variational problem matching onto the principal eigenvector or the matrix is described\nLDDMM Tensor Image Matching.\n\nHigh angular resolution diffusion imaging (HARDI) addresses the well-known limitation of DTI, that is, DTI can only reveal one dominant fiber orientation at each location. HARDI measures diffusion along formula_132 uniformly distributed directions on the sphere and can characterize more complex fiber geometries. HARDI can be used to reconstruct an orientation distribution function (ODF) that characterizes the angular profile of the diffusion probability density function of water molecules. The ODF is a function defined on a unit sphere, formula_133.\n\nDense LDDMM ODF matching takes the HARDI data as ODF at each voxel and solves the LDDMM variational problem in the space of ODF. In the field of information geometry, the space of ODF forms a Riemannian manifold with the Fisher-Rao metric. For the purpose of LDDMM ODF mapping, the square-root representation is chosen because it is one of the most efficient representations found to date as the various Riemannian operations, such as geodesics, exponential maps, and logarithm maps, are available in closed form. In the following, denote square-root ODF (formula_134) as formula_135, where formula_135 is non-negative to ensure uniqueness and formula_137. The variational problem for matching assumes that two ODF volumes can be generated from one to another via flows of diffeomorphisms formula_138, which are solutions of ordinary differential equations formula_139 starting from the identity map formula_140. Denote the action of the diffeomorphism on template as formula_141, formula_142, formula_143 are respectively the coordinates of the unit sphere, formula_144 and the image domain, with the target indexed similarly, formula_145,formula_142,formula_143.\n\nThe group action of the diffeomorphism on the template is given according to\nwhere formula_149 is the Jacobian of the affined transformed ODF and is defined as\nformula_150\nThis group action of diffeomorphisms on ODF reorients the ODF and reflects changes in both the magnitude of formula_151 and the sampling directions of formula_152 due to affine transformation. It guarantees that the volume fraction of fibers oriented toward a small patch must remain the same after the patch is transformed. \nThe LDDMM variational problem is defined as\n\nwhere the logarithm of formula_154 is defined as\nwhere formula_156 is the normal dot product between points in the sphere under the formula_157 metric.\n\nThis LDDMM-ODF mapping algorithm has been widely used to study brain white matter degeneration in aging, Alzheimer's disease, and vascular dementia. The brain white matter atlas generated based on ODF is constructed via Bayesian estimation. Regression analysis on ODF is developed in the ODF manifold space in.\n\nThe principle mode of variation represented by the orbit model is change of coordinates. For setting in which pairs of images are not related by diffeomorphisms but have photometric variation or image variation not represented by the template, active appearance modelling has been introduced, originally by Edwards-Cootes-Taylor and in 3D medical imaging in. In the context of computational anatomy in which metrics on the anatomical orbit has been studied, metamorphosis for modelling structures such as tumors and photometric changes which are not resident in the template was introduced in for Magnetic Resonance image models, with many subsequent developments extending the metamorphosis framework.\n\nFor image matching the image metamorphosis framework enlarges the action so that formula_158 with action formula_159. In this setting metamorphosis combines both the diffeomorphic coordinate system transformation of computational anatomy as well as the early morphing technologies which only faded or modified the photometric or image intensity alone.\n\nThen the matching problem takes a form with equality boundary conditions:\n\nTransforming coordinate systems based on Landmark point or fiducial marker features dates back to Bookstein's early work on small deformation spline methods for interpolating correspondences defined by fiducial points to the two-dimensional or three-dimensional background space in which the fiducials are defined. Large deformation landmark methods came on in the late 1990s. The above Figure depicts a series of landmarks associated three brain structures, the amygdala, entorhinal cortex, and hippocampus.\n\nMatching geometrical objects like unlabelled point distributions, curves or surfaces is another common problem in computational anatomy. Even in the discrete setting where these are commonly given as vertices with meshes, there are no predetermined correspondences between points as opposed to the situation of landmarks described above. From the theoretical point of view, while any submanifold formula_161 in formula_20, formula_163 can be parameterized in local charts formula_164, all reparametrizations of these charts give geometrically the same manifold. Therefore, early on in computational anatomy, investigators have identified the necessity of parametrization invariant representations. One indispensable requirement is that the end-point matching term between two submanifolds is itself independent of their parametrizations. This can be achieved via concepts and methods borrowed from Geometric measure theory, in particular currents and varifolds which have been used extensively for curve and surface matching.\n\nDenoted the landmarked shape formula_165 with endpoint formula_166, the variational problem becomes\n\n</math>|}}The geodesic Eulerian momentum is a generalized function formula_167, supported on the landmarked set in the variational problem.The endpoint condition with conservation implies the initial momentum at the identity of the group:\n\nThe iterative algorithm \nfor large deformation diffeomorphic metric mapping for landmarks is given.\n\nGlaunes and co-workers first introduced diffeomorphic matching of pointsets in the general setting of matching distributions. As opposed to landmarks, this includes in particular the situation of weighted point clouds with no predefined correspondences and possibly different cardinalities. The template and target discrete point clouds are represented as two weighted sums of Diracs formula_169 and formula_170 living in the space of signed measures of formula_171. The space is equipped with a Hilbert metric obtained from a real positive kernel formula_172 on formula_171, giving the following norm:\n\nThe matching problem between a template and target point cloud may be then formulated using this kernel metric for the endpoint matching term:\n\nwhere formula_176 is the distribution transported by the deformation.\n\nIn the one dimensional case, a curve in 3D can be represented by an embedding formula_177, and the group action of \"Diff\" becomes formula_178. However, the correspondence between curves and embeddings is not one to one as the any reparametrization formula_179, for formula_180 a diffeomorphism of the interval [0,1], represents geometrically the same curve. In order to preserve this invariance in the end-point matching term, several extensions of the previous 0-dimensional measure matching approach can be considered.\n\n- Curve matching with currents\nIn the situation of oriented curves, currents give an efficient setting to construct invariant matching terms. In such representation, curves are interpreted as elements of a functional space dual to the space vector fields, and compared through kernel norms on these spaces. Matching of two curves formula_181 and formula_182 writes eventually as the variational problem\n\nwith the endpoint term formula_184 is obtained from the norm\n\nthe derivative formula_186 being the tangent vector to the curve and formula_187 a given matrix kernel of formula_20. Such expressions are invariant to any positive reparametrizations of formula_181 and formula_190, and thus still depend on the orientation of the two curves.\n\n- Curve matching with varifolds\nVarifold is an alternative to currents when orientation becomes an issue as for instance in situations involving multiple bundles of curves for which no \"consistent\" orientation may be defined. Varifolds directly extend 0-dimensional measures by adding an extra tangent space direction to the position of points, leading to represent curves as measures on the product of formula_20 and the Grassmannian of all straight lines in formula_20. The matching problem between two curves then consists in replacing the endpoint matching term by formula_193 with varifold norms of the form:\n\nwhere formula_195 is the non-oriented line directed by tangent vector formula_186 and formula_197 two scalar kernels respectively on formula_2 and the Grassmannian. Due to the inherent non-oriented nature of the Grassmannian representation, such expressions are invariant to positive and negative reparametrizations.\n\nSurface matching share many similarities with the case of curves. Surfaces in formula_20 are parametrized in local charts by embeddings formula_200, with all reparametrizations formula_201 with formula_202 a diffeomorphism of U being equivalent geometrically. Currents and varifolds can be also used to formalize surface matching.\n\n- Surface matching with currents\nOriented surfaces can be represented as 2-currents which are dual to differential 2-forms. In formula_20, one can further identify 2-forms with vector fields through the standard wedge product of 3D vectors. In that setting, surface matching writes again:\n\nwith the endpoint term formula_184 given through the norm\n\nwith formula_207 the normal vector to the surface parametrized by formula_208.\n\nThis surface mapping algorithm has been validated for brain cortical surfaces against CARET and FreeSurfer. LDDMM mapping for multiscale surfaces is discussed in.\n\n- Surface matching with varifolds\nFor non-orientable or non-oriented surfaces, the varifold framework is often more adequate. Identifying the parametric surface formula_208 with a varifold formula_210 in the space of measures on the product of formula_20 and the Grassmannian, one simply replaces the previous current metric formula_212 by:\n\nwhere formula_214 is the (non-oriented) line directed by the normal vector to the surface.\n\nThere are many settings in which there are a series of measurements, a time-series to which the underlying\ncoordinate systems will be matched and flowed onto. This occurs for example\nin the dynamic growth and atrophy models and motion tracking such as have been explored in An observed time sequence is given and the goal is to infer the time flow of geometric change of coordinates carrying the exemplars or templars through the period of observations.\n\nThe generic time-series matching problem considers the series of times is formula_215. The flow optimizes at the series of costs formula_216 giving optimization problems of the form\n\nThere have been at least three solutions offered thus far, piecewise geodesic, principal geodesic and splines.\n\nThe random orbit model of computational anatomy first appeared in modelling the change in coordinates associated to the randomness of the group acting on the templates, which induces the randomness on the source of images in the anatomical orbit of shapes and forms and resulting observations through the medical imaging devices. Such a random orbit model in which randomness on the group induces randomness on the images was examined for the Special Euclidean Group for object recognition in.\n\nDepicted in the figure is a depiction of the random orbits around each exemplar, formula_218, generated by randomizing the flow by generating the initial tangent space vector field at the identity formula_94, and then generating random object formula_220.\n\nThe random orbit model induces the prior on shapes and images formula_221 conditioned on a particular atlas formula_222. For this the generative model generates the mean field formula_223 as a random change in coordinates of the template according to formula_224, where the diffeomorphic change in coordinates is generated randomly via the geodesic flows. The prior on random transformations formula_225 on formula_226 is induced by the flow formula_227, with formula_228 constructed as a Gaussian random field prior formula_229. The density on the random observables at the output of the sensor formula_230 are given by\nformula_231\n\nShown in the Figure on the right the cartoon orbit, are a random spray of the subcortical manifolds generated by randomizing the vector fields formula_118 supported over the submanifolds.\n\nThe central statistical model of computational anatomy in the context of medical imaging has been the source-channel model of Shannon theory; the source is the deformable template of images formula_233, the channel outputs are the imaging sensors with observables formula_234 (see Figure).\n\nSee The Bayesian model of computational anatomy for discussions (i) MAP estimation with multiple atlases, (ii)\nMAP segmentation with multiple atlases, MAP estimation of templates from populations.\n\nShape in computational anatomy is a local theory, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape in computational anatomy is the empirical study of diffeomorphic correspondences between populations and common template coordinate systems. This is a strong departure from Procrustes Analyses and shape theories pioneered by David G. Kendall in that the central group of Kendall's theories are the finite-dimensional Lie groups, whereas the theories of shape in computational anatomy have focused on the diffeomorphism group, which to first order via the Jacobian can be thought of as a field–thus infinite dimensional–of low-dimensional Lie groups of scale and rotations. \n\nThe random orbit model provides the natural setting to understand empirical shape and shape statistics within computational anatomy since the non-linearity of the induced probability law on anatomical shapes and forms formula_6 is induced via the reduction to the vector fields formula_236 at the tangent space at the identity of the diffeomorphism group. The successive flow of the Euler equation induces the random space of shapes and forms formula_237.\n\nPerforming empirical statistics on this tangent space at the identity is the natural way for inducing probability laws on the statistics of shape. Since both the vector fields and the Eulerian momentum formula_119 are in a Hilbert space the natural model is one of a Gaussian random field, so that given test function formula_239, then the inner-products with the test functions are Gaussian distributed with mean and covariance.\n\nThis is depicted in the accompanying figure where sub-cortical brain structures are depicted in a two-dimensional coordinate system based on inner products of their initial vector fields that generate them from the template is shown in a 2-dimensional span of the Hilbert space.\n\nThe study of shape and statistics in populations are local theories, indexing shapes and structures to templates to which they are bijectively mapped. Statistical shape is then the study of diffeomorphic correspondences relative to the template. A core operation is the generation of templates from populations, estimating a shape that is matched to the population. There are several important methods for generating templates including methods based on Frechet averaging, and statistical approaches based on the expectation-maximization algorithm and the Bayes Random orbit models of computational anatomy. Shown in the accompanying figure is a subcortical template reconstruction from the population of MRI subjects.\n\nSoftware suites containing a variety of diffeomorphic mapping algorithms include the following:\n\n- ANTS\n- DARTEL Voxel-based morphometry(VBM)\n- DEFORMETRICA\n- DEMONS\n- LDDMM Large deformation diffeomorphic metric mapping\n- LDDMM based on frame-based kernel\n- StationaryLDDMM\n\n- MRICloud\n\n", "related": "\n- Bayesian estimation of templates in computational anatomy\n- Computational neuroanatomy\n- Geometric data analysis\n- Large deformation diffeomorphic metric mapping\n- Procrustes analysis\n- Riemannian metric and Lie-bracket in computational anatomy\n- Shape analysis (disambiguation)\n- Statistical shape analysis\n"}
{"id": "49418115", "url": "https://en.wikipedia.org/wiki?curid=49418115", "title": "Large deformation diffeomorphic metric mapping", "text": "Large deformation diffeomorphic metric mapping\n\nLarge deformation diffeomorphic metric mapping (LDDMM) is a specific suite of algorithms used for diffeomorphic mapping and manipulating dense imagery based on diffeomorphic metric mapping within the academic discipline of computational anatomy, to be distinguished from its precursor based on diffeomorphic mapping. The distinction between the two is that diffeomorphic metric maps satisfy the property that the length associated to their flow away from the identity induces a metric on the group of diffeomorphisms, which in turn induces a metric on the orbit of shapes and forms within the field of Computational Anatomy. The study of shapes and forms with the metric of diffeomorphic metric mapping is called .\n\nA diffeomorphic mapping system is a system designed to map, manipulate, and transfer information which is stored in many types of spatially distributed medical imagery.\n\nDiffeomorphic mapping is the underlying technology for mapping and analyzing information measured in human anatomical coordinate systems which have been measured via Medical imaging. Diffeomorphic mapping is a broad term that actually refers to a number of different algorithms, processes, and methods. It is attached to many operations and has many applications for analysis and visualization. Diffeomorphic mapping can be used to relate various sources of information which are indexed as a function of spatial position as the key index variable. Diffeomorphisms are by their Latin root structure preserving transformations, which are in turn differentiable and therefore smooth, allowing for the calculation of metric based quantities such as arc length and surface areas. Spatial location and extents in human anatomical coordinate systems can be recorded via a variety of Medical imaging modalities, generally termed multi-modal medical imagery, providing either scalar and or vector quantities at each spatial location. Examples are scalar T1 or T2 magnetic resonance imagery, or as 3x3 diffusion tensor matrices diffusion MRI and diffusion-weighted imaging, to scalar densities associated to computed tomography (CT), or functional imagery such as temporal data of functional magnetic resonance imaging and scalar densities such as Positron emission tomography (PET).\n\nComputational anatomy is a subdiscipline within the broader field of neuroinformatics within bioinformatics and medical imaging. The first algorithm for dense image mapping via diffeomorphic metric mapping was Beg's LDDMM for volumes and Joshi's landmark matching for point sets with correspondence, with LDDMM algorithms now available for computing diffeomorphic metric maps between non-corresponding landmarks and landmark matching intrinsic to spherical manifolds, curves, currents and surfaces, tensors, varifolds, and time-series. The term LDDMM was first established as part of the National Institutes of Health supported Biomedical Informatics Research Network.\n\nIn a more general sense, diffeomorphic mapping is any solution that registers or builds correspondences between dense coordinate systems in medical imaging by ensuring the solutions are diffeomorphic. There are now many codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, StationaryLDDMM, FastLDDMM, as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.\n\nThe distinction between diffeomorphic metric mapping forming the basis for LDDMM and the earliest methods of diffeomorphic mapping is the introduction of a Hamilton principle of least-action in which large deformations are selected of shortest length corresponding to geodesic flows. This important distinction arises from the original formulation of the Riemannian metric corresponding to the right-invariance. The lengths of these geodesics give the metric in the metric space structure of human anatomy. Non-geodesic formulations of diffeomorphic mapping in general does not correspond to any metric formulation.\n\nDiffeomorphic mapping 3-dimensional information across coordinate systems is central to high-resolution Medical imaging and the area of Neuroinformatics within the newly emerging field of bioinformatics. Diffeomorphic mapping 3-dimensional coordinate systems as measured via high resolution dense imagery has a long history in 3-D beginning with Computed Axial Tomography (CAT scanning) in the early 80's by the University of Pennsylvania group led by Ruzena Bajcsy, and subsequently the Ulf Grenander school at Brown University with the HAND experiments. In the 90's there were several solutions for image registration which were associated to linearizations of small deformation and non-linear elasticity.\n\nThe central focus of the sub-field of Computational anatomy (CA) within medical imaging is mapping information across anatomical coordinate systems at the 1 millimeter morphome scale. In CA mapping of dense information measured within Magnetic resonance image (MRI) based coordinate systems such as in the brain has been solved via inexact matching of 3D MR images one onto the other. The earliest introduction of the use of diffeomorphic mapping via large deformation flows of diffeomorphisms for transformation of coordinate systems in image analysis and medical imaging was by Christensen, Rabbitt and Miller and Trouve. The introduction of flows, which are akin to the equations of motion used in fluid dynamics, exploit the notion that dense coordinates in image analysis follow the Lagrangian and Eulerian equations of motion. This model becomes more appropriate for cross-sectional studies in which brains and or hearts are not necessarily deformations of one to the other. Methods based on linear or non-linear elasticity energetics which grows with distance from the identity mapping of the template, is not appropriate for cross-sectional study. Rather, in models based on Lagrangian and Eulerian flows of diffeomorphisms, the constraint is associated to topological properties, such as open sets being preserved, coordinates not crossing implying uniqueness and existence of the inverse mapping, and connected sets remaining connected. The use of diffeomorphic methods grew quickly to dominate the field of mapping methods post Christensen's original paper, with fast and symmetric methods becoming available.\n\nSuch methods are powerful in that they introduce notions of regularity of the solutions so that they can be differentiated and local inverses can be calculated. The disadvantages of these methods is that there was no associated global least-action property which could score the flows of minimum energy. This contrasts the geodesic motions which are central to the study of Rigid body kinematics and the many problems solved in Physics via Hamilton's principle of least action. In 1998, Dupuis, Grenander and Miller established the conditions for guaranteeing the existence of solutions for dense image matching in the space of flows of diffeomorphisms. These conditions require an action penalizing kinetic energy measured via the Sobolev norm on spatial derivatives of the flow of vector fields.\n\nThe large deformation diffeomorphic metric mapping (LDDMM) code that Faisal Beg derived and implemented for his PhD at Johns Hopkins University developed the earliest algorithmic code which solved for flows with fixed points satisfying the necessary conditions for the dense image matching problem subject to least-action. Computational anatomy now has many existing codes organized around diffeomorphic registration including ANTS, DARTEL, DEMONS, LDDMM, StationaryLDDMM as examples of actively used computational codes for constructing correspondences between coordinate systems based on dense images.\n\nThese large deformation methods have been extended to landmarks without registration via measure matching, curves, surfaces, dense vector and tensor imagery, and varifolds removing orientation.\n\nDeformable shape in Computational Anatomy (CA)is studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinates in Medical Imaging. In this setting, three dimensional medical images are modelled as a random deformation of some exemplar, termed the template formula_1, with the set of observed images element in the random orbit model of CA for images formula_2. The template is mapped onto the target by defining a variational problem in which the template is transformed via the diffeomorphism used as a change of coordinate to minimize a squared-error matching condition between the transformed template and the target.\n\nThe diffeomorphisms are generated via smooth flows formula_3 , with formula_4, satisfying the Lagrangian and Eulerian specification of the flow field associated to the ordinary differential equation,\nwith \nformula_6 the Eulerian vector fields determining the flow.\nThe vector fields are guaranteed to be 1-time continuously differentiable formula_7 by modelling them to be in a \nsmooth Hilbert space formula_8 supporting 1-continuous derivative. The inverse formula_9 is defined by the Eulerian vector-field with flow given by\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields with components in formula_10 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_11 using the Sobolev embedding theorems so that each element formula_12 has 3-times square-integrable weak-derivatives. Thus formula_11 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm\n\nIn CA the space of vector fields formula_11 are modelled as a reproducing Kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_15 determining the norm formula_16 \nwhere the integral is calculated by integration by parts when formula_17 is a generalized function in the dual space formula_18. \nThe differential operator is selected so that the Green's kernel, the inverse of the operator, is continuously differentiable in each variable implying that the vector fields support 1-continuous derivative; see for the necessary conditions on the norm for existence of solutions.\n\nThe original large deformation diffeomorphic metric mapping (LDDMM) algorithms of Beg, Miller, Trouve, Younes was derived taking variations with respect to the vector field parameterization of the group, since formula_19 are in a vector spaces. Beg solved the dense image matching minimizing the action integral of kinetic energy of diffeomorphic flow while\nminimizing endpoint matching term according to\n- Beg's Iterative Algorithm for Dense Image Matching\nUpdate until convergence, formula_20 each iteration, with formula_21:\nThis implies that the fixed point at formula_22 satisfies \nwhich in turn implies it satisfies the Conservation equation given by the according to \n\nThe landmark matching problem has a pointwise correspondence defining the endpoint condition with geodesics given by the following minimum:\n\n- Iterative Algorithm for Landmark Matching\nJoshi originally defined the registered landmark matching probleme. Update until convergence, formula_20 each iteration, with formula_21:\nThis implies that the fixed point satisfy \nwith\n\nThe Calculus of variations was used in Beg to derive the iterative algorithm as a solution which when it converges satisfies the necessary maximizer conditions given by the necessary conditions for a first order variation requiring the variation of the endpoint with respect to a first order variation of the vector field. The directional derivative calculates the Gateaux derivative as calculated in Beg's original paper and.\n\n</math>\n\nThe LDDMM variational problem is defined as\n\nBeg solved the early LDDMM algorithms by solving the variational matching taking variations with respect to the vector fields. Another solution by Vialard, reparameterizes the optimization problem in terms of the state formula_31, for image formula_32, with the dynamics equation controlling the state by the control given in terms of the advection equation according to formula_33. The endpoint matching term\nformula_34 gives the variational problem:\n", "related": "NONE"}
{"id": "49400436", "url": "https://en.wikipedia.org/wiki?curid=49400436", "title": "Riemannian metric and Lie bracket in computational anatomy", "text": "Riemannian metric and Lie bracket in computational anatomy\n\nComputational anatomy (CA) is the study of shape and form in medical imaging. The study of deformable shapes in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2. In CA, this orbit is in general considered a smooth Riemannian manifold\nsince at every point of the manifold formula_3 there is an inner product inducing the norm formula_4 on the tangent space\nthat varies smoothly from point to point in the manifold of shapes formula_3. This is generated by viewing the\ngroup of diffeomorphisms formula_1 as a Riemannian manifold with formula_7, associated to the tangent space at formula_8 . This induces the norm and metric on the orbit formula_3 under the action from the group of diffeomorphisms.\n\nThe diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_10, generated via the ordinary differential equation\nwith the Eulerian vector fields formula_11 in formula_12 for formula_13, with the inverse for the flow given by\nand the formula_14 Jacobian matrix for flows in formula_15 given as formula_16\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_12 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_18 using the Sobolev embedding theorems so that each element formula_19 has 3-square-integrable derivatives thusly implies formula_18 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\n\nShapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_21, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_22, with for charts representing sub-manifolds denoted as formula_23.\n\nThe orbit of shapes and forms in Computational Anatomy are generated by the group actionformula_24. This is made into a Riemannian orbit by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_25 in the group of diffeomorphisms \nwith the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_18. We model formula_28 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_29. For formula_30 a distribution or generalized function, the linear form formula_31 determines the norm:and inner product for formula_32 according to \nwhere the integral is calculated by integration by parts for formula_34 a generalized function formula_35 the dual-space.\nThe differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative.\n\nThe metric on the group of diffeomorphisms is defined by the distance as defined on pairs of elements in the group of diffeomorphisms according to\n\nThis distance provides a right-invariant metric of diffeomorphometry, invariant to reparameterization of space since for all formula_1,\n\nThe Lie bracket gives the adjustment of the velocity term resulting from a perturbation of the motion in the setting of curved spaces. Using Hamilton's principle of least-action derives the optimizing flows as a critical point for the action integral of the integral of the kinetic energy. The Lie bracket for vector fields in Computational Anatomy was first introduced in Miller, Trouve and Younes. The derivation calculates the perturbation formula_38 on the vector fields\nformula_39 in terms of the derivative in time of the group perturbation adjusted by the correction of the Lie bracket of vector fields in this function setting involving the Jacobian matrix, unlike the matrix group case:\nProof:\nProving Lie bracket of vector fields take a first order perturbation of the flow at point formula_1.\n\nThe Lie bracket gives the first order variation of the vector field with respect to first order variation of the flow.\n\nThe Euler–Lagrange equation can be used to calculate geodesic flows through the group which form the basis for the metric. The action integral for the Lagrangian of the kinetic energy for Hamilton's principle becomes \n\nThe action integral in terms of the vector field corresponds to integrating the kinetic energy\nThe shortest paths geodesic connections in the orbit are defined via Hamilton's Principle of least action requires first order variations of the solutions in the orbits of Computational Anatomy which are based on computing critical points on the metric length or energy of the path.\nThe original derivation of the Euler equation associated to the geodesic flow of diffeomorphisms exploits the was a generalized function equation whenformula_43 is a distribution, or generalized function, take the first order variation of the action integral using the adjoint operator for the Lie bracket () gives for all smooth formula_44, \nUsing the bracket formula_46 and formula_47 gives\nmeaning for all smooth formula_48\nEquation () is the Euler-equation when diffeomorphic shape momentum is a generalized function.\n\nThis equation has been called EPDiff, Euler–Poincare equation for diffeomorphisms and has been studied in the context of fluid mechanics for incompressible fluids with formula_50 metric.\n\nIn the random orbit model of Computational anatomy, the entire flow is reduced to the initial condition which forms the coordinates encoding the diffeomorphism, as well as providing the means of positioning information in the orbit. This was first terms a geodesic positioning system in Miller, Trouve, and Younes. From the initial condition formula_51 then geodesic positioning with respect to the Riemannian metric of Computational anatomy solves for the flow of the Euler–Lagrange equation. Solving the geodesic from the initial condition formula_51 is termed the Riemannian-exponential, a mapping formula_53 at identity to the group.\n\nThe Riemannian exponential satisfies formula_54 for initial condition formula_55, vector field dynamics formula_56, \n- for classical equation on the diffeomorphic shape momentum as a smooth vector formula_57 with formula_58 the Euler equation exists in the classical sense as first derived for the density:\n\n- for generalized equation, formula_60, then\n\nIt is\nextended to the entire group,\nformula_62.\n\nMatching information across coordinate systems is central to computational anatomy. Adding a matching term formula_63 to the action integral of Equation ()\nwhich represents the target endpoint \nThe endpoint term adds a boundary condition for the Euler–Lagrange equation ()\nwhich gives the Euler equation with boundary term. Taking the variation gives\n- Necessary geodesic condition:\n\nProof:<ref>M.I. Miller, A. Trouve, L Younes,\nOn the Metrics and Euler–Lagrange equations of Computational Anatomy,\nAnnu. Rev. Biomed. Eng. 2002. 4:375–405\n= {} &-\\int_X(I \\circ \\varphi_1^{-1} -J) \\nabla (I\\circ \\varphi_1^{-1}) \\delta \\varphi \\, dx.\n</math>\n", "related": "NONE"}
{"id": "52907991", "url": "https://en.wikipedia.org/wiki?curid=52907991", "title": "Front (physics)", "text": "Front (physics)\n\nIn physics, a front can be understood as an interface between two different possible states (either stable or unstable) in a physical system. For example, a Weather front is the interface between two different density masses of air, in combustion where the flame is the interface between burned and unburned material or in population dynamics where the front is the interface between populated and unpopulated places. Fronts can be static or mobile depending on the conditions of the system, and the causes of the motion can be the variation of a free energy, where the most energetically favorable state invades the less favorable one, according to Pomeau or shape induced motion due to non-variation dynamics in the system, according to Alvarez-Socorro, Clerc, González-Cortés and Wilson.\n\nFrom a mathematical point of view, fronts are solutions of spatially extended systems connecting two steady states, and from dynamical systems point of view, a front corresponds to a heteroclinic orbit of the system in the co-mobile frame (or proper frame).\nThe most simple example of front solution connecting a homogeneous stable state with a homogeneous unstable state can be shown in the one-dimensional Fisher-Kolmogorov equation:\n\nthat describes a simple model for the density formula_2 of population. This equation has two steady states, formula_3, and formula_4. This solution corresponds to extinction and saturation of population. Observe that this model is spatially-extended, because it includes a diffusion term given by the second derivative. The state formula_5 is stable as a simple linear analysis can show and the state formula_6 is unstable. There exist a family of front solutions connecting formula_7 with formula_8, and such solution are propagative. Particularly, there exist one solution of the form formula_9, with formula_10 is a velocity that only depends on formula_11 and formula_12\n", "related": "NONE"}
{"id": "102847", "url": "https://en.wikipedia.org/wiki?curid=102847", "title": "Solid-state physics", "text": "Solid-state physics\n\nSolid-state physics is the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy. It is the largest branch of condensed matter physics. Solid-state physics studies how the large-scale properties of solid materials result from their atomic-scale properties. Thus, solid-state physics forms a theoretical basis of materials science. It also has direct applications, for example in the technology of transistors and semiconductors.\n\nSolid materials are formed from densely packed atoms, which interact intensely. These interactions produce the mechanical (e.g. hardness and elasticity), thermal, electrical, magnetic and optical properties of solids. Depending on the material involved and the conditions in which it was formed, the atoms may be arranged in a regular, geometric pattern (crystalline solids, which include metals and ordinary water ice) or irregularly (an amorphous solid such as common window glass).\n\nThe bulk of solid-state physics, as a general theory, is focused on crystals. Primarily, this is because the periodicity of atoms in a crystal — its defining characteristic — facilitates mathematical modeling. Likewise, crystalline materials often have electrical, magnetic, optical, or mechanical properties that can be exploited for engineering purposes.\n\nThe forces between the atoms in a crystal can take a variety of forms. For example, in a crystal of sodium chloride (common salt), the crystal is made up of ionic sodium and chlorine, and held together with ionic bonds. In others, the atoms share electrons and form covalent bonds. In metals, electrons are shared amongst the whole crystal in metallic bonding. Finally, the noble gases do not undergo any of these types of bonding. In solid form, the noble gases are held together with van der Waals forces resulting from the polarisation of the electronic charge cloud on each atom. The differences between the types of solid result from the differences between their bonding.\n\nThe physical properties of solids have been common subjects of scientific inquiry for centuries, but a separate field going by the name of solid-state physics did not emerge until the 1940s, in particular with the establishment of the Division of Solid State Physics (DSSP) within the American Physical Society. The DSSP catered to industrial physicists, and solid-state physics became associated with the technological applications made possible by research on solids. By the early 1960s, the DSSP was the largest division of the American Physical Society.\n\nLarge communities of solid state physicists also emerged in Europe after World War II, in particular in England, Germany, and the Soviet Union. In the United States and Europe, solid state became a prominent field through its investigations into semiconductors, superconductivity, nuclear magnetic resonance, and diverse other phenomena. During the early Cold War, research in solid state physics was often not restricted to solids, which led some physicists in the 1970s and 1980s to found the field of condensed matter physics, which organized around common techniques used to investigate solids, liquids, plasmas, and other complex matter. Today, solid-state physics is broadly considered to be the subfield of condensed matter physics that focuses on the properties of solids with regular crystal lattices.\n\nMany properties of materials are affected by their crystal structure. This structure can be investigated using a range of crystallographic techniques, including X-ray crystallography, neutron diffraction and electron diffraction.\n\nThe sizes of the individual crystals in a crystalline solid material vary depending on the material involved and the conditions when it was formed. Most crystalline materials encountered in everyday life are polycrystalline, with the individual crystals being microscopic in scale, but macroscopic single crystals can be produced either naturally (e.g. diamonds) or artificially.\n\nReal crystals feature defects or irregularities in the ideal arrangements, and it is these defects that critically determine many of the electrical and mechanical properties of real materials.\n\nProperties of materials such as electrical conduction and heat capacity are investigated by solid state physics. An early model of electrical conduction was the Drude model, which applied kinetic theory to the electrons in a solid. By assuming that the material contains immobile positive ions and an \"electron gas\" of classical, non-interacting electrons, the Drude model was able to explain electrical and thermal conductivity and the Hall effect in metals, although it greatly overestimated the electronic heat capacity.\n\nArnold Sommerfeld combined the classical Drude model with quantum mechanics in the free electron model (or Drude-Sommerfeld model). Here, the electrons are modelled as a Fermi gas, a gas of particles which obey the quantum mechanical Fermi–Dirac statistics. The free electron model gave improved predictions for the heat capacity of metals, however, it was unable to explain the existence of insulators.\n\nThe nearly free electron model is a modification of the free electron model which includes a weak periodic perturbation meant to model the interaction between the conduction electrons and the ions in a crystalline solid. By introducing the idea of electronic bands, the theory explains the existence of conductors, semiconductors and insulators.\n\nThe nearly free electron model rewrites the Schrödinger equation for the case of a periodic potential. The solutions in this case are known as Bloch states. Since Bloch's theorem applies only to periodic potentials, and since unceasing random movements of atoms in a crystal disrupt periodicity, this use of Bloch's theorem is only an approximation, but it has proven to be a tremendously valuable approximation, without which most solid-state physics analysis would be intractable. Deviations from periodicity are treated by quantum mechanical perturbation theory.\n\nModern research topics in solid-state physics include:\n- High-temperature superconductivity\n- Quasicrystals\n- Spin glass\n- Strongly correlated materials\n- Two-dimensional materials\n- Nanomaterials\n\n", "related": "\n- Condensed matter physics\n- Crystallography\n- Nuclear spectroscopy\n\n- Neil W. Ashcroft and N. David Mermin, \"Solid State Physics\" (Harcourt: Orlando, 1976).\n- Charles Kittel, \"Introduction to Solid State Physics\" (Wiley: New York, 2004).\n- H. M. Rosenberg, \"The Solid State\" (Oxford University Press: Oxford, 1995).\n- Steven H. Simon, \"The Oxford Solid State Basics\" (Oxford University Press: Oxford, 2013).\n- \"Out of the Crystal Maze. Chapters from the History of Solid State Physics\", ed. Lillian Hoddeson, Ernest Braun, Jürgen Teichmann, Spencer Weart (Oxford: Oxford University Press, 1992).\n- M. A. Omar, \"Elementary Solid State Physics\" (Revised Printing, Addison-Wesley, 1993).\n"}
{"id": "41232", "url": "https://en.wikipedia.org/wiki?curid=41232", "title": "Harmonic", "text": "Harmonic\n\nA harmonic is any member of the harmonic series. The term is employed in various disciplines, including music, physics, acoustics, electronic power transmission, radio technology, and other fields. It is typically applied to repeating signals, such as sinusoidal waves. A harmonic of such a wave is a wave with a frequency that is a positive integer multiple of the frequency of the original wave, known as the fundamental frequency. The original wave is also called the 1st harmonic, the following harmonics are known as higher harmonics. As all harmonics are periodic at the fundamental frequency, the sum of harmonics is also periodic at that frequency. For example, if the fundamental frequency is 50 Hz, a common AC power supply frequency, the frequencies of the first three higher harmonics are 100 Hz (2nd harmonic), 150 Hz (3rd harmonic), 200 Hz (4th harmonic) and any addition of waves with these frequencies is periodic at 50 Hz.\n\nIn music, harmonics are used on string instruments and wind instruments as a way of producing sound on the instrument, particularly to play higher notes and, with strings, obtain notes that have a unique sound quality or \"tone colour\". On strings, harmonics that are bowed have a \"glassy\", pure tone. On stringed instruments, harmonics are played by touching (but not fully pressing down the string) at an exact point on the string while sounding the string (plucking, bowing, etc.); this allows the harmonic to sound, a pitch which is always higher than the fundamental frequency of the string.\n\nHarmonics may also be called \"overtones\", \"partials\" or \"upper partials\". The difference between \"harmonic\" and \"overtone\" is that the term \"harmonic\" includes all of the notes in a series, including the fundamental frequency (e.g., the open string of a guitar). The term \"overtone\" only includes the pitches above the fundamental. In some music contexts, the terms \"harmonic\", \"overtone\" and \"partial\" are used fairly interchangeably.\n\nMost acoustic instruments emit complex tones containing many individual partials (component simple tones or sinusoidal waves), but the untrained human ear typically does not perceive those partials as separate phenomena. Rather, a musical note is perceived as one sound, the quality or timbre of that sound being a result of the relative strengths of the individual partials. Many acoustic oscillators, such as the human voice or a bowed violin string, produce complex tones that are more or less periodic, and thus are composed of partials that are near matches to integer multiples of the fundamental frequency and therefore resemble the ideal harmonics and are called \"harmonic partials\" or simply \"harmonics\" for convenience (although it's not strictly accurate to call a partial a harmonic, the first being real and the second being ideal).\n\nOscillators that produce harmonic partials behave somewhat like one-dimensional resonators, and are often long and thin, such as a guitar string or a column of air open at both ends (as with the modern orchestral transverse flute). Wind instruments whose air column is open at only one end, such as trumpets and clarinets, also produce partials resembling harmonics. However they only produce partials matching the odd harmonics, at least in theory. The reality of acoustic instruments is such that none of them behaves as perfectly as the somewhat simplified theoretical models would predict.\n\nPartials whose frequencies are not integer multiples of the fundamental are referred to as \"inharmonic partials\". Some acoustic instruments emit a mix of harmonic and inharmonic partials but still produce an effect on the ear of having a definite fundamental pitch, such as pianos, strings plucked pizzicato, vibraphones, marimbas, and certain pure-sounding bells or chimes. Antique singing bowls are known for producing multiple harmonic partials or multiphonics.\n\nAn overtone is any partial higher than the lowest partial in a compound tone. The relative strengths and frequency relationships of the component partials determine the timbre of an instrument. The similarity between the terms overtone and partial sometimes leads to their being loosely used interchangeably in a musical context, but they are counted differently, leading to some possible confusion. In the special case of instrumental timbres whose component partials closely match a harmonic series (such as with most strings and winds) rather than being inharmonic partials (such as with most pitched percussion instruments), it is also convenient to call the component partials \"harmonics\" but not strictly correct (because harmonics are numbered the same even when missing, while partials and overtones are only counted when present). This chart demonstrates how the three types of names (partial, overtone, and harmonic) are counted (assuming that the harmonics are present):\n\nIn many musical instruments, it is possible to play the upper harmonics without the fundamental note being present. In a simple case (e.g., recorder) this has the effect of making the note go up in pitch by an octave, but in more complex cases many other pitch variations are obtained. In some cases it also changes the timbre of the note. This is part of the normal method of obtaining higher notes in wind instruments, where it is called \"overblowing\". The extended technique of playing multiphonics also produces harmonics. On string instruments it is possible to produce very pure sounding notes, called harmonics or \"flageolets\" by string players, which have an eerie quality, as well as being high in pitch. Harmonics may be used to check at a unison the tuning of strings that are not tuned to the unison. For example, lightly fingering the node found halfway down the highest string of a cello produces the same pitch as lightly fingering the node of the way down the second highest string. For the human voice see Overtone singing, which uses harmonics.\n\nWhile it is true that electronically produced periodic tones (e.g. square waves or other non-sinusoidal waves) have \"harmonics\" that are whole number multiples of the fundamental frequency, practical instruments do not all have this characteristic. For example, higher \"harmonics\"' of piano notes are not true harmonics but are \"overtones\" and can be very sharp, i.e. a higher frequency than given by a pure harmonic series. This is especially true of instruments other than stringed or brass/woodwind ones, e.g., xylophone, drums, bells etc., where not all the overtones have a simple whole number ratio with the fundamental frequency. The fundamental frequency is the reciprocal of the period of the periodic phenomenon.\n\nThe following table displays the stop points on a stringed instrument, such as the guitar (guitar harmonics), at which gentle touching of a string will force it into a harmonic mode when vibrated. String harmonics (flageolet tones) are described as having a \"flutelike, silvery quality\" that can be highly effective as a special color or tone color (timbre) when used and heard in orchestration. It is unusual to encounter natural harmonics higher than the fifth partial on any stringed instrument except the double bass, on account of its much longer strings. Harmonics are widely used in plucked string instruments, such as acoustic guitar, electric guitar and electric bass. On an electric guitar played loudly through a guitar amplifier with distortion, harmonics are more sustained and can be used in guitar solos. In the heavy metal music lead guitar style known as shred guitar, harmonics, both natural and artificial, are widely used.\n\nAlthough harmonics are most often used on open strings (natural harmonics), occasionally a score will call for an artificial harmonic, produced by playing an overtone on an already stopped string. As a performance technique, it is accomplished by using two fingers on the fingerboard, the first to shorten the string to the desired fundamental, with the second touching the node corresponding to the appropriate harmonic. On fretted instruments, such as an electric guitar, the performer can look at the frets to determine where to stop the string and where to touch the node. On unfretted instruments, such as the violin and related instruments, playing artificial harmonics is an advanced technique, as it requires the performer to find two precise locations on the same string.\n\nHarmonics may be either used or considered as the basis of just intonation systems. Composer Arnold Dreyblatt is able to bring out different harmonics on the single string of his modified double bass by slightly altering his unique bowing technique halfway between hitting and bowing the strings. Composer Lawrence Ball uses harmonics to generate music electronically.\n\n", "related": "\n- Aristoxenus\n- Electronic tuner\n- Formant\n- Fourier series\n- Guitar harmonic\n- Harmonics (electrical power)\n- Harmonic oscillator\n- Harmonic series (music)\n- Harmony\n- Pure tone\n- Pythagorean tuning\n- Scale of harmonics\n- Spherical harmonics\n- Stretched octave\n- Subharmonic\n- Xenharmonic music\n\n- Harmonics, partials and overtones from fundamental frequency\n- Discussion of Sciarrino's violin etudes and notation issues\n- Harmonics\n- Hear and see harmonics on a Piano\n"}
{"id": "53790076", "url": "https://en.wikipedia.org/wiki?curid=53790076", "title": "Conformon", "text": "Conformon\n\nFrom a biological standpoint, the goal-directed molecular motions inside living cells are carried out by biopolymers acting like molecular machines (e.g. myosin, RNA/DNA polymerase, ion pumps, etc.). These molecular machines are driven by conformons, that is sequence-specific mechanical strains generated by free energy released in chemical reactions or stress induced destabilisations in supercoiled biopolymer chains. Therefore, conformons can be defined as packets of conformational energy generated from substrate binding or chemical reactions and confined within biopolymers.\n\nOn the other hand, from a physics standpoint, the conformon is a localization of elastic and electronic energy which may propagate in space with or without dissipation. The mechanism which involves dissipationless propagation is a form of molecular superconductivity. On quantum mechanical level both elastic/vibrational and electronic energy can be quantised, therefore the conformon carries a fixed portion of energy. This has led to the definition of quantum of conformation (shape).\n", "related": "NONE"}
{"id": "784621", "url": "https://en.wikipedia.org/wiki?curid=784621", "title": "Many-body theory", "text": "Many-body theory\n\nMany-body theory (or many-body physics) is an area of physics which provides the framework for understanding the collective behavior of large numbers of interacting particles, often on the order of Avogadro's number. In general terms, many-body theory deals with effects that manifest themselves only in systems containing large numbers of constituents. While the underlying physical laws that govern the motion of each individual particle may (or may not) be simple, the study of the collection of particles can be extremely complex. In some cases emergent phenomena may arise which bear little resemblance to the underlying elementary laws.\n\nMany-body theory plays a central role in condensed matter physics.\n\n", "related": "\n- Many-body problem\n- Fock space\n- Dynamical mean field theory\n- Collective behavior in social sciences\n"}
{"id": "55503653", "url": "https://en.wikipedia.org/wiki?curid=55503653", "title": "Camelback potential", "text": "Camelback potential\n\nA camelback potential is potential energy curve that looks like a normal distribution with a distinct dip where the peak would be, so named because it resembles the humps on a camel's back. The term was applied to a configuration of a superconducting quantum interference device in 2009, and to an arrangement of magnets in 2014.\n\nThe latter system consists of two parallel diametric cylindrical magnets, that is, magnets that are magnetized perpendicular to their axis, with the north and south poles located on the curved surface as opposed to either end. When a diamagnetic rod (usually graphite) is placed between the magnets, it will remain in place and move back and forth in harmonic motion when disturbed. This arrangement, also known as a \"PDL trap\" for \"parallel dipole line\", was the subject of the 2017 International Physics Olympiad.\n\nIn the magnetic system, the camelback potential effect only occurs when the length of the diamagnetic rod is between two critical lengths. Below the minimum length, the magnet is hypothesized to align with magnetic field lines, hence not maintaining its orientation and touching the magnet. The maximum length is limited by the distance between the peaks of the camelback humps; thus, a rod longer than that will be unstable and fall out of the trap. Both the radius and the length of the rod determine the damping of the system. The damping is primarily due to Stokes drag, as damping is non-observable under vacuum.\n\nPossible practical uses of the concept include being a platform for custom-designed 1D potentials, a highly sensitive force-distance transducer or a trap for semiconductor nanowires.\n", "related": "NONE"}
{"id": "3339367", "url": "https://en.wikipedia.org/wiki?curid=3339367", "title": "Elitzur–Vaidman bomb tester", "text": "Elitzur–Vaidman bomb tester\n\nThe Elitzur–Vaidman bomb-tester is a quantum mechanics thought experiment that uses interaction-free measurements to verify that a bomb is functional without having to detonate it. It was conceived in 1993 by Avshalom Elitzur and Lev Vaidman. Since their publication, real-world experiments have confirmed that their theoretical method works as predicted.\n\nThe bomb tester takes advantage of two characteristics of elementary particles, such as photons or electrons: nonlocality and wave-particle duality. By placing the particle in a quantum superposition, it is possible for the experiment to verify that the bomb works \"without\" triggering its detonation, although there is still a 50% chance that the bomb will detonate in the effort.\n\nThe bomb test is an interaction-free measurement. The idea of getting information about an object without interacting with it is not a new one. For example, there are two boxes, one of which contains something, the other of which contains nothing. If you open one box and see nothing, you know that the other contains something, without ever opening it.\n\nThis experiment has its roots in the double-slit experiment and other, more complex concepts which inspired it, including Schrödinger's cat, and Wheeler's delayed-choice experiment. The behavior of elementary particles is very different from what we experience in our macroscopic world. Their observed behavior can be that of a wave or of a particle (see wave–particle duality), their wave-like behavior implies what is called \"superposition\".\nIn this state, some properties of the particle, for example, its location, are not definite. While in a superposition, any and all possibilities are equally real. So, if the particle could feasibly exist in more than one location, in certain senses that are experimentally useful, it exists in all of them simultaneously. The particle's wave can later be \"collapsed\" by observing it, at which time its location (or other measured property) at the moment of observation is definite. Information can then be gleaned not only about the actual state of the particle, but also other states or locations in which it \"existed\" before the collapse. This gleaning of information is possible even if the particle was never factually in any of the particular states or locations that are of interest.\n\nConsider a collection of light-sensitive bombs, of which some are duds. When their triggers detect any light, even a single photon, the light is absorbed and the bomb explodes. The triggers on the dud bombs have no sensor, so the photon cannot be absorbed. Thus, the dud bomb will not detect the photon and will not detonate. Is it possible to determine which bombs are functional and which are duds without detonating all of the live ones?\n\n- A light-sensitive bomb: it is not known whether it is live or a dud.\n- A photon emitter: it produces a single photon for the purposes of the experiment.\n- A photon: after being emitted, it travels through the box below.\n- A \"box\" which contains:\n- An initial half-silvered mirror: The photon enters the box when it encounters this \"beam splitter\". The photon will either pass through the mirror and travel along the \"lower path\" inside the box, or be reflected at a 90-degree angle and travel along the box's \"upper path\".\n- The bomb in question: The bomb is placed inside the box beforehand on the \"lower path\". If the bomb is live and comes into contact with a photon, it will detonate and destroy itself and the photon. If, however, the bomb is a dud, the photon passes it by and continues on its way along the lower path.\n- A pair of ordinary mirrors: One mirror is located on each beam path. They are positioned to redirect the photon so that the two paths intersect one another at the same position as the second beam splitter.\n- A second beam splitter: Identical to the initial one. This beam splitter is positioned opposite the first, at the intersection between the lower path and upper path (after they have been redirected by the ordinary mirrors), at the exit of the box.\n- A pair of photon detectors: Located outside the box, they are aligned with the second beam-splitter. The photon can be detected at either or neither, but never both.\n\nA superposition in the bomb tester is created with an angled half-silvered mirror, which allows a photon to either pass through it, or be reflected off it at a 90-degree angle (see figure 3). There is equal probability it will do either. The photon enters a superposition, in which it does both. The single particle both passes through, and is reflected off the half-silvered mirror. From that moment on, the single photon exists in two different locations.\n\nAlong both the upper and lower path, the particle will encounter an ordinary mirror, positioned to redirect the two routes toward one another. They then intersect at a second half-silvered mirror. On the other side, a pair of detectors are placed such that the photon can be detected by either detector, but never by both. It is also possible that it will not be detected by either. Based on this outcome, with a live bomb, there is a 50% chance it will explode, a 25% chance it will be identified as good without exploding and a 25% chance there will be no result.\n\nA light-sensitive bomb is placed along the lower path. If the bomb is good, when a photon arrives, it will explode and both will be destroyed. If it is a dud, the photon will pass by unaffected (see figure 4). To understand how this experiment works, it is important to know that the bomb is a kind of observer and that this encounter is a kind of observation. It can therefore collapse the photon's superposition, in which the photon is travelling along both the upper and lower paths. When it reaches the live bomb, or the detectors, however, it can only have been on one or the other. But, like the radioactive material in the box with Schrödinger's famous cat, upon its encounter with the half-silvered mirror at the beginning of the experiment, the photon, paradoxically does and does not interact with the bomb. According to the authors, the bomb both explodes and does not explode. This is only in the case of a live bomb, however. In any event, once observed by the detectors, it will have only traveled one of the paths.\n\nWhen two waves collide, the process by which they affect each other is called interference. They can either strengthen each other by \"constructive interference\", or weaken each other by \"destructive interference\". This is true whether the wave is in water, or a single photon in a superposition. So even though there is only one photon in the experiment, because of its encounter with the half-silvered mirror, it acts like two. When \"it\" or \"they\" are reflected off the ordinary mirrors, it will interfere with itself as if it were two different photons. \"But that is only true if the bomb is a dud.\" A live bomb will absorb the photon when it explodes and there will be no opportunity for the photon to interfere with itself.\n\nWhen it reaches the second half-silvered mirror, if the photon in the experiment is behaving like a particle (in other words, if it is not in a superposition), then it has a fifty-fifty chance it will pass through or be reflected and be detected by one or the other detector. \"But that is only possible if the bomb is live.\" If the bomb \"observed\" the photon, it detonated and destroyed the photon on the lower path, therefore only the photon that takes the upper path will be detected, either at Detector C or Detector D.\n\nDetector D is the key to confirming that the bomb is live.\n\nThe two detectors and the second half-silvered mirror are precisely aligned with one another. Detector C is positioned to detect the particle if the bomb is a dud and the particle traveled both paths in its superposition and then constructively interfered with itself. Detector D is positioned to detect the photon only in the event of destructive interference—an impossibility (see figure 6). In other words, if the photon is in a superposition at the time it arrives at the second half-silvered mirror, it will always arrive at detector C and never at detector D.\n\nIf the bomb is live, there is a 50/50 chance that the photon took the upper path. If it \"factually\" did so, then it \"counter-factually\" took the lower path (see figure 7). That counter-factual event destroyed that photon and left only the photon on the upper path to arrive at the second half-silvered mirror. At which point it will, again, have a 50/50 chance of passing through it or being reflected off it, and, subsequently, it will be detected at either of the two detectors with the same probability. This is what makes it possible for the experiment to verify the bomb is live without actually blowing it up.\n\nWith a live bomb, there can be three possible outcomes:\n\n1. No photon was detected (50% chance).\n2. The photon was detected at C (25% chance).\n3. The photon was detected at D (25% chance).\n\nThese correspond with the following conditions of the bomb being tested:\n\n1. No photon was detected: The bomb exploded and destroyed the photon before it could be detected. This is because the photon in fact took the lower path and triggered the bomb, destroying itself in the process. There is a 50% chance that this will be the outcome if the bomb is live.\n2. The photon was detected at C: This will always be the outcome if a bomb is a dud, however, there is a 25% chance that this will be the outcome if the bomb is live. If the bomb is a dud, this is because the photon remained in its superposition until it reached the second half-silvered mirror and constructively interfered with itself. If the bomb is live, this is because the photon in fact took the upper path and reflected off the second half-silvered mirror.\n3. The photon was detected at D: The bomb is live but unexploded. That is because the photon in fact took the upper path and passed through the second half-silvered mirror, something possible only because there was no photon from the lower path with which it could interfere. \"This is the only way that a photon can ever be detected at D.\" If this is the outcome, the experiment has successfully verified that the bomb is live despite the fact that the photon never \"factually\" encountered the bomb itself. There is a 25% chance that this will be the outcome if the bomb is live.\n\nIf the result is 2, the experiment is repeated. If the photon continues to be observed at C and the bomb does not explode, it can eventually be concluded that the bomb is a dud.\n\nWith this process 25% of live bombs can be identified without being detonated, 50% will be detonated and 25% remain uncertain. By repeating the process with the uncertain ones, the ratio of identified non-detonated live bombs approaches 33% of the initial population of bombs. See below for a modified experiment that can identify the live bombs with a yield rate approaching 100%.\n\nThe authors state that the ability to obtain information about the bomb's functionality without ever \"touching\" it appears to be a paradox. That, they argue, is based on the assumption that there is only a single \"real\" result. But according to the many-worlds interpretation, each possible state of a particle's superposition is real. The authors therefore argue that the particle does actually interact with the bomb and it does explode, just not in our \"world\".\n\nIn 1994, Anton Zeilinger, Paul Kwiat, Harald Weinfurter, and Thomas Herzog actually performed an equivalent of the above experiment, proving interaction-free measurements are indeed possible.\n\nIn 1996, Kwiat \"et al.\" devised a method, using a sequence of polarising devices, that efficiently increases the yield rate to a level arbitrarily close to one. The key idea is to split a fraction of the photon beam into a large number of beams of very small amplitude and reflect all of them off the mirror, recombining them with the original beam afterwards.\nIt can also be argued that this revised construction is simply equivalent to a resonant cavity and the result looks much less shocking in this language; see Watanabe and Inoue (2000).\n\nIn 2016, Carsten Robens, Wolfgang Alt, Clive Emary, Dieter Meschede, and Andrea Alberti demonstrated that the Elitzur–Vaidman bomb testing experiment can be recast in a rigorous test of the macro-realistic worldview based on the violation of the Leggett–Garg inequality using ideal negative measurements. In their experiment they perform the bomb test with a single atom trapped in a polarization-synthesized optical lattice. This optical lattice enables interaction-free measurements by entangling the spin and position of atoms.\n\n", "related": "\n- Counterfactual definiteness\n- Renninger negative-result experiment\n\n- Z. Blanco-Garcia and O. Rosas-Ortiz, Interaction-Free Measurements of Optical Semitransparent Objects, J. Phys.: Conf. Ser. 698:012013, 2016\n- A. Peruzzo, P. Shadbolt, N. Brunner, S. Popescu and J.L. O'Brien, A Quantum Delayed-Choice Experiment, Science 338:634–637, 2012\n- F. Kaiser, T. Coudreau, P. Milman, D.B. Ostroswsky and S. Tanzilli, Entanglement-Enabled Delayed-Choice Experiment Science 338:637–640, 2012\n\n- Penrose, R. (2004). \"The Road to Reality: A Complete Guide to the Laws of Physics\". Jonathan Cape, London.\n"}
{"id": "55823152", "url": "https://en.wikipedia.org/wiki?curid=55823152", "title": "Diffeomorphometry", "text": "Diffeomorphometry\n\nDiffeomorphometry is the metric study of imagery, shape and form in the discipline of computational anatomy (CA) in medical imaging. The study of images in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2, in which images formula_3 can be dense scalar magnetic resonance or computed axial tomography images. For deformable shapes these are the collection of manifolds formula_4, points, curves and surfaces. The diffeomorphisms move the images and shapes through the orbit according to formula_5 which are defined as the group actions of computational anatomy.\n\nThe orbit of shapes and forms is made into a metric space by inducing a metric on the group of diffeomorphisms. The study of metrics on groups of diffeomorphisms and the study of metrics between manifolds and surfaces has been an area of significant investigation. In Computational anatomy, the diffeomorphometry metric measures how close and far two shapes or images are from each other. Informally, the metric is constructed by defining a flow of diffemorphisms formula_6 which connect the group elements from one to another, so for formula_7 then formula_8. The metric between two coordinate systems or diffeomorphisms is then the shortest length or geodesic flow connecting them. The metric on the space associated to the geodesics is given byformula_9. The metrics on the orbits formula_10 are inherited from the metric induced on the diffeomorphism group.\n\nThe group formula_1 is thusly made into a smooth Riemannian manifold with Riemannian metric formula_12 associated to the tangent spaces at all formula_13. The Riemannian metric satisfies at every point of the manifold formula_14 there is an inner product inducing the norm on the tangent space formula_15 that varies smoothly across formula_16.\n\nOftentimes, the familiar Euclidean metric is not directly applicable because the patterns of shapes and images don't form a vector space. In the Riemannian orbit model of Computational anatomy, diffeomorphisms acting on the forms formula_17 don't act linearly. There are many ways to define metrics, and for the sets associated to shapes the Hausdorff metric is another. The method used to induce the Riemannian metric is to induce the metric on the orbit of shapes by defining it in terms of the metric length between diffeomorphic coordinate system transformations of the flows. Measuring the lengths of the geodesic flow between coordinates systems in the orbit of shapes is called diffeomorphometry.\n\nThe diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_18, generated via the ordinary differential equation\n\nwith the Eulerian vector fields formula_19 in formula_20 for formula_21. The inverse for the flow is given by\nformula_22\nand the formula_23 Jacobian matrix for flows in formula_24 given as formula_25\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_20 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_27 using the Sobolev embedding theorems so that each element formula_28 has 3-square-integrable derivatives thusly implies formula_27 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\n\nShapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_30, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_31, with for charts representing sub-manifolds denoted as formula_32.\n\nThe orbit of shapes and forms in Computational Anatomy are generated by the group action formula_33 , formula_34. These are made into a Riemannian orbits by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_35 in the group of diffeomorphisms\n\nwith the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_27. We model formula_38 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operator formula_39, where formula_40 is the dual-space. In general, formula_41 is a generalized function or distribution, the linear form associated to the inner-product and norm for generalized functions are interpreted by integration by parts according to for formula_42,\n\nWhen formula_44, a vector density, formula_45\n\nThe differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative. The Sobolev embedding theorem arguments were made in demonstrating that 1-continuous derivative is required for smooth flows. The Green's operator generated from the Green's function(scalar case) associated to the differential operator smooths.\n\nFor proper choice of formula_46 then formula_47 is an RKHS with the operator formula_48. The Green's kernels associated to the differential operator smooths since for controlling enough derivatives in the square-integral sense the kernel formula_49 is continuously differentiable in both variables implying\n\n(I,J)=\\inf_{\\phi \\in \\operatorname{Diff}_V: \\phi \\cdot I = J } d_{\\operatorname{Diff}_V}(id,\\phi) \\ ;\n\nThe distance on shapes and forms, formula_51,\n\nFor calculating the metric, the geodesics are a dynamical system, the flow of coordinates formula_52 and the control the vector field formula_53 related via formula_54 The Hamiltonian view\n\nThe Pontryagin maximum principle gives the Hamiltonian formula_59\nThe optimizing vector field formula_60 with dynamics formula_61. Along the geodesic the Hamiltonian is constant:\nformula_62. The metric distance between coordinate systems connected via the geodesic determined by the induced distance between identity and group element:\n\nFor landmarks, formula_64, the Hamiltonian momentum\n\nwith Hamiltonian dynamics taking the form\n\nwith\nThe metric between landmarks formula_68\n\nThe dynamics associated to these geodesics is shown in the accompanying figure.\n\nFor surfaces, the Hamiltonian momentum is defined across the surface has Hamiltonian\n\nand dynamics\n\nFor volumes the Hamiltonian\n", "related": "NONE"}
{"id": "56310192", "url": "https://en.wikipedia.org/wiki?curid=56310192", "title": "Self-propulsion", "text": "Self-propulsion\n\nSelf-propulsion is the autonomous displacement of nano-, micro- and macroscopic natural and artificial objects, containing their own means of motion. Self-propulsion is driven mainly by interfacial phenomena. Various mechanisms of self-propelling have been introduced and investigated, which exploited phoretic effects, gradient surfaces, breaking the wetting symmetry of a droplet on a surface, the Leidenfrost effect, the self-generated hydrodynamic and chemical fields originating from the geometrical confinements, and soluto- and thermo-capillary Marangoni flows. Self-propelled system demonstrate a potential as micro-fluidics devices and micro-mixers. Self-propelled liquid marbles have been demonstrated.\n", "related": "NONE"}
{"id": "50311973", "url": "https://en.wikipedia.org/wiki?curid=50311973", "title": "Microfluidic cell culture", "text": "Microfluidic cell culture\n\nMicrofluidic cell culture integrates knowledge from biology, biochemistry, engineering, and physics to develop devices and techniques for culturing, maintaining, analyzing, and experimenting with cells at the microscale. It merges microfluidics, a set of technologies used for the manipulation of small fluid volumes (μL, nL, pL) within artificially fabricated microsystems, and cell culture, which involves the maintenance and growth of cells in a controlled laboratory environment. Microfluidics has been used for cell biology studies as the dimensions of the microfluidic channels are well suited for the physical scale of cells. For example, eukaryotic cells have linear dimensions between 10-100 μm which falls within the range of microfluidic dimensions. A key component of microfluidic cell culture is being able to mimic the cell microenvironment which includes soluble factors that regulate cell structure, function, behavior, and growth. Another important component for the devices is the ability to produce stable gradients that are present \"in vivo\" as these gradients play a significant role in understanding chemotactic, durotactic, and haptotactic effects on cells.\n\nSome considerations for microfluidic devices relating to cell culture include:\n- fabrication material (e.g., polydimethylsiloxane (PDMS), polystyrene)\n- culture region geometry\n- control system for delivering and removing media when needed using either passive methods (e.g., gravity-driven flow, capillary pumps, or Laplace pressure based ‘passive pumping’) or a flow-rate controlled device (i.e., perfusion system)\nFabrication material is crucial as not all polymers are biocompatible, with some materials such as PDMS causing undesirable adsorption or absorption of small molecules. Additionally, uncured PDMS oligomers can leach into the cell culture media, which can harm the microenvironment. As an alternative to commonly used PDMS, there have been advances in the use of thermoplastics (e.g., polystyrene) as a replacement material.\n\nSpatial organization of cells in microscale devices largely depends on the culture region geometry for cells to perform functions \"in vivo\". For example, long, narrow channels may be desired to culture neurons. The perfusion system chosen might also affect the geometry chosen. For example, in a system that incorporates syringe pumps, channels for perfusion inlet, perfusion outlet, waste, and cell loading would need to be added for the cell culture maintenance. Perfusion in microfluidic cell culture is important to enable long culture periods on-chip and cell differentiation.\n\nOther critical aspects for controlling the microenvironment include: cell seeding density, reduction of air bubbles as they can rupture cell membranes, evaporation of media due to an insufficiently humid environment, and cell culture maintenance (i.e. regular, timely media changes).\n\nCell's health is defined as the collective equilibrium activities of essential and specialized cellular processes; while a cell stressor is defined as a stimulus that causes excursion from its equilibrium state. Hence, cell health may be perturbed within microsystems based on platform design or operating conditions. Exposure to stressors within microsystems can impact cells through direct and indirect ways. Therefore, it is important to design the microfluidics system for cell culture in a manner that minimizes cell stress situations. For example, by minimizing cell suspension, by avoiding abrupt geometries (which tend to favor bubble formation), designing higher and wider channels (to avoid shear stress), avoid thermosensitive hydrogels...\n\nSome of the major advantages of microfluidic cell culture include reduced sample volumes (especially important when using primary cells, which are often limited) and the flexibility to customize and study multiple microenvironments within the same device. A reduced cell population can also be used in a microscale system (e.g., a few hundred cells) in comparison to macroscale culture systems (which often require 10 – 10 cells); this can make studying certain cell-cell interactions more accessible. These reduced cell numbers make studying non-dividing or slow dividing cells (e.g., stem cells) easier than traditional culture methods (e.g., flasks, petri dishes, or well plates) due to the smaller sample volumes. Given the small dimensions in microfluidics, laminar flow can be achieved, allowing manipulations with the culture system to be done easily without affecting other culture chambers. Laminar flow is also useful as is it mimics \"in vivo\" fluid dynamics more accurately, often making microscale culture more relevant than traditional culture methods. Compartmentalized microfluidic cultures have also been combined with live cell calcium imaging, where depolarizing stimuli have been delivered to the peripheral terminals of neurons, and calcium responses recorded in the cell body. This technique has demonstrated a stark difference in the sensitivity of the peripheral terminals compared to the neuronal cell body to certain stimuli such as protons. This gives an excellent example as to why it is so important to study the peripheral terminals in isolation using microfluidic cell culture devices.\n\nTraditional two-dimensional (2D) cell culture is cell culture that takes place on a flat surface, e.g. the bottom of a well-plate, and is known as the conventional method. While these platforms are useful for growing and passaging cells to be used in subsequent experiments, they are not ideal environments to monitor cell responses to stimuli as cells cannot freely move or perform functions as observed \"in vivo\" that are dependent on cell-extracellular matrix material interactions. To address this issue many methods have been developed to create a three-dimensional (3D) native cell environment. One example of a 3D method is the hanging drop, where a droplet with growing cells is suspended and hangs downwards, which allows cells to grow around and atop of one another, forming a spheroid. The hanging drop method has been used to culture tumor cells but is limited to the geometry of a sphere. Since the advent of poly(dimethylsiloxane) (PDMS) microfluidic device fabrication through soft lithography microfluidic devices have progressed and have proven to be very beneficial for mimicking a natural 3D environment for cell culture.\n\nMicrofluidic devices make possible the study of a single cell to a few hundred cells in a 3D environment. Comparatively, macroscopic 2D cultures have 10 to 10 cells on a flat surface. Microfluidics also allow for chemical gradients, the continuous flow of fresh media, high through put testing, and direct output to analytical instruments. Many microfluidic systems employ the use of hydrogels as the extracellular matrix (ECM) support which can be modulated for fiber thickness and pore size and have been demonstrated to allow the growth of cancer cells. Gel free 3D cell cultures have been developed to allow cells to grow in either a cell dense environment or an ECM poor environment. Although these devices have proven very useful, there are certain disadvantages such as cells sticking to the PDMS surface, small molecules diffusing into the PDMS, and unreacted PDMS polymers washing into cell culture media.\n\nThe use of 3D cell cultures in microfluidic devices has led to a field of study called organ-on-a-chip. The first report of these types of microfluidic cultures was used to study the toxicity of naphthalene metabolites on the liver and lung (Viravaidya et al.). These devices can grow a stripped-down version of an organ-like system that can be used to understand many biological processes. By adding an additional dimension, more advanced cell architectures can be achieved, and cell behavior is more representative of \"in vivo\" dynamics; cells can engage in enhanced communication with neighboring cells and cell-extracellular matrix interactions can be modeled. In these devices, chambers or collagen layers containing different cell types can interact with one another for multiple days while various channels deliver nutrients to the cells. An advantage of these devices is that tissue function can be characterized and observed under controlled conditions (e.g., effect of shear stress on cells, effect of cyclic strain or other forces) to better understand the overall function of the organ. While these 3D models ofter better model organ function on a cellular level compared with 2D models, there are still challenges. Some of the challenges include: imaging of the cells, control of gradients in static models (i.e., without a perfusion system), and difficulty recreating vasculature. Despite these challenges, 3D models are still used as tools for studying and testing drug responses in pharmacological studies. In recent years, there are microfluidic devices reproducing the complex \"in vivo\" microvascular network. Organs-on-a-chip have also been used to replicate very complex systems like lung epithelial cells in an exposed airway and provides valuable insight for how multicellular systems and tissues function \"in vivo.\" These devices are able to create a physiologically realistic 3D environment, which is desirable as a tool for drug screening, drug delivery, cell-cell interactions, tumor metastasis etc. In one study, researchers grew tumor cells and tested the drug delivery of cis platin, resveratrol, tirapazamine (TPZ) and then measured the effects the drugs have on cell viability.\n\n", "related": "\n- Microphysiometry\n"}
{"id": "24489", "url": "https://en.wikipedia.org/wiki?curid=24489", "title": "Outline of physics", "text": "Outline of physics\n\nThe following outline is provided as an overview of and topical guide to physics:\n\nPhysics – natural science that involves the study of matter and its motion through spacetime, along with related concepts such as energy and force. More broadly, it is the general analysis of nature, conducted in order to understand how the universe behaves.\n\nPhysics can be described as all of the following:\n- An academic discipline – one with academic departments, curricula and degrees; national and international societies; and specialized journals.\n- A scientific field (a branch of science) – widely recognized category of specialized expertise within science, and typically embodies its own terminology and nomenclature. Such a field will usually be represented by one or more scientific journals, where peer-reviewed research is published.\n- A natural science – one that seeks to elucidate the rules that govern the natural world using empirical and scientific method.\n-  A physical science – one that studies non-living systems.\n-  A biological science – one that studies the role of physical processes in living organisms. \"See Outline of biophysics.\"\n\n- Astronomy – studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation).\n- Astrodynamics – application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft.\n- Astrometry – branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies.\n- Astrophysics – study of the physical aspects of celestial objects\n- Celestial mechanics - the branch of theoretical astronomy that deals with the calculation of the motions of celestial objects such as planets.\n- Extragalactic astronomy – branch of astronomy concerned with objects outside our own Milky Way Galaxy\n- Galactic astronomy – study of our own Milky Way galaxy and all its contents.\n- Physical cosmology – study of the largest-scale structures and dynamics of the universe and is concerned with fundamental questions about its formation and evolution.\n- Planetary science – scientific study of planets (including Earth), moons, and planetary systems, in particular those of the Solar System and the processes that form them.\n- Stellar astronomy – natural science that deals with the study of celestial objects (such as stars, planets, comets, nebulae, star clusters and galaxies) and phenomena that originate outside the atmosphere of Earth (such as cosmic background radiation)\n- Atmospheric physics – study of the application of physics to the atmosphere\n- Atomic, molecular, and optical physics – study of how matter and light interact\n- Optics – branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it.\n- Biophysics – interdisciplinary science that uses the methods of physics to study biological systems\n- Neurophysics – branch of biophysics dealing with the nervous system.\n- Polymer physics – field of physics that studies polymers, their fluctuations, mechanical properties, as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively.\n- Quantum biology - application of quantum mechanics to biological phenomenon.\n- Chemical physics – branch of physics that studies chemical processes from the point of view of physics.\n- Computational physics – study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists.\n- Condensed matter physics – study of the physical properties of condensed phases of matter.\n- Electricity – the study of electrical phenomena.\n- Electromagnetism – branch of science concerned with the forces that occur between electrically charged particles.\n- Geophysics – the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods\n- Magnetism – the study of physical phenomena that are mediated by magnetic field.\n- Mathematical physics – application of mathematics to problems in physics and the development of mathematical methods for such applications and for the formulation of physical theories.\n- Mechanics – branch of physics concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment.\n- Aerodynamics – study of the motion of air.\n- Biomechanics – study of the structure and function of biological systems such as humans, animals, plants, organs, and cells by means of the methods of mechanics.\n- Classical mechanics – one of the two major sub-fields of mechanics, which is concerned with the set of physical laws describing the motion of bodies under the action of a system of forces.\n-  Kinematics – branch of classical mechanics that describes the motion of points, bodies (objects) and systems of bodies (groups of objects) without consideration of the causes of motion.\n-   Homeokinetics - the physics of complex, self-organizing systems\n- Continuum mechanics – branch of mechanics that deals with the analysis of the kinematics and the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles.\n- Dynamics – study of the causes of motion and changes in motion\n- Fluid mechanics – study of fluids and the forces on them.\n-  Fluid statics – study of fluids at rest\n-  Fluid kinematics – study of fluids in motion\n-  Fluid dynamics – study of the effect of forces on fluid motion\n- Statics – branch of mechanics concerned with the analysis of loads (force, torque/moment) on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity.\n- Statistical mechanics – branch of physics which studies any physical system that has a large number of degrees of freedom.\n- Thermodynamics – branch of physical science concerned with heat and its relation to other forms of energy and work.\n- Nuclear physics – field of physics that studies the building blocks and interactions of atomic nuclei.\n- Particle physics – branch of physics that studies the properties and interactions of the fundamental constituents of matter and energy.\n- Psychophysics – quantitatively investigates the relationship between physical stimuli and the sensations and perceptions they affect.\n- Plasma physics – the study of plasma, a state of matter similar to gas in which a certain portion of the particles are ionized.\n- Quantum physics – branch of physics dealing with physical phenomena where the action is on the order of the Planck constant.\n- Relativity – theory of physics which describes the relationship between space and time.\n- General Relativity - the geometric theory of gravitation the current description of gravitation in modern physics.\n- Special Relativity - a theory that describes the propagation of matter and light at high speeds.\n\n- Other\n- Agrophysics – study of physics applied to agroecosystems\n- Soil physics – study of soil physical properties and processes.\n- Cryogenics – cryogenics is the study of the production of very low temperature (below −150 °C, −238 °F or 123K) and the behavior of materials at those temperatures.\n- Econophysics – interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics\n- Materials physics – use of physics to describe materials in many different ways such as force, heat, light and mechanics.\n- Vehicle dynamics – dynamics of vehicles, here assumed to be ground vehicles.\n\nHistory of physics – history of the physical science that studies matter and its motion through space-time, and related concepts such as energy and force\n- History of acoustics – history of the study of mechanical waves in solids, liquids, and gases (such as vibration and sound)\n- History of agrophysics – history of the study of physics applied to agroecosystems\n- History of soil physics – history of the study of soil physical properties and processes.\n- History of astrophysics – history of the study of the physical aspects of celestial objects\n- History of astronomy – history of the studies the universe beyond Earth, including its formation and development, and the evolution, physics, chemistry, meteorology, and motion of celestial objects (such as galaxies, planets, etc.) and phenomena that originate outside the atmosphere of Earth (such as the cosmic background radiation).\n- History of astrodynamics – history of the application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft.\n- History of astrometry – history of the branch of astronomy that involves precise measurements of the positions and movements of stars and other celestial bodies.\n- History of cosmology – history of the discipline that deals with the nature of the Universe as a whole.\n- History of extragalactic astronomy – history of the branch of astronomy concerned with objects outside our own Milky Way Galaxy\n- History of galactic astronomy – history of the study of our own Milky Way galaxy and all its contents.\n- History of physical cosmology – history of the study of the largest-scale structures and dynamics of the universe and is concerned with fundamental questions about its formation and evolution.\n- History of planetary science – history of the scientific study of planets (including Earth), moons, and planetary systems, in particular those of the Solar System and the processes that form them.\n- History of stellar astronomy – history of the natural science that deals with the study of celestial objects (such as stars, planets, comets, nebulae, star clusters and galaxies) and phenomena that originate outside the atmosphere of Earth (such as cosmic background radiation)\n- History of atmospheric physics – history of the study of the application of physics to the atmosphere\n- History of atomic, molecular, and optical physics – history of the study of how matter and light interact\n- History of biophysics – history of the study of physical processes relating to biology\n- History of medical physics – history of the application of physics concepts, theories and methods to medicine.\n- History of neurophysics – history of the branch of biophysics dealing with the nervous system.\n- History of chemical physics – history of the branch of physics that studies chemical processes from the point of view of physics.\n- History of computational physics – history of the study and implementation of numerical algorithms to solve problems in physics for which a quantitative theory already exists.\n- History of condensed matter physics – history of the study of the physical properties of condensed phases of matter.\n- History of cryogenics – history of the cryogenics is the study of the production of very low temperature (below −150 °C, −238 °F or 123K) and the behavior of materials at those temperatures.\n- Dynamics – history of the study of the causes of motion and changes in motion\n- History of econophysics – history of the interdisciplinary research field, applying theories and methods originally developed by physicists in order to solve problems in economics\n- History of electromagnetism – history of the branch of science concerned with the forces that occur between electrically charged particles.\n- History of geophysics – history of the physics of the Earth and its environment in space; also the study of the Earth using quantitative physical methods\n- History of materials physics – history of the use of physics to describe materials in many different ways such as force, heat, light and mechanics.\n- History of mathematical physics – history of the application of mathematics to problems in physics and the development of mathematical methods for such applications and for the formulation of physical theories.\n- History of mechanics – history of the branch of physics concerned with the behavior of physical bodies when subjected to forces or displacements, and the subsequent effects of the bodies on their environment.\n- History of biomechanics – history of the study of the structure and function of biological systems such as humans, animals, plants, organs, and cells by means of the methods of mechanics.\n- History of classical mechanics – history of the one of the two major sub-fields of mechanics, which is concerned with the set of physical laws describing the motion of bodies under the action of a system of forces.\n- History of continuum mechanics – history of the branch of mechanics that deals with the analysis of the kinematics and the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles.\n- History of fluid mechanics – history of the study of fluids and the forces on them.\n- History of quantum mechanics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant.\n- History of thermodynamics – history of the branch of physical science concerned with heat and its relation to other forms of energy and work.\n- History of nuclear physics – history of the field of physics that studies the building blocks and interactions of atomic nuclei.\n- History of optics – history of the branch of physics which involves the behavior and properties of light, including its interactions with matter and the construction of instruments that use or detect it.\n- History of particle physics – history of the branch of physics that studies the existence and interactions of particles that are the constituents of what is usually referred to as matter or radiation.\n- History of psychophysics – history of the quantitative investigations of the relationship between physical stimuli and the sensations and perceptions they affect.\n- History of plasma physics – history of the state of matter similar to gas in which a certain portion of the particles are ionized.\n- History of polymer physics – history of the field of physics that studies polymers, their fluctuations, mechanical properties, as well as the kinetics of reactions involving degradation and polymerisation of polymers and monomers respectively.\n- History of quantum physics – history of the branch of physics dealing with physical phenomena where the action is on the order of the Planck constant.\n- History of theory of relativity - history of the special and the general theory of relativity\n- History of special relativity - history of the study of the relationship between space and time in the absence of gravity\n- History of general relativity - history of the non-quantum theory of gravity\n- History of statics – history of the branch of mechanics concerned with the analysis of loads (force, torque/moment) on physical systems in static equilibrium, that is, in a state where the relative positions of subsystems do not vary over time, or where components and structures are at a constant velocity.\n- History of solid state physics – history of the study of rigid matter, or solids, through methods such as quantum mechanics, crystallography, electromagnetism, and metallurgy.\n- History of vehicle dynamics – history of the dynamics of vehicles, here assumed to be ground vehicles.\n\nPhysics – branch of science that studies matter and its motion through space and time, along with related concepts such as energy and force. Physics is one of the \"fundamental sciences\" because the other natural sciences (like biology, geology etc.) deal with systems that seem to obey the laws of physics. According to physics, the physical laws of matter, energy and the fundamental forces of nature govern the interactions between particles and physical entities (such as planets, molecules, atoms or the subatomic particles). Some of the basic pursuits of physics, which include some of the most prominent developments in modern science in the last millennium, include:\n\n- Describing the nature, measuring and quantifying of bodies and their motion, dynamics etc.\n- Newton's laws of motion\n- Mass, force and weight (Mass versus weight)\n- Momentum and conservation of energy\n- Gravity, theories of gravity\n- Energy, work, and their relationship\n- Motion, position, and energy\n- Different forms of Energy, their inter-conversion and the inevitable loss of energy in the form of heat (Thermodynamics)\n- Energy conservation, conversion, and transfer.\n- Energy source the transfer of energy from one source to work in another.\n- Kinetic molecular theory\n- Phases of matter and phase transitions\n- Temperature and thermometers\n- Energy and heat\n- Heat flow: conduction, convection, and radiation\n- The three laws of thermodynamics\n- The principles of waves and sound\n- The principles of electricity, magnetism, and electromagnetism\n- The principles, sources, and properties of light\n- Basic quantities\n- Acceleration\n- Electric charge\n- Energy\n- Entropy\n- Force\n- Length\n- Mass\n- Matter\n- Momentum\n- Potential energy\n- Space\n- Temperature\n- Time\n- Velocity\nGravity, light, physical system, physical observation, physical quantity, physical state, physical unit, physical theory, physical experiment\n\nTheoretical concepts\nMass–energy equivalence, particle, physical field, physical interaction, physical law, fundamental force, physical constant, wave\n\n- Measurement\n- SI units\n- Conversion of units\n- Length\n- Time\n- Mass\n- Density\n\n- Motion\n- Velocity\n- Speed\n- Acceleration\n- Constant acceleration\n- Newton's laws of motion\n\nPhysics\nThis is a list of the primary theories in physics, major subtopics, and concepts.\n\nIndex of physics articles\n- List of common physics notations\n- List of equations in classical mechanics\n- List of important publications in physics\n- List of laws in science\n- List of letters used in mathematics and science\n- List of noise topics\n- List of optical topics\n- List of physicists\n- List of physics journals\n- List of scientific units named after people\n- Index of wave articles\n- Variables commonly used in physics\n\n", "related": "\n- Elementary physics formulae\n- Glossary of classical physics\n- List of physics concepts in primary and secondary education curricula\n\n\n- AIP.org is the website of the American Institute of Physics\n- IOP.org is the website of the Institute of Physics\n- APS.org is the website of the American Physical Society\n- SPS National is the website of the American Society of Physics Students\n- CAP.ca is the website of the Canadian Association of Physicists\n- EPS.org is the website of the European Physical Society\n- Meta Institute for Computational Physics - Popular Talks\n- ScienceMathMastery - Compilation of YouTube Physics Courses\n- Physics | Channel | MIT Video\n- How to become a GOOD Theoretical Physicist, a website with outline of theoretical physics by Gerard 't Hooft\n- \"The Feynman Lectures on Physics\", 3 vols., free online, Caltech & The Feynman Lectures Website\n- Resource recommendations - List of freely available physics books - Physics Stack Exchange\n"}
{"id": "57243717", "url": "https://en.wikipedia.org/wiki?curid=57243717", "title": "Nonlinear frictiophoresis", "text": "Nonlinear frictiophoresis\n\nNonlinear frictiophoresis is the unidirectional drift of a particle in a medium caused by periodic driving force with zero mean. The effect is possible due to nonlinear dependence of the friction-drag force on the particle's velocity. It was discovered theoretically.,\nand is mainly known as nonlinear electrofrictiophoresis\nAt first glance, a periodic driving force with zero mean is able to entrain a particle into an oscillating movement without unidirectional drift, because integral momentum provided to the particle by the force is zero. The possibility of unidirectional drift can be recognized if one takes into account that the particle itself loses momentum through transferring it further to the medium it moves in/at. If the friction is nonlinear, then it may so happen that the momentum loss during movement in one direction does not equal to that in the opposite direction and this causes unidirectional drift. For this to happen, the driving force time-dependence must be more complicated than it is in a single sinusoidal harmonic.\n\nThe simplest case of friction-velocity dependence law is the\nStokes's one:\nwhere formula_2 is the friction/drag force applied to a particle moving with velocity formula_3 in a medium. The friction-velocity law (1) is observed for a slowly moving spherical particle in a Newtonian fluid. It is linear, see Fig. 1, and is not suitable for nonlinear frictiophoresis to take place. The characteristic property of the law (1) is that any, even a very small driving force is able to get particle moving. This is not the case for such media as Bingham plastic. For those media, it is necessary to apply some threshold force, formula_4, to get the particle moving. This kind of friction-velocity (dry friction) law has a jump discontinuity at formula_5:\n\nIt is nonlinear, see Fig. 2, and is used in this example.\n\nLet formula_7 denote the period of driving force. Chose a time\nvalue formula_8\nsuch that formula_9\nand two force values, formula_10, formula_11\nsuch that the following relations are\nsatisfied:\nThe periodic driving force formula_14\nused in this example is as follows:\nIt is clear that, due to (3), formula_14 has zero mean:\n\nSee also Fig. 3.\n\nFor the sake of simplicity, we consider here the physical situation when inertia may be neglected. The latter can be achieved if\nparticle's mass is small, velocity is low and friction is high. This conditions have to ensure that formula_18,\nwhere formula_19 is the relaxation time. In this situation, the particle driven with force (4) immediately starts moving with constant velocity \nformula_20 during interval\nformula_21\nand will immediately\nstop moving during interval formula_22, see Fig. 4.\n\nThis results in the\npositive mean velocity of unidirectional drift:\n\nAnalysis of possibility to get a nonzero drift by periodic force with\nzero integral has been made in\nThe dimensionless\nequation of motion for a particle driven by periodic force\nformula_14, formula_25, formula_26\nis as follows:\nwhere the friction/drag force\nformula_28 satisfies\nthe following:\nIt is proven in \nthat any solution to (5)\nsettles down onto periodic regime\nformula_30, formula_31, which has nonzero mean:\nalmost certainly, provided formula_14\nis not antiperiodic.\n\nFor formula_34, two cases of formula_14 have been considered explicitly:\n\n1. Saw-shaped driving force, see Fig. 5:\n\nIn this case, found in \nfirst order in formula_37 approximation to formula_30,\nformula_39, has the following mean value:\nThis estimate is made expecting formula_41.\n\n2. Two harmonics driving force,\nIn this case, the first order in formula_37 approximation\nhas the following mean value:\nThis value is maximized in formula_45, formula_46, keeping formula_47 constant. Interesting that the drift value depends on formula_45 and changes its direction twice as formula_45 spans over the interval formula_50. Another type of analysis, based on symmetry breaking suggests as well that a zero mean driving force is able to generate a directed drift.\n\n In applications, the nature of force formula_14 in (5), is usually electric, similar to forces acting during standard electrophoresis. The only differences are that the force is periodic and without constant component.\nFor the effect to show up, the dependence of friction/drag force on velocity must be nonlinear. This is the case for numerous substances known as non-Newtonian fluids. Among these are gels, and dilatant fluids, pseudoplastic fluids, liquid crystals.\n\nDedicated experiments\nhave determined formula_52 for a standard DNA ladder up to 1500 bp long in 1.5% agarose gel. The dependence found, see Fig. 6, supports the possibility of nonlinear frictiophoresis in such a system. Based on data in Fig. 6, an optimal time course for driving electric field with zero mean, formula_53, has been found in, which ensures maximal drift for 1500 b.p. long fragment,\nsee Fig. 7.\nThe effect of unidirectional drift caused by periodic force with zero integral value has a peculiar dependence on the time course of the force applied. See the previous section for examples. This offers a new dimension to a set of separation problems.\n\nIn the DNA fragments separation,\nzero mean periodic electric field is used in zero-integrated-field electrophoresis (ZIFE),\nwhere the field time dependence similar to that shown in Fig. 3 is used.\nThis allows to separate long fragments in agarose gel,\nnonseparable by standard constant field electrophoresis.\nThe long DNA geometry and its manner of movement in a gel,\nknown as reptation\ndo not allow to apply directly the consideration based on Eq. (5), above.\n\nIt was observed,\n\nthat under certain physical conditions\nthe mechanism described in Mathematical analysis section, above,\ncan be used for separation with respect to specific mass,\nlike particles made of isotopes of the same material.\n\nThe idea of organizing directed drift with zero mean periodic drive\nhave obtained further development for other configurations\nand other physical mechanism of nonlinearity.\n\nAn electric dipole\nrotating freely around formula_54-axis\nin a medium with nonlinear friction can be manipulated by\napplying electromagnetic wave polarized circularly along formula_55\nand composed of two harmonics. The equation of motion\nfor this system is as follows:\nwhere formula_57 is the torque acting on the dipole due to\ncircular wave: \nwhere formula_59 is the dipole moment component orthogonal\nto formula_54-axis\nand formula_61 defines the dipole direction in the formula_62 plane. By choosing proper\nphase shift formula_45 in (6) it is possible to\norient the dipole in any desired direction, formula_64.\nThe direction formula_64 is\nattained due to angular directed drift, which becomes zero when\nformula_66.\nA small detuning between the first and second harmonic in (6) results in continuous rotational drift.\n\nIf a particle undergoes a directed drift while moving freely in accordance\nwith Eq. (5), then it drifts similarly if a shallow enough\npotential field formula_67 is imposed. Equation of motion in that case is:\nwhere formula_69 is the force due to potential field. The drift continues until a steep enough region in the course of formula_67 is met, which is able to stop the drift. This kind of behavior, as rigorous mathematical analysis shows, \nresults in modification of formula_67 by adding a linear in formula_72 term. This may change the formula_67 qualitatively, by, e.g. changing the number of equilibrium points, see Fig. 8. The effect may be essential during high frequency\nelectric field acting on biopolymers. \nFor electrophoresis of colloid particles under a small strength electric field, the force formula_14\nin the right-hand side of Eq. (5) is linearly proportional to the\nstrength formula_53 of the electric field applied. For a high strength,\nthe linearity is broken due to nonlinear polarization.\nAs a result, the force may depend nonlinearly on the applied field:\nIn the last expression, even if\nthe applied field, formula_53 has zero mean, the applied force formula_14 may happen to have a constant component that can cause a directed drift.\n\nAs above, for this to happen, formula_53 must have more than a single sinusoidal harmonic.\nThis same effect for a liquid in a tube may serve in\nelectroosmotic pump\ndriven with zero mean electric field.\n", "related": "NONE"}
{"id": "53031776", "url": "https://en.wikipedia.org/wiki?curid=53031776", "title": "Stochastic thermodynamics", "text": "Stochastic thermodynamics\n\nStochastic thermodynamics is an emergent field of research in statistical mechanics that uses stochastic variables to better understand the non-equilibrium dynamics present in microscopic systems such as colloidal particles, biopolymers (e.g. DNA, RNA, and proteins), enzymes, molecular motors and many other types of systems.\n\nWhen a microscopic machine (e.g. a MEM) performs useful work it generates heat and entropy as a byproduct of the process, however it is also predicted that this machine will operate in \"reverse\" or \"backwards\" over appreciable short periods. That is, heat energy from the surroundings will be converted into useful work. For larger engines, this would be described as a violation of the second law of thermodynamics, as entropy is consumed rather than generated. Loschmidt's paradox states that in a time reversible system, for every trajectory there exists a time-reversed anti-trajectory. As the entropy production of a trajectory and its equal anti-trajectory are of identical magnitude but opposite sign, then, so the argument goes, one cannot prove that entropy production is positive.\n\nFor a long time, exact results in thermodynamics were only possible in linear systems capable of reaching equilibrium, leaving other questions like the Loschmidt paradox unsolved. During the last few decades fresh approaches have revealed general laws applicable to non-equilibrium system which are described by nonlinear equations, pushing the range of exact thermodynamic statements beyond the realm of traditional linear solutions. These exact results are particularly relevant for small systems where appreciable (typically non-Gaussian) fluctuations occur. Thanks to stochastic thermodynamics it is now possible to accurately predict distribution functions of thermodynamic quantities relating to exchanged heat, applied work or entropy production for these systems.\n\nThe mathematical resolution to Loschmidt's paradox is called the (steady state) fluctuation theorem (FT), which is a generalisation of the second law of thermodynamics. The FT shows that as a system gets larger or the trajectory duration becomes longer, entropy-consuming trajectories become more unlikely, and the expected second law behaviour is recovered. The FT was first put forward by and much of the work done in developing and extending the theorem was accomplished by theoreticians and mathematicians interested in nonequilibrium statistical mechanics.\n\nThe first observation and experimental proof of Evan's fluctuation theorem (FT) was performed by \n\nA recent review states that \"proved a remarkable relation which allows to express the free energy difference between two equilibrium systems by a nonlinear average over the work required to drive the system in a non-equilibrium process from one state to the other. By comparing probability distributions for the work spent in the original process with the time-reversed one, Crooks found a “refinement” of the Jarzynski relation (JR), now called the Crooks fluctuation theorem. Both, this relation and another refinement of the JR, the Hummer-Szabo relation became particularly useful for determining free energy differences and landscapes of biomolecules. These relations are the most prominent ones within a class of exact results (some of which found even earlier and then rediscovered) valid for non-equilibrium systems driven by time-dependent forces. A close analogy to the JR, which relates different equilibrium states, is the Hatano-Sasa relation that applies to transitions between two different non-equilibrium steady states\". \n\nThis is shown to be a special case of a more general relation.\n\nClassical thermodynamics, at its heart, deals with general laws governing the transformations of a system, in particular, those involving the exchange of heat, work and matter with an environment. As a central result, total entropy production is identified that in any such process can never decrease, leading, inter alia, to fundamental limits on the efficiency of heat engines and refrigerators.\n\nThe thermodynamic characterisation of systems in equilibrium got its microscopic justification from equilibrium statistical mechanics which states that for a system in contact with a heat bath the probability to find it in any specific microstate is given by the Boltzmann factor. For small deviations from equilibrium, linear response theory allows to express transport properties caused by small external fields through equilibrium correlation functions. On a more phenomenological level, linear irreversible thermodynamics provides a relation between such transport coefficients and entropy production in terms of forces and fluxes. Beyond this linear response regime, for a long time, no universal exact results were available.\n\nDuring the last 20 years fresh approaches have revealed general laws applicable to non-equilibrium system thus pushing the range of validity of exact thermodynamic statements beyond the realm of linear response deep into the genuine non-equilibrium region. These exact results, which become particularly relevant for small systems with appreciable (typically non-Gaussian) fluctuations, generically refer to distribution functions of thermodynamic quantities like exchanged heat, applied work or entropy production.\n\nStochastic thermodynamics combines the stochastic energetics introduced by with the idea that entropy can consistently be assigned to a single fluctuating trajectory.\n\nStochastic thermodynamics can be applied to driven (i.e. open) quantum systems whenever the effects of quantum coherence can be ignored. The dynamics of an open quantum system is then equivalent to a classical stochastic one. However, this is sometimes at the cost of requiring unrealistic measurements at the beginning and end of a process.\n\nUnderstanding non-equilibrium quantum thermodynamics more broadly is an important and active area of research. The efficiency of some computing and information theory tasks can be greatly enhanced when using quantum correlated states; quantum correlations can be used not as a valuable resource in quantum computation, but also in the realm of quantum thermodynamics. New types of quantum devices in non-equilibrium states function very differently to their classical counterparts. For example, it has been theoretically shown that non-equilibrium quantum ratchet systems function far more efficiently then that predicted by classical thermodynamics. It has also been shown that quantum coherence can be used to enhance the efficiency of systems beyond the classical Carnot limit. This is because it could be possible to extract work, in the form of photons, from a single heat bath. Quantum coherence can be used in effect to play the role of Maxwell's demon though the broader information theory based interpretation of the second law of thermodynamics is not violated.\n\nQuantum versions of stochastic thermodynamics have been studied for some time and the past few years have seen a surge of interest in this topic. Quantum mechanics involves profound issues around the interpretation of reality (e.g. the Copenhagen interpretation, many-worlds, de Broglie-Bohm theory etc are all competing interpretations that try to explain the unintuitive results of quantum theory) . It is hoped that by trying to specify the quantum-mechanical definition of work, dealing with open quantum systems, analyzing exactly solvable models, or proposing and performing experiments to test non-equilibrium predictions, important insights into the interpretation of quantum mechanics and the true nature of reality will be gained.\n\nApplications of non-equilibrium work relations, like the Jarzynski equality, have recently been proposed for the purposes of detecting quantum entanglement and to improving optimization problems (minimize or maximize a function of multivariables called the cost function) via quantum annealing .\n\nUntil recently thermodynamics has only considered systems coupled to a thermal bath and, therefore, satisfying Boltzmann statistics. However, systems satisfying these conditions do not include many systems that are far from equilibrium such as living matter, for which fluctuations are expected to be non-Gaussian.\n\nActive particle systems are able to take energy from their environment and drive themselves far from equilibrium. An important example of active matter is constituted by objects capable of self propulsion. Thanks to this property, they feature a series of novel behaviours that are not attainable by matter at thermal equilibrium, including, for example, swarming and the emergence of other collective properties. A passive particle is considered in an active bath when it is in an environment where a wealth of active particles are present. These particles will exert nonthermal forces on the passive object so that it will experience non-thermal fluctuations and will behave widely different from a passive Brownian particle in a thermal bath. The presence of an active bath can significantly influence the microscopic thermodynamics of a particle. Experiments have suggested that the Jarzynski equality does not hold in some cases due to the presence of non-Boltzmann statistics in active baths. This observation points towards a new direction in the study of non-equilibrium statistical physics and stochastic thermodynamics, where also the environment itself is far from equilibrium.\n\nActive baths are a question of particular importance in biochemistry. For example, biomolecules within cells are coupled with an active bath due to the presence of molecular motors within the cytoplasm, which leads to striking and largely not yet understood phenomena such as the emergence of anomalous diffusion (Barkai et al., 2012). Also, protein folding might be facilitated by the presence of active fluctuations (Harder et al., 2014b) and active matter dynamics could play a central role in several biological functions (Mallory et al., 2015; Shin et al., 2015; Suzuki et al., 2015). It is an open question to what degree stochastic thermodynamics can be applied to systems coupled to active baths.\n", "related": "NONE"}
{"id": "59833659", "url": "https://en.wikipedia.org/wiki?curid=59833659", "title": "Gapped Hamiltonian", "text": "Gapped Hamiltonian\n\nIn many-body physics, most commonly within condensed-matter physics, a gapped Hamiltonian is a Hamiltonian for an infinitely large many-body system where there is a finite energy gap separating the (possibly degenerate) ground space from the first excited states. A Hamiltonian that is not gapped is called gapless.\n\nThe property of being gapped or gapless is formally defined through a sequence of Hamiltonians on finite lattices in the thermodynamic limit.\n\nAn example is the BCS Hamiltonian in the theory of superconductivity.\n\nIn quantum many-body systems, ground states of gapped Hamiltonians have exponential decay of correlations.\n\nIn quantum field theory, a continuum limit of many-body physics, a gapped Hamiltonian induces a mass gap.\n", "related": "NONE"}
{"id": "244629", "url": "https://en.wikipedia.org/wiki?curid=244629", "title": "Scientific law", "text": "Scientific law\n\nScientific laws or laws of science are statements, based on repeated experiments or observations, that describe or predict a range of natural phenomena. The term \"law\" has diverse usage in many cases (approximate, accurate, broad, or narrow) across all fields of natural science (physics, chemistry, biology, Earth science). Laws are developed from data and can be further developed through mathematics; in all cases they are directly or indirectly based on empirical evidence. It is generally understood that they implicitly reflect, though they do not explicitly assert, causal relationships fundamental to reality, and are discovered rather than invented.\n\nScientific laws summarize the results of experiments or observations, usually within a certain range of application. In general, the accuracy of a law does not change when a new theory of the relevant phenomenon is worked out, but rather the scope of the law's application, since the mathematics or statement representing the law does not change. As with other kinds of scientific knowledge, laws do not have absolute certainty (as mathematical theorems or identities do), and it is always possible for a law to be contradicted, restricted, or extended by future observations. A law can usually be formulated as one or several statements or equations, so that it can be used to predict the outcome of an experiment, given the circumstances of the processes taking place.\n\nLaws differ from hypotheses and postulates, which are proposed during the scientific process before and during validation by experiment and observation. Hypotheses and postulates are not laws since they have not been verified to the same degree, although they may lead to the formulation of laws. Laws are narrower in scope than scientific theories, which may entail one or several laws. Science distinguishes a law or theory from facts. Calling a law a fact is ambiguous, an overstatement, or an equivocation. The nature of scientific laws has been much discussed in philosophy, but in essence scientific laws are simply empirical conclusions reached by scientific method; they are intended to be neither laden with ontological commitments nor statements of logical absolutes.\n\nA scientific law always applies to a physical system under the same conditions, and it implies that there is a causal relationship involving the elements of the system. Factual and well-confirmed statements like \"Mercury is liquid at standard temperature and pressure\" are considered too specific to qualify as scientific laws. A central problem in the philosophy of science, going back to David Hume, is that of distinguishing causal relationships (such as those implied by laws) from principles that arise due to constant conjunction.\n\nLaws differ from scientific theories in that they do not posit a mechanism or explanation of phenomena: they are merely distillations of the results of repeated observation. As such, a law is limited in applicability to circumstances resembling those already observed, and may be found false when extrapolated. Ohm's law only applies to linear networks, Newton's law of universal gravitation only applies in weak gravitational fields, the early laws of aerodynamics such as Bernoulli's principle do not apply in case of compressible flow such as occurs in transonic and supersonic flight, Hooke's law only applies to strain below the elastic limit, Boyle's law applies with perfect accuracy only to the ideal gas, etc. These laws remain useful, but only under the conditions where they apply.\n\nMany laws take mathematical forms, and thus can be stated as an equation; for example, the law of conservation of energy can be written as formula_1, where E is the total amount of energy in the universe. Similarly, the first law of thermodynamics can be written as formula_2, and Newton's Second law can be written as \"F\" = . While these scientific laws explain what our senses perceive, they are still empirical, and so are not like mathematical theorems (which can be proved purely by mathematics and not by scientific experiment).\n\nLike theories and hypotheses, laws make predictions (specifically, they predict that new observations will conform to the law), and can be falsified if they are found in contradiction with new data.\n\nSome laws are only approximations of other more general laws, and are good approximations with a restricted domain of applicability. For example, Newtonian dynamics (which is based on Galilean transformations) is the low-speed limit of special relativity (since the Galilean transformation is the low-speed approximation to the Lorentz transformation). Similarly, the Newtonian gravitation law is a low-mass approximation of general relativity, and Coulomb's law is an approximation to Quantum Electrodynamics at large distances (compared to the range of weak interactions). In such cases it is common to use the simpler, approximate versions of the laws, instead of the more accurate general laws.\n\nLaws are constantly being tested experimentally to higher and higher degrees of precision. This is one of the main goals of science. Just because laws have never been observed to be violated does not preclude testing them at increased accuracy or in new kinds of conditions to confirm whether they continue to hold, or whether they break, and what can be discovered in the process. It is always possible for laws to be invalidated or proven to have limitations, by repeatable experimental evidence, should any be observed. Well-established laws have indeed been invalidated in some special cases, but the new formulations created to explain the discrepancies generalize upon, rather than overthrow, the originals. That is, the invalidated laws have been found to be only close approximations, to which other terms or factors must be added to cover previously unaccounted-for conditions, e.g. very large or very small scales of time or space, enormous speeds or masses, etc. Thus, rather than unchanging knowledge, physical laws are better viewed as a series of improving and more precise generalizations.\n\nScientific laws are typically conclusions based on repeated scientific experiments and observations over many years and which have become accepted universally within the scientific community. A scientific law is \"inferred from particular facts, applicable to a defined group or class of phenomena, and expressible by the statement that a particular phenomenon always occurs if certain conditions be present.\" The production of a summary description of our environment in the form of such laws is a fundamental aim of science.\n\nSeveral general properties of scientific laws, particularly when referring to laws in physics, have been identified. They are:\n\n- True, at least within their regime of validity. By definition, there have never been repeatable contradicting observations.\n- Universal. They appear to apply everywhere in the universe.\n- Simple. They are typically expressed in terms of a single mathematical equation.\n- Absolute. Nothing in the universe appears to affect them.\n- Stable. Unchanged since first discovered (although they may have been shown to be approximations of more accurate laws),\n- Omnipotent. Everything in the universe apparently must comply with them (according to observations).\n- Generally conservative of quantity.\n- Often expressions of existing homogeneities (symmetries) of space and time.\n- Typically theoretically reversible in time (if non-quantum), although time itself is irreversible.\n\nThe term \"scientific law\" is traditionally associated with the natural sciences, though the social sciences also contain laws. For example, Zipf's law is a law in the social sciences which is based on mathematical statistics. In these cases, laws may describe general trends or expected behaviors rather than being absolutes.\n\nSome laws reflect mathematical symmetries found in Nature (e.g. the Pauli exclusion principle reflects identity of electrons, conservation laws reflect homogeneity of space, time, and Lorentz transformations reflect rotational symmetry of spacetime). Many fundamental physical laws are mathematical consequences of various symmetries of space, time, or other aspects of nature. Specifically, Noether's theorem connects some conservation laws to certain symmetries. For example, conservation of energy is a consequence of the shift symmetry of time (no moment of time is different from any other), while conservation of momentum is a consequence of the symmetry (homogeneity) of space (no place in space is special, or different than any other). The indistinguishability of all particles of each fundamental type (say, electrons, or photons) results in the Dirac and Bose quantum statistics which in turn result in the Pauli exclusion principle for fermions and in Bose–Einstein condensation for bosons. The rotational symmetry between time and space coordinate axes (when one is taken as imaginary, another as real) results in Lorentz transformations which in turn result in special relativity theory. Symmetry between inertial and gravitational mass results in general relativity.\n\nThe inverse square law of interactions mediated by massless bosons is the mathematical consequence of the 3-dimensionality of space.\n\nOne strategy in the search for the most fundamental laws of nature is to search for the most general mathematical symmetry group that can be applied to the fundamental interactions.\n\nMost significant laws in science are conservation laws. These fundamental laws follow from homogeneity of space, time and phase, in other words \"symmetry\".\n\n- Noether's theorem: Any quantity which has a continuous differentiable symmetry in the action has an associated conservation law.\n- Conservation of mass was the first law of this type to be understood, since most macroscopic physical processes involving masses, for example collisions of massive particles or fluid flow, provide the apparent belief that mass is conserved. Mass conservation was observed to be true for all chemical reactions. In general this is only approximative, because with the advent of relativity and experiments in nuclear and particle physics: mass can be transformed into energy and vice versa, so mass is not always conserved, but part of the more general conservation of mass-energy.\n- Conservation of energy, momentum and angular momentum for isolated systems can be found to be symmetries in time, translation, and rotation.\n- Conservation of charge was also realized since charge has never been observed to be created or destroyed, and only found to move from place to place.\n\nConservation laws can be expressed using the general continuity equation (for a conserved quantity) can be written in differential form as:\n\nwhere ρ is some quantity per unit volume, J is the flux of that quantity (change in quantity per unit time per unit area). Intuitively, the divergence (denoted ∇•) of a vector field is a measure of flux diverging radially outwards from a point, so the negative is the amount piling up at a point, hence the rate of change of density in a region of space must be the amount of flux leaving or collecting in some region (see main article for details). In the table below, the fluxes, flows for various physical quantities in transport, and their associated continuity equations, are collected for comparison.\n\nMore general equations are the convection–diffusion equation and Boltzmann transport equation, which have their roots in the continuity equation.\n\nAll of classical mechanics, including Newton's laws, Lagrange's equations, Hamilton's equations, etc., can be derived from this very simple principle:\n\nwhere formula_5 is the action; the integral of the Lagrangian\n\nof the physical system between two times \"t\" and \"t\". The kinetic energy of the system is \"T\" (a function of the rate of change of the configuration of the system), and potential energy is \"V\" (a function of the configuration and its rate of change). The configuration of a system which has \"N\" degrees of freedom is defined by generalized coordinates q = (\"q\", \"q\", ... \"q\").\n\nThere are generalized momenta conjugate to these coordinates, p = (\"p\", \"p\", ..., \"p\"), where:\n\nThe action and Lagrangian both contain the dynamics of the system for all times. The term \"path\" simply refers to a curve traced out by the system in terms of the generalized coordinates in the configuration space, i.e. the curve q(\"t\"), parameterized by time (see also parametric equation for this concept).\n\nThe action is a \"functional\" rather than a \"function\", since it depends on the Lagrangian, and the Lagrangian depends on the path q(\"t\"), so the action depends on the \"entire\" \"shape\" of the path for all times (in the time interval from \"t\" to \"t\"). Between two instants of time, there are infinitely many paths, but one for which the action is stationary (to the first order) is the true path. The stationary value for the \"entire continuum\" of Lagrangian values corresponding to some path, \"not just one value\" of the Lagrangian, is required (in other words it is \"not\" as simple as \"differentiating a function and setting it to zero, then solving the equations to find the points of maxima and minima etc\", rather this idea is applied to the entire \"shape\" of the function, see calculus of variations for more details on this procedure).\n\nNotice \"L\" is \"not\" the total energy \"E\" of the system due to the difference, rather than the sum:\n\nThe following general approaches to classical mechanics are summarized below in the order of establishment. They are equivalent formulations, Newton's is very commonly used due to simplicity, but Hamilton's and Lagrange's equations are more general, and their range can extend into other branches of physics with suitable modifications.\n\nFrom the above, any equation of motion in classical mechanics can be derived.\n\n- Corollaries in mechanics\n\n- Euler's laws of motion\n- Euler's equations (rigid body dynamics)\n\n- Corollaries in fluid mechanics\n\nEquations describing fluid flow in various situations can be derived, using the above classical equations of motion and often conservation of mass, energy and momentum. Some elementary examples follow.\n\n- Archimedes' principle\n- Bernoulli's principle\n- Poiseuille's law\n- Stokes's law\n- Navier–Stokes equations\n- Faxén's law\n\nSome of the more famous laws of nature are found in Isaac Newton's theories of (now) classical mechanics, presented in his \"Philosophiae Naturalis Principia Mathematica\", and in Albert Einstein's theory of relativity. \n\n- Special relativity\n\nPostulates of special relativity are not \"laws\" in themselves, but assumptions of their nature in terms of \"relative motion\".\n\nOften two are stated as \"the laws of physics are the same in all inertial frames\" and \"the speed of light is constant\". However the second is redundant, since the speed of light is predicted by Maxwell's equations. Essentially there is only one.\n\nThe said postulate leads to the Lorentz transformations – the transformation law between two frame of references moving relative to each other. For any 4-vector\n\nthis replaces the Galilean transformation law from classical mechanics. The Lorentz transformations reduce to the Galilean transformations for low velocities much less than the speed of light \"c\".\n\nThe magnitudes of 4-vectors are invariants - \"not\" \"conserved\", but the same for all inertial frames (i.e. every observer in an inertial frame will agree on the same value), in particular if \"A\" is the four-momentum, the magnitude can derive the famous invariant equation for mass-energy and momentum conservation (see invariant mass):\n\nin which the (more famous) mass-energy equivalence \"E\" = \"mc\" is a special case.\n\n- General relativity\n\nGeneral relativity is governed by the Einstein field equations, which describe the curvature of space-time due to mass-energy equivalent to the gravitational field. Solving the equation for the geometry of space warped due to the mass distribution gives the metric tensor. Using the geodesic equation, the motion of masses falling along the geodesics can be calculated.\n\n- Gravitomagnetism\n\nIn a relatively flat spacetime due to weak gravitational fields, gravitational analogues of Maxwell's equations can be found; the GEM equations, to describe an analogous \"gravitomagnetic field\". They are well established by the theory, and experimental tests form ongoing research.\n\nThese equations can be modified to include magnetic monopoles, and are consistent with our observations of monopoles either existing or not existing; if they do not exist, the generalized equations reduce to the ones above, if they do, the equations become fully symmetric in electric and magnetic charges and currents. Indeed, there is a duality transformation where electric and magnetic charges can be \"rotated into one another\", and still satisfy Maxwell's equations.\n\n- Pre-Maxwell laws\n\nThese laws were found before the formulation of Maxwell's equations. They are not fundamental, since they can be derived from Maxwell's Equations. Coulomb's Law can be found from Gauss' Law (electrostatic form) and the Biot–Savart Law can be deduced from Ampere's Law (magnetostatic form). Lenz' Law and Faraday's Law can be incorporated into the Maxwell-Faraday equation. Nonetheless they are still very effective for simple calculations.\n\n- Lenz's law\n- Coulomb's law\n- Biot–Savart law\n\n- Other laws\n\n- Ohm's law\n- Kirchhoff's laws\n- Joule's law\n\nClassically, optics is based on a variational principle: light travels from one point in space to another in the shortest time.\n\n- Fermat's principle\n\nIn geometric optics laws are based on approximations in Euclidean geometry (such as the paraxial approximation).\n\n- Law of reflection\n- Law of refraction, Snell's law\n\nIn physical optics, laws are based on physical properties of materials.\n\n- Brewster's angle\n- Malus's law\n- Beer–Lambert law\n\nIn actuality, optical properties of matter are significantly more complex and require quantum mechanics.\n\nQuantum mechanics has its roots in postulates. This leads to results which are not usually called \"laws\", but hold the same status, in that all of quantum mechanics follows from them.\n\nOne postulate that a particle (or a system of many particles) is described by a wavefunction, and this satisfies a quantum wave equation: namely the Schrödinger equation (which can be written as a non-relativistic wave equation, or a relativistic wave equation). Solving this wave equation predicts the time-evolution of the system's behaviour, analogous to solving Newton's laws in classical mechanics.\n\nOther postulates change the idea of physical observables; using quantum operators; some measurements can't be made at the same instant of time (Uncertainty principles), particles are fundamentally indistinguishable. Another postulate; the wavefunction collapse postulate, counters the usual idea of a measurement in science.\n\nApplying electromagnetism, thermodynamics, and quantum mechanics, to atoms and molecules, some laws of electromagnetic radiation and light are as follows.\n\n- Stefan-Boltzmann law\n- Planck's law of black body radiation\n- Wien's displacement law\n- Radioactive decay law\n\nChemical laws are those laws of nature relevant to chemistry. Historically, observations led to many empirical laws, though now it is known that chemistry has its foundations in quantum mechanics.\n\n- Quantitative analysis\n\nThe most fundamental concept in chemistry is the law of conservation of mass, which states that there is no detectable change in the quantity of matter during an ordinary chemical reaction. Modern physics shows that it is actually energy that is conserved, and that energy and mass are related; a concept which becomes important in nuclear chemistry. Conservation of energy leads to the important concepts of equilibrium, thermodynamics, and kinetics.\n\nAdditional laws of chemistry elaborate on the law of conservation of mass. Joseph Proust's law of definite composition says that pure chemicals are composed of elements in a definite formulation; we now know that the structural arrangement of these elements is also important.\n\nDalton's law of multiple proportions says that these chemicals will present themselves in proportions that are small whole numbers; although in many systems (notably biomacromolecules and minerals) the ratios tend to require large numbers, and are frequently represented as a fraction.\n\nThe law of definite composition and the law of multiple proportions are the first two of the three laws of stoichiometry, the proportions by which the chemical elements combine to form chemical compounds. The third law of stoichiometry is the law of reciprocal proportions, which provides the basis for establishing equivalent weights for each chemical element. Elemental equivalent weights can then be used to derive atomic weights for each element.\nMore modern laws of chemistry define the relationship between energy and its transformations.\n\n- Reaction kinetics and equilibria\n\n- In equilibrium, molecules exist in mixture defined by the transformations possible on the timescale of the equilibrium, and are in a ratio defined by the intrinsic energy of the molecules—the lower the intrinsic energy, the more abundant the molecule. Le Chatelier's principle states that the system opposes changes in conditions from equilibrium states, i.e. there is an opposition to change the state of an equilibrium reaction.\n- Transforming one structure to another requires the input of energy to cross an energy barrier; this can come from the intrinsic energy of the molecules themselves, or from an external source which will generally accelerate transformations. The higher the energy barrier, the slower the transformation occurs.\n- There is a hypothetical intermediate, or \"transition structure\", that corresponds to the structure at the top of the energy barrier. The Hammond–Leffler postulate states that this structure looks most similar to the product or starting material which has intrinsic energy closest to that of the energy barrier. Stabilizing this hypothetical intermediate through chemical interaction is one way to achieve catalysis.\n- All chemical processes are reversible (law of microscopic reversibility) although some processes have such an energy bias, they are essentially irreversible.\n- The reaction rate has the mathematical parameter known as the rate constant. The Arrhenius equation gives the temperature and activation energy dependence of the rate constant, an empirical law.\n\n- Thermochemistry\n\n- Dulong–Petit law\n- Gibbs–Helmholtz equation\n- Hess's law\n\n- Gas laws\n\n- Raoult's law\n- Henry's law\n\n- Chemical transport\n\n- Fick's laws of diffusion\n- Graham's law\n- Lamm equation\n\n- Archie's law\n- Buys-Ballot's law\n- Birch's law\n- Byerlee's law\n- Principle of original horizontality\n- Law of superposition\n- Principle of lateral continuity\n- Principle of cross-cutting relationships\n- Principle of faunal succession\n- Principle of inclusions and components\n- Walther's law\n\nSome mathematical theorems and axioms are referred to as laws because they provide logical foundation to empirical laws.\n\nExamples of other observed phenomena sometimes described as laws include the Titius–Bode law of planetary positions, Zipf's law of linguistics, and Moore's law of technological growth. Many of these laws fall within the scope of uncomfortable science. Other laws are pragmatic and observational, such as the law of unintended consequences. By analogy, principles in other fields of study are sometimes loosely referred to as \"laws\". These include Occam's razor as a principle of philosophy and the Pareto principle of economics.\n\nThe observation that there are underlying regularities in nature dates from prehistoric times, since the recognition of cause-and-effect relationships is an implicit recognition that there are laws of nature. The recognition of such regularities as independent scientific laws \"per se\", though, was limited by their entanglement in animism, and by the attribution of many effects that do not have readily obvious causes—such as meteorological, astronomical and biological phenomena—to the actions of various gods, spirits, supernatural beings, etc. Observation and speculation about nature were intimately bound up with metaphysics and morality.\n\nAccording to a positivist view, when compared to pre-modern accounts of causality, laws of nature replace the need for divine causality on the one hand, and accounts such as Plato's theory of forms on the other.\n\nIn Europe, systematic theorizing about nature (\"physis\") began with the early Greek philosophers and scientists and continued into the Hellenistic and Roman imperial periods, during which times the intellectual influence of Roman law increasingly became paramount.The formula \"law of nature\" first appears as \"a live metaphor\" favored by Latin poets Lucretius, Virgil, Ovid, Manilius, in time gaining a firm theoretical presence in the prose treatises of Seneca and Pliny. Why this Roman origin? According to [historian and classicist Daryn] Lehoux's persuasive narrative, the idea was made possible by the pivotal role of codified law and forensic argument in Roman life and culture.\nFor the Romans . . . the place par excellence where ethics, law, nature, religion and politics overlap is the law court. When we read Seneca's \"Natural Questions\", and watch again and again just how he applies standards of evidence, witness evaluation, argument and proof, we can recognize that we are reading one of the great Roman rhetoricians of the age, thoroughly immersed in forensic method. And not Seneca alone. Legal models of scientific judgment turn up all over the place, and for example prove equally integral to Ptolemy's approach to verification, where the mind is assigned the role of magistrate, the senses that of disclosure of evidence, and dialectical reason that of the law itself.\n\nThe precise formulation of what are now recognized as modern and valid statements of the laws of nature dates from the 17th century in Europe, with the beginning of accurate experimentation and development of advanced forms of mathematics. During this period, natural philosophers such as Isaac Newton were influenced by a religious view which held that God had instituted absolute, universal and immutable physical laws. In chapter 7 of \"The World\", René Descartes described \"nature\" as matter itself, unchanging as created by God, thus changes in parts \"are to be attributed to nature. The rules according to which these changes take place I call the 'laws of nature'.\" The modern scientific method which took shape at this time (with Francis Bacon and Galileo) aimed at total separation of science from theology, with minimal speculation about metaphysics and ethics. Natural law in the political sense, conceived as universal (i.e., divorced from sectarian religion and accidents of place), was also elaborated in this period (by Grotius, Spinoza, and Hobbes, to name a few).\n\nThe distinction between natural law in the political-legal sense and law of nature or physical law in the scientific sense is a modern one, both concepts being equally derived from \"physis\", the Greek word (translated into Latin as \"natura\") for \"nature\".\n\n", "related": "\n- Empirical method\n- Empirical research\n- Empirical statistical laws\n- List of laws\n- Hypothesis\n- Law (principle)\n- Philosophy of science\n- Physical constant\n- Scientific laws named after people\n- Theory\n\n- John Barrow (1991). \"Theories of Everything: The Quest for Ultimate Explanations\". ()\n- Francis Bacon (1620). \"Novum Organum\".\n- Daryn Lehoux (2012). \"What Did the Romans Know? An Inquiry into Science and Worldmaking\". University of Chicago Press. ()\n\n- Physics Formulary, a useful book in different formats containing many or the physical laws and formulae.\n- Eformulae.com, website containing most of the formulae in different disciplines.\n- Stanford Encyclopedia of Philosophy: \"Laws of Nature\" by John W. Carroll.\n- Baaquie, Belal E. \"Laws of Physics : A Primer\". Core Curriculum, National University of Singapore.\n- Francis, Erik Max. \"The laws list\".. Physics. Alcyone Systems\n- Pazameta, Zoran. \"The laws of nature\". Committee for the scientific investigation of Claims of the Paranormal.\n- The Internet Encyclopedia of Philosophy. \"Laws of Nature\" – By Norman Swartz\n- \"Laws of Nature\", \"In Our Time\", BBC Radio 4 discussion with Mark Buchanan, Frank Close and Nancy Cartwright (Oct. 19, 2000)\n"}
{"id": "60657382", "url": "https://en.wikipedia.org/wiki?curid=60657382", "title": "Random cluster model", "text": "Random cluster model\n\nIn physics, probability theory, graph theory, etc. the random cluster model is a random graph that generalizes and unifies the Ising model, Potts model, and percolation. It is used to study random combinatorial structures, electrical networks, etc. It is also referred to as the RC model or sometimes the KF-theory, after its founders.\n\nLet \"G\" be a graph. Suppose an edge formula_1 is open with probability \"p\", wherein we say formula_2, and is otherwise closed formula_3. The probability of a given configuration is then\n\nAnd this would give you the Erdős–Rényi model (independent edges, product measure). However, suppose you weight these in the following way. Let formula_5 be the number of open clusters of the configuration (the number of connected components in the subgraph of all open edges formula_2). Let \"q\" be a positive real. Then define the new weighted measure as\n\nHere \"Z\" is the partition function or sum over all configurations:\n\nThis resulting model is known as the random cluster model or RCM for short.\n\nThere are two cases: \"q\" ≤ 1 and \"q\" ≥ 1. The former favors fewer clusters, whereas the latter favors many clusters. When \"q\" = 1, edges are open and closed independently of one another, and the model reduces to percolation and random graphs.\n\nIt is a generalization of the Tutte polynomial. The limit as \"q\" ↓ 0 describes linear resistance networks.\n\nIt is a special case of the exponential random graph models.\n\nRC models were introduced in 1969 by Fortuin and Kasteleyn, mainly to solve combinatorial problems. After their founders, it is sometimes referred to as FK models. In 1971 they used it to obtain the FKG inequality. Post 1987, interest in the model and applications in statistical physics reignited. It became the inspiration for the Swendsen–Wang algorithm describing the time-evolution of Potts models. Michael Aizenman, et al. used it to study the phase boundaries in 1D Ising and Potts models.\n\n", "related": "\n- Tutte polynomial\n- Ising model\n- Random graph\n- Swendsen–Wang algorithm\n- FKG inequality\n\n- Random-Cluster Model – Wolfram MathWorld\n"}
{"id": "60672870", "url": "https://en.wikipedia.org/wiki?curid=60672870", "title": "Surface growth", "text": "Surface growth\n\nIn mathematics and physics, a surface growth model is the dynamical study of growth of a surface, usually by means of a stochastic differential equation of a field. Popular growth models include:\n\n- KPZ equation\n- dimer model\n- Eden growth model\n- SOS model\n- Self-avoiding walk\n- Abelian sandpile model\n- Kuramoto–Sivashinsky equation (or the flame equation, for studying the surface of a flame front)\n\nThey are studied for their fractal properties, scaling behavior, critical exponents, universality classes, and relations to chaos theory, dynamical system, non-equilibrium / disordered / complex systems.\n\nPopular tools include statistical mechanics, renormalization group, rough path theory, etc.\n\n", "related": "\n- Stochastic heat equation\n- Domino tiling\n- Stochastic partial differential equation\n"}
{"id": "27146693", "url": "https://en.wikipedia.org/wiki?curid=27146693", "title": "Physical metallurgy", "text": "Physical metallurgy\n\nPhysical metallurgy is one of the two main branches of the scientific approach to metallurgy, which considers in a systematic way the physical properties of metals and alloys. It is basically the fundamentals and applications of the theory of phase transformations in metal and alloys, as the title of classic, challenging monograph on the subject with this title . While chemical metallurgy involves the domain of reduction/oxidation of metals, physical metallurgy deals mainly with mechanical and magnetic/electric/thermal properties of metals – treated by the discipline of solid state physics. Calphad methodology, able to produce Phase diagrams which is the basis for evaluating or estimating physical properties of metals, relies on Computational thermodynamics i.e. on Chemical thermodynamics and could be considered a common and useful field for both the two sub-disciplines.\n\n", "related": "\n- Extractive metallurgy\n- Metallurgical (and Materials) Transactions, a peer-review journal covering Physical Metallurgy and Materials Science\n\n- MIT Ocw MIT OpenCourseWare Course on Physical Metallurgy\n- The classic, extensive book single authored book on the subject\n- A concise, yet not simplified single authored textbook on Physical Metallurgy\n- A series of Lectures by Prof. \"Harry\" Harshad Bhadeshia, University of Cambridge on the Physical Metallurgy of Steels\n- [http://www.phase-trans.msm.cam.ac.uk/teaching.html Additional teaching materials by Prof. by Prof. \"Harry\" Harshad Bhadeshia, University of Cambridge, at the Phase Transformations & Complex\n\n- Metallurgical and Materials Transactions A, open access articles\n- Metallurgical and Materials Transactions B, open access articles\n- Acta Materialia, open access articles\n- [https://www.researchgate.net/journal/0925-8388_Journal_of_Alloys_and_Compounds], open access articles\n"}
{"id": "59478233", "url": "https://en.wikipedia.org/wiki?curid=59478233", "title": "Perpendicular paramagnetic bond", "text": "Perpendicular paramagnetic bond\n\nA perpendicular paramagnetic bond is a type of chemical bond (in contrast to covalent or ionic bonds) that does not exist under normal, atmospheric conditions. Such a phenomenon was first hypothesized through simulation to exist in the atmospheres of white dwarf stars whose magnetic fields, on the order of 10 teslas, allow such interactions to exist. Normally, at such intense temperatures as those near a white dwarf, more common molecular bonds cannot form and existing ones decompose.\n", "related": "NONE"}
{"id": "43474327", "url": "https://en.wikipedia.org/wiki?curid=43474327", "title": "Dollar (reactivity)", "text": "Dollar (reactivity)\n\nA dollar is a unit of reactivity for a nuclear reactor, calibrated to the interval between the conditions of delayed criticality and prompt criticality. One dollar is defined to be the threshold of slow criticality, which means a steady reaction rate. Two dollars is defined to be the threshold of prompt criticality, which means a nuclear excursion or explosion. A cent is of a dollar.\n\nEach nuclear fission produces several neutrons that can be absorbed, escape from the reactor, or go on to cause more fissions in a chain reaction. When an average of one neutron from each fission goes on to cause another fission, the reactor is just barely \"critical\" and the chain reaction proceeds at a constant power level.\n\nMost neutrons produced in fission are \"prompt\", i.e., created with the fission products in less than about 10 nanoseconds (a \"shake\" of time). But certain fission products produce additional neutrons when they decay up to several minutes after their creation by fission. These delayed-release neutrons, a few percent of the total, are key to stable nuclear reactor control. Without delayed neutrons, in a reactor that was just barely above critical, reactor power would increase exponentially on millisecond or even microsecond timescales – much too fast to be controlled. Such a rapid power increase can also happen in a real reactor when the chain reaction is sustained without the help of the delayed neutrons. This is prompt criticality, the most extreme example of which is an exploding nuclear weapon where considerable design effort goes into keeping the core deep into prompt criticality for as long as possible until the greatest attainable percentage of material has fissioned.\n\nBy definition, a reactivity of one dollar is just barely on the edge of criticality using both prompt and delayed neutrons. A reactivity less than one dollar is subcritical; if not already one, the power level will decrease exponentially and a sustained chain reaction will not occur. Two dollars is defined as the threshold between delayed and prompt criticality. At prompt criticality, on average each prompt neutron will cause exactly one additional fission, and the delayed neutrons will then increase power. Any reactivity above $1 is supercritical and power will increase exponentially, but between $1 and $2 the power rise will be slow enough to be easily and safely controlled with mechanical control rods because the chain reaction partly depends on the delayed neutrons. A power reactor operating at steady state (constant power) will therefore have an average reactivity of $1, with small fluctuations above and below this value.\n\nReactivity can also be expressed in relative terms, such as \"5 cents above prompt critical\".\n\nWhile power reactors are carefully designed and operated to avoid prompt criticality under all circumstances, many small research or \"zero power\" reactors are designed to be intentionally placed into prompt criticality (reactivity > $2) with complete safety by rapidly withdrawing their control rods. Their fuel elements are designed so that as they heat up, reactivity is automatically and quickly reduced through effects such as doppler broadening and thermal expansion. Such reactors can be \"pulsed\" to very high power levels (e.g., several GW) for a few milliseconds, after which reactivity automatically drops to $1 and a relatively low and constant power level (e.g. several hundred kW) is maintained until shut down manually by reinserting the control rods. \n\nAccording to Alvin Weinberg and Eugene Wigner, Louis Slotin was the first to propose the name \"dollar\" for the interval of reactivity between barely critical and prompt criticality, and \"cents\" for the decimal fraction of the dollar.\n", "related": "NONE"}
{"id": "62446002", "url": "https://en.wikipedia.org/wiki?curid=62446002", "title": "Chemical gardening", "text": "Chemical gardening\n\nChemical gardening refers to the process of creating complex biological-looking structures by mixing chemicals together wherever large amounts of such chemicals naturally occur. More simply, forming natural minerals to mimic biology. For example, mixing iron-rich particles with alkaline liquids containing the chemicals silicate or carbonate have created biological-looking structures. Such structures are actually non-biological even though they may appear to be biological and/or fossils. According to researchers, \"Chemical reactions like these have been studied for hundreds of years but they had not previously been shown to mimic these tiny iron-rich structures inside rocks. These results call for a re-examination of many ancient real-world examples to see if they are more likely to be fossils or non-biological mineral deposits.\"\n\nOne use of the study of chemical gardening is to be better able to distinguish biological structures, including fossils, from non-biological structures on the planet Mars.\n", "related": "NONE"}
{"id": "62606505", "url": "https://en.wikipedia.org/wiki?curid=62606505", "title": "KTHNY theory", "text": "KTHNY theory\n\nThe KTHNY-theory describes melting of crystals in two dimensions (2D). The name is derived from the initials of the surnames of John Michael Kosterlitz, David J. Thouless, Bertrand Halperin, David R. Nelson, and A. Peter Young, who developed the theory in the 1970s. It is, beside the Ising model in 2D and the XY model in 2D, one of the few theories, which can be solved analytically and which predicts a phase transition at a temperature formula_1.\n\nMelting of 2D crystals is mediated by the dissociation of topological defects, which destroy the order of the crystal. In 2016, Michael Kosterlitz and David Thouless were awarded with the Nobel prize in physics for their idea, how thermally excited pairs of ``virtual´´ dislocations induce a softening (described by renormalization group theory) of the crystal during heating. The shear elasticity disappears simultaneously with the dissociation of the dislocations, indicating a fluid phase. Based on this work, David Nelson and Bertrand Halperin showed, that the resulting hexatic phase is not yet an isotropic fluid. Starting from a hexagonal crystal (which is the densest packed structure in 2D), the hexatic phase has a six-folded director field, similar to liquid crystals. Orientational order only disappears due to the dissociations of a second class of topological defects, named disclinations. Peter Young calculated the critical exponent of the diverging correlations length at the transition between crystalline and hexatic.\nKTHNY theory predicts two continuous phase transitions, thus latent heat and phase coexistence is ruled out. The thermodynamic phases can be distinguished based on discrete versus continuous translational and orientational order. One of the transitions separates a solid phase with quasi-long range translational order and perfect long ranged orientational order from the hexatic phase. The hexatic phase shows short ranged translational order and quasi-long ranged orientational order. The second phase transition separates the hexatic phase from the isotropic fluid, where both, translational and orientational order is short ranged. The system is dominated by critical fluctuations, since for continuous transitions, the difference of energy between the thermodynamic phases disappears in the vicinity of the transition. This implies, that ordered and disordered regions fluctuate strongly in space and time. The size of those regions grows strongly near the transitions and diverges at the transition itself. At this point, the pattern of symmetry broken versus symmetric domains is fractal. Fractals are characterized by a scaling invariance – they appear similar on an arbitrary scale or by arbitrarily zooming in (this is true on any scale larger than the atomic distance). The scale invariance is the basis to use the renormalization group theory to describe the phase transitions. Both transitions are accompanied by spontaneous symmetry breaking. Unlike for melting in three dimensions, translational and orientational symmetry breaking does not need to appear simultaneously in 2D, since two different types of topological defects destroy the different types of order.\n\nMichael Kosterlitz and David Thouless tried to resolve a contradiction about 2D crystals: on one hand side, the Mermin-Wagner theorem claims that symmetry breaking of a continuous order-parameter cannot exist in two dimensions. This implies, that perfect long range positional order is ruled out in 2D crystals. On the other side, very early computer simulations of Berni Alder and Thomas E. Wainwright indicated crystallization in 2D. The KTHNY theory shows implicitly that periodicity is not a sufficient criterion for a solid (this is already indicated by the existence of amorphous solids like glasses. Following M. Kosterlitz, a finite shear elasticity defines a 2D solid, including quasicrystals in this description.\n\nAll three thermodynamic phases and their corresponding symmetries can be visualized using the structure factor :formula_2. The double sum runs over all positions of particle pairs I and j and the brackets denote an average about various configurations. The isotropic phase is characterized by concentric rings at formula_3, if formula_4 is the average particle distance calculated by the 2D particle density formula_5. The (closed packed) crystalline phase is characterized by six-fold symmetry based on the orientational order. Unlike in 3D, where the peaks are arbitrarily sharp (formula_6-peaks), the 2D peaks have a finite width described with a Lorenz-curve. This is due to the fact, that the translational order is only quasi-long ranged as predicted by the Mermin-Wagner theorem. The hexatic phase is characterized by six segments, which reflect the quasi-long ranged orientational order. The structure factor of Figure 1 is calculated from the positions of a colloidal monolayer (crosses at high intensity are artefacts from the Fourier transformation due to the finite (rectangular) field of view of the ensemble).\n\nTo analyse melting due to the dissociation of dislocations, one starts with the energy formula_7 as function of distance between two dislocations. An isolated dislocation in 2D is a local distortions of the six-folded lattice, where neighbouring particles have five- and seven nearest neighbours, instead of six. It is important to note, that dislocations can only be created in pairs, due to topological reasons. A bound pair of dislocations is a local configuration with 5-7-7-5 neighbourhood.\nThe double sum runs over all positions of defect pairs formula_9 and formula_10, formula_11 measures the distance between the dislocations. formula_12 is the Burgers vector and denotes the orientation of the dislocation at position Orte formula_13. The second term in the brackets brings dislocations to arrange preferentially antiparallel due to energetic reasons. Its contribution is small and can be neglected for large distance between defects. The main contribution stems from the logarithmic term (the first one in the brackets) which describes, how the energy of a dislocation pair diverges with increasing distance. Since the shortest distance between two dislocations is given approximatively by the average particle distance formula_14, the scaling of distances with formula_14 prevents the logarithm formula_16 to become negative. The strength of the interaction is proportional to Young's modulus formula_17 given by the stiffness of the crystal lattice. To create a dislocation from an undisturbed lattice, a small displacement on a scale smaller than the average particle distance formula_14 is needed. The discrete energy associated with this displacement is usually called core energy Energie formula_19 and has to be counted for each of the formula_20 dislocations individually (last term).\nAn easy argument for the dominating logarithmic term is, that the magnitude of the strain induced by an isolated dislocation decays according mit formula_21 with distance. Assuming Hooke's approximation, the associated stress is linear with the strain. Integrating the strain ~1/r gives the energy proportional to the logarithm. The logarithmic distance dependence of the energy is the reason, why KTHNY-theory is one of the few theories of phase transitions which can be solved analytically: in statistical physics one has to calculate partition functions, e.g. the probability distribution for ``all´´ possible configurations of dislocation pairs given by the Boltzmann distribution formula_22. Here, formula_23 is the thermal energy with Boltzmann constant formula_24. For the majority of problems in statistical physics one can hardly solve the partition function due to the enormous amount of particles and degrees of freedoms. This is different in KTHNY theory due to the logarithmic energy functions of dislocations formula_7 and the e-function from the Boltzmann factor as inverse which can be solved easily.\n\nWe want to calculate the mean squared distance between two dislocations considering only the dominant logarithmic term for simplicity:\nThis mean distance formula_27 tends to zero for low temparatures – dislocations will annihilate and the crystal is free of defects. The expression diverges formula_28, if the denominator tends to zero. This happens, when\nformula_29. A diverging distance of dislocations implies, that they are dissociated and do not form a bound pair. The crystal is molten, if several isolated dislocations are thermally excited and the melting temperature formula_30 is given by Young’s modulus:\nThe dimensionless quantity formula_32 is a universal constant for melting in 2D and is independent of details of the system under investigation. \nThis example investigated only an isolated pair of dislocations. In general, a multiplicity of dislocations will appear during melting. The strain field of an isolated dislocation will be shielded and the crystal will get softer in the vicinity of the phase transition; Young’s modulus will decrease due to dislocations. In KTHNY theory, this feedback of dislocations on elasticity, and especially on Young’s modulus acting as coupling constant in the energy function, is described within the framework of renormalization group theory.\n\nIf a 2D crystal is heated, ``virtual´´ dislocation pairs will be excited due to thermal fluctuations in the vicinity of the phase transition. Virtual means, that the average thermal energy is not large enough to overcome (two times) the core-energy and to dissociate (unbind) dislocation pairs. Nonetheless, dislocation pairs can appear locally on very short time scales due to thermal fluctuations, before they annihilate again. Although they annihilate, they have a detectable impact on elasticity: they soften the crystal. The principle is completely analogue to calculating the bare charge of the electron in quantum electrodynamics (QED). In QED, the charge of the electron is shielded due to virtual electron-positron pairs due to quantum fluctuations of the vacuum. Roughly spoken one can summarize: If the crystal is softened due to the presence of virtual pairs of dislocation, the probability (fugacity) formula_33 for creating additional virtual dislocations is enhanced, proportional to the Boltzmann factor of the core-energy of a dislocation formula_34. If additional (virtual) dislocations are present, the crystal will get additionally softer. If the crystal is additionally softer, the fugacity will increase further... and so on and so forth.\nDavid Nelson, Bertrand Halperin and independently Peter Young formulated this in a mathematically precise way, using renormalization group theory for the fugacity and the elasticity: In the vicinity of the continuous phase transition, the system becomes critical – this means that it becomes self-similar on all length scales formula_35. Executing a transformation of all length scales by an factor of formula_36, the energy formula_37 and fugacity formula_38 will depend on this factor, but the system has to appear identically, simultaneously due to the self similarity. Especially the energy function (Hamiltonian) of the dislocations have to be invariant in structure. The softening of the system after a length scale transformation (zooming out to visualize a larger area implies to count more dislocations) is now covered in a renormalized (reduced) elasticity. The recursion relation for elasticity and fugacity are:\nSimilar recursion relations can be derived for the shear modulus and the bulk modulus. formula_41 and formula_42 are Bessel functions, respectively. Depending on the starting point, the recursion relation can run into two directions. formula_43 implies no defects, the ensemble is crystalline. formula_44, implies arbitrary many defects, the ensemble is fluid. The recursion relation have a fix-point at formula_45 with formula_46. Now, formula_47 is the renormalized value instead of the bare one. Figure 2 shows Youngs’modulus as function of the dimensionless control parameter formula_48. It measures the ratio of the repelling energy between two particles and the thermal energy (which was constant in this experiment). It can be interpreted as pressure or inverse temperature. The black curve is a thermodynamic calculation of a perfect hexagonal crystal at formula_49. The blue curve is from computer simulations and shows a reduced elasticity due to lattice vibrations at formula_50. The red curve is the renormalization following the recursion relations, Young's modulus disappears discontinuously to zero at formula_32. Turquoise symbols are from measurements of elasticity in a colloidal monolayer, and confirm the melting point at formula_52.\n\nThe system enters the hexatic phase after the dissociation of dislocations. To reach the isotropic fluid, dislocations (5-7-pairs) have to dissociate into disclinations, consisting of isolated 5-folded and isolated 7-folded particles. Similar arguments for the interaction of disclinations compared to dislocations can be used. Again, disclinations can only be created as pairs due to topological reasons. Starting with the energy formula_53 as function of distance between two disclinations one finds:\nThe logarithmic term is again dominating. The sign of the interaction gives attraction or repulsion for the winding numbers formula_55 and formula_56 of the five- and seven-folded disclinations in a way that ``charges´´ with opposite sign have attraction. The overall strength is given by the stiffness against twist. The coupling constant formula_57 is called Frank's constant, following the theory of liquid crystals. \nformula_58 is the discrete energy of a dislocation to dissociate into two disclinations. The squared distance of two disclinations can be calculated the same way, as for dislocations, only the prefactor, denoting the coupling constant, has to be changed accordingly. It diverges for formula_59. The system is molten from the hexatic phase into the isotropic liquid, if unbound disclinations are present. This transition temperature formula_60 is given by Frank's constant:\nformula_62 is again a universal constan. Figure 3 shows measurements of the orientational stiffness of a colloidal monolayer; Frank's constant drops below this universal constant at formula_60.\n\nContinuous phase transitions (or second order phase transition following Ehrenfest notation) show critical fluctuations of ordered and disordered regions in the vicinity of the transition. The correlation length measuring the size of those regions diverges algebraically in typical 3D systems. formula_64\nHere, formula_65 is the transition temperature and formula_66 is a critical exponent. Another special feature of Kosterlitz–Thouless transitions is, that translational and orientational correlation length in 2D diverge exponentially (see also hexatic phase for the definition of those correlation functions):\nThe critical exponent becomes formula_68 for the diverging translational correlation length at the hexatic – crystalline transition. D. Nelson and B. Halperin predicted, that Frank's constant diverges exponentially with formula_69 at formula_60, too. The red curve shows a fit of experimental data covering the critical behaviour; the critical exponent is measured to be formula_71. This value is compatible with the prediction of KTHNY theory within the error bars. The orientational correlation length at the hexatic – isotropic transition is predicted to diverge with an exponent formula_72. This rational value is compatible with mean-field-theories and implies that a renormalization of Frank's constant is not necessary. The increasing shielding of orientational stiffness due to disclinations has not to be taken into account – this is already done by dislocations which are frequently present at formula_60. Experiments measured a critical exponent of formula_74.\nKTHNY-theory has been tested in experiment and in computer simulations. For short range particle interaction (hard discs), simulations found a weakly first order transition for the hexatic – isotropic transition, slightly beyond KTHNY-theory.\n", "related": "NONE"}
{"id": "62421802", "url": "https://en.wikipedia.org/wiki?curid=62421802", "title": "Perturbed angular correlation", "text": "Perturbed angular correlation\n\nThe perturbed γ-γ angular correlation, PAC for short or PAC-Spectroscopy, is a method of nuclear solid-state physics with which magnetic and electric fields in crystal structures can be measured. In doing so, electrical field gradients and the Larmor frequency in magnetic fields as well as dynamic effects are determined. With this very sensitive method, which requires only about 10-1000 billion atoms of a radioactive isotope per measurement, material properties in the local structure, phase transitions, magnetism and diffusion can be investigated. The PAC method is related to nuclear magnetic resonance and the Mössbauer effect, but shows no signal attenuation at very high temperatures.\nToday only the time-differential perturbed angular correlation (TDPAC) is used.\n\nPAC goes back to a theoretical work by Donald R. Hamilton from 1940. The first successful experiment was carried out by Brady and Deutsch in 1947. Essentially spin and parity of nuclear spins were investigated in these first PAC experiments. However, it was recognized early on that electric and magnetic fields interact with the nuclear moment, providing the basis for a new form of material investigation: nuclear solid-state spectroscopy.\n\nStep by step the theory was developed.\nAfter Abragam and Pound published their work on the theory of PAC in 1953 including extra nuclear fields, many studies with PAC were carried out afterwards. In the 1960s and 1970s, interest in PAC experiments sharply increased, focusing mainly on magnetic and electric fields in crystals into which the probe nuclei were introduced. In the mid-1960s, ion implantation was discovered, providing new opportunities for sample preparation. The rapid electronic development of the 1970s brought significant improvements in signal processing. From the 1980s to the present, PAC has emerged as an important method for the study and characterization of materials. B. for the study of semiconductor materials, intermetallic compounds, surfaces and interfaces. Lars Hemmingsen et al. Recently, PAC also applied in biological systems.\n\nWhile until about 2008 PAC instruments used conventional high-frequency electronics of the 1970s, in 2008 Christian Herden and Jens Röder et al. developed the first fully digitized PAC instrument that enables extensive data analysis and parallel use of multiple probes. Replicas and further developments followed.\n\nPAC uses radioactive probes, which have an intermediate state with decay times of 2 ns to approx. 10 μs, see example In in the picture on the right. After electron capture (EC), indium transmutates to cadmium. Immediately thereafter, the cadmium nucleus is predominantly in the excited 7/2+ nuclear spin and only to a very small extent in the 11/2- nuclear spin, the latter should not be considered further. The 7/2+ excited state transitions to the 5/2+ intermediate state by emitting a 171 keV γ-quantum. The intermediate state has a lifetime of 84.5 ns and is the sensitive state for the PAC. This state in turn decays into the 1/2+ ground state by emitting a γ-quantum with 245 keV. PAC now detects both γ-quanta and evaluates the first as a start signal, the second as a stop signal.\nNow one measures the time between start and stop for each event. This is called coincidence when a start and stop pair has been found. Since the intermediate state decays according to the laws of radioactive decay, one obtains an exponential curve with the lifetime of this intermediate state after plotting the frequency over time. Due to the non-spherically symmetric radiation of the second γ-quantum, the so-called anisotropy, which is an intrinsic property of the nucleus in this transition, it comes with the surrounding electrical and/or magnetic fields to a periodic disorder (hyperfine interaction). The illustration of the individual spectra on the right shows the effect of this disturbance as a wave pattern on the exponential decay of two detectors, one pair at 90° and one at 180° to each other. The waveforms to both detector pairs are shifted from each other. Very simply, one can imagine a fixed observer looking at a lighthouse whose light intensity periodically becomes lighter and darker. Correspondingly, a detector arrangement, usually four detectors in a planar 90 ° arrangement or six detectors in an octahedral arrangement, \"sees\" the rotation of the core on the order of magnitude of MHz to GHz.\nAccording to the number n of detectors, the number of individual spectra (z) results after z=n²-n, for n=4 therefore 12 and for n=6 thus 30. In order to obtain a PAC spectrum, the 90° and 180° single spectra are calculated in such a way that the exponential functions cancel each other out and, in addition, the different detector properties shorten themselves. The pure perturbation function remains, as shown in the example of a complex PAC spectrum. Its Fourier transform gives the transition frequencies as peaks.\n\nformula_1, the count rate ratio, is obtained from the single spectra by using:\n\nDepending on the spin of the intermediate state, a different number of transition frequencies show up. For 5/2 spin, 3 transition frequencies can be observed with the ratio ω+ω=ω. As a rule, a different combination of 3 frequencies can be observed for each associated site in the unit cell.\nPAC is a statistical method: Each radioactive probe atom sits in its own environment. In crystals, due to the high regularity of the arrangement of the atoms or ions, the environments are identical or very similar, so that probes on identical lattice sites experience the same hyperfine field or magnetic field, which then becomes measurable in a PAC spectrum. On the other hand, for probes in very different environments, such as in amorphous materials, a broad frequency distribution or no is usually observed and the PAC spectrum appears flat, without frequency response. With single crystals, depending on the orientation of the crystal to the detectors, certain transition frequencies can be reduced or extinct, as can be seen in the example of the PAC spectrum of zinc oxide (ZnO).\n\nIn the typical PAC spectrometer, a setup of four 90° and 180° planar arrayed detectors or six octahedral arrayed detectors are placed around the radioactive source sample. The detectors used are scintillation crystals of BaF or NaI. For modern instruments today mainly LaBr:Ce or CeBr are used. Photomultipliers convert the weak flashes of light into electrical signals generated in the scintillator by gamma radiation. In classical instruments these signals are amplified and processed in logical AND/OR circuits in combination with time windows the different detector combinations (for 4 detectors: 12, 13, 14, 21, 23, 24, 31, 32, 34, 41, 42, 43) assigned and counted. Modern digital spectrometers use digitizer cards that directly use the signal and convert it into energy and time values and store them on hard drives. These are then searched by software for coincidences. Whereas in classical instruments, \"windows\" limiting the respective γ-energies must be set before processing, this is not necessary for the digital PAC during the recording of the measurement. The analysis only takes place in the second step. In the case of probes with complex cascades, this makes it makes it possible to perform a data optimization or to evaluate several cascades in parallel, as well as measuríng different probes simultaneously. The resulting data volumes can be between 60 and 300 GB per measurement.\n\nAs materials for the investigation (samples) are in principle all materials that can be solid and liquid. Depending on the question and the purpose of the investigation, certain framework conditions arise. For the observation of clear perturbation frequencies it is necessary, due to the statistical method, that a certain proportion of the probe atoms are in a similar environment and e.g. experiences the same electric field gradient. Furthermore, during the time window between the start and stop, or approximately 5 half-lives of the intermediate state, the direction of the electric field gradient must not change. In liquids, therefore, no interference frequency can be measured as a result of the frequent collisions, unless the probe is complexed in large molecules, such as in proteins. The samples with proteins or peptides are usually frozen to improve the measurement.\n\nThe most studied materials with PAC are solids such as semiconductors, metals, insulators, and various types of functional materials. For the investigations, these are usually crystalline. Amorphous materials do not have highly ordered structures. However, they have close proximity, which can be seen in PAC spectroscopy as a broad distribution of frequencies. Nano-materials have a crystalline core and a shell that has a rather amorphous structure. This is called core-shell model. The smaller the nanoparticle becomes, the larger the volume fraction of this amorphous portion becomes. In PAC measurements, this is shown by the decrease of the crystalline frequency component in a reduction of the amplitude (attenuation).\n\nThe amount of suitable PAC isotopes required for a measurement is between about 10 to 1000 billion atoms (10-10). The right amount depends on the particular properties of the isotope. 10 billion atoms are a very small amount of substance. For comparison, one mol contains about 6.22x10 particles. 10 atoms in one cubic centimeter of beryllium give a concentration of about 8 nmol/L (nanomol=10 mol). The radioactive samples each have an activity of 0.1-5 MBq, which is in the order of the exemption limit for the respective isotope.\n\nHow the PAC isotopes are brought into the sample to be examined is up to the experimenter and the technical possibilities. The following methods are usual:\n\nDuring implantation, a radioactive ion beam is generated, which is directed onto the sample material. Due to the kinetic energy of the ions (1-500 keV) these fly into the crystal lattice and are slowed down by impacts. They either come to a stop at interstitial sites or push a lattice-atom out of its place and replace it. This leads to a disruption of the crystal structure. These disorders can be investigated with PAC. By tempering these disturbances can be healed. If, on the other hand, radiation defects in the crystal and their healing are to be examined, unperseived samples are measured, which are then annealed step by step.\n\nThe implantation is usually the method of choice, because it can be used to produce very well-defined samples.\n\nIn a vacuum, the PAC probe can be evaporated onto the sample. The radioactive probe is applied to a hot plate or filament, where it is brought to the evaporation temperature and condensed on the opposite sample material. With this method, e.g. surfaces are examined. Furthermore, by vapor deposition of other materials, interfaces can be produced. They can be studied during tempering with PAC and their changes can be observed. Similarly, the PAC probe can be transferred to sputtering using a plasma.\n\nIn the diffusion method, the radioactive probe is usually diluted in a solvent applied to the sample, dried and it is diffused into the material by tempering it. The solution with the radioactive probe should be as pure as possible, since all other substances can diffuse into the sample and affect thereby the measurement results. The sample should be sufficiently diluted in the sample. Therefore, the diffusion process should be planned so that a uniform distribution or sufficient penetration depth is achieved.\n\nPAC probes may also be added during the synthesis of sample materials to achieve the most uniform distribution in the sample. This method is particularly well suited if, for example, the PAC probe diffuses only poorly in the material and a higher concentration in grain boundaries is to be expected. Since only very small samples are necessary with PAC (about 5 mm), micro-reactors can be used. Ideally, the probe is added to the liquid phase of the sol-gel process or one of the later precursor phases.\n\nIn neutron activation, the probe is prepared directly from the sample material by converting very small part of one of the elements of the sample material into the desired PAC probe or its parent isotope by neutron capture. As with implantation, radiation damage must be healed. This method is limited to sample materials containing elements from which neutron capture PAC probes can be made. Furthermore, samples can be intentionally contaminated with those elements that are to be activated. For example, hafnium is excellently suited for activation because of its large capture cross section for neutrons.\n\nRarely used are direct nuclear reactions in which nuclei are converted into PAC probes by bombardment by high-energy elementary particles or protons. This causes major radiation damage, which must be healed. This method is used with PAD, which belongs to the PAC methods.\n\nThe currently largest PAC laboratory in the world is located at ISOLDE in CERN with about 10 PAC instruments, that receives its major funding form BMBF. Radioactive ion beams are produced at the ISOLDE by bombarding protons from the booster onto target materials (uranium carbide, liquid tin, etc.) and evaporating the spallation products at high temperatures (up to 2000 °C), then ionizing them and then accelerating them. With the subsequent mass separation usually very pure isotope beams can be produced, which can be implanted in PAC samples. Of particular interest to the PAC are short-lived isomeric probes such as: Cd, Hg, Pb, and various rare earth probes.\n\nThe first formula_3-quantum (formula_4) will be emitted isotopically. Detecting this quantum in a detector selects a subset with an orientation of the many possible directions that has a given. The second formula_3-quantum (formula_6) has an anisotropic emission and shows the effect of the angle correlation. The goal is to measure the relative probability formula_7 with the detection of formula_8 at the fixed angle formula_9 in relation to formula_10. The probability is given with the angle correlation (perturbation theory):\n\nFor a formula_3-formula_3-cascade, formula_14 is due to the preservation of parity:\n\nWhere formula_16 is the spin of the intermediate state and formula_17 with formula_18 the multipolarity of the two transitions. For pure multipole transitions, is formula_19.\n\nformula_20 is the anisotropy coefficient that depends on the angular momentum of the intermediate state and the multipolarities of the transition.\n\nThe radioactive nucleus is built into the sample material and emits two formula_3-quanta upon decay. During the lifetime of the intermediate state, i.e. the time between formula_10 and formula_8, the core experiences a disturbance due to the hyperfine interaction through its electrical and magnetic environment. This disturbance changes the angular correlation to:\n\nformula_25 is the perturbation factor. Due to the electrical and magnetic interaction, the angular momentum of the intermediate state formula_17 experiences a torque about its axis of symmetry. Quantum-mechanically, this means that the interaction leads to transitions between the M states. The second formula_3-quantum (formula_8) is then sent from the intermediate level. This population change is the reason for the attenuation of the correlation.\n\nThe interaction occurs between the magnetic core dipole moment formula_29 and the intermediate state formula_16 or/and an external magnetic field formula_31. The interaction also takes place between nuclear quadrupole moment and the off-core electric field gradient formula_32.\n\nFor the magnetic dipole interaction, the frequency of the precession of the nuclear spin around the axis of the magnetic field formula_31 is given by:\n\nformula_36 is the Landé g-factor und formula_37 is the nuclear magneton.\n\nWith formula_38 follows:\n\nFrom the general theory we get:\n\nFor the magnetic interaction follows:\n\nThe energy of the hyperfine electrical interaction between the charge distribution of the core and the extranuclear static electric field can be extended to multipoles. The monopole term only causes an energy shift and the dipole term disappears, so that the first relevant expansion term is the quadrupole term:\n\nThis can be written as a product of the quadrupole moment formula_43 and the electric field gradient formula_44. Both [tensor]s are of second order. Higher orders have too small effect to be measured with PAC.\n\nThe electric field gradient is the second derivative of the electric potential formula_45 at the core:\n\nformula_44 becomes diagonalized, that:\n\nThe matrix is free of traces in the main axis system (Laplace equation)\n\nTypically, the electric field gradient is defined with the largest proportion formula_32 and formula_51:\n\nIn cubic crystals, the axis parameters of the unit cell x, y, z are of the same length. Therefore:\n\nIn axisymmetric systems is formula_55.\n\nFor axially symmetric electric field gradients, the energy of the substates has the values:\n\nThe energy difference between two substates, formula_58 and formula_59, is given by:\n\nThe quadrupole frequency formula_61 is introduced.\nThe formulas in the colored frames are important for the evaluation:\n\nThe publications mostly list formula_64. formula_65 as elementary charge and formula_66 as Planck constant are well known or well defined. \nThe nuclear quadrupole moment formula_67 is often determined only very inaccurately (often only with 2-3 digits).\nBecause formula_64 can be determined much more accurately than formula_67, it is not useful to specify only formula_32 because of the error propagation.\nIn addition, formula_64 is independent of spin! This means that measurements of two different isotopes of the same element can be compared, such as Hg(5/2−), Hg(5/2−) and Hg(9/2−). Further, formula_64 can be used as finger print method.\n\nFor the energy difference then follows:\n\nIf formula_55, then:\nwith:\n\nFor integer spins applies:\nFor half integer spins applies:\n\nThe perturbation factor is given by:\n\nWith the factor for the probabilities of the observed frequencies:\n\nAs far as the magnetic dipole interaction is concerned, the electrical quadrupole interaction also induces a precision of the angular correlation in time and this modulates the quadrupole interaction frequency. This frequency is an overlap of the different transition frequencies formula_83. The relative amplitudes of the various components depend on the orientation of the electric field gradient relative to the detectors (symmetry axis) and the asymmetry parameter formula_51. For a probe with different probe nuclei, one needs a parameter that allows a direct comparison: Therefore, the quadrupole coupling constant formula_64 independent of the nuclear spin formula_86 is introduced.\n\nIf there is a magnetic and electrical interaction at the same time on the radioactive nucleus as described above, combined interactions result. This leads to the splitting of the respectively observed frequencies. The analysis may not be trivial due to the higher number of frequencies that must be allocated. These then depend in each case on the direction of the electric and magnetic field to each other in the crystal. PAC is one of the few ways in which these directions can be determined.\n\nIf the hyperfine field fluctuates during the lifetime formula_87 of the intermediate level due to jumps of the probe into another lattice position or from jumps of a near atom into another lattice position, the correlation is lost. For the simple case with an undistorted lattice of cubic symmetry, for a jump rate of formula_88 for equivalent places formula_89, an exponential damping of the static formula_90-terms is observed:\n\nHere formula_93 is a constant to be determined, which should not be confused with the decay constant formula_94. For large values of formula_95, only pure exponential decay can be observed:\n\nThe boundary case after Abragam-Pound is formula_93, if formula_98, then:\n\nCores that transmute beforehand of the formula_3-formula_3-cascade usually cause a charge change in ionic crystals (In) to Cd). As a result, the lattice must respond to these changes. Defects or neighboring ions can also migrate. Likewise, the high-energy transition process may cause the Auger effect, that can bring the core into higher ionization states. The normalization of the state of charge then depends on the conductivity of the material. In metals, the process takes place very quickly. This takes considerably longer in semiconductors and insulators. In all these processes, the hyperfine field changes. If this change falls within the formula_3-formula_3-cascade, it may be observed as an after effect.\n\nThe number of nuclei in state (a) in the image on the right is depopulated both by the decay after state (b) and after state (c):\n\nmit: formula_105\n\nFrom this one obtains the exponential case:\n\nFor the total number of nuclei in the static state (c) follows:\n\nThe initial occupation probabilities formula_108 are for static and dynamic environments:\n\nIn the general theory for a transition formula_111 is given:\n\nwith:\n", "related": "NONE"}
{"id": "43239622", "url": "https://en.wikipedia.org/wiki?curid=43239622", "title": "Physics education research", "text": "Physics education research\n\n<noinclude>\n\nPhysics education research (PER) is a form of discipline-based education research specifically related to the study of the teaching and learning of physics, often with the aim of improving the effectiveness of student learning. Approximately eighty-five institutions in the United States conduct research in science and physics education.\n\nOne primary goal of PER is to develop pedagogical techniques and strategies that will help students learn physics more effectively and help instructors to implement these techniques. Because even basic ideas in physics can be confusing, together with the possibility of scientific misconceptions formed from teaching through analogies, lecturing often does not erase common misconceptions about physics that students acquire before they are taught physics. Research often focuses on learning more about common misconceptions that students bring to the physics classroom so that techniques can be devised to help students overcome these misconceptions.\n\nIn most introductory physics courses, mechanics is usually the first area of physics that is taught. Newton's laws of motion about interactions between forces and objects are central to the study of mechanics. Many students hold the Aristotelian misconception that a net force is required to keep a body moving; instead, motion is modeled in modern physics with Newton's first law of inertia, stating that a body will keep its state of rest or movement unless a net force acts on the body. Like students who hold this misconception, Newton arrived at his three laws of motion through empirical analysis, although he did it with an extensive study of data that included astronomical observations. Students can erase such as misconception in a nearly frictionless environment, where they find that objects move at an almost constant velocity without a constant force.\n\nThe broad goal of the PER community is to understand the processes involved in the teaching and learning of physics through rigorous scientific investigation.\n\nAccording to the University of Washington PER group, one of the pioneers in the field, work within PER tends to fall within one or more of several broad descriptions, including:\n\n- identifying student difficulties\n- developing methods to address these difficulties and measure learning gains\n- developing surveys to measure student performance and other characteristics\n- investigating student attitudes and beliefs as relating to physics\n- studying small and large group dynamics analyzing student patterns using framing and other new and existing epistemological methods\n\n\"An Introduction to Physics Education Research\", by Robert Beichner, identifies eight trends in PER:\n\n- Conceptual understanding: Investigating what students know and how they learn it is a centerpiece of PER. Early research involved identifying and treating misconceptions about the principles of physics. The term has since evolved to \"student difficulties\" based on the consideration of alternative theoretical frameworks for student learning. A difficulty with a concept can be built into a correct concept; a misconception is rooted out and replaced by a correct conception. The PER group at the University of Washington specializes in research about conceptual understanding and student difficulty.\n- Epistemology: PER began as a trial-and-error approach to improve learning. Because of the downsides of such an approach, theoretical bases for research were developed early on, most notably through the University of Maryland. The theoretical underpinnings of PER are mostly built around Piagettean constructivism. Theories on cognition in physics learning were put forward by Redish, Hammer, Elby and Scherr, who built off of diSessa's \"Knowledge in Pieces\". The Resources Framework, developed from this work, builds off of research in neuroscience, sociology, linguistics, education and psychology. Additional frameworks are forthcoming, most recently the \"Possibilities Framework\", which builds off of deductive reasoning research started by Wason and Philip Johnson-Laird.\n- Problem solving: It plays an important role in the processes that advance physics research, featured in high numbers of exercises in conventional textbooks. Most research in this area rests on examining the difference between novice and expert problem solvers (freshmen and sophomores, and graduate-level and postdoctorate students, respectively). Approaches in researching problem solving have been a focus for the University of Minnesota's PER group. Recently, a paper was published in PRL Special Section: PER that identified over 30 behaviors, attitudes, and skills that are used in the solving of a typical physics problem. Greater resolution and specific attention to the details are used in the field of problem solving.\n- Attitudes: The University of Colorado developed an instrument that reveals student attitudes and expectations about physics as a subject and as a class. Student attitudes are often found to decline after traditional instruction, but recent work by Redish and Hammer show that this can be reversed and positive attitudinal gains can be seen if attention is paid to \"explicate the epistemological elements of the implicit curriculum.\"\n- Social aspects: Research has been conducted into gender, race, and other socioeconomic issues that can influence learning in physics and other fields. Other research has investigated the impacts on learning physics of body language, group dynamics, and classroom setup.\n- Technology: Student response systems (clickers) are based on Eric Mazur's work in Peer Instruction. Research in PER examines the influence, applications of, and possibilities for technology in the classroom.\n- Instructional interventions: PER's curriculum design is based on more than two decades of research in physics education. Notable textbooks include \"Tutorials in Physics\", \"Physics by Inquiry\", \"Investigative Science Learning Environment\", and \"Paradigms in Physics\", as well as many new textbooks in introductory and junior level coursework. The Kansas State University Physics Education Research Group has developed a program, Visual Quantum Mechanics (VQM), to teach quantum mechanics to high school and college students who do not have advanced backgrounds in physics or math.\n- Instructional materials: For undergraduates, publishers now emphasize a PER basis for their physics textbooks as a major selling point. One of the earliest comprehensive physics textbooks to incorporate PER findings was written by Serway and Beichner. Apart from textbooks, instructional material for pre-college physics students now include PhET (Physics Education Technology) simulations. This is made possible through advances in personal computer hardware, platform-independent software such as Adobe Flash Player and Java, and more recently HTML5, CSS3 and JavaScript. According to Wieman, PhET simulations offer a direct and powerful tool for probing student thinking and learning.\n\nPhysics education research papers in the United States are primarily issued among four publishing venues. Papers submitted to the \"American Journal of Physics: Physics Education Research Section (PERS)\" are mostly to consumers of physics education research. The \"Journal of the Learning Sciences\" (JLS) publishes papers that regard real-life or non-laboratory environments, often in the context of technology, and are about learning, not teaching. Meanwhile, papers at \"\" (PRST:PER) are aimed at those for whom research is conducted on PER rather than to consumers. The audience for \"Physics Education Research Conference Proceedings\" (PERC) is designed for a mix of consumers and researchers. The latter provides a snapshot of the field and as such is open to preliminary results and research in progress, as well as papers that would simply be thought-provoking to the PER community. Other journals include \"Physics Education\" (UK), the \"European Journal of Physics\" (UK), and \"The Physics Teacher\". Leon Hsu and others published an article about publishing and refereeing papers in physics education research in 2007.\n", "related": "NONE"}
{"id": "63107907", "url": "https://en.wikipedia.org/wiki?curid=63107907", "title": "Unification (physics)", "text": "Unification (physics)\n\nUnification of the observable fundamental phenomena of nature is one of the primary goals of physics.\n\nThe \"first great unification\" was Isaac Newton's 17th century unification of gravity, which brought together the understandings of the observable phenomena of gravity on Earth with the observable behaviour of celestial bodies in space. The \"second great unification\" was James Clerk Maxwell's 19th century unification of electromagnetism, brought together the understandings of the observable phenomena of magnetism, electricity and light (and more broadly, the spectrum of electromagnetic radiation. This was followed in the 20th century by Albert Einstein's unification of space and time, and of mass and energy. Later, quantum field theory unified quantum mechanics and special relativity.\n\nThis process of \"unifying\" forces continues today, with the ultimate goal of finding a theory of everything – it remains perhaps the most important of the unsolved problems in physics. There remain four fundamental forces which have not been decisively unified: the gravitational and electromagnetic interactions, which produce significant long-range forces whose effects can be seen directly in everyday life, and the strong and weak interactions, which produce forces at minuscule, subatomic distances and govern nuclear interactions. Gravity and electromagnetism have been proposed together in the theory of Gravitoelectromagnetism. Electromagnetism and the weak interactions are widely considered to be two aspects of the electroweak interaction. Attempt to unify quantum mechanics and general relativity into a single theory of quantum gravity, a program ongoing for over half a century, have not yet been decisively resolved; current leading candidates are M-theory, superstring theory and loop quantum gravity.\n\nThe ancient Chinese observed that certain rocks (lodestone and magnetite) were attracted to one another by an invisible force. This effect was later called magnetism, which was first rigorously studied in the 17th century. But even before the Chinese discovered magnetism, the ancient Greeks knew of other objects such as amber, that when rubbed with fur would cause a similar invisible attraction between the two. This was also first studied rigorously in the 17th century and came to be called electricity. Thus, physics had come to understand two observations of nature in terms of some root cause (electricity and magnetism). However, further work in the 19th century revealed that these two forces were just two different aspects of one force—electromagnetism. This process of \"unifying\" forces continues today, and electromagnetism and the weak nuclear force are now considered to be two aspects of the electroweak interaction. Physics hopes to find an ultimate reason (theory of everything) for why nature is as it is (see section \"Current research\" below for more information).\n", "related": "NONE"}
{"id": "14539370", "url": "https://en.wikipedia.org/wiki?curid=14539370", "title": "Segregation (materials science)", "text": "Segregation (materials science)\n\nIn materials science, segregation is the enrichment of atoms, ions, or molecules at a microscopic region in a materials system. While the terms segregation and adsorption are essentially synonymous, in practice, segregation is often used to describe the partitioning of molecular constituents to defects from \"solid\" solutions, whereas adsorption is generally used to describe such partitioning from liquids and gases to surfaces. The molecular-level segregation discussed in this article is distinct from other types of materials phenomena that are often called segregation, such as particle segregation in granular materials, and phase separation or precipitation, wherein molecules are segregated in to macroscopic regions of different compositions. Segregation has many practical consequences, ranging from the formation of soap bubbles, to microstructural engineering in materials science, to the stabilization of colloidal suspensions.\n\nSegregation can occur in various materials classes. In polycrystalline solids, segregation occurs at defects, such as dislocations, grain boundaries, stacking faults, or the interface between two phases. In liquid solutions, chemical gradients exist near second phases and surfaces due to combinations of chemical and electrical effects.\n\nSegregation which occurs in well-equilibrated systems due to the instrinsic to the chemical properties of the system is termed equilibrium segregation. Segregation that occurs due to the processing history of the sample (but that would disappear at long times) is termed non-equilibrium segregation.\n\nEquilibrium segregation is associated with the lattice disorder at interfaces, where there are sites of energy different from those within the lattice at which the solute atoms can deposit themselves. The equilibrium segregation is so termed because the solute atoms segregate themselves to the interface or surface in accordance with the statistics of thermodynamics in order to minimize the overall free energy of the system. This sort of partitioning of solute atoms between the grain boundary and the lattice was predicted by McLean in 1957.\n\nNon-equilibrium segregation, first theorized by Westbrook in 1964, occurs as a result of solutes coupling to vacancies which are moving to grain boundary sources or sinks during quenching or application of stress. It can also occur as a result of solute pile-up at a moving interface.\n\nThere are two main features of non-equilibrium segregation, by which it is most easily distinguished from equilibrium segregation. In the non-equilibrium effect, the magnitude of the segregation increases with increasing temperature and the alloy can be homogenized without further quenching because its lowest energy state corresponds to a uniform solute distribution. In contrast, the equilibrium segregated state, by definition, is the lowest energy state in a system that exhibits equilibrium segregation, and the extent of the segregation effect decreases with increasing temperature. The details of non-equilibrium segregation are not going to be discussed here, but can be found in the review by Harries and Marwick.\n\nSegregation of a solute to surfaces and grain boundaries in a solid produces a section of material with a discrete composition and its own set of properties that can have important (and often deleterious) effects on the overall properties of the material. These ‘zones’ with an increased concentration of solute can be thought of as the cement between the bricks of a building. The structural integrity of the building depends not only on the material properties of the brick, but also greatly on the properties of the long lines of mortar in between.\n\nSegregation to grain boundaries, for example, can lead to grain boundary fracture as a result of temper brittleness, creep embrittlement, stress relief cracking of weldments, hydrogen embrittlement, environmentally assisted fatigue, grain boundary corrosion, and some kinds of intergranular stress corrosion cracking. A very interesting and important field of study of impurity segregation processes involves AES of grain boundaries of materials. This technique includes tensile fracturing of special specimens directly inside the UHV chamber of the Auger Electron Spectrometer that was developed by Ilyin.\nSegregation to grain boundaries can also affect their respective migration rates, and so affects sinterability, as well as the grain boundary diffusivity (although sometimes these effects can be used advantageously).\n\nSegregation to free surfaces also has important consequences involving the purity of metallurgical samples. Because of the favorable segregation of some impurities to the surface of the material, a very small concentration of impurity in the bulk of the sample can lead to a very significant coverage of the impurity on a cleaved surface of the sample. In applications where an ultra-pure surface is needed (for example, in some nanotechnology applications), the segregation of impurities to surfaces requires a much higher purity of bulk material than would be needed if segregation effects didn’t exist. The following figure illustrates this concept with two cases in which the total fraction of impurity atoms is 0.25 (25 impurity atoms in 100 total). In the representation on the left, these impurities are equally distributed throughout the sample, and so the fractional surface coverage of impurity atoms is also approximately 0.25. In the representation to the right, however, the same number of impurity atoms are shown segregated on the surface, so that an observation of the surface composition would yield a much higher impurity fraction (in this case, about 0.69). In fact, in this example, were impurities to completely segregate to the surface, an impurity fraction of just 0.36 could completely cover the surface of the material. In an application where surface interactions are important, this result could be disastrous.\n\nWhile the intergranular failure problems noted above are sometimes severe, they are rarely the cause of major service failures (in structural steels, for example), as suitable safety margins are included in the designs. Perhaps the greater concern is that with the development of new technologies and materials with new and more extensive mechanical property requirements, and with the increasing impurity contents as a result of the increased recycling of materials, we may see intergranular failure in materials and situations not seen currently. Thus, a greater understanding of all of the mechanisms surrounding segregation might lead to being able to control these effects in the future. Modeling potentials, experimental work, and related theories are still being developed to explain these segregation mechanisms for increasingly complex systems.\n\nSeveral theories describe the equilibrium segregation activity in materials. The adsorption theories for the solid-solid interface and the solid-vacuum surface are direct analogues of theories well known in the field of gas adsorption on the free surfaces of solids.\n\nThis is the earliest theory specifically for grain boundaries, in which McLean uses a model of P solute atoms distributed at random amongst N lattice sites and p solute atoms distributed at random amongst n independent grain boundary sites. The total free energy due to the solute atoms is then:\n\nwhere E and e are energies of the solute atom in the lattice and in the grain boundary, respectively and the kln term represents the configurational entropy of the arrangement of the solute atoms in the bulk and grain boundary. McLean used basic statistical mechanics to find the fractional monolayer of segregant, formula_2, at which the system energy was minimized (at the equilibrium state), differentiating \"G\" with respect to \"p\", noting that the sum of \"p\" and \"P\" is constant. Here the grain boundary analogue of Langmuir adsorption at free surfaces becomes:\n\nHere, formula_4 is the fraction of the grain boundary monolayer available for segregated atoms at saturation, formula_2 is the actual fraction covered with segregant, formula_6 is the bulk solute molar fraction, and formula_7 is the free energy of segregation per mole of solute.\n\nValues of formula_7 were estimated by McLean using the elastic strain energy, formula_9, released by the segregation of solute atoms. The solute atom is represented by an elastic sphere fitted into a spherical hole in an elastic matrix continuum. The elastic energy associated with the solute atom is given by:\n\nwhere formula_11 is the solute bulk modulus, formula_12 is the matrix shear modulus, and formula_13 and formula_14 are the atomic radii of the matrix and impurity atoms, respectively. This method gives values correct to within a factor of two (as compared with experimental data for grain boundary segregation), but a greater accuracy is obtained using the method of Seah and Hondros, described in the following section.\n\nUsing truncated BET theory (the gas adsorption theory developed by Brunauer, Emmett, and Teller), Seah and Hondros write the solid-state analogue as:\n\nwhere formula_17\n\nformula_18 is the solid solubility, which is known for many elements (and can be found in metallurgical handbooks). In the dilute limit, a slightly soluble substance has formula_19, so the above equation reduces to that found with the Langmuir-McLean theory. This equation is only valid for formula_20. If there is an excess of solute such that a second phase appears, the solute content is limited to formula_18 and the equation becomes\n\nThis theory for grain boundary segregation, derived from truncated BET theory, provides excellent agreement with experimental data obtained by Auger electron spectroscopy and other techniques.\n\nOther models exist to model more complex binary systems. The above theories operate on the assumption that the segregated atoms are non-interacting. If, in a binary system, adjacent adsorbate atoms are allowed an interaction energy formula_23, such that they can attract (when formula_23 is negative) or repel (when formula_23 is positive) each other, the solid-state analogue of the Fowler adsorption theory is developed as\n\nWhen formula_23 is zero, this theory reduces to that of Langmuir and McLean. However, as formula_23 becomes more negative, the segregation shows progressively sharper rises as the temperature falls until eventually the rise in segregation is discontinuous at a certain temperature, as shown in the following figure.\n\nGuttman, in 1975, extended the Fowler theory to allow for interactions between two co-segregating species in multicomponent systems. This modification is vital to explaining the segregation behavior that results in the intergranular failures of engineering materials. More complex theories are detailed in the work by Guttmann and McLean and Guttmann.\n\nThe Langmuir–McLean equation for segregation, when using the regular solution model for a binary system, is valid for surface segregation (although sometimes the equation will be written replacing formula_2 with formula_30). The free energy of surface segregation is formula_31. The enthalpy is given by\n\nwhere formula_33 and formula_34 are matrix surface energies without and with solute, formula_35 is their heat of mixing, Z and formula_36 are the coordination numbers in the matrix and at the surface, and formula_37 is the coordination number for surface atoms to the layer below. The last term in this equation is the elastic strain energy formula_9, given above, and is governed by the mismatch between the solute and the matrix atoms. For solid metals, the surface energies scale with the melting points. The surface segregation enrichment ratio increases when the solute atom size is larger than the matrix atom size and when the melting point of the solute is lower than that of the matrix.\n\nA chemisorbed gaseous species on the surface can also have an effect on the surface composition of a binary alloy. In the presence of a coverage of a chemisorbed species theta, it is proposed that the Langmuir-McLean model is valid with the free energy of surface segregation given by formula_39, where\n\nformula_41 and formula_42 are the chemisorption energies of the gas on solute A and matrix B and formula_43 is the fractional coverage. At high temperatures, evaporation from the surface can take place, causing a deviation from the McLean equation. At lower temperatures, both grain boundary and surface segregation can be limited by the diffusion of atoms from the bulk to the surface or interface.\n\nIn some situations where segregation is important, the segregant atoms do not have sufficient time to reach their equilibrium level as defined by the above adsorption theories. The kinetics of segregation become a limiting factor and must be analyzed as well. Most existing models of segregation kinetics follow the McLean approach. In the model for equilibrium monolayer segregation, the solute atoms are assumed to segregate to a grain boundary from two infinite half-crystals or to a surface from one infinite half-crystal. The diffusion in the crystals is described by Fick’s laws. The ratio of the solute concentration in the grain boundary to that in the adjacent atomic layer of the bulk is given by an enrichment ratio, formula_44. Most models assume formula_44 to be a constant, but in practice this is only true for dilute systems with low segregation levels. In this dilute limit, if formula_4 is one monolayer, formula_44 is given as formula_48.\n\nThe kinetics of segregation can be described by the following equation:\n\nwhere formula_51 for grain boundaries and 1 for the free surface, formula_52 is the boundary content at time formula_53, formula_54 is the solute bulk diffusivity, formula_55 is related to the atomic sizes of the solute and the matrix, formula_56 and formula_57, respectively, by formula_58. For short times, this equation is approximated by:\n\nIn practice, formula_44 is not a constant but generally falls as segregation proceeds due to saturation. If formula_44 starts high and falls rapidly as the segregation saturates, the above equation is valid until the point of saturation.\n\nAll metal castings experience segregation to some extent, and a distinction is made between \"macro\"segregation and \"micro\"segregation. Microsegregation refers to localized differences in composition between dendrite arms, and can be significantly reduced by a homogenizing heat treatment. This is possible because the distances involved (typically on the order of 10 to 100 µm) are sufficiently small for diffusion to be a significant mechanism. This is not the case in macrosegregation. Therefore, macrosegregation in metal castings cannot be remedied or removed using heat treatment.\n\n", "related": "\n- Adsorption\n- Absorption (chemistry)\n- BET theory\n- Freundlich equation\n- Langmuir equation\n- Reactions on surfaces\n- Wetting\n- Henry adsorption constant\n"}
{"id": "54738", "url": "https://en.wikipedia.org/wiki?curid=54738", "title": "Interpretations of quantum mechanics", "text": "Interpretations of quantum mechanics\n\nAn interpretation of quantum mechanics is an attempt to explain how the mathematical theory of quantum mechanics \"corresponds\" to reality. Although quantum mechanics has held up to rigorous and extremely precise tests in an extraordinarily broad range of experiments (not one prediction from quantum mechanics is found to be contradicted by experiments), there exist a number of contending schools of thought over their interpretation. These views on interpretation differ on such fundamental questions as whether quantum mechanics is deterministic or stochastic, which elements of quantum mechanics can be considered \"real\", and what is the nature of measurement, among other matters.\n\nDespite nearly a century of debate and experiment, no consensus has been reached among physicists and philosophers of physics concerning which interpretation best \"represents\" reality.\n\nThe definition of quantum theorists' terms, such as \"wave functions\" and \"matrix mechanics\", progressed through many stages. For instance, Erwin Schrödinger originally viewed the electron's wave function as its charge density smeared across space, whereas Max Born reinterpreted the absolute square value of the wave function as the electron's probability density distributed across space.\n\nThe views of several early pioneers of quantum mechanics, such as Niels Bohr and Werner Heisenberg, are often grouped together as the \"Copenhagen interpretation\", though physicists and historians of physics have argued that this terminology obscures differences between the views so designated. While Copenhagen-type ideas were never universally embraced, challenges to a perceived Copenhagen orthodoxy gained increasing attention in the 1950s with the pilot-wave interpretation of David Bohm and the many-worlds interpretation of Hugh Everett III.\n\nMoreover, the strictly formalist position, shunning interpretation, has been challenged by proposals for falsifiable experiments that might one day distinguish among interpretations, as by measuring an AI consciousness or via quantum computing.\n\nThe physicist N. David Mermin once quipped, \"New interpretations appear every year. None ever disappear.\" As a rough guide to development of the mainstream view during the 1990s to 2000s, consider the \"snapshot\" of opinions collected in a poll by Schlosshauer et al. at the \"Quantum Physics and the Nature of Reality\" conference of July 2011.\nThe authors reference a similarly informal poll carried out by Max Tegmark at the \"Fundamental Problems in Quantum Theory\" conference in August 1997. The main conclusion of the authors is that \"the Copenhagen interpretation still reigns supreme\", receiving the most votes in their poll (42%), besides the rise to mainstream notability of the many-worlds interpretations:\n\nMore or less, all interpretations of quantum mechanics share two qualities:\n1. They interpret a \"formalism\"—a set of equations and principles to generate predictions via input of initial conditions\n2. They interpret a \"phenomenology\"—a set of observations, including those obtained by empirical research and those obtained informally, such as humans' experience of an unequivocal world\nTwo qualities vary among interpretations:\n1. Ontology—claims about what things, such as categories and entities, \"exist\" in the world\n2. Epistemology—claims about the possibility, scope, and means toward relevant \"knowledge\" of the world\n\nIn philosophy of science, the distinction of knowledge versus reality is termed \"epistemic\" versus \"ontic\". A general law is a \"regularity\" of outcomes (epistemic), whereas a causal mechanism may \"regulate\" the outcomes (ontic). A phenomenon can receive interpretation either ontic or epistemic. For instance, indeterminism may be attributed to limitations of human observation and perception (epistemic), or may be explained as a real existing \"maybe\" encoded in the universe (ontic). Confusing the epistemic with the ontic, like if one were to presume that a general law actually \"governs\" outcomes—and that the statement of a regularity has the role of a causal mechanism—is a category mistake.\n\nIn a broad sense, scientific theory can be viewed as offering scientific realism—approximately true description or explanation of the natural world—or might be perceived with antirealism. A realist stance seeks the epistemic and the ontic, whereas an antirealist stance seeks epistemic but not the ontic. In the 20th century's first half, antirealism was mainly logical positivism, which sought to exclude unobservable aspects of reality from scientific theory.\n\nSince the 1950s, antirealism is more modest, usually instrumentalism, permitting talk of unobservable aspects, but ultimately discarding the very question of realism and posing scientific theory as a tool to help humans make predictions, not to attain metaphysical understanding of the world. The instrumentalist view is carried by the famous quote of David Mermin, \"Shut up and calculate\", often misattributed to Richard Feynman.\n\nOther approaches to resolve conceptual problems introduce new mathematical formalism, and so propose alternative theories with their interpretations. An example is Bohmian mechanics, whose empirical equivalence with the three standard formalisms—Schrödinger's wave mechanics, Heisenberg's matrix mechanics, and Feynman's path integral formalism—has been demonstrated.\n\n1. Abstract, mathematical nature of quantum field theories: the mathematical structure of quantum mechanics is mathematically abstract without clear interpretation of its quantities.\n2. Existence of apparently indeterministic and irreversible processes: in classical field theory, a physical property at a given location in the field is readily derived. In most mathematical formulations of quantum mechanics, measurement is given a special role in the theory, as it is the sole process that can cause a nonunitary, irreversible evolution of the state.\n3. Role of the observer in determining outcomes: the Copenhagen Interpretation implies that the wavefunction is a calculational tool, and represents reality only immediately after a measurement, perhaps performed by an observer; Everettian interpretations grant that all the possibilities can be real, and that the process of measurement-type interactions cause an effective branching process.\n4. Classically unexpected correlations between remote objects: entangled quantum systems, as illustrated in the EPR paradox, obey statistics that seem to violate principles of local causality.\n5. Complementarity of proffered descriptions: complementarity holds that no set of classical physical concepts can simultaneously refer to all properties of a quantum system. For instance, wave description \"A\" and particulate description \"B\" can each describe quantum system \"S\", but not simultaneously. This implies the composition of physical properties of \"S\" does not obey the rules of classical propositional logic when using propositional connectives (see \"Quantum logic\"). Like contextuality, the \"origin of complementarity lies in the non-commutativity of operators\" that describe quantum objects (Omnès 1999).\n6. Rapidly rising intricacy, far exceeding humans' present calculational capacity, as a system's size increases: since the state space of a quantum system is exponential in the number of subsystems, it is difficult to derive classical approximations.\n7. Contextual behaviour of systems locally: Quantum contextuality demonstrates that classical intuitions in which properties of a system hold definite values, independent of the manner of their measurement, fails even for local systems. Also, physical principles such as Leibniz's Principle of the identity of indiscernibles no longer apply in the quantum domain, signalling that most classical intuitions may be incorrect about the quantum world.\n\nAs well as the mainstream interpretations discussed below, a number of other interpretations have been proposed which have not made a significant scientific impact for whatever reason. These range from proposals by mainstream physicists to the more occult ideas of quantum mysticism.\n\nThe current usage of realism and completeness originated in the 1935 paper in which Einstein and others proposed the EPR paradox. In that paper the authors proposed the concepts \"element of reality\" and the \"completeness of a physical theory\". They characterised element of reality as a quantity whose value can be predicted with certainty before measuring or otherwise disturbing it, and defined a complete physical theory as one in which every element of physical reality is accounted for by the theory. In a semantic view of interpretation, an interpretation is complete if every element of the interpreting structure is present in the mathematics. Realism is also a property of each of the elements of the maths; an element is real if it corresponds to something in the interpreting structure. For example, in some interpretations of quantum mechanics (such as the many-worlds interpretation) the ket vector associated to the system state is said to correspond to an element of physical reality, while in other interpretations it is not.\n\nDeterminism is a property characterizing state changes due to the passage of time, namely that the state at a future instant is a function of the state in the present (see time evolution). It may not always be clear whether a particular interpretation is deterministic or not, as there may not be a clear choice of a time parameter. Moreover, a given theory may have two interpretations, one of which is deterministic and the other not.\n\nLocal realism has two aspects:\n- The value returned by a measurement corresponds to the value of some function in the state space. In other words, that value is an element of reality;\n- The effects of measurement have a propagation speed not exceeding some universal limit (e.g. the speed of light). In order for this to make sense, measurement operations in the interpreting structure must be localized.\n\nA precise formulation of local realism in terms of a local hidden-variable theory was proposed by John Bell.\n\nBell's theorem, combined with experimental testing, restricts the kinds of properties a quantum theory can have, the primary implication being that quantum mechanics cannot satisfy both the principle of locality and counterfactual definiteness.\n\nRegardless of Einstein's concerns about interpretation issues, Dirac and other quantum notables embraced the technical advances of the new theory while devoting little or no attention to interpretational aspects.\n\nThe Copenhagen interpretation is the \"standard\" interpretation of quantum mechanics formulated by Niels Bohr and Werner Heisenberg while collaborating in Copenhagen around 1927. Bohr and Heisenberg extended the probabilistic interpretation of the wavefunction proposed originally by Max Born. The Copenhagen interpretation rejects questions like \"where was the particle before I measured its position?\" as meaningless. The measurement process randomly picks out exactly one of the many possibilities allowed for by the state's wave function in a manner consistent with the well-defined probabilities that are assigned to each possible state. According to the interpretation, the interaction of an observer or apparatus that is external to the quantum system is the cause of wave function collapse, thus according to Paul Davies, \"reality is in the observations, not in the electron\". In general, after a measurement (click of a Geiger counter or a trajectory in a spark or bubble chamber) it ceases to be relevant unless subsequent experimental observations can be performed.\n\nQuantum informational approaches have attracted growing support. They subdivide into two kinds.\n- Information ontologies, such as J. A. Wheeler's \"it from bit\". These approaches have been described as a revival of immaterialism.\n- Interpretations where quantum mechanics is said to describe an observer's knowledge of the world, rather than the world itself. This approach has some similarity with Bohr's thinking. Collapse (also known as reduction) is often interpreted as an observer acquiring information from a measurement, rather than as an objective event. These approaches have been appraised as similar to instrumentalism.\n\nThe state is not an objective property of an individual system but is that information, obtained from a knowledge of how a system was prepared, which can be used for making predictions about future measurements.\n...A quantum mechanical state being a summary of the observer's information about an individual physical system changes both by dynamical laws, and whenever the observer acquires new information about the system through the process of measurement. The existence of two laws for the evolution of the state vector...becomes problematical only if it is believed that the state vector is an objective property of the system...The \"reduction of the wavepacket\" does take place in the consciousness of the observer, not because of any unique physical process which takes place there, but only because the state is a construct of the observer and not an objective property of the physical system.\n\nThe essential idea behind relational quantum mechanics, following the precedent of special relativity, is that different observers may give different accounts of the same series of events: for example, to one observer at a given point in time, a system may be in a single, \"collapsed\" eigenstate, while to another observer at the same time, it may be in a superposition of two or more states. Consequently, if quantum mechanics is to be a complete theory, relational quantum mechanics argues that the notion of \"state\" describes not the observed system itself, but the relationship, or correlation, between the system and its observer(s). The state vector of conventional quantum mechanics becomes a description of the correlation of some \"degrees of freedom\" in the observer, with respect to the observed system. However, it is held by relational quantum mechanics that this applies to all physical objects, whether or not they are conscious or macroscopic. Any \"measurement event\" is seen simply as an ordinary physical interaction, an establishment of the sort of correlation discussed above. Thus the physical content of the theory has to do not with objects themselves, but the relations between them.\n\nAn independent relational approach to quantum mechanics was developed in analogy with David Bohm's elucidation of special relativity, in which a detection event is regarded as establishing a relationship between the quantized field and the detector. The inherent ambiguity associated with applying Heisenberg's uncertainty principle is subsequently avoided.\n\nQuantum Bayesianism (also called QBism) is an interpretation of quantum mechanics that takes an agent's actions and experiences as the central concerns of the theory. This interpretation is distinguished by its use of a subjective Bayesian account of probabilities to understand the quantum mechanical Born rule as a normative addition to good decision-making. QBism draws from the fields of quantum information and Bayesian probability and aims to eliminate the interpretational conundrums that have beset quantum theory.\n\nQBism deals with common questions in the interpretation of quantum theory about the nature of wavefunction superposition, quantum measurement, and entanglement. According to QBism, many, but not all, aspects of the quantum formalism are subjective in nature. For example, in this interpretation, a quantum state is not an element of reality—instead it represents the degrees of belief an agent has about the possible outcomes of measurements. For this reason, some philosophers of science have deemed QBism a form of anti-realism. The originators of the interpretation disagree with this characterization, proposing instead that the theory more properly aligns with a kind of realism they call \"participatory realism\", wherein reality consists of \"more\" than can be captured by any putative third-person account of it.\n\nThe many-worlds interpretation is an interpretation of quantum mechanics in which a universal wavefunction obeys the same deterministic, reversible laws at all times; in particular there is no (indeterministic and irreversible) wavefunction collapse associated with measurement. The phenomena associated with measurement are claimed to be explained by decoherence, which occurs when states interact with the environment producing entanglement, repeatedly \"splitting\" the universe into mutually unobservable alternate histories—effectively distinct universes within a greater multiverse.\n\nThe consistent histories interpretation generalizes the conventional Copenhagen interpretation and attempts to provide a natural interpretation of quantum cosmology. The theory is based on a consistency criterion that allows the history of a system to be described so that the probabilities for each history obey the additive rules of classical probability. It is claimed to be consistent with the Schrödinger equation.\n\nAccording to this interpretation, the purpose of a quantum-mechanical theory is to predict the relative probabilities of various alternative histories (for example, of a particle).\n\nThe ensemble interpretation, also called the statistical interpretation, can be viewed as a minimalist interpretation. That is, it claims to make the fewest assumptions associated with the standard mathematics. It takes the statistical interpretation of Born to the fullest extent. The interpretation states that the wave function does not apply to an individual systemfor example, a single particlebut is an abstract statistical quantity that only applies to an ensemble (a vast multitude) of similarly prepared systems or particles. In the words of Einstein:\n\nThe most prominent current advocate of the ensemble interpretation is Leslie E. Ballentine, professor at Simon Fraser University, author of the text book \"Quantum Mechanics, A Modern Development\". An experiment illustrating the ensemble interpretation is provided in Akira Tonomura's Video clip 1. \nThe de Broglie–Bohm theory of quantum mechanics (also known as the pilot wave theory) is a theory by Louis de Broglie and extended later by David Bohm to include measurements. Particles, which always have positions, are guided by the wavefunction. The wavefunction evolves according to the Schrödinger wave equation, and the wavefunction never collapses. The theory takes place in a single space-time, is non-local, and is deterministic. The simultaneous determination of a particle's position and velocity is subject to the usual uncertainty principle constraint. The theory is considered to be a hidden-variable theory, and by embracing non-locality it satisfies Bell's inequality. The measurement problem is resolved, since the particles have definite positions at all times. Collapse is explained as phenomenological.\n\nQuantum Darwinism is a theory meant to explain the emergence of the classical world from the quantum world as due to a process of Darwinian natural selection induced by the environment interacting with the quantum system; where the many possible quantum states are selected against in favor of a stable pointer state. It was proposed in 2003 by Wojciech Zurek and a group of collaborators including Ollivier, Poulin, Paz and Blume-Kohout. The development of the theory is due to the integration of a number of Zurek’s research topics pursued over the course of twenty-five years including: pointer states, einselection and decoherence.\n\nThe transactional interpretation of quantum mechanics (TIQM) by John G. Cramer is an interpretation of quantum mechanics inspired by the Wheeler–Feynman absorber theory. It describes the collapse of the wave function as resulting from a time-symmetric transaction between a possibility wave from the source to the receiver (the wave function) and a possibility wave from the receiver to source (the complex conjugate of the wave function). This interpretation of quantum mechanics is unique in that it not only views the wave function as a real entity, but the complex conjugate of the wave function, which appears in the Born rule for calculating the expected value for an observable, as also real.\n\nAn entirely classical derivation and interpretation of Schrödinger's wave equation by analogy with Brownian motion was suggested by Princeton University professor Edward Nelson in 1966. Similar considerations had previously been published, for example by R. Fürth (1933), I. Fényes (1952), and Walter Weizel (1953), and are referenced in Nelson's paper. More recent work on the stochastic interpretation has been done by M. Pavon. An alternative stochastic interpretation was developed by Roumen Tsekov.\n\nObjective collapse theories differ from the Copenhagen interpretation by regarding both the wave function and the process of collapse as ontologically objective (meaning these exist and occur independent of the observer). In objective theories, collapse occurs either randomly (\"spontaneous localization\") or when some physical threshold is reached, with observers having no special role. Thus, objective-collapse theories are realistic, indeterministic, no-hidden-variables theories. Standard quantum mechanics does not specify any mechanism of collapse; QM would need to be extended if objective collapse is correct. The requirement for an extension to QM means that objective collapse is more of a theory than an interpretation. Examples include \n- the Ghirardi-Rimini-Weber theory\n- the Penrose interpretation.\n- the deterministic variant of an objective collapse theory\n\nIn his treatise \"The Mathematical Foundations of Quantum Mechanics\", John von Neumann deeply analyzed the so-called measurement problem. He concluded that the entire physical universe could be made subject to the Schrödinger equation (the universal wave function). He also described how measurement could cause a collapse of the wave function. This point of view was prominently expanded on by Eugene Wigner, who argued that human experimenter consciousness (or maybe even dog consciousness) was critical for the collapse, but he later abandoned this interpretation.\n\nVariations of the consciousness causes collapse interpretation include:\n\nOther physicists have elaborated their own variations of the consciousness causes collapse interpretation; including:\n- Henry P. Stapp (\"Mindful Universe: Quantum Mechanics and the Participating Observer\")\n- Bruce Rosenblum and Fred Kuttner (\"Quantum Enigma: Physics Encounters Consciousness\")\n- Amit Goswami (\"The Self-Aware Universe\")\n\nQuantum logic can be regarded as a kind of propositional logic suitable for understanding the apparent anomalies regarding quantum measurement, most notably those concerning composition of measurement operations of complementary variables. This research area and its name originated in the 1936 paper by Garrett Birkhoff and John von Neumann, who attempted to reconcile some of the apparent inconsistencies of classical boolean logic with the facts related to measurement and observation in quantum mechanics.\n\nModal interpretations of quantum mechanics were first conceived of in 1972 by B. van Fraassen, in his paper \"A formal approach to the philosophy of science.\" However, this term now is used to describe a larger set of models that grew out of this approach. The Stanford Encyclopedia of Philosophy describes several versions:\n- The Copenhagen variant\n- Kochen-Dieks-Healey Interpretations\n- Motivating Early Modal Interpretations, based on the work of R. Clifton, M. Dickson and J. Bub.\n\nSeveral theories have been proposed which modify the equations of quantum mechanics to be symmetric with respect to time reversal. (E.g. see Wheeler-Feynman time-symmetric theory). This creates retrocausality: events in the future can affect ones in the past, exactly as events in the past can affect ones in the future. In these theories, a single measurement cannot fully determine the state of a system (making them a type of hidden-variables theory), but given two measurements performed at different times, it is possible to calculate the exact state of the system at all intermediate times. The collapse of the wavefunction is therefore not a physical change to the system, just a change in our knowledge of it due to the second measurement. Similarly, they explain entanglement as not being a true physical state but just an illusion created by ignoring retrocausality. The point where two particles appear to \"become entangled\" is simply a point where each particle is being influenced by events that occur to the other particle in the future.\n\nNot all advocates of time-symmetric causality favour modifying the unitary dynamics of standard quantum mechanics. Thus a leading exponent of the two-state vector formalism, Lev Vaidman, states that the two-state vector formalism dovetails well with Hugh Everett's many-worlds interpretation.\n\nBST theories resemble the many worlds interpretation; however, \"the main difference is that the BST interpretation takes the branching of history to be a feature of the topology of the set of events with their causal relationships... rather than a consequence of the separate evolution of different components of a state vector.\" In MWI, it is the wave functions that branches, whereas in BST, the space-time topology itself branches.\nBST has applications to Bell's theorem, quantum computation and quantum gravity. It also has some resemblance to hidden-variable theories and the ensemble interpretation: particles in BST have multiple well defined trajectories at the microscopic level. These can only be treated stochastically at a coarse grained level, in line with the ensemble interpretation.\n\nThe most common interpretations are summarized in the table below. The values shown in the cells of the table are not without controversy, for the precise meanings of some of the concepts involved are unclear and, in fact, are themselves at the center of the controversy surrounding the given interpretation. For another table comparing interpretations of quantum theory, see reference.\n\nNo experimental evidence exists that distinguishes among these interpretations. To that extent, the physical theory stands, and is consistent with itself and with reality; difficulties arise only when one attempts to \"interpret\" the theory. Nevertheless, designing experiments which would test the various interpretations is the subject of active research.\n\nMost of these interpretations have variants. For example, it is difficult to get a precise definition of the Copenhagen interpretation as it was developed and argued about by many people.\n\n- According to Bohr, the concept of a physical state independent of the conditions of its experimental observation does not have a well-defined meaning. According to Heisenberg the wavefunction represents a probability, but not an objective reality itself in space and time.\n- According to the Copenhagen interpretation, the wavefunction collapses when a measurement is performed.\n- Both particle guiding wavefunction are real.\n- Unique particle history, but multiple wave histories.\n- But quantum logic is more limited in applicability than Coherent Histories.\n- Quantum mechanics is regarded as a way of predicting observations, or a theory of measurement.\n- Observers separate the universal wavefunction into orthogonal sets of experiences.\n- In the TI the collapse of the state vector is interpreted as the completion of the transaction between emitter and absorber.\n- Comparing histories between systems in this interpretation has no well-defined meaning.\n- Any physical interaction is treated as a collapse event relative to the systems involved, not just macroscopic or conscious observers.\n- The state of the system is observer-dependent, i.e., the state is specific to the reference frame of the observer.\n- The transactional interpretation is explicitly non-local.\n- The assumption of intrinsic periodicity is an element of non-locality consistent with relativity as the periodicity varies in a causal way.\n- In the stochastic interpretation is not possible to define velocities for particles, i.e. the paths are not smooth. Moreover, to know the motion of the particles at any moment, you have to know what the Markov process is. However, once we know the exactly initial conditions and the Markov process, the theory is in fact a realistic interpretation of quantum mechanics.\n- A wavefunction merely encodes an agent’s expectations for future experiences. It is no more real than a probability distribution is in subjective Bayesianism.\n- Quantum theory is a tool any agent may use to help manage their expectations. The past comes into play only insofar as an agent’s individual experiences and temperament influence their priors.\n- Although QBism would eschew this terminology. A change in the wavefunction that an agent ascribes to a system as a result of having an experience represents a change in his or her beliefs about further experiences they may have. See Doxastic logic.\n- Observers, or more properly, participants, are as essential to the formalism as the systems they interact with.\n\nAlthough interpretational opinions are openly and widely discussed today, that was not always the case. A notable exponent of a tendency of silence was Paul Dirac who once wrote: \"The interpretation of quantum mechanics has been dealt with by many authors, and I do not want to discuss it here. I want to deal with more fundamental things.\" This position is not uncommon among practitioners of quantum mechanics. Others, like Nico van Kampen and Willis Lamb, have openly criticized non-orthodox interpretations of quantum mechanics.\n\n", "related": "\n- Afshar experiment\n- Bohr–Einstein debates\n- Einstein's thought experiments\n- Glossary of quantum philosophy\n- Macroscopic quantum phenomena\n- Path integral formulation\n- Philosophical interpretation of classical physics\n- Popper's experiment\n- Quantum foundations\n- Quantum gravity\n- Quantum Zeno effect\n- Rudolf Carnap, 1939, \"The interpretation of physics\", in \"Foundations of Logic and Mathematics\" of the \"International Encyclopedia of Unified Science\". University of Chicago Press.\n- Dickson, M., 1994, \"Wavefunction tails in the modal interpretation\" in Hull, D., Forbes, M., and Burian, R., eds., \"Proceedings of the PSA\" 1\" 366–76. East Lansing, Michigan: Philosophy of Science Association.\n- --------, and Clifton, R., 1998, \"Lorentz-invariance in modal interpretations\" in Dieks, D. and Vermaas, P., eds., \"The Modal Interpretation of Quantum Mechanics\". Dordrecht: Kluwer Academic Publishers: 9–48.\n- Fuchs, Christopher, 2002, \"Quantum Mechanics as Quantum Information (and only a little more).\"\n- -------- and A. Peres, 2000, \"Quantum theory needs no ‘interpretation’\", \"Physics Today\".\n- Herbert, N., 1985. \"Quantum Reality: Beyond the New Physics\". New York: Doubleday. .\n- Hey, Anthony, and Walters, P., 2003. \"The New Quantum Universe\", 2nd ed. Cambridge Univ. Press. .\n- Max Jammer, 1966. \"The Conceptual Development of Quantum Mechanics\". McGraw-Hill.\n- --------, 1974. \"The Philosophy of Quantum Mechanics\". Wiley & Sons.\n- Al-Khalili, 2003. \"Quantum: A Guide for the Perplexed\". London: Weidenfeld & Nicolson.\n- de Muynck, W. M., 2002. \"Foundations of quantum mechanics, an empiricist approach\". Dordrecht: Kluwer Academic Publishers. .\n- Roland Omnès, 1999. \"Understanding Quantum Mechanics\". Princeton Univ. Press.\n- Karl Popper, 1963. \"Conjectures and Refutations\". London: Routledge and Kegan Paul. The chapter \"Three views Concerning Human Knowledge\" addresses, among other things, instrumentalism in the physical sciences.\n- Hans Reichenbach, 1944. \"Philosophic Foundations of Quantum Mechanics\". Univ. of California Press.\n- Bas van Fraassen, 1972, \"A formal approach to the philosophy of science\", in R. Colodny, ed., \"Paradigms and Paradoxes: The Philosophical Challenge of the Quantum Domain\". Univ. of Pittsburgh Press: 303-66.\n- John A. Wheeler and Wojciech Hubert Zurek (eds), \"Quantum Theory and Measurement\", Princeton: Princeton University Press, , LoC QC174.125.Q38 1983.\n\nAlmost all authors below are professional physicists.\n- David Z Albert, 1992. \"Quantum Mechanics and Experience\". Harvard Univ. Press. .\n- John S. Bell, 1987. \"Speakable and Unspeakable in Quantum Mechanics\". Cambridge Univ. Press, . The 2004 edition () includes two additional papers and an introduction by Alain Aspect.\n- Dmitrii Ivanovich Blokhintsev, 1968. \"The Philosophy of Quantum Mechanics\". D. Reidel Publishing Company. .\n- David Bohm, 1980. \"Wholeness and the Implicate Order\". London: Routledge. .\n- David Deutsch, 1997. \"The Fabric of Reality\". London: Allen Lane. ; . Argues forcefully \"against\" instrumentalism. For general readers.\n- Provides a \"pragmatic\" perspective on interpretations. For general readers.\n- Bernard d'Espagnat, 1976. \"Conceptual Foundation of Quantum Mechanics\", 2nd ed. Addison Wesley. .\n- Bernard d'Espagnat, 1983. \"In Search of Reality\". Springer. .\n- Bernard d'Espagnat, 2003. \"Veiled Reality: An Analysis of Quantum Mechanical Concepts\". Westview Press.\n- Bernard d'Espagnat, 2006. \"On Physics and Philosophy\". Princeton Univ. Press.\n- Arthur Fine, 1986. \"The Shaky Game: Einstein Realism and the Quantum Theory. Science and its Conceptual Foundations\". Univ. of Chicago Press. .\n- Ghirardi, Giancarlo, 2004. \"Sneaking a Look at God's Cards\". Princeton Univ. Press.\n- Gregg Jaeger (2009) \"Entanglement, Information, and the Interpretation of Quantum Mechanics\". Springer. .\n- N. David Mermin (1990) \"Boojums all the way through.\" Cambridge Univ. Press. .\n- Roland Omnès, 1994. \"The Interpretation of Quantum Mechanics\". Princeton Univ. Press. .\n- Roland Omnès, 1999. \"Understanding Quantum Mechanics\". Princeton Univ. Press.\n- Roland Omnès, 1999. \"Quantum Philosophy: Understanding and Interpreting Contemporary Science\". Princeton Univ. Press.\n- Roger Penrose, 1989. \"The Emperor's New Mind\". Oxford Univ. Press. . Especially chpt. 6.\n- Roger Penrose, 1994. \"Shadows of the Mind\". Oxford Univ. Press. .\n- Roger Penrose, 2004. \"The Road to Reality\". New York: Alfred A. Knopf. Argues that quantum theory is incomplete.\n\n- \"Stanford Encyclopedia of Philosophy\":\n- \"Bohmian mechanics\" by Sheldon Goldstein.\n- \"Collapse Theories.\" by Giancarlo Ghirardi.\n- \"Copenhagen Interpretation of Quantum Mechanics\" by Jan Faye.\n- \"Everett's Relative State Formulation of Quantum Mechanics\" by Jeffrey Barrett.\n- \"Many-Worlds Interpretation of Quantum Mechanics\" by Lev Vaidman.\n- \"Modal Interpretation of Quantum Mechanics\" by Michael Dickson and Dennis Dieks.\n- \"Philosophical Issues in Quantum Theory\" by Wayne Myrvold.\n- \"Quantum-Bayesian and Pragmatist Views of Quantum Theory\" by Richard Healey.\n- \"Quantum Entanglement and Information\" by Jeffrey Bub.\n- \"Quantum mechanics\" by Jenann Ismael.\n- \"Quantum Logic and Probability Theory\" by Alexander Wilce.\n- \"Relational Quantum Mechanics\" by Federico Laudisa and Carlo Rovelli.\n- \"The Role of Decoherence in Quantum Mechanics\" by Guido Bacciagaluppi.\n- \"Internet Encyclopedia of Philosophy\":\n- \"Interpretations of Quantum Mechanics\" by Peter J. Lewis.\n- \"Everettian Interpretations of Quantum Mechanics\" by Christina Conroy.\n"}
{"id": "63555971", "url": "https://en.wikipedia.org/wiki?curid=63555971", "title": "International Association of Seismology and Physics of the Earth’s Interior", "text": "International Association of Seismology and Physics of the Earth’s Interior\n\nInternational Association of Seismology and Physics of the Earth’s Interior (IASPEI) is an international organization that promotes study of earthquakes and other seismic sources, propagation of seismic waves, and the internal structure, properties and processes of the Earth. IASPEI is one of eight associations of the International Union of Geodesy and Geophysics (IUGG). IASPEI initiates and co-ordinates international researches and scientific discussion on scientific and applied seismology. The activities of IASPEI focus on the societal impacts of earthquakes and tsunamis, with four regional commissions promoting high standards of seismological education, outreach and international scientific cooperation.\n\n- Commission on Education and Outreach\n- Commission on Seismological Observation and Interpretation (CoSOI)\n- Commission on Tectonophysics and Crustal Structure\n- Commission on Earthquake Generation Process - Physics, Modelling, and Monitoring for Forecast\n- Commission on Earth Structure and Geodynamics\n- Commission on Earthquake Hazard, Risk and Strong Ground Motion (SHR)\n- Commission on Earthquake Source Mechanics (ESM)\n\n- European Seismological Commission (ESC)\n- Asian Seismological Commission (ASC)\n- African Seismological Commission (AfSC)\n- Latin American and Caribbean Seismological Commission (LACSC)\n\nIn 1899, a Permanent Seismological Commission was created at the Seventh International Congress of Geography in Berlin, and in 1903 at subsequent conference in Strasbourg, the \"International Seismological Association (ISA)\" was founded. On 1 April 1904, the ISA convention entered into force with 18 founding member-states with a plan to continue during next 12 years.\n\nIn 1922, ISA became one of constituent Sections of International Union of Geodesy and Geophysics (IUGG), taking its present name - International Association of Seismology and Physics of the Earth’s Interior (IASPEI) – at the IX IUGG General Assembly in 1951 in Bruxelles.\n\nManagement of the organization is dealt by bureau that consists of President, 1 and 2 Vice-Presidents, as well as Secretary-General/Treasurer. The Bureau is responsible to the Executive Committee consisting of nine members. Current IASPEI President is Kenji Satake (Japan).\n\nIASPEI medal is being awarded since 2011 for sustaining IASPEI goals and activities and for scientific merits in for scientific merits in the field of seismology and physics of the Earth’s interior. The most recent IASPEI medal winner is Dr. Eric Robert Engdahl who was awarded in 2017.\n", "related": "NONE"}
